{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf2-bug.txt", "bug_report_text": "CID: 492835\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: SECURE_CODING\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\nFunction: amdgpu_fru_get_product_info\nLine: 131\n\nProblem:\nPotential buffer overflow when formatting device serial number using sprintf\n\nAbstract:\nThe code uses sprintf to write a hexadecimal string representation of a 64-bit \nunique ID into the fru_info->serial buffer without size checking. This could \nlead to a buffer overflow if the destination buffer is smaller than the \nformatted string length (including null terminator).\n\nPath:\n  drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c:131\n    sprintf(fru_info->serial, \"%llx\", adev->unique_id)\n\nDetails:\nConverting a 64-bit value to hexadecimal format using sprintf can produce up to \n16 characters plus a null terminator. Without bounds checking, this operation \nrisks writing beyond the end of the serial buffer if it's smaller than 17 bytes. \nThe code handles critical device identification information in kernel space, \nmaking buffer overflows particularly dangerous.\n\nFix:\nReplace sprintf with snprintf to enforce buffer size limits:\n  snprintf(fru_info->serial, sizeof(fru_info->serial), \"%llx\", adev->unique_id)\n", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\nindex ceb5163480f4..a4696976c809 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\n@@ -131,7 +131,7 @@ int amdgpu_fru_get_product_info(struct amdgpu_device *adev)\n         * so convert it to a 16-digit HEX string for convenience and\n         * backwards-compatibility.\n         */\n-       sprintf(fru_info->serial, \"%llx\", adev->unique_id);\n+       snprintf(fru_info->serial, sizeof(fru_info->serial), \"%llx\", adev->unique_id);\n\n        /* If algo exists, it means that the i2c_adapter's initialized */\n        if (!adev->pm.fru_eeprom_i2c_bus || !adev->pm.fru_eeprom_i2c_bus->algo) {", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c", "line_number": 131, "code": "/*\n * Copyright 2019 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include <linux/pci.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_i2c.h\"\n#include \"smu_v11_0_i2c.h\"\n#include \"atom.h\"\n#include \"amdgpu_fru_eeprom.h\"\n#include \"amdgpu_eeprom.h\"\n\n#define FRU_EEPROM_MADDR_6      0x60000\n#define FRU_EEPROM_MADDR_8      0x80000\n\nstatic bool is_fru_eeprom_supported(struct amdgpu_device *adev, u32 *fru_addr)\n{\n\t/* Only server cards have the FRU EEPROM\n\t * TODO: See if we can figure this out dynamically instead of\n\t * having to parse VBIOS versions.\n\t */\n\tstruct atom_context *atom_ctx = adev->mode_info.atom_context;\n\n\t/* The i2c access is blocked on VF\n\t * TODO: Need other way to get the info\n\t * Also, FRU not valid for APU devices.\n\t */\n\tif (amdgpu_sriov_vf(adev) || (adev->flags & AMD_IS_APU))\n\t\treturn false;\n\n\t/* The default I2C EEPROM address of the FRU.\n\t */\n\tif (fru_addr)\n\t\t*fru_addr = FRU_EEPROM_MADDR_8;\n\n\t/* VBIOS is of the format ###-DXXXYYYY-##. For SKU identification,\n\t * we can use just the \"DXXX\" portion. If there were more models, we\n\t * could convert the 3 characters to a hex integer and use a switch\n\t * for ease/speed/readability. For now, 2 string comparisons are\n\t * reasonable and not too expensive\n\t */\n\tswitch (amdgpu_ip_version(adev, MP1_HWIP, 0)) {\n\tcase IP_VERSION(11, 0, 2):\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_VEGA20:\n\t\t\t/* D161 and D163 are the VG20 server SKUs */\n\t\t\tif (strnstr(atom_ctx->vbios_pn, \"D161\",\n\t\t\t\t    sizeof(atom_ctx->vbios_pn)) ||\n\t\t\t    strnstr(atom_ctx->vbios_pn, \"D163\",\n\t\t\t\t    sizeof(atom_ctx->vbios_pn))) {\n\t\t\t\tif (fru_addr)\n\t\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_6;\n\t\t\t\treturn true;\n\t\t\t} else {\n\t\t\t\treturn false;\n\t\t\t}\n\t\tcase CHIP_ARCTURUS:\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase IP_VERSION(11, 0, 7):\n\t\tif (strnstr(atom_ctx->vbios_pn, \"D603\",\n\t\t\t    sizeof(atom_ctx->vbios_pn))) {\n\t\t\tif (strnstr(atom_ctx->vbios_pn, \"D603GLXE\",\n\t\t\t\t    sizeof(atom_ctx->vbios_pn))) {\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tif (fru_addr)\n\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_6;\n\t\t\treturn true;\n\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\tcase IP_VERSION(13, 0, 2):\n\t\t/* All Aldebaran SKUs have an FRU */\n\t\tif (!strnstr(atom_ctx->vbios_pn, \"D673\",\n\t\t\t     sizeof(atom_ctx->vbios_pn)))\n\t\t\tif (fru_addr)\n\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_6;\n\t\treturn true;\n\tcase IP_VERSION(13, 0, 6):\n\tcase IP_VERSION(13, 0, 14):\n\t\t\tif (fru_addr)\n\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_8;\n\t\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nint amdgpu_fru_get_product_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_fru_info *fru_info;\n\tunsigned char buf[8], *pia;\n\tu32 addr, fru_addr;\n\tint size, len;\n\tu8 csum;\n\n\tif (!is_fru_eeprom_supported(adev, &fru_addr))\n\t\treturn 0;\n\n\tif (!adev->fru_info) {\n\t\tadev->fru_info = kzalloc(sizeof(*adev->fru_info), GFP_KERNEL);\n\t\tif (!adev->fru_info)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tfru_info = adev->fru_info;\n\t/* For Arcturus-and-later, default value of serial_number is unique_id\n\t * so convert it to a 16-digit HEX string for convenience and\n\t * backwards-compatibility.\n\t */\n\tsprintf(fru_info->serial, \"%llx\", adev->unique_id);\n\n\t/* If algo exists, it means that the i2c_adapter's initialized */\n\tif (!adev->pm.fru_eeprom_i2c_bus || !adev->pm.fru_eeprom_i2c_bus->algo) {\n\t\tDRM_WARN(\"Cannot access FRU, EEPROM accessor not initialized\");\n\t\treturn -ENODEV;\n\t}\n\n\t/* Read the IPMI Common header */\n\tlen = amdgpu_eeprom_read(adev->pm.fru_eeprom_i2c_bus, fru_addr, buf,\n\t\t\t\t sizeof(buf));\n\tif (len != 8) {\n\t\tDRM_ERROR(\"Couldn't read the IPMI Common Header: %d\", len);\n\t\treturn len < 0 ? len : -EIO;\n\t}\n\n\tif (buf[0] != 1) {\n\t\tDRM_ERROR(\"Bad IPMI Common Header version: 0x%02x\", buf[0]);\n\t\treturn -EIO;\n\t}\n\n\tfor (csum = 0; len > 0; len--)\n\t\tcsum += buf[len - 1];\n\tif (csum) {\n\t\tDRM_ERROR(\"Bad IPMI Common Header checksum: 0x%02x\", csum);\n\t\treturn -EIO;\n\t}\n\n\t/* Get the offset to the Product Info Area (PIA). */\n\taddr = buf[4] * 8;\n\tif (!addr)\n\t\treturn 0;\n\n\t/* Get the absolute address to the PIA. */\n\taddr += fru_addr;\n\n\t/* Read the header of the PIA. */\n\tlen = amdgpu_eeprom_read(adev->pm.fru_eeprom_i2c_bus, addr, buf, 3);\n\tif (len != 3) {\n\t\tDRM_ERROR(\"Couldn't read the Product Info Area header: %d\", len);\n\t\treturn len < 0 ? len : -EIO;\n\t}\n\n\tif (buf[0] != 1) {\n\t\tDRM_ERROR(\"Bad IPMI Product Info Area version: 0x%02x\", buf[0]);\n\t\treturn -EIO;\n\t}\n\n\tsize = buf[1] * 8;\n\tpia = kzalloc(size, GFP_KERNEL);\n\tif (!pia)\n\t\treturn -ENOMEM;\n\n\t/* Read the whole PIA. */\n\tlen = amdgpu_eeprom_read(adev->pm.fru_eeprom_i2c_bus, addr, pia, size);\n\tif (len != size) {\n\t\tkfree(pia);\n\t\tDRM_ERROR(\"Couldn't read the Product Info Area: %d\", len);\n\t\treturn len < 0 ? len : -EIO;\n\t}\n\n\tfor (csum = 0; size > 0; size--)\n\t\tcsum += pia[size - 1];\n\tif (csum) {\n\t\tDRM_ERROR(\"Bad Product Info Area checksum: 0x%02x\", csum);\n\t\tkfree(pia);\n\t\treturn -EIO;\n\t}\n\n\t/* Now extract useful information from the PIA.\n\t *\n\t * Read Manufacturer Name field whose length is [3].\n\t */\n\taddr = 3;\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->manufacturer_name, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->manufacturer_name),\n\t\t     pia[addr] & 0x3F));\n\tfru_info->manufacturer_name[sizeof(fru_info->manufacturer_name) - 1] =\n\t\t'\\0';\n\n\t/* Read Product Name field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->product_name, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->product_name), pia[addr] & 0x3F));\n\tfru_info->product_name[sizeof(fru_info->product_name) - 1] = '\\0';\n\n\t/* Go to the Product Part/Model Number field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->product_number, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->product_number),\n\t\t     pia[addr] & 0x3F));\n\tfru_info->product_number[sizeof(fru_info->product_number) - 1] = '\\0';\n\n\t/* Go to the Product Version field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\n\t/* Go to the Product Serial Number field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->serial, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->serial), pia[addr] & 0x3F));\n\tfru_info->serial[sizeof(fru_info->serial) - 1] = '\\0';\n\n\t/* Asset Tag field */\n\taddr += 1 + (pia[addr] & 0x3F);\n\n\t/* FRU File Id field. This could be 'null'. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif ((addr + 1 >= len) || !(pia[addr] & 0x3F))\n\t\tgoto Out;\n\tmemcpy(fru_info->fru_id, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->fru_id), pia[addr] & 0x3F));\n\tfru_info->fru_id[sizeof(fru_info->fru_id) - 1] = '\\0';\n\nOut:\n\tkfree(pia);\n\treturn 0;\n}\n\n/**\n * DOC: product_name\n *\n * The amdgpu driver provides a sysfs API for reporting the product name\n * for the device\n * The file product_name is used for this and returns the product name\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_product_name_show(struct device *dev,\n\t\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t\t    char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->product_name);\n}\n\nstatic DEVICE_ATTR(product_name, 0444, amdgpu_fru_product_name_show, NULL);\n\n/**\n * DOC: product_number\n *\n * The amdgpu driver provides a sysfs API for reporting the part number\n * for the device\n * The file product_number is used for this and returns the part number\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_product_number_show(struct device *dev,\n\t\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t\t      char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->product_number);\n}\n\nstatic DEVICE_ATTR(product_number, 0444, amdgpu_fru_product_number_show, NULL);\n\n/**\n * DOC: serial_number\n *\n * The amdgpu driver provides a sysfs API for reporting the serial number\n * for the device\n * The file serial_number is used for this and returns the serial number\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_serial_number_show(struct device *dev,\n\t\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t\t     char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->serial);\n}\n\nstatic DEVICE_ATTR(serial_number, 0444, amdgpu_fru_serial_number_show, NULL);\n\n/**\n * DOC: fru_id\n *\n * The amdgpu driver provides a sysfs API for reporting FRU File Id\n * for the device.\n * The file fru_id is used for this and returns the File Id value\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_id_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->fru_id);\n}\n\nstatic DEVICE_ATTR(fru_id, 0444, amdgpu_fru_id_show, NULL);\n\n/**\n * DOC: manufacturer\n *\n * The amdgpu driver provides a sysfs API for reporting manufacturer name from\n * FRU information.\n * The file manufacturer returns the value as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_manufacturer_name_show(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->manufacturer_name);\n}\n\nstatic DEVICE_ATTR(manufacturer, 0444, amdgpu_fru_manufacturer_name_show, NULL);\n\nstatic const struct attribute *amdgpu_fru_attributes[] = {\n\t&dev_attr_product_name.attr,\n\t&dev_attr_product_number.attr,\n\t&dev_attr_serial_number.attr,\n\t&dev_attr_fru_id.attr,\n\t&dev_attr_manufacturer.attr,\n\tNULL\n};\n\nint amdgpu_fru_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (!is_fru_eeprom_supported(adev, NULL) || !adev->fru_info)\n\t\treturn 0;\n\n\treturn sysfs_create_files(&adev->dev->kobj, amdgpu_fru_attributes);\n}\n\nvoid amdgpu_fru_sysfs_fini(struct amdgpu_device *adev)\n{\n\tif (!is_fru_eeprom_supported(adev, NULL) || !adev->fru_info)\n\t\treturn;\n\n\tsysfs_remove_files(&adev->dev->kobj, amdgpu_fru_attributes);\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/use-freed-mem0-bug.txt", "bug_report_text": "File: drivers/gpu/drm/amd/amdgpu/amdgpu_preempt_mgr.c\n\nLine: 69\n\nFunction: amdgpu_preempt_mgr_new(\n\nDescription: Use of freed memory:\n *res may have been freed inside of ttm_resource_init\n\nConsider adding a check for NULL.\n", "diff_path": "dataset/raw_data/bugs/dev-set/use-freed-mem0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_preempt_mgr.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_preempt_mgr.c\n@@ -66,7 +66,9 @@ static int amdgpu_preempt_mgr_new(struct ttm_resource_manager *man,\n                return -ENOMEM;\n\n        ttm_resource_init(tbo, place, *res);\n-       (*res)->start = AMDGPU_BO_INVALID_OFFSET;\n+       if (*res) {\n+           (*res)->start = AMDGPU_BO_INVALID_OFFSET;\n+       }\n        return 0;\n }", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_preempt_mgr.c", "line_number": 69, "code": "// SPDX-License-Identifier: GPL-2.0 OR MIT\n/*\n * Copyright 2016-2021 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Christian KÃ¶nig, Felix Kuehling\n */\n\n#include \"amdgpu.h\"\n\n/**\n * DOC: mem_info_preempt_used\n *\n * The amdgpu driver provides a sysfs API for reporting current total amount of\n * used preemptible memory.\n * The file mem_info_preempt_used is used for this, and returns the current\n * used size of the preemptible block, in bytes\n */\nstatic ssize_t mem_info_preempt_used_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct ttm_resource_manager *man = &adev->mman.preempt_mgr;\n\n\treturn sysfs_emit(buf, \"%llu\\n\", ttm_resource_manager_usage(man));\n}\n\nstatic DEVICE_ATTR_RO(mem_info_preempt_used);\n\n/**\n * amdgpu_preempt_mgr_new - allocate a new node\n *\n * @man: TTM memory type manager\n * @tbo: TTM BO we need this range for\n * @place: placement flags and restrictions\n * @res: TTM memory object\n *\n * Dummy, just count the space used without allocating resources or any limit.\n */\nstatic int amdgpu_preempt_mgr_new(struct ttm_resource_manager *man,\n\t\t\t\t  struct ttm_buffer_object *tbo,\n\t\t\t\t  const struct ttm_place *place,\n\t\t\t\t  struct ttm_resource **res)\n{\n\t*res = kzalloc(sizeof(**res), GFP_KERNEL);\n\tif (!*res)\n\t\treturn -ENOMEM;\n\n\tttm_resource_init(tbo, place, *res);\n\t(*res)->start = AMDGPU_BO_INVALID_OFFSET;\n\treturn 0;\n}\n\n/**\n * amdgpu_preempt_mgr_del - free ranges\n *\n * @man: TTM memory type manager\n * @res: TTM memory object\n *\n * Free the allocated GTT again.\n */\nstatic void amdgpu_preempt_mgr_del(struct ttm_resource_manager *man,\n\t\t\t\t   struct ttm_resource *res)\n{\n\tttm_resource_fini(man, res);\n\tkfree(res);\n}\n\nstatic const struct ttm_resource_manager_func amdgpu_preempt_mgr_func = {\n\t.alloc = amdgpu_preempt_mgr_new,\n\t.free = amdgpu_preempt_mgr_del,\n};\n\n/**\n * amdgpu_preempt_mgr_init - init PREEMPT manager and DRM MM\n *\n * @adev: amdgpu_device pointer\n *\n * Allocate and initialize the GTT manager.\n */\nint amdgpu_preempt_mgr_init(struct amdgpu_device *adev)\n{\n\tstruct ttm_resource_manager *man = &adev->mman.preempt_mgr;\n\tint ret;\n\n\tman->use_tt = true;\n\tman->func = &amdgpu_preempt_mgr_func;\n\n\tttm_resource_manager_init(man, &adev->mman.bdev, (1 << 30));\n\n\tret = device_create_file(adev->dev, &dev_attr_mem_info_preempt_used);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to create device file mem_info_preempt_used\\n\");\n\t\treturn ret;\n\t}\n\n\tttm_set_driver_manager(&adev->mman.bdev, AMDGPU_PL_PREEMPT, man);\n\tttm_resource_manager_set_used(man, true);\n\treturn 0;\n}\n\n/**\n * amdgpu_preempt_mgr_fini - free and destroy GTT manager\n *\n * @adev: amdgpu_device pointer\n *\n * Destroy and free the GTT manager, returns -EBUSY if ranges are still\n * allocated inside it.\n */\nvoid amdgpu_preempt_mgr_fini(struct amdgpu_device *adev)\n{\n\tstruct ttm_resource_manager *man = &adev->mman.preempt_mgr;\n\tint ret;\n\n\tttm_resource_manager_set_used(man, false);\n\n\tret = ttm_resource_manager_evict_all(&adev->mman.bdev, man);\n\tif (ret)\n\t\treturn;\n\n\tdevice_remove_file(adev->dev, &dev_attr_mem_info_preempt_used);\n\n\tttm_resource_manager_cleanup(man);\n\tttm_set_driver_manager(&adev->mman.bdev, AMDGPU_PL_PREEMPT, NULL);\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/buffer-overflow-memset0-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 463\n\nDescription:\nA potential buffer overflow has been detected in the function gfx_v9_4_2_wait_for_waves_assigned(). \nThe function uses a fixed-size buffer of 256 bytes to store a formatted string, but does not implement proper bounds checking. \nThe size of the string written to str depends on the values of constants CU_ID_MAX, SIMD_ID_MAX, and WAVE_ID_MAX, which are not taken \ninto account when determining the buffer size. This could lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size.\n", "diff_path": "dataset/raw_data/bugs/dev-set/buffer-overflow-memset0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\n@@ -460,7 +460,8 @@ static int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n        char *str;\n        int str_size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_buff_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+       str = kmalloc(max_buff_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -470,7 +471,7 @@ static int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n\n                for (se = 0; se < adev->gfx.config.max_shader_engines; se++)\n                        for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                               memset(str, 0, 256);\n+                               memset(str, 0, max_buff_size);\n                                str_size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                                for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                        str_size += sprintf(str + str_size, \"[\");", "source_code_path": "drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c", "line_number": 463, "code": "/*\n * Copyright 2020 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include \"amdgpu.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n\n#include \"gc/gc_9_4_2_offset.h\"\n#include \"gc/gc_9_4_2_sh_mask.h\"\n#include \"gfx_v9_0.h\"\n\n#include \"gfx_v9_4_2.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_gfx.h\"\n\n#define SE_ID_MAX 8\n#define CU_ID_MAX 16\n#define SIMD_ID_MAX 4\n#define WAVE_ID_MAX 10\n\nenum gfx_v9_4_2_utc_type {\n\tVML2_MEM,\n\tVML2_WALKER_MEM,\n\tUTCL2_MEM,\n\tATC_L2_CACHE_2M,\n\tATC_L2_CACHE_32K,\n\tATC_L2_CACHE_4K\n};\n\nstruct gfx_v9_4_2_utc_block {\n\tenum gfx_v9_4_2_utc_type type;\n\tuint32_t num_banks;\n\tuint32_t num_ways;\n\tuint32_t num_mem_blocks;\n\tstruct soc15_reg idx_reg;\n\tstruct soc15_reg data_reg;\n\tuint32_t sec_count_mask;\n\tuint32_t sec_count_shift;\n\tuint32_t ded_count_mask;\n\tuint32_t ded_count_shift;\n\tuint32_t clear;\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x141dc920),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0x3b458b93),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x1a4f5583),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0x317717f6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x107cc1e6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x351),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_1[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x2591aa38),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0xac9e88b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x2bc3369b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0xfb74ee),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x21f0a2fe),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x49),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regGB_ADDR_CONFIG, 0xffff77ff, 0x2a114042),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTA_CNTL_AUX, 0xfffffeef, 0x10b0000),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_UTCL1_CNTL1, 0xffffffff, 0x30800400),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCI_CNTL_3, 0xff, 0x20),\n};\n\n/*\n * This shader is used to clear VGPRS and LDS, and also write the input\n * pattern into the write back buffer, which will be used by driver to\n * check whether all SIMDs have been covered.\n*/\nstatic const u32 vgpr_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xd3d94000,\n\t0x18000080, 0xd3d94001, 0x18000080, 0xd3d94002, 0x18000080, 0xd3d94003,\n\t0x18000080, 0xd3d94004, 0x18000080, 0xd3d94005, 0x18000080, 0xd3d94006,\n\t0x18000080, 0xd3d94007, 0x18000080, 0xd3d94008, 0x18000080, 0xd3d94009,\n\t0x18000080, 0xd3d9400a, 0x18000080, 0xd3d9400b, 0x18000080, 0xd3d9400c,\n\t0x18000080, 0xd3d9400d, 0x18000080, 0xd3d9400e, 0x18000080, 0xd3d9400f,\n\t0x18000080, 0xd3d94010, 0x18000080, 0xd3d94011, 0x18000080, 0xd3d94012,\n\t0x18000080, 0xd3d94013, 0x18000080, 0xd3d94014, 0x18000080, 0xd3d94015,\n\t0x18000080, 0xd3d94016, 0x18000080, 0xd3d94017, 0x18000080, 0xd3d94018,\n\t0x18000080, 0xd3d94019, 0x18000080, 0xd3d9401a, 0x18000080, 0xd3d9401b,\n\t0x18000080, 0xd3d9401c, 0x18000080, 0xd3d9401d, 0x18000080, 0xd3d9401e,\n\t0x18000080, 0xd3d9401f, 0x18000080, 0xd3d94020, 0x18000080, 0xd3d94021,\n\t0x18000080, 0xd3d94022, 0x18000080, 0xd3d94023, 0x18000080, 0xd3d94024,\n\t0x18000080, 0xd3d94025, 0x18000080, 0xd3d94026, 0x18000080, 0xd3d94027,\n\t0x18000080, 0xd3d94028, 0x18000080, 0xd3d94029, 0x18000080, 0xd3d9402a,\n\t0x18000080, 0xd3d9402b, 0x18000080, 0xd3d9402c, 0x18000080, 0xd3d9402d,\n\t0x18000080, 0xd3d9402e, 0x18000080, 0xd3d9402f, 0x18000080, 0xd3d94030,\n\t0x18000080, 0xd3d94031, 0x18000080, 0xd3d94032, 0x18000080, 0xd3d94033,\n\t0x18000080, 0xd3d94034, 0x18000080, 0xd3d94035, 0x18000080, 0xd3d94036,\n\t0x18000080, 0xd3d94037, 0x18000080, 0xd3d94038, 0x18000080, 0xd3d94039,\n\t0x18000080, 0xd3d9403a, 0x18000080, 0xd3d9403b, 0x18000080, 0xd3d9403c,\n\t0x18000080, 0xd3d9403d, 0x18000080, 0xd3d9403e, 0x18000080, 0xd3d9403f,\n\t0x18000080, 0xd3d94040, 0x18000080, 0xd3d94041, 0x18000080, 0xd3d94042,\n\t0x18000080, 0xd3d94043, 0x18000080, 0xd3d94044, 0x18000080, 0xd3d94045,\n\t0x18000080, 0xd3d94046, 0x18000080, 0xd3d94047, 0x18000080, 0xd3d94048,\n\t0x18000080, 0xd3d94049, 0x18000080, 0xd3d9404a, 0x18000080, 0xd3d9404b,\n\t0x18000080, 0xd3d9404c, 0x18000080, 0xd3d9404d, 0x18000080, 0xd3d9404e,\n\t0x18000080, 0xd3d9404f, 0x18000080, 0xd3d94050, 0x18000080, 0xd3d94051,\n\t0x18000080, 0xd3d94052, 0x18000080, 0xd3d94053, 0x18000080, 0xd3d94054,\n\t0x18000080, 0xd3d94055, 0x18000080, 0xd3d94056, 0x18000080, 0xd3d94057,\n\t0x18000080, 0xd3d94058, 0x18000080, 0xd3d94059, 0x18000080, 0xd3d9405a,\n\t0x18000080, 0xd3d9405b, 0x18000080, 0xd3d9405c, 0x18000080, 0xd3d9405d,\n\t0x18000080, 0xd3d9405e, 0x18000080, 0xd3d9405f, 0x18000080, 0xd3d94060,\n\t0x18000080, 0xd3d94061, 0x18000080, 0xd3d94062, 0x18000080, 0xd3d94063,\n\t0x18000080, 0xd3d94064, 0x18000080, 0xd3d94065, 0x18000080, 0xd3d94066,\n\t0x18000080, 0xd3d94067, 0x18000080, 0xd3d94068, 0x18000080, 0xd3d94069,\n\t0x18000080, 0xd3d9406a, 0x18000080, 0xd3d9406b, 0x18000080, 0xd3d9406c,\n\t0x18000080, 0xd3d9406d, 0x18000080, 0xd3d9406e, 0x18000080, 0xd3d9406f,\n\t0x18000080, 0xd3d94070, 0x18000080, 0xd3d94071, 0x18000080, 0xd3d94072,\n\t0x18000080, 0xd3d94073, 0x18000080, 0xd3d94074, 0x18000080, 0xd3d94075,\n\t0x18000080, 0xd3d94076, 0x18000080, 0xd3d94077, 0x18000080, 0xd3d94078,\n\t0x18000080, 0xd3d94079, 0x18000080, 0xd3d9407a, 0x18000080, 0xd3d9407b,\n\t0x18000080, 0xd3d9407c, 0x18000080, 0xd3d9407d, 0x18000080, 0xd3d9407e,\n\t0x18000080, 0xd3d9407f, 0x18000080, 0xd3d94080, 0x18000080, 0xd3d94081,\n\t0x18000080, 0xd3d94082, 0x18000080, 0xd3d94083, 0x18000080, 0xd3d94084,\n\t0x18000080, 0xd3d94085, 0x18000080, 0xd3d94086, 0x18000080, 0xd3d94087,\n\t0x18000080, 0xd3d94088, 0x18000080, 0xd3d94089, 0x18000080, 0xd3d9408a,\n\t0x18000080, 0xd3d9408b, 0x18000080, 0xd3d9408c, 0x18000080, 0xd3d9408d,\n\t0x18000080, 0xd3d9408e, 0x18000080, 0xd3d9408f, 0x18000080, 0xd3d94090,\n\t0x18000080, 0xd3d94091, 0x18000080, 0xd3d94092, 0x18000080, 0xd3d94093,\n\t0x18000080, 0xd3d94094, 0x18000080, 0xd3d94095, 0x18000080, 0xd3d94096,\n\t0x18000080, 0xd3d94097, 0x18000080, 0xd3d94098, 0x18000080, 0xd3d94099,\n\t0x18000080, 0xd3d9409a, 0x18000080, 0xd3d9409b, 0x18000080, 0xd3d9409c,\n\t0x18000080, 0xd3d9409d, 0x18000080, 0xd3d9409e, 0x18000080, 0xd3d9409f,\n\t0x18000080, 0xd3d940a0, 0x18000080, 0xd3d940a1, 0x18000080, 0xd3d940a2,\n\t0x18000080, 0xd3d940a3, 0x18000080, 0xd3d940a4, 0x18000080, 0xd3d940a5,\n\t0x18000080, 0xd3d940a6, 0x18000080, 0xd3d940a7, 0x18000080, 0xd3d940a8,\n\t0x18000080, 0xd3d940a9, 0x18000080, 0xd3d940aa, 0x18000080, 0xd3d940ab,\n\t0x18000080, 0xd3d940ac, 0x18000080, 0xd3d940ad, 0x18000080, 0xd3d940ae,\n\t0x18000080, 0xd3d940af, 0x18000080, 0xd3d940b0, 0x18000080, 0xd3d940b1,\n\t0x18000080, 0xd3d940b2, 0x18000080, 0xd3d940b3, 0x18000080, 0xd3d940b4,\n\t0x18000080, 0xd3d940b5, 0x18000080, 0xd3d940b6, 0x18000080, 0xd3d940b7,\n\t0x18000080, 0xd3d940b8, 0x18000080, 0xd3d940b9, 0x18000080, 0xd3d940ba,\n\t0x18000080, 0xd3d940bb, 0x18000080, 0xd3d940bc, 0x18000080, 0xd3d940bd,\n\t0x18000080, 0xd3d940be, 0x18000080, 0xd3d940bf, 0x18000080, 0xd3d940c0,\n\t0x18000080, 0xd3d940c1, 0x18000080, 0xd3d940c2, 0x18000080, 0xd3d940c3,\n\t0x18000080, 0xd3d940c4, 0x18000080, 0xd3d940c5, 0x18000080, 0xd3d940c6,\n\t0x18000080, 0xd3d940c7, 0x18000080, 0xd3d940c8, 0x18000080, 0xd3d940c9,\n\t0x18000080, 0xd3d940ca, 0x18000080, 0xd3d940cb, 0x18000080, 0xd3d940cc,\n\t0x18000080, 0xd3d940cd, 0x18000080, 0xd3d940ce, 0x18000080, 0xd3d940cf,\n\t0x18000080, 0xd3d940d0, 0x18000080, 0xd3d940d1, 0x18000080, 0xd3d940d2,\n\t0x18000080, 0xd3d940d3, 0x18000080, 0xd3d940d4, 0x18000080, 0xd3d940d5,\n\t0x18000080, 0xd3d940d6, 0x18000080, 0xd3d940d7, 0x18000080, 0xd3d940d8,\n\t0x18000080, 0xd3d940d9, 0x18000080, 0xd3d940da, 0x18000080, 0xd3d940db,\n\t0x18000080, 0xd3d940dc, 0x18000080, 0xd3d940dd, 0x18000080, 0xd3d940de,\n\t0x18000080, 0xd3d940df, 0x18000080, 0xd3d940e0, 0x18000080, 0xd3d940e1,\n\t0x18000080, 0xd3d940e2, 0x18000080, 0xd3d940e3, 0x18000080, 0xd3d940e4,\n\t0x18000080, 0xd3d940e5, 0x18000080, 0xd3d940e6, 0x18000080, 0xd3d940e7,\n\t0x18000080, 0xd3d940e8, 0x18000080, 0xd3d940e9, 0x18000080, 0xd3d940ea,\n\t0x18000080, 0xd3d940eb, 0x18000080, 0xd3d940ec, 0x18000080, 0xd3d940ed,\n\t0x18000080, 0xd3d940ee, 0x18000080, 0xd3d940ef, 0x18000080, 0xd3d940f0,\n\t0x18000080, 0xd3d940f1, 0x18000080, 0xd3d940f2, 0x18000080, 0xd3d940f3,\n\t0x18000080, 0xd3d940f4, 0x18000080, 0xd3d940f5, 0x18000080, 0xd3d940f6,\n\t0x18000080, 0xd3d940f7, 0x18000080, 0xd3d940f8, 0x18000080, 0xd3d940f9,\n\t0x18000080, 0xd3d940fa, 0x18000080, 0xd3d940fb, 0x18000080, 0xd3d940fc,\n\t0x18000080, 0xd3d940fd, 0x18000080, 0xd3d940fe, 0x18000080, 0xd3d940ff,\n\t0x18000080, 0xb07c0000, 0xbe8a00ff, 0x000000f8, 0xbf11080a, 0x7e000280,\n\t0x7e020280, 0x7e040280, 0x7e060280, 0x7e080280, 0x7e0a0280, 0x7e0c0280,\n\t0x7e0e0280, 0x808a880a, 0xbe80320a, 0xbf84fff5, 0xbf9c0000, 0xd28c0001,\n\t0x0001007f, 0xd28d0001, 0x0002027e, 0x10020288, 0xbe8b0004, 0xb78b4000,\n\t0xd1196a01, 0x00001701, 0xbe8a0087, 0xbefc00c1, 0xd89c4000, 0x00020201,\n\t0xd89cc080, 0x00040401, 0x320202ff, 0x00000800, 0x808a810a, 0xbf84fff8,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry vgpr_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 4 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0xbf },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x400006 },  /* 64KB LDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x3F }, /*  63 - accum-offset = 256 */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * The below shaders are used to clear SGPRS, and also write the input\n * pattern into the write back buffer. The first two dispatch should be\n * scheduled simultaneously which make sure that all SGPRS could be\n * allocated, so the dispatch 1 need check write back buffer before scheduled,\n * make sure that waves of dispatch 0 are all dispacthed to all simds\n * balanced. both dispatch 0 and dispatch 1 should be halted until all waves\n * are dispatched, and then driver write a pattern to the shared memory to make\n * all waves continue.\n*/\nstatic const u32 sgpr112_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbeda0080, 0xbedb0080, 0xbedc0080, 0xbedd0080, 0xbede0080, 0xbedf0080,\n\t0xbee00080, 0xbee10080, 0xbee20080, 0xbee30080, 0xbee40080, 0xbee50080,\n\t0xbf810000\n};\n\nconst struct soc15_reg_entry sgpr112_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 8 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x340 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic const u32 sgpr96_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr96_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0xc },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x2c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * This shader is used to clear the uninitiated sgprs after the above\n * two dispatches, because of hardware feature, dispath 0 couldn't clear\n * top hole sgprs. Therefore need 4 waves per SIMD to cover these sgprs\n*/\nstatic const u32 sgpr64_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr64_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0x10 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x1c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic int gfx_v9_4_2_run_shader(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_ib *ib,\n\t\t\t\t const u32 *shader_ptr, u32 shader_size,\n\t\t\t\t const struct soc15_reg_entry *init_regs, u32 regs_size,\n\t\t\t\t u32 compute_dim_x, u64 wb_gpu_addr, u32 pattern,\n\t\t\t\t struct dma_fence **fence_ptr)\n{\n\tint r, i;\n\tuint32_t total_size, shader_offset;\n\tu64 gpu_addr;\n\n\ttotal_size = (regs_size * 3 + 4 + 5 + 5) * 4;\n\ttotal_size = ALIGN(total_size, 256);\n\tshader_offset = total_size;\n\ttotal_size += ALIGN(shader_size, 256);\n\n\t/* allocate an indirect buffer to put the commands in */\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, total_size,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\t/* load the compute shaders */\n\tfor (i = 0; i < shader_size/sizeof(u32); i++)\n\t\tib->ptr[i + (shader_offset / 4)] = shader_ptr[i];\n\n\t/* init the ib length to 0 */\n\tib->length_dw = 0;\n\n\t/* write the register state for the compute dispatch */\n\tfor (i = 0; i < regs_size; i++) {\n\t\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);\n\t\tib->ptr[ib->length_dw++] = SOC15_REG_ENTRY_OFFSET(init_regs[i])\n\t\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\t\tib->ptr[ib->length_dw++] = init_regs[i].reg_value;\n\t}\n\n\t/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */\n\tgpu_addr = (ib->gpu_addr + (u64)shader_offset) >> 8;\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_PGM_LO)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(gpu_addr);\n\n\t/* write the wb buffer address */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 3);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_USER_DATA_0)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = pattern;\n\n\t/* write dispatch packet */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);\n\tib->ptr[ib->length_dw++] = compute_dim_x; /* x */\n\tib->ptr[ib->length_dw++] = 1; /* y */\n\tib->ptr[ib->length_dw++] = 1; /* z */\n\tib->ptr[ib->length_dw++] =\n\t\tREG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);\n\n\t/* shedule the ib on the ring */\n\tr = amdgpu_ib_schedule(ring, 1, ib, NULL, fence_ptr);\n\tif (r) {\n\t\tdev_err(adev->dev, \"ib submit failed (%d).\\n\", r);\n\t\tamdgpu_ib_free(adev, ib, NULL);\n\t}\n\treturn r;\n}\n\nstatic void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t *wb_ptr)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n\t\tfor (cu = 0; cu < CU_ID_MAX; cu++) {\n\t\t\tmemset(str, 0, 256);\n\t\t\tsize = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++) {\n\t\t\t\tsize += sprintf(str + size, \"[\");\n\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\tsize += sprintf(str + size, \"%x\", wb_ptr[offset]);\n\t\t\t\t\toffset++;\n\t\t\t\t}\n\t\t\t\tsize += sprintf(str + size, \"]  \");\n\t\t\t}\n\t\t\tdev_dbg(adev->dev, \"%s\\n\", str);\n\t\t}\n\t}\n\n\tkfree(str);\n}\n\nstatic int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t *wb_ptr, uint32_t mask,\n\t\t\t\t\t      uint32_t pattern, uint32_t num_wave, bool wait)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t loop = 0;\n\tuint32_t wave_cnt;\n\tuint32_t offset;\n\tchar *str;\n\tint str_size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\t\n\tdo {\n\t\twave_cnt = 0;\n\t\toffset = 0;\n\n\t\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++)\n\t\t\tfor (cu = 0; cu < CU_ID_MAX; cu++) {\n\t\t\t\tmemset(str, 0, 256);\n\t\t\t\tstr_size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n\t\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++) {\n\t\t\t\t\tstr_size += sprintf(str + str_size, \"[\");\n\t\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\t\tif (((1 << wave) & mask) &&\n\t\t\t\t\t\t    (wb_ptr[offset] == pattern))\n\t\t\t\t\t\t\twave_cnt++;\n\n\t\t\t\t\t\tstr_size += sprintf(str + str_size, \"%x\", wb_ptr[offset]);\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t}\n\t\t\t\t\tstr_size += sprintf(str + str_size, \"]  \");\t\n\t\t\t\t}\n\t\t\t}\n\n\t\tif (wave_cnt == num_wave)\n\t\t\treturn 0;\n\n\t\tmdelay(1);\n\t} while (++loop < 2000 && wait);\n\n\tdev_err(adev->dev, \"actual wave num: %d, expected wave num: %d\\n\",\n\t\twave_cnt, num_wave);\n\n\tgfx_v9_4_2_log_wave_assignment(adev, wb_ptr);\n\n\treturn -EBADSLT;\n}\n\nstatic int gfx_v9_4_2_do_sgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ibs[3];\n\tstruct dma_fence *fences[3];\n\tu32 pattern[3] = { 0x1, 0x5, 0xa };\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready ||\n\t\t !adev->gfx.compute_ring[1].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[0],\n\t\t\tsgpr112_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr112_init_compute_shader_aldebaran),\n\t\t\tsgpr112_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr112_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[0], &fences[0]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 224 sgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11,\n\t\t\tpattern[0],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 2,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 224 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[1],\n\t\t\t&disp_ibs[1],\n\t\t\tsgpr96_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr96_init_compute_shader_aldebaran),\n\t\t\tsgpr96_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr96_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number * 2,\n\t\t\twb_ib.gpu_addr, pattern[1], &fences[1]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear next 576 sgprs\\n\");\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11111100,\n\t\t\tpattern[1], adev->gfx.cu_info.number * SIMD_ID_MAX * 6,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 576 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp1_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fences[0], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 224 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = dma_fence_wait(fences[1], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 576 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[2],\n\t\t\tsgpr64_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr64_init_compute_shader_aldebaran),\n\t\t\tsgpr64_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr64_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[2], &fences[2]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 256 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1111,\n\t\t\tpattern[2],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 4,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 256 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp2_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\tr = dma_fence_wait(fences[2], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 256 sgprs\\n\");\n\t\tgoto disp2_failed;\n\t}\n\ndisp2_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[2], NULL);\n\tdma_fence_put(fences[2]);\ndisp1_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[1], NULL);\n\tdma_fence_put(fences[1]);\ndisp0_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[0], NULL);\n\tdma_fence_put(fences[0]);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init SGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init SGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_2_do_vgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\t/* CU_ID: 0~15, SIMD_ID: 0~3, WAVE_ID: 0 ~ 9 */\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ib;\n\tstruct dma_fence *fence;\n\tu32 pattern = 0xa;\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb.\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ib,\n\t\t\tvgpr_init_compute_shader_aldebaran,\n\t\t\tsizeof(vgpr_init_compute_shader_aldebaran),\n\t\t\tvgpr_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(vgpr_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern, &fence);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear vgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fence, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1,\n\t\t\tpattern,\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX,\n\t\t\tfalse);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to cover all simds when clearing vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\ndisp_failed:\n\tamdgpu_ib_free(adev, &disp_ib, NULL);\n\tdma_fence_put(fence);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init VGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init VGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nint gfx_v9_4_2_do_edc_gpr_workarounds(struct amdgpu_device *adev)\n{\n\t/* only support when RAS is enabled */\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn 0;\n\n\t/* Workaround for ALDEBARAN, skip GPRs init in GPU reset.\n\t   Will remove it once GPRs init algorithm works for all CU settings. */\n\tif (amdgpu_in_reset(adev))\n\t\treturn 0;\n\n\tgfx_v9_4_2_do_sgprs_init(adev);\n\n\tgfx_v9_4_2_do_vgprs_init(adev);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev);\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev);\n\nvoid gfx_v9_4_2_init_golden_registers(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t die_id)\n{\n\tsoc15_program_register_sequence(adev,\n\t\t\t\t\tgolden_settings_gc_9_4_2_alde,\n\t\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde));\n\n\t/* apply golden settings per die */\n\tswitch (die_id) {\n\tcase 0:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_0,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_0));\n\t\tbreak;\n\tcase 1:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_1,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_1));\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(adev->dev,\n\t\t\t \"invalid die id %d, ignore channel fabricid remap settings\\n\",\n\t\t\t die_id);\n\t\tbreak;\n\t}\n}\n\nvoid gfx_v9_4_2_debug_trap_config_init(struct amdgpu_device *adev,\n\t\t\t\tuint32_t first_vmid,\n\t\t\t\tuint32_t last_vmid)\n{\n\tuint32_t data;\n\tint i;\n\n\tmutex_lock(&adev->srbm_mutex);\n\n\tfor (i = first_vmid; i < last_vmid; i++) {\n\t\tdata = 0;\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, TRAP_EN, 1);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_EN, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_REPLACE,\n\t\t\t\t\t0);\n\t\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_PER_VMID_CNTL), data);\n\t}\n\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, 0);\n\tmutex_unlock(&adev->srbm_mutex);\n\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA0), 0);\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA1), 0);\n}\n\nvoid gfx_v9_4_2_set_power_brake_sequence(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tgfx_v9_0_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL, PATTERN_MODE, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL, tmp);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL1, PWRBRK_STALL_EN, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL1, tmp);\n\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_INDEX, ixPWRBRK_STALL_PATTERN_CTRL);\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, PWRBRK_STALL_PATTERN_CTRL, PWRBRK_END_STEP, 0x12);\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_DATA, tmp);\n}\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_edc_counter_regs[] = {\n\t/* CPF */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT), 0, 1, 1 },\n\t/* CPC */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT), 0, 1, 1 },\n\t/* GDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT), 0, 1, 1 },\n\t/* RLC */\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2), 0, 1, 1 },\n\t/* SPI */\n\t{ SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT), 0, 8, 1 },\n\t/* SQC */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3), 0, 8, 7 },\n\t/* SQ */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT), 0, 8, 14 },\n\t/* TCP */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW), 0, 8, 14 },\n\t/* TCI */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT), 0, 1, 69 },\n\t/* TCC */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2), 0, 1, 16 },\n\t/* TCA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT), 0, 1, 2 },\n\t/* TCX */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT), 0, 1, 2 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2), 0, 1, 2 },\n\t/* TD */\n\t{ SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT), 0, 8, 14 },\n\t/* TA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT), 0, 8, 14 },\n\t/* GCEA */\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 1, 16 },\n};\n\nstatic void gfx_v9_4_2_select_se_sh(struct amdgpu_device *adev, u32 se_num,\n\t\t\t\t  u32 sh_num, u32 instance)\n{\n\tu32 data;\n\n\tif (instance == 0xffffffff)\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX,\n\t\t\t\t     instance);\n\n\tif (se_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);\n\n\tif (sh_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);\n\n\tWREG32_SOC15_RLC_SHADOW_EX(reg, GC, 0, regGRBM_GFX_INDEX, data);\n}\n\nstatic const struct soc15_ras_field_entry gfx_v9_4_2_ras_fields[] = {\n\t/* CPF */\n\t{ \"CPF_ROQ_ME2\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME2),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME2) },\n\t{ \"CPF_ROQ_ME1\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME1) },\n\t{ \"CPF_TCIU_TAG\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT) },\n\n\t/* CPC */\n\t{ \"CPC_SCRATCH\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT) },\n\t{ \"CPC_UCODE\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT) },\n\t{ \"CPC_DC_STATE_RAM_ME1\", SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT1_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT1_ME1) },\n\n\t/* GDS */\n\t{ \"GDS_GRBM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, DED) },\n\t{ \"GDS_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED) },\n\t{ \"GDS_PHY_CMD_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED) },\n\t{ \"GDS_PHY_DATA_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_DED) },\n\t{ \"GDS_ME0_CS_PIPE_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE0_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE1_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE2_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE3_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED) },\n\t{ \"GDS_ME0_GFXHP3D_PIX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_PIX_DED) },\n\t{ \"GDS_ME0_GFXHP3D_VTX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_VTX_DED) },\n\t{ \"GDS_ME0_CS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_CS_DED) },\n\t{ \"GDS_ME0_GFXHP3D_GS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_GS_DED) },\n\t{ \"GDS_ME1_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE0_DED) },\n\t{ \"GDS_ME1_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE1_DED) },\n\t{ \"GDS_ME1_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE2_DED) },\n\t{ \"GDS_ME1_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE3_DED) },\n\t{ \"GDS_ME2_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE0_DED) },\n\t{ \"GDS_ME2_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE1_DED) },\n\t{ \"GDS_ME2_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE2_DED) },\n\t{ \"GDS_ME2_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE3_DED) },\n\n\t/* RLC */\n\t{ \"RLCG_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCG_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLCV_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCV_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_TCTAG_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_DATA_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_ADDR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE0_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE1_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE2_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE3_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE4_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE5_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE6_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE7_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_DED_COUNT) },\n\n\t/* SPI */\n\t{ \"SPI_SR_MEM\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_DED_COUNT) },\n\t{ \"SPI_GDS_EXPREQ\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_DED_COUNT) },\n\t{ \"SPI_WB_GRANT_30\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_DED_COUNT) },\n\t{ \"SPI_LIFE_CNT\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT */\n\t{ \"SQC_DATA_CU0_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU0_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT2 */\n\t{ \"SQC_INST_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_INST_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT3 */\n\t{ \"SQC_INST_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_PARITY_CNT3 */\n\t{ \"SQC_INST_BANKA_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_DED_COUNT) },\n\n\t/* SQ */\n\t{ \"SQ_LDS_D\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT) },\n\t{ \"SQ_LDS_I\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT) },\n\t{ \"SQ_SGPR\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT) },\n\t{ \"SQ_VGPR0\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT) },\n\t{ \"SQ_VGPR1\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT) },\n\t{ \"SQ_VGPR2\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT) },\n\t{ \"SQ_VGPR3\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT) },\n\n\t/* TCP */\n\t{ \"TCP_CACHE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT) },\n\t{ \"TCP_LFIFO_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT) },\n\t{ \"TCP_CMD_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_DED_COUNT) },\n\t{ \"TCP_VM_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_DED_COUNT) },\n\t{ \"TCP_DB_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO0\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO1\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT) },\n\n\t/* TCI */\n\t{ \"TCI_WRITE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_DED_COUNT) },\n\n\t/* TCC */\n\t{ \"TCC_CACHE_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT) },\n\t{ \"TCC_CACHE_DIRTY\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT) },\n\t{ \"TCC_HIGH_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_LOW_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_SRC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO_NEXT_RAM\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_DED_COUNT) },\n\t{ \"TCC_CACHE_TAG_PROBE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_DED_COUNT) },\n\t{ \"TCC_UC_ATOMIC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_DED_COUNT) },\n\t{ \"TCC_WRITE_CACHE_READ\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_DED_COUNT) },\n\t{ \"TCC_RETURN_CONTROL\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_DED_COUNT) },\n\t{ \"TCC_IN_USE_TRANSFER\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_DED_COUNT) },\n\t{ \"TCC_IN_USE_DEC\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_DED_COUNT) },\n\t{ \"TCC_WRITE_RETURN\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_DED_COUNT) },\n\t{ \"TCC_RETURN_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_DED_COUNT) },\n\n\t/* TCA */\n\t{ \"TCA_HOLE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_DED_COUNT) },\n\t{ \"TCA_REQ_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_DED_COUNT) },\n\n\t/* TCX */\n\t{ \"TCX_GROUP0\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_DED_COUNT) },\n\t{ \"TCX_GROUP1\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_DED_COUNT) },\n\t{ \"TCX_GROUP2\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_DED_COUNT) },\n\t{ \"TCX_GROUP3\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_DED_COUNT) },\n\t{ \"TCX_GROUP4\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_DED_COUNT) },\n\t{ \"TCX_GROUP5\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP5_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP6\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP6_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP7\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP7_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP8\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP8_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP9\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP9_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP10\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP10_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP11\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP11_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP12\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP12_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP13\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP13_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP14\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP14_SED_COUNT), 0, 0 },\n\n\t/* TD */\n\t{ \"TD_SS_FIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT) },\n\t{ \"TD_SS_FIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT) },\n\t{ \"TD_CS_FIFO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_DED_COUNT) },\n\n\t/* TA */\n\t{ \"TA_FS_DFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_DED_COUNT) },\n\t{ \"TA_FL_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_DED_COUNT) },\n\t{ \"TA_FX_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_DED_COUNT) },\n\t{ \"TA_FS_CFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT */\n\t{ \"EA_DRAMRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_RRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_WRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_IOWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT), 0, 0 },\n\n\t/* EA - regGCEA_EDC_CNT2 */\n\t{ \"EA_GMIRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_MAM_D0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_DED_COUNT) },\n\t{ \"EA_MAM_D1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_DED_COUNT) },\n\t{ \"EA_MAM_D2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_DED_COUNT) },\n\t{ \"EA_MAM_D3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT3 */\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IORD_CMDMEM_DED_COUNT) },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IOWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_MAM_A0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_DED_COUNT) },\n\t{ \"EA_MAM_A1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_DED_COUNT) },\n\t{ \"EA_MAM_A2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_DED_COUNT) },\n\t{ \"EA_MAM_A3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_DED_COUNT) },\n\t{ \"EA_MAM_AFMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_DED_COUNT) },\n};\n\nstatic const char * const vml2_walker_mems[] = {\n\t\"UTC_VML2_CACHE_PDE0_MEM0\",\n\t\"UTC_VML2_CACHE_PDE0_MEM1\",\n\t\"UTC_VML2_CACHE_PDE1_MEM0\",\n\t\"UTC_VML2_CACHE_PDE1_MEM1\",\n\t\"UTC_VML2_CACHE_PDE2_MEM0\",\n\t\"UTC_VML2_CACHE_PDE2_MEM1\",\n\t\"UTC_VML2_RDIF_ARADDRS\",\n\t\"UTC_VML2_RDIF_LOG_FIFO\",\n\t\"UTC_VML2_QUEUE_REQ\",\n\t\"UTC_VML2_QUEUE_RET\",\n};\n\nstatic struct gfx_v9_4_2_utc_block gfx_v9_4_2_utc_blocks[] = {\n\t{ VML2_MEM, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ VML2_WALKER_MEM, ARRAY_SIZE(vml2_walker_mems), 1, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_WALKER_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ UTCL2_MEM, 18, 1, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, UTCL2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_2M, 8, 2, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_2M_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_32K, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_32K_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_4K, 8, 2, 8,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_4K_DSM_CNTL, WRITE_COUNTERS, 1) },\n};\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_ea_err_status_regs = {\n\tSOC15_REG_ENTRY(GC, 0, regGCEA_ERR_STATUS), 0, 1, 16\n};\n\nstatic int gfx_v9_4_2_get_reg_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t  const struct soc15_reg_entry *reg,\n\t\t\t\t\t  uint32_t se_id, uint32_t inst_id,\n\t\t\t\t\t  uint32_t value, uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i;\n\tuint32_t sec_cnt, ded_cnt;\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_ras_fields); i++) {\n\t\tif (gfx_v9_4_2_ras_fields[i].reg_offset != reg->reg_offset ||\n\t\t    gfx_v9_4_2_ras_fields[i].seg != reg->seg ||\n\t\t    gfx_v9_4_2_ras_fields[i].inst != reg->inst)\n\t\t\tcontinue;\n\n\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], sec);\n\t\tif (sec_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], SEC %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t sec_cnt);\n\t\t\t*sec_count += sec_cnt;\n\t\t}\n\n\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], ded);\n\t\tif (ded_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], DED %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t ded_cnt);\n\t\t\t*ded_count += ded_cnt;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_2_query_sram_edc_count(struct amdgpu_device *adev,\n\t\t\t\tuint32_t *sec_count, uint32_t *ded_count)\n{\n\tuint32_t i, j, k, data;\n\tuint32_t sec_cnt = 0, ded_cnt = 0;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_edc_counter_regs); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_edc_counter_regs[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_2_edc_counter_regs[i].instance;\n\t\t\t     k++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, j, 0, k);\n\n\t\t\t\t/* if sec/ded_count is null, just clear counter */\n\t\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]));\n\n\t\t\t\tif (!data)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tgfx_v9_4_2_get_reg_error_count(adev,\n\t\t\t\t\t&gfx_v9_4_2_edc_counter_regs[i],\n\t\t\t\t\tj, k, data, &sec_cnt, &ded_cnt);\n\n\t\t\t\t/* clear counter after read */\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count += sec_cnt;\n\t\t*ded_count += ded_cnt;\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_log_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t struct gfx_v9_4_2_utc_block *blk,\n\t\t\t\t\t uint32_t instance, uint32_t sec_cnt,\n\t\t\t\t\t uint32_t ded_cnt)\n{\n\tuint32_t bank, way, mem;\n\tstatic const char * const vml2_way_str[] = { \"BIGK\", \"4K\" };\n\tstatic const char * const utcl2_rounter_str[] = { \"VMC\", \"APT\" };\n\n\tmem = instance % blk->num_mem_blocks;\n\tway = (instance / blk->num_mem_blocks) % blk->num_ways;\n\tbank = instance / (blk->num_mem_blocks * blk->num_ways);\n\n\tswitch (blk->type) {\n\tcase VML2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_VML2_BANK_CACHE_%d_%s_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, vml2_way_str[way], mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase VML2_WALKER_MEM:\n\t\tdev_info(adev->dev, \"GFX SubBlock %s, SED %d, DED %d\\n\",\n\t\t\t vml2_walker_mems[bank], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase UTCL2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTCL2_ROUTER_IFIF%d_GROUP0_%s, SED %d, DED %d\\n\",\n\t\t\tbank, utcl2_rounter_str[mem], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_2M:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_2M_BANK%d_WAY%d_MEM, SED %d, DED %d\\n\",\n\t\t\tbank, way, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_32K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_32K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_4K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_4K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_2_query_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i, j, data;\n\tuint32_t sec_cnt, ded_cnt;\n\tuint32_t num_instances;\n\tstruct gfx_v9_4_2_utc_block *blk;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_utc_blocks); i++) {\n\t\tblk = &gfx_v9_4_2_utc_blocks[i];\n\t\tnum_instances =\n\t\t\tblk->num_banks * blk->num_ways * blk->num_mem_blocks;\n\t\tfor (j = 0; j < num_instances; j++) {\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->idx_reg), j);\n\n\t\t\t/* if sec/ded_count is NULL, just clear counter */\n\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t\t       blk->clear);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg));\n\t\t\tif (!data)\n\t\t\t\tcontinue;\n\n\t\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, sec);\n\t\t\t*sec_count += sec_cnt;\n\t\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, ded);\n\t\t\t*ded_count += ded_cnt;\n\n\t\t\t/* clear counter after read */\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t       blk->clear);\n\n\t\t\t/* print the edc count */\n\t\t\tif (sec_cnt || ded_cnt)\n\t\t\t\tgfx_v9_4_2_log_utc_edc_count(adev, blk, j, sec_cnt,\n\t\t\t\t\t\t\t     ded_cnt);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t    void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tuint32_t sec_count = 0, ded_count = 0;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\terr_data->ue_count = 0;\n\terr_data->ce_count = 0;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n\tgfx_v9_4_2_query_utc_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n}\n\nstatic void gfx_v9_4_2_reset_utc_err_status(struct amdgpu_device *adev)\n{\n\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n}\n\nstatic void gfx_v9_4_2_reset_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\tvalue = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\t\t\tvalue = REG_SET_FIELD(value, GCEA_ERR_STATUS, CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), value);\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, NULL, NULL);\n\tgfx_v9_4_2_query_utc_edc_count(adev, NULL, NULL);\n}\n\nstatic void gfx_v9_4_2_query_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t reg_value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\treg_value = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\n\t\t\tif (REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_WRRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_DATAPARITY_ERROR)) {\n\t\t\t\tdev_warn(adev->dev, \"GCEA err detected at instance: %d, status: 0x%x!\\n\",\n\t\t\t\t\t\tj, reg_value);\n\t\t\t}\n\t\t\t/* clear after read */\n\t\t\treg_value = REG_SET_FIELD(reg_value, GCEA_ERR_STATUS,\n\t\t\t\t\t\t  CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), reg_value);\n\t\t}\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_query_utc_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX UTCL2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Walker Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_ea_err_status(adev);\n\tgfx_v9_4_2_query_utc_err_status(adev);\n\tgfx_v9_4_2_query_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_reset_utc_err_status(adev);\n\tgfx_v9_4_2_reset_ea_err_status(adev);\n\tgfx_v9_4_2_reset_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_enable_watchdog_timer(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\tuint32_t data;\n\n\tdata = REG_SET_FIELD(0, SQ_TIMEOUT_CONFIG, TIMEOUT_FATAL_DISABLE,\n\t\t\t     amdgpu_watchdog_timer.timeout_fatal_disable ? 1 :\n\t\t\t\t\t\t\t\t\t   0);\n\n\tif (amdgpu_watchdog_timer.timeout_fatal_disable &&\n\t    (amdgpu_watchdog_timer.period < 1 ||\n\t     amdgpu_watchdog_timer.period > 0x23)) {\n\t\tdev_warn(adev->dev, \"Watchdog period range is 1 to 0x23\\n\");\n\t\tamdgpu_watchdog_timer.period = 0x23;\n\t}\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, PERIOD_SEL,\n\t\t\t     amdgpu_watchdog_timer.period);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tgfx_v9_4_2_select_se_sh(adev, i, 0xffffffff, 0xffffffff);\n\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_CONFIG, data);\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t address)\n{\n\tWREG32_SOC15_RLC_EX(reg, GC, 0, regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(address << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK));\n\treturn RREG32_SOC15(GC, 0, regSQ_IND_DATA);\n}\n\nstatic void gfx_v9_4_2_log_cu_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t status)\n{\n\tstruct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;\n\tuint32_t i, simd, wave;\n\tuint32_t wave_status;\n\tuint32_t wave_pc_lo, wave_pc_hi;\n\tuint32_t wave_exec_lo, wave_exec_hi;\n\tuint32_t wave_inst_dw0, wave_inst_dw1;\n\tuint32_t wave_ib_sts;\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (!((i << 1) & status))\n\t\t\tcontinue;\n\n\t\tsimd = i / cu_info->max_waves_per_simd;\n\t\twave = i % cu_info->max_waves_per_simd;\n\n\t\twave_status = wave_read_ind(adev, simd, wave, ixSQ_WAVE_STATUS);\n\t\twave_pc_lo = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_LO);\n\t\twave_pc_hi = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_HI);\n\t\twave_exec_lo =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_LO);\n\t\twave_exec_hi =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_HI);\n\t\twave_inst_dw0 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW0);\n\t\twave_inst_dw1 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW1);\n\t\twave_ib_sts = wave_read_ind(adev, simd, wave, ixSQ_WAVE_IB_STS);\n\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"\\t SIMD %d, Wave %d: status 0x%x, pc 0x%llx, exec 0x%llx, inst 0x%llx, ib_sts 0x%x\\n\",\n\t\t\tsimd, wave, wave_status,\n\t\t\t((uint64_t)wave_pc_hi << 32 | wave_pc_lo),\n\t\t\t((uint64_t)wave_exec_hi << 32 | wave_exec_lo),\n\t\t\t((uint64_t)wave_inst_dw1 << 32 | wave_inst_dw0),\n\t\t\twave_ib_sts);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\tuint32_t status;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tstatus = RREG32_SOC15(GC, 0,\n\t\t\t\t\t\t      regSQ_TIMEOUT_STATUS);\n\t\t\t\tif (status != 0) {\n\t\t\t\t\tdev_info(\n\t\t\t\t\t\tadev->dev,\n\t\t\t\t\t\t\"GFX Watchdog Timeout: SE %d, SH %d, CU %d\\n\",\n\t\t\t\t\t\tse_idx, sh_idx, cu_idx);\n\t\t\t\t\tgfx_v9_4_2_log_cu_timeout_status(\n\t\t\t\t\t\tadev, status);\n\t\t\t\t}\n\t\t\t\t/* clear old status */\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n\n\nstruct amdgpu_ras_block_hw_ops  gfx_v9_4_2_ras_ops = {\n\t\t.query_ras_error_count = &gfx_v9_4_2_query_ras_error_count,\n\t\t.reset_ras_error_count = &gfx_v9_4_2_reset_ras_error_count,\n\t\t.query_ras_error_status = &gfx_v9_4_2_query_ras_error_status,\n\t\t.reset_ras_error_status = &gfx_v9_4_2_reset_ras_error_status,\n};\n\nstruct amdgpu_gfx_ras gfx_v9_4_2_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &gfx_v9_4_2_ras_ops,\n\t},\n\t.enable_watchdog_timer = &gfx_v9_4_2_enable_watchdog_timer,\n};\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf3-bug.txt", "bug_report_text": "CID: 501947\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: SECURE_CODING\nFile: drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\nFunction: vcn_v2_0_sw_init\nLine: 169\n\nProblem: \nUnbounded string copy when initializing VCN decoder ring name using sprintf\n\nAbstract:\nThe code uses sprintf to write a fixed string into ring->name without size checking. \nAdditionally, there appears to be a syntax error in the fix attempt, as the sizeof \nparameter is malformed. This could potentially overflow the destination buffer and \ncorrupt adjacent memory.\n\nPath:\n drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c:169\n   sprintf(ring->name, \"vcn_dec\")\n\nDetails:\nThe VCN decoder ring initialization code copies a hardcoded string \"vcn_dec\" into \nthe ring name buffer using sprintf without verifying the buffer size. While the \nsource string is constant in this case, using unbounded string operations in \nkernel code is risky. Additionally, the attempted fix contains a syntax error in \nthe sizeof usage.\n\nFix:\nReplace with properly formatted snprintf call:\n snprintf(ring->name, sizeof(ring->name), \"vcn_dec\")", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf3-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c b/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\nindex bfd067e2d2f1..6419ab97d922 100644\n--- a/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\n@@ -169,7 +169,7 @@ static int vcn_v2_0_sw_init(void *handle)\n        ring->doorbell_index = adev->doorbell_index.vcn.vcn_ring0_1 << 1;\n        ring->vm_hub = AMDGPU_MMHUB0(0);\n\n-       sprintf(ring->name, \"vcn_dec\");\n+       snprintf(ring->name, sizeof(ring->name), \"vcn_dec\");\n        r = amdgpu_ring_init(adev, ring, 512, &adev->vcn.inst->irq, 0,\n                             AMDGPU_RING_PRIO_DEFAULT, NULL);\n        if (r)", "source_code_path": "drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c", "line_number": 169, "code": "/*\n * Copyright 2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/firmware.h>\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_vcn.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_psp.h\"\n#include \"mmsch_v2_0.h\"\n#include \"vcn_v2_0.h\"\n\n#include \"vcn/vcn_2_0_0_offset.h\"\n#include \"vcn/vcn_2_0_0_sh_mask.h\"\n#include \"ivsrcid/vcn/irqsrcs_vcn_2_0.h\"\n\n#define VCN_VID_SOC_ADDRESS_2_0\t\t\t\t\t0x1fa00\n#define VCN1_VID_SOC_ADDRESS_3_0\t\t\t\t0x48200\n\n#define mmUVD_CONTEXT_ID_INTERNAL_OFFSET\t\t\t0x1fd\n#define mmUVD_GPCOM_VCPU_CMD_INTERNAL_OFFSET\t\t\t0x503\n#define mmUVD_GPCOM_VCPU_DATA0_INTERNAL_OFFSET\t\t\t0x504\n#define mmUVD_GPCOM_VCPU_DATA1_INTERNAL_OFFSET\t\t\t0x505\n#define mmUVD_NO_OP_INTERNAL_OFFSET\t\t\t\t0x53f\n#define mmUVD_GP_SCRATCH8_INTERNAL_OFFSET\t\t\t0x54a\n#define mmUVD_SCRATCH9_INTERNAL_OFFSET\t\t\t\t0xc01d\n\n#define mmUVD_LMI_RBC_IB_VMID_INTERNAL_OFFSET\t\t\t0x1e1\n#define mmUVD_LMI_RBC_IB_64BIT_BAR_HIGH_INTERNAL_OFFSET\t\t0x5a6\n#define mmUVD_LMI_RBC_IB_64BIT_BAR_LOW_INTERNAL_OFFSET\t\t0x5a7\n#define mmUVD_RBC_IB_SIZE_INTERNAL_OFFSET\t\t\t0x1e2\n\nstatic const struct amdgpu_hwip_reg_entry vcn_reg_list_2_0[] = {\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_POWER_STATUS),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_STATUS),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_CONTEXT_ID),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_CONTEXT_ID2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_GPCOM_VCPU_DATA0),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_GPCOM_VCPU_DATA1),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_GPCOM_VCPU_CMD),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_PGFSM_CONFIG),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_PGFSM_STATUS),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_LMA_CTL),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_LMA_DATA),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_LMA_MASK),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_PAUSE)\n};\n\nstatic void vcn_v2_0_set_dec_ring_funcs(struct amdgpu_device *adev);\nstatic void vcn_v2_0_set_enc_ring_funcs(struct amdgpu_device *adev);\nstatic void vcn_v2_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic int vcn_v2_0_set_powergating_state(void *handle,\n\t\t\t\tenum amd_powergating_state state);\nstatic int vcn_v2_0_pause_dpg_mode(struct amdgpu_device *adev,\n\t\t\t\tint inst_idx, struct dpg_pause_state *new_state);\nstatic int vcn_v2_0_start_sriov(struct amdgpu_device *adev);\n/**\n * vcn_v2_0_early_init - set function pointers and load microcode\n *\n * @handle: amdgpu_device pointer\n *\n * Set ring and irq function pointers\n * Load microcode from filesystem\n */\nstatic int vcn_v2_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tadev->vcn.num_enc_rings = 1;\n\telse\n\t\tadev->vcn.num_enc_rings = 2;\n\n\tvcn_v2_0_set_dec_ring_funcs(adev);\n\tvcn_v2_0_set_enc_ring_funcs(adev);\n\tvcn_v2_0_set_irq_funcs(adev);\n\n\treturn amdgpu_vcn_early_init(adev);\n}\n\n/**\n * vcn_v2_0_sw_init - sw init for VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * Load firmware and sw initialization\n */\nstatic int vcn_v2_0_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint i, r;\n\tuint32_t reg_count = ARRAY_SIZE(vcn_reg_list_2_0);\n\tuint32_t *ptr;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tvolatile struct amdgpu_fw_shared *fw_shared;\n\n\t/* VCN DEC TRAP */\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VCN,\n\t\t\t      VCN_2_0__SRCID__UVD_SYSTEM_MESSAGE_INTERRUPT,\n\t\t\t      &adev->vcn.inst->irq);\n\tif (r)\n\t\treturn r;\n\n\t/* VCN ENC TRAP */\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VCN,\n\t\t\t\t      i + VCN_2_0__SRCID__UVD_ENC_GENERAL_PURPOSE,\n\t\t\t\t      &adev->vcn.inst->irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vcn_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vcn_setup_ucode(adev);\n\n\tr = amdgpu_vcn_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tring = &adev->vcn.inst->ring_dec;\n\n\tring->use_doorbell = true;\n\tring->doorbell_index = adev->doorbell_index.vcn.vcn_ring0_1 << 1;\n\tring->vm_hub = AMDGPU_MMHUB0(0);\n\n\tsprintf(ring->name, \"vcn_dec\");\n\tr = amdgpu_ring_init(adev, ring, 512, &adev->vcn.inst->irq, 0,\n\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\tif (r)\n\t\treturn r;\n\n\tadev->vcn.internal.context_id = mmUVD_CONTEXT_ID_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_vmid = mmUVD_LMI_RBC_IB_VMID_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_bar_low = mmUVD_LMI_RBC_IB_64BIT_BAR_LOW_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_bar_high = mmUVD_LMI_RBC_IB_64BIT_BAR_HIGH_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_size = mmUVD_RBC_IB_SIZE_INTERNAL_OFFSET;\n\tadev->vcn.internal.gp_scratch8 = mmUVD_GP_SCRATCH8_INTERNAL_OFFSET;\n\n\tadev->vcn.internal.scratch9 = mmUVD_SCRATCH9_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.scratch9 = SOC15_REG_OFFSET(UVD, 0, mmUVD_SCRATCH9);\n\tadev->vcn.internal.data0 = mmUVD_GPCOM_VCPU_DATA0_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.data0 = SOC15_REG_OFFSET(UVD, 0, mmUVD_GPCOM_VCPU_DATA0);\n\tadev->vcn.internal.data1 = mmUVD_GPCOM_VCPU_DATA1_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.data1 = SOC15_REG_OFFSET(UVD, 0, mmUVD_GPCOM_VCPU_DATA1);\n\tadev->vcn.internal.cmd = mmUVD_GPCOM_VCPU_CMD_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.cmd = SOC15_REG_OFFSET(UVD, 0, mmUVD_GPCOM_VCPU_CMD);\n\tadev->vcn.internal.nop = mmUVD_NO_OP_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.nop = SOC15_REG_OFFSET(UVD, 0, mmUVD_NO_OP);\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tenum amdgpu_ring_priority_level hw_prio = amdgpu_vcn_get_enc_ring_prio(i);\n\n\t\tring = &adev->vcn.inst->ring_enc[i];\n\t\tring->use_doorbell = true;\n\t\tring->vm_hub = AMDGPU_MMHUB0(0);\n\t\tif (!amdgpu_sriov_vf(adev))\n\t\t\tring->doorbell_index = (adev->doorbell_index.vcn.vcn_ring0_1 << 1) + 2 + i;\n\t\telse\n\t\t\tring->doorbell_index = (adev->doorbell_index.vcn.vcn_ring0_1 << 1) + 1 + i;\n\t\tsprintf(ring->name, \"vcn_enc%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 512, &adev->vcn.inst->irq, 0,\n\t\t\t\t     hw_prio, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tadev->vcn.pause_dpg_mode = vcn_v2_0_pause_dpg_mode;\n\n\tr = amdgpu_virt_alloc_mm_table(adev);\n\tif (r)\n\t\treturn r;\n\n\tfw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\tfw_shared->present_flag_0 = cpu_to_le32(AMDGPU_VCN_MULTI_QUEUE_FLAG);\n\n\tif (amdgpu_vcnfw_log)\n\t\tamdgpu_vcn_fwlog_init(adev->vcn.inst);\n\n\t/* Allocate memory for VCN IP Dump buffer */\n\tptr = kcalloc(adev->vcn.num_vcn_inst * reg_count, sizeof(uint32_t), GFP_KERNEL);\n\tif (!ptr) {\n\t\tDRM_ERROR(\"Failed to allocate memory for VCN IP Dump\\n\");\n\t\tadev->vcn.ip_dump = NULL;\n\t} else {\n\t\tadev->vcn.ip_dump = ptr;\n\t}\n\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_sw_fini - sw fini for VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * VCN suspend and free up sw allocation\n */\nstatic int vcn_v2_0_sw_fini(void *handle)\n{\n\tint r, idx;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tfw_shared->present_flag_0 = 0;\n\t\tdrm_dev_exit(idx);\n\t}\n\n\tamdgpu_virt_free_mm_table(adev);\n\n\tr = amdgpu_vcn_suspend(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vcn_sw_fini(adev);\n\n\tkfree(adev->vcn.ip_dump);\n\n\treturn r;\n}\n\n/**\n * vcn_v2_0_hw_init - start and test VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * Initialize the hardware, boot up the VCPU and do some testing\n */\nstatic int vcn_v2_0_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring = &adev->vcn.inst->ring_dec;\n\tint i, r;\n\n\tadev->nbio.funcs->vcn_doorbell_range(adev, ring->use_doorbell,\n\t\t\t\t\t     ring->doorbell_index, 0);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tvcn_v2_0_start_sriov(adev);\n\n\tr = amdgpu_ring_test_helper(ring);\n\tif (r)\n\t\treturn r;\n\n\t//Disable vcn decode for sriov\n\tif (amdgpu_sriov_vf(adev))\n\t\tring->sched.ready = false;\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tring = &adev->vcn.inst->ring_enc[i];\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_hw_fini - stop the hardware block\n *\n * @handle: amdgpu_device pointer\n *\n * Stop the VCN block, mark ring as not ready any more\n */\nstatic int vcn_v2_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcancel_delayed_work_sync(&adev->vcn.idle_work);\n\n\tif ((adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG) ||\n\t    (adev->vcn.cur_state != AMD_PG_STATE_GATE &&\n\t      RREG32_SOC15(VCN, 0, mmUVD_STATUS)))\n\t\tvcn_v2_0_set_powergating_state(adev, AMD_PG_STATE_GATE);\n\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_suspend - suspend VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * HW fini and suspend VCN block\n */\nstatic int vcn_v2_0_suspend(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = vcn_v2_0_hw_fini(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vcn_suspend(adev);\n\n\treturn r;\n}\n\n/**\n * vcn_v2_0_resume - resume VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * Resume firmware and hw init VCN block\n */\nstatic int vcn_v2_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = amdgpu_vcn_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = vcn_v2_0_hw_init(adev);\n\n\treturn r;\n}\n\n/**\n * vcn_v2_0_mc_resume - memory controller programming\n *\n * @adev: amdgpu_device pointer\n *\n * Let the VCN memory controller know it's offsets\n */\nstatic void vcn_v2_0_mc_resume(struct amdgpu_device *adev)\n{\n\tuint32_t size = AMDGPU_GPU_PAGE_ALIGN(adev->vcn.fw[0]->size + 4);\n\tuint32_t offset;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\t/* cache window 0: fw */\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW,\n\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_lo));\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH,\n\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_hi));\n\t\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET0, 0);\n\t\toffset = 0;\n\t} else {\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW,\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr));\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH,\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr));\n\t\toffset = size;\n\t\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET0,\n\t\t\tAMDGPU_UVD_FIRMWARE_OFFSET >> 3);\n\t}\n\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_SIZE0, size);\n\n\t/* cache window 1: stack */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW,\n\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH,\n\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset));\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET1, 0);\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_SIZE1, AMDGPU_VCN_STACK_SIZE);\n\n\t/* cache window 2: context */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_LOW,\n\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_HIGH,\n\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE));\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET2, 0);\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_SIZE2, AMDGPU_VCN_CONTEXT_SIZE);\n\n\t/* non-cache window */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_LOW,\n\t\tlower_32_bits(adev->vcn.inst->fw_shared.gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_HIGH,\n\t\tupper_32_bits(adev->vcn.inst->fw_shared.gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_NONCACHE_OFFSET0, 0);\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_NONCACHE_SIZE0,\n\t\tAMDGPU_GPU_PAGE_ALIGN(sizeof(struct amdgpu_fw_shared)));\n\n\tWREG32_SOC15(UVD, 0, mmUVD_GFX10_ADDR_CONFIG, adev->gfx.config.gb_addr_config);\n}\n\nstatic void vcn_v2_0_mc_resume_dpg_mode(struct amdgpu_device *adev, bool indirect)\n{\n\tuint32_t size = AMDGPU_GPU_PAGE_ALIGN(adev->vcn.fw[0]->size + 4);\n\tuint32_t offset;\n\n\t/* cache window 0: fw */\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tif (!indirect) {\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_lo), 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_hi), 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET0), 0, 0, indirect);\n\t\t} else {\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW), 0, 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH), 0, 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET0), 0, 0, indirect);\n\t\t}\n\t\toffset = 0;\n\t} else {\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr), 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr), 0, indirect);\n\t\toffset = size;\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET0),\n\t\t\tAMDGPU_UVD_FIRMWARE_OFFSET >> 3, 0, indirect);\n\t}\n\n\tif (!indirect)\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE0), size, 0, indirect);\n\telse\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE0), 0, 0, indirect);\n\n\t/* cache window 1: stack */\n\tif (!indirect) {\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset), 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset), 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET1), 0, 0, indirect);\n\t} else {\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW), 0, 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH), 0, 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET1), 0, 0, indirect);\n\t}\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE1), AMDGPU_VCN_STACK_SIZE, 0, indirect);\n\n\t/* cache window 2: context */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_LOW),\n\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_HIGH),\n\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET2), 0, 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE2), AMDGPU_VCN_CONTEXT_SIZE, 0, indirect);\n\n\t/* non-cache window */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_LOW),\n\t\tlower_32_bits(adev->vcn.inst->fw_shared.gpu_addr), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_HIGH),\n\t\tupper_32_bits(adev->vcn.inst->fw_shared.gpu_addr), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_NONCACHE_OFFSET0), 0, 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_NONCACHE_SIZE0),\n\t\tAMDGPU_GPU_PAGE_ALIGN(sizeof(struct amdgpu_fw_shared)), 0, indirect);\n\n\t/* VCN global tiling registers */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_GFX10_ADDR_CONFIG), adev->gfx.config.gb_addr_config, 0, indirect);\n}\n\n/**\n * vcn_v2_0_disable_clock_gating - disable VCN clock gating\n *\n * @adev: amdgpu_device pointer\n *\n * Disable clock gating for VCN block\n */\nstatic void vcn_v2_0_disable_clock_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\t/* UVD disable CGC */\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tif (adev->cg_flags & AMD_CG_SUPPORT_VCN_MGCG)\n\t\tdata |= 1 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\telse\n\t\tdata &= ~UVD_CGC_CTRL__DYN_CLOCK_MODE_MASK;\n\tdata |= 1 << UVD_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\tdata |= 4 << UVD_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_GATE);\n\tdata &= ~(UVD_CGC_GATE__SYS_MASK\n\t\t| UVD_CGC_GATE__UDEC_MASK\n\t\t| UVD_CGC_GATE__MPEG2_MASK\n\t\t| UVD_CGC_GATE__REGS_MASK\n\t\t| UVD_CGC_GATE__RBC_MASK\n\t\t| UVD_CGC_GATE__LMI_MC_MASK\n\t\t| UVD_CGC_GATE__LMI_UMC_MASK\n\t\t| UVD_CGC_GATE__IDCT_MASK\n\t\t| UVD_CGC_GATE__MPRD_MASK\n\t\t| UVD_CGC_GATE__MPC_MASK\n\t\t| UVD_CGC_GATE__LBSI_MASK\n\t\t| UVD_CGC_GATE__LRBBM_MASK\n\t\t| UVD_CGC_GATE__UDEC_RE_MASK\n\t\t| UVD_CGC_GATE__UDEC_CM_MASK\n\t\t| UVD_CGC_GATE__UDEC_IT_MASK\n\t\t| UVD_CGC_GATE__UDEC_DB_MASK\n\t\t| UVD_CGC_GATE__UDEC_MP_MASK\n\t\t| UVD_CGC_GATE__WCB_MASK\n\t\t| UVD_CGC_GATE__VCPU_MASK\n\t\t| UVD_CGC_GATE__SCPU_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_GATE, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tdata &= ~(UVD_CGC_CTRL__UDEC_RE_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_CM_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_IT_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_DB_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MP_MODE_MASK\n\t\t| UVD_CGC_CTRL__SYS_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPEG2_MODE_MASK\n\t\t| UVD_CGC_CTRL__REGS_MODE_MASK\n\t\t| UVD_CGC_CTRL__RBC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_MC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_UMC_MODE_MASK\n\t\t| UVD_CGC_CTRL__IDCT_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPRD_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LBSI_MODE_MASK\n\t\t| UVD_CGC_CTRL__LRBBM_MODE_MASK\n\t\t| UVD_CGC_CTRL__WCB_MODE_MASK\n\t\t| UVD_CGC_CTRL__VCPU_MODE_MASK\n\t\t| UVD_CGC_CTRL__SCPU_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\t/* turn on */\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_GATE);\n\tdata |= (UVD_SUVD_CGC_GATE__SRE_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_MASK\n\t\t| UVD_SUVD_CGC_GATE__SMP_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_MASK\n\t\t| UVD_SUVD_CGC_GATE__SRE_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SRE_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCLR_MASK\n\t\t| UVD_SUVD_CGC_GATE__UVD_SC_MASK\n\t\t| UVD_SUVD_CGC_GATE__ENT_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_HEVC_DEC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_HEVC_ENC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SITE_MASK\n\t\t| UVD_SUVD_CGC_GATE__SRE_VP9_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_VP9_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_VP9_DEC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_VP9_MASK\n\t\t| UVD_SUVD_CGC_GATE__IME_HEVC_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_GATE, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL);\n\tdata &= ~(UVD_SUVD_CGC_CTRL__SRE_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SIT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SMP_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCM_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SDB_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCLR_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__UVD_SC_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__ENT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__IME_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SITE_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL, data);\n}\n\nstatic void vcn_v2_0_clock_gating_dpg_mode(struct amdgpu_device *adev,\n\t\tuint8_t sram_sel, uint8_t indirect)\n{\n\tuint32_t reg_data = 0;\n\n\t/* enable sw clock gating control */\n\tif (adev->cg_flags & AMD_CG_SUPPORT_VCN_MGCG)\n\t\treg_data = 1 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\telse\n\t\treg_data = 0 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\treg_data |= 1 << UVD_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\treg_data |= 4 << UVD_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\treg_data &= ~(UVD_CGC_CTRL__UDEC_RE_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_CM_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_IT_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_DB_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_MP_MODE_MASK |\n\t\t UVD_CGC_CTRL__SYS_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_MODE_MASK |\n\t\t UVD_CGC_CTRL__MPEG2_MODE_MASK |\n\t\t UVD_CGC_CTRL__REGS_MODE_MASK |\n\t\t UVD_CGC_CTRL__RBC_MODE_MASK |\n\t\t UVD_CGC_CTRL__LMI_MC_MODE_MASK |\n\t\t UVD_CGC_CTRL__LMI_UMC_MODE_MASK |\n\t\t UVD_CGC_CTRL__IDCT_MODE_MASK |\n\t\t UVD_CGC_CTRL__MPRD_MODE_MASK |\n\t\t UVD_CGC_CTRL__MPC_MODE_MASK |\n\t\t UVD_CGC_CTRL__LBSI_MODE_MASK |\n\t\t UVD_CGC_CTRL__LRBBM_MODE_MASK |\n\t\t UVD_CGC_CTRL__WCB_MODE_MASK |\n\t\t UVD_CGC_CTRL__VCPU_MODE_MASK |\n\t\t UVD_CGC_CTRL__SCPU_MODE_MASK);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_CGC_CTRL), reg_data, sram_sel, indirect);\n\n\t/* turn off clock gating */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_CGC_GATE), 0, sram_sel, indirect);\n\n\t/* turn on SUVD clock gating */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_SUVD_CGC_GATE), 1, sram_sel, indirect);\n\n\t/* turn on sw mode in UVD_SUVD_CGC_CTRL */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_SUVD_CGC_CTRL), 0, sram_sel, indirect);\n}\n\n/**\n * vcn_v2_0_enable_clock_gating - enable VCN clock gating\n *\n * @adev: amdgpu_device pointer\n *\n * Enable clock gating for VCN block\n */\nstatic void vcn_v2_0_enable_clock_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data = 0;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\t/* enable UVD CGC */\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tif (adev->cg_flags & AMD_CG_SUPPORT_VCN_MGCG)\n\t\tdata |= 1 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\telse\n\t\tdata |= 0 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\tdata |= 1 << UVD_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\tdata |= 4 << UVD_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tdata |= (UVD_CGC_CTRL__UDEC_RE_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_CM_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_IT_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_DB_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MP_MODE_MASK\n\t\t| UVD_CGC_CTRL__SYS_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPEG2_MODE_MASK\n\t\t| UVD_CGC_CTRL__REGS_MODE_MASK\n\t\t| UVD_CGC_CTRL__RBC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_MC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_UMC_MODE_MASK\n\t\t| UVD_CGC_CTRL__IDCT_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPRD_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LBSI_MODE_MASK\n\t\t| UVD_CGC_CTRL__LRBBM_MODE_MASK\n\t\t| UVD_CGC_CTRL__WCB_MODE_MASK\n\t\t| UVD_CGC_CTRL__VCPU_MODE_MASK\n\t\t| UVD_CGC_CTRL__SCPU_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL);\n\tdata |= (UVD_SUVD_CGC_CTRL__SRE_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SIT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SMP_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCM_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SDB_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCLR_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__UVD_SC_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__ENT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__IME_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SITE_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL, data);\n}\n\nstatic void vcn_v2_0_disable_static_power_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data = 0;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN) {\n\t\tdata = (1 << UVD_PGFSM_CONFIG__UVDM_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDU_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDF_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDC_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDB_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIL_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIR_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTD_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTE_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDE_PWR_CONFIG__SHIFT);\n\n\t\tWREG32_SOC15(VCN, 0, mmUVD_PGFSM_CONFIG, data);\n\t\tSOC15_WAIT_ON_RREG(VCN, 0, mmUVD_PGFSM_STATUS,\n\t\t\tUVD_PGFSM_STATUS__UVDM_UVDU_PWR_ON_2_0, 0xFFFFF);\n\t} else {\n\t\tdata = (1 << UVD_PGFSM_CONFIG__UVDM_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDU_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDF_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDC_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDB_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDIL_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDIR_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDTD_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDTE_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDE_PWR_CONFIG__SHIFT);\n\t\tWREG32_SOC15(VCN, 0, mmUVD_PGFSM_CONFIG, data);\n\t\tSOC15_WAIT_ON_RREG(VCN, 0, mmUVD_PGFSM_STATUS, 0,  0xFFFFF);\n\t}\n\n\t/* polling UVD_PGFSM_STATUS to confirm UVDM_PWR_STATUS,\n\t * UVDU_PWR_STATUS are 0 (power on) */\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS);\n\tdata &= ~0x103;\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN)\n\t\tdata |= UVD_PGFSM_CONFIG__UVDM_UVDU_PWR_ON |\n\t\t\tUVD_POWER_STATUS__UVD_PG_EN_MASK;\n\n\tWREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS, data);\n}\n\nstatic void vcn_v2_0_enable_static_power_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data = 0;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN) {\n\t\t/* Before power off, this indicator has to be turned on */\n\t\tdata = RREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS);\n\t\tdata &= ~UVD_POWER_STATUS__UVD_POWER_STATUS_MASK;\n\t\tdata |= UVD_POWER_STATUS__UVD_POWER_STATUS_TILES_OFF;\n\t\tWREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS, data);\n\n\n\t\tdata = (2 << UVD_PGFSM_CONFIG__UVDM_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDU_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDF_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDC_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDB_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIL_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIR_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTD_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTE_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDE_PWR_CONFIG__SHIFT);\n\n\t\tWREG32_SOC15(VCN, 0, mmUVD_PGFSM_CONFIG, data);\n\n\t\tdata = (2 << UVD_PGFSM_STATUS__UVDM_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDU_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDF_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDC_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDB_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDIL_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDIR_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDTD_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDTE_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDE_PWR_STATUS__SHIFT);\n\t\tSOC15_WAIT_ON_RREG(VCN, 0, mmUVD_PGFSM_STATUS, data, 0xFFFFF);\n\t}\n}\n\nstatic int vcn_v2_0_start_dpg_mode(struct amdgpu_device *adev, bool indirect)\n{\n\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\tstruct amdgpu_ring *ring = &adev->vcn.inst->ring_dec;\n\tuint32_t rb_bufsz, tmp;\n\n\tvcn_v2_0_enable_static_power_gating(adev);\n\n\t/* enable dynamic power gating mode */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_POWER_STATUS);\n\ttmp |= UVD_POWER_STATUS__UVD_PG_MODE_MASK;\n\ttmp |= UVD_POWER_STATUS__UVD_PG_EN_MASK;\n\tWREG32_SOC15(UVD, 0, mmUVD_POWER_STATUS, tmp);\n\n\tif (indirect)\n\t\tadev->vcn.inst->dpg_sram_curr_addr = (uint32_t *)adev->vcn.inst->dpg_sram_cpu_addr;\n\n\t/* enable clock gating */\n\tvcn_v2_0_clock_gating_dpg_mode(adev, 0, indirect);\n\n\t/* enable VCPU clock */\n\ttmp = (0xFF << UVD_VCPU_CNTL__PRB_TIMEOUT_VAL__SHIFT);\n\ttmp |= UVD_VCPU_CNTL__CLK_EN_MASK;\n\ttmp |= UVD_VCPU_CNTL__MIF_WR_LOW_THRESHOLD_BP_MASK;\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CNTL), tmp, 0, indirect);\n\n\t/* disable master interupt */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MASTINT_EN), 0, 0, indirect);\n\n\t/* setup mmUVD_LMI_CTRL */\n\ttmp = (UVD_LMI_CTRL__WRITE_CLEAN_TIMER_EN_MASK |\n\t\tUVD_LMI_CTRL__REQ_MODE_MASK |\n\t\tUVD_LMI_CTRL__CRC_RESET_MASK |\n\t\tUVD_LMI_CTRL__MASK_MC_URGENT_MASK |\n\t\tUVD_LMI_CTRL__DATA_COHERENCY_EN_MASK |\n\t\tUVD_LMI_CTRL__VCPU_DATA_COHERENCY_EN_MASK |\n\t\t(8 << UVD_LMI_CTRL__WRITE_CLEAN_TIMER__SHIFT) |\n\t\t0x00100000L);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_CTRL), tmp, 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_CNTL),\n\t\t0x2 << UVD_MPC_CNTL__REPLACEMENT_MODE__SHIFT, 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_SET_MUXA0),\n\t\t((0x1 << UVD_MPC_SET_MUXA0__VARA_1__SHIFT) |\n\t\t (0x2 << UVD_MPC_SET_MUXA0__VARA_2__SHIFT) |\n\t\t (0x3 << UVD_MPC_SET_MUXA0__VARA_3__SHIFT) |\n\t\t (0x4 << UVD_MPC_SET_MUXA0__VARA_4__SHIFT)), 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_SET_MUXB0),\n\t\t((0x1 << UVD_MPC_SET_MUXB0__VARB_1__SHIFT) |\n\t\t (0x2 << UVD_MPC_SET_MUXB0__VARB_2__SHIFT) |\n\t\t (0x3 << UVD_MPC_SET_MUXB0__VARB_3__SHIFT) |\n\t\t (0x4 << UVD_MPC_SET_MUXB0__VARB_4__SHIFT)), 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_SET_MUX),\n\t\t((0x0 << UVD_MPC_SET_MUX__SET_0__SHIFT) |\n\t\t (0x1 << UVD_MPC_SET_MUX__SET_1__SHIFT) |\n\t\t (0x2 << UVD_MPC_SET_MUX__SET_2__SHIFT)), 0, indirect);\n\n\tvcn_v2_0_mc_resume_dpg_mode(adev, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_REG_XX_MASK), 0x10, 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_RBC_XX_IB_REG_CHECK), 0x3, 0, indirect);\n\n\t/* release VCPU reset to boot */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_SOFT_RESET), 0, 0, indirect);\n\n\t/* enable LMI MC and UMC channels */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_CTRL2),\n\t\t0x1F << UVD_LMI_CTRL2__RE_OFLD_MIF_WR_REQ_NUM__SHIFT, 0, indirect);\n\n\t/* enable master interrupt */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MASTINT_EN),\n\t\tUVD_MASTINT_EN__VCPU_EN_MASK, 0, indirect);\n\n\tif (indirect)\n\t\tamdgpu_vcn_psp_update_sram(adev, 0, 0);\n\n\t/* force RBC into idle state */\n\trb_bufsz = order_base_2(ring->ring_size);\n\ttmp = REG_SET_FIELD(0, UVD_RBC_RB_CNTL, RB_BUFSZ, rb_bufsz);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_BLKSZ, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_FETCH, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_UPDATE, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_RPTR_WR_EN, 1);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_CNTL, tmp);\n\n\t/* Stall DPG before WPTR/RPTR reset */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\tUVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK,\n\t\t~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\tfw_shared->multi_queue.decode_queue_mode |= FW_QUEUE_RING_RESET;\n\n\t/* set the write pointer delay */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR_CNTL, 0);\n\n\t/* set the wb address */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR_ADDR,\n\t\t(upper_32_bits(ring->gpu_addr) >> 2));\n\n\t/* program the RB_BASE for ring buffer */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_LOW,\n\t\tlower_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_HIGH,\n\t\tupper_32_bits(ring->gpu_addr));\n\n\t/* Initialize the ring buffer's read and write pointers */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR, 0);\n\n\tWREG32_SOC15(UVD, 0, mmUVD_SCRATCH2, 0);\n\n\tring->wptr = RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR,\n\t\tlower_32_bits(ring->wptr));\n\n\tfw_shared->multi_queue.decode_queue_mode &= ~FW_QUEUE_RING_RESET;\n\t/* Unstall DPG */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\t0, ~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\treturn 0;\n}\n\nstatic int vcn_v2_0_start(struct amdgpu_device *adev)\n{\n\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\tstruct amdgpu_ring *ring = &adev->vcn.inst->ring_dec;\n\tuint32_t rb_bufsz, tmp;\n\tuint32_t lmi_swap_cntl;\n\tint i, j, r;\n\n\tif (adev->pm.dpm_enabled)\n\t\tamdgpu_dpm_enable_uvd(adev, true);\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG)\n\t\treturn vcn_v2_0_start_dpg_mode(adev, adev->vcn.indirect_sram);\n\n\tvcn_v2_0_disable_static_power_gating(adev);\n\n\t/* set uvd status busy */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_STATUS) | UVD_STATUS__UVD_BUSY;\n\tWREG32_SOC15(UVD, 0, mmUVD_STATUS, tmp);\n\n\t/*SW clock gating */\n\tvcn_v2_0_disable_clock_gating(adev);\n\n\t/* enable VCPU clock */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_VCPU_CNTL),\n\t\tUVD_VCPU_CNTL__CLK_EN_MASK, ~UVD_VCPU_CNTL__CLK_EN_MASK);\n\n\t/* disable master interrupt */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_MASTINT_EN), 0,\n\t\t~UVD_MASTINT_EN__VCPU_EN_MASK);\n\n\t/* setup mmUVD_LMI_CTRL */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_LMI_CTRL);\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_CTRL, tmp |\n\t\tUVD_LMI_CTRL__WRITE_CLEAN_TIMER_EN_MASK\t|\n\t\tUVD_LMI_CTRL__MASK_MC_URGENT_MASK |\n\t\tUVD_LMI_CTRL__DATA_COHERENCY_EN_MASK |\n\t\tUVD_LMI_CTRL__VCPU_DATA_COHERENCY_EN_MASK);\n\n\t/* setup mmUVD_MPC_CNTL */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_MPC_CNTL);\n\ttmp &= ~UVD_MPC_CNTL__REPLACEMENT_MODE_MASK;\n\ttmp |= 0x2 << UVD_MPC_CNTL__REPLACEMENT_MODE__SHIFT;\n\tWREG32_SOC15(VCN, 0, mmUVD_MPC_CNTL, tmp);\n\n\t/* setup UVD_MPC_SET_MUXA0 */\n\tWREG32_SOC15(UVD, 0, mmUVD_MPC_SET_MUXA0,\n\t\t((0x1 << UVD_MPC_SET_MUXA0__VARA_1__SHIFT) |\n\t\t(0x2 << UVD_MPC_SET_MUXA0__VARA_2__SHIFT) |\n\t\t(0x3 << UVD_MPC_SET_MUXA0__VARA_3__SHIFT) |\n\t\t(0x4 << UVD_MPC_SET_MUXA0__VARA_4__SHIFT)));\n\n\t/* setup UVD_MPC_SET_MUXB0 */\n\tWREG32_SOC15(UVD, 0, mmUVD_MPC_SET_MUXB0,\n\t\t((0x1 << UVD_MPC_SET_MUXB0__VARB_1__SHIFT) |\n\t\t(0x2 << UVD_MPC_SET_MUXB0__VARB_2__SHIFT) |\n\t\t(0x3 << UVD_MPC_SET_MUXB0__VARB_3__SHIFT) |\n\t\t(0x4 << UVD_MPC_SET_MUXB0__VARB_4__SHIFT)));\n\n\t/* setup mmUVD_MPC_SET_MUX */\n\tWREG32_SOC15(UVD, 0, mmUVD_MPC_SET_MUX,\n\t\t((0x0 << UVD_MPC_SET_MUX__SET_0__SHIFT) |\n\t\t(0x1 << UVD_MPC_SET_MUX__SET_1__SHIFT) |\n\t\t(0x2 << UVD_MPC_SET_MUX__SET_2__SHIFT)));\n\n\tvcn_v2_0_mc_resume(adev);\n\n\t/* release VCPU reset to boot */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET), 0,\n\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\n\t/* enable LMI MC and UMC channels */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_LMI_CTRL2), 0,\n\t\t~UVD_LMI_CTRL2__STALL_ARB_UMC_MASK);\n\n\ttmp = RREG32_SOC15(VCN, 0, mmUVD_SOFT_RESET);\n\ttmp &= ~UVD_SOFT_RESET__LMI_SOFT_RESET_MASK;\n\ttmp &= ~UVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK;\n\tWREG32_SOC15(VCN, 0, mmUVD_SOFT_RESET, tmp);\n\n\t/* disable byte swapping */\n\tlmi_swap_cntl = 0;\n#ifdef __BIG_ENDIAN\n\t/* swap (8 in 32) RB and IB */\n\tlmi_swap_cntl = 0xa;\n#endif\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_SWAP_CNTL, lmi_swap_cntl);\n\n\tfor (i = 0; i < 10; ++i) {\n\t\tuint32_t status;\n\n\t\tfor (j = 0; j < 100; ++j) {\n\t\t\tstatus = RREG32_SOC15(UVD, 0, mmUVD_STATUS);\n\t\t\tif (status & 2)\n\t\t\t\tbreak;\n\t\t\tmdelay(10);\n\t\t}\n\t\tr = 0;\n\t\tif (status & 2)\n\t\t\tbreak;\n\n\t\tDRM_ERROR(\"VCN decode not responding, trying to reset the VCPU!!!\\n\");\n\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\t\tUVD_SOFT_RESET__VCPU_SOFT_RESET_MASK,\n\t\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET), 0,\n\t\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t\tr = -1;\n\t}\n\n\tif (r) {\n\t\tDRM_ERROR(\"VCN decode not responding, giving up!!!\\n\");\n\t\treturn r;\n\t}\n\n\t/* enable master interrupt */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_MASTINT_EN),\n\t\tUVD_MASTINT_EN__VCPU_EN_MASK,\n\t\t~UVD_MASTINT_EN__VCPU_EN_MASK);\n\n\t/* clear the busy bit of VCN_STATUS */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_STATUS), 0,\n\t\t~(2 << UVD_STATUS__VCPU_REPORT__SHIFT));\n\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_VMID, 0);\n\n\t/* force RBC into idle state */\n\trb_bufsz = order_base_2(ring->ring_size);\n\ttmp = REG_SET_FIELD(0, UVD_RBC_RB_CNTL, RB_BUFSZ, rb_bufsz);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_BLKSZ, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_FETCH, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_UPDATE, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_RPTR_WR_EN, 1);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_CNTL, tmp);\n\n\tfw_shared->multi_queue.decode_queue_mode |= FW_QUEUE_RING_RESET;\n\t/* program the RB_BASE for ring buffer */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_LOW,\n\t\tlower_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_HIGH,\n\t\tupper_32_bits(ring->gpu_addr));\n\n\t/* Initialize the ring buffer's read and write pointers */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR, 0);\n\n\tring->wptr = RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR,\n\t\t\tlower_32_bits(ring->wptr));\n\tfw_shared->multi_queue.decode_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\tfw_shared->multi_queue.encode_generalpurpose_queue_mode |= FW_QUEUE_RING_RESET;\n\tring = &adev->vcn.inst->ring_enc[0];\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO, ring->gpu_addr);\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI, upper_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE, ring->ring_size / 4);\n\tfw_shared->multi_queue.encode_generalpurpose_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\tfw_shared->multi_queue.encode_lowlatency_queue_mode |= FW_QUEUE_RING_RESET;\n\tring = &adev->vcn.inst->ring_enc[1];\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR2, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO2, ring->gpu_addr);\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI2, upper_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE2, ring->ring_size / 4);\n\tfw_shared->multi_queue.encode_lowlatency_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_stop_dpg_mode(struct amdgpu_device *adev)\n{\n\tstruct dpg_pause_state state = {.fw_based = VCN_DPG_STATE__UNPAUSE};\n\tuint32_t tmp;\n\n\tvcn_v2_0_pause_dpg_mode(adev, 0, &state);\n\t/* Wait for power status to be 1 */\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS, 1,\n\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\n\t/* wait for read ptr to be equal to write ptr */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR);\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_RB_RPTR, tmp, 0xFFFFFFFF);\n\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2);\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_RB_RPTR2, tmp, 0xFFFFFFFF);\n\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR) & 0x7FFFFFFF;\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_RBC_RB_RPTR, tmp, 0xFFFFFFFF);\n\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS, 1,\n\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\n\t/* disable dynamic power gating mode */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS), 0,\n\t\t\t~UVD_POWER_STATUS__UVD_PG_MODE_MASK);\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_stop(struct amdgpu_device *adev)\n{\n\tuint32_t tmp;\n\tint r;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG) {\n\t\tr = vcn_v2_0_stop_dpg_mode(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto power_off;\n\t}\n\n\t/* wait for uvd idle */\n\tr = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_STATUS, UVD_STATUS__IDLE, 0x7);\n\tif (r)\n\t\treturn r;\n\n\ttmp = UVD_LMI_STATUS__VCPU_LMI_WRITE_CLEAN_MASK |\n\t\tUVD_LMI_STATUS__READ_CLEAN_MASK |\n\t\tUVD_LMI_STATUS__WRITE_CLEAN_MASK |\n\t\tUVD_LMI_STATUS__WRITE_CLEAN_RAW_MASK;\n\tr = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_LMI_STATUS, tmp, tmp);\n\tif (r)\n\t\treturn r;\n\n\t/* stall UMC channel */\n\ttmp = RREG32_SOC15(VCN, 0, mmUVD_LMI_CTRL2);\n\ttmp |= UVD_LMI_CTRL2__STALL_ARB_UMC_MASK;\n\tWREG32_SOC15(VCN, 0, mmUVD_LMI_CTRL2, tmp);\n\n\ttmp = UVD_LMI_STATUS__UMC_READ_CLEAN_RAW_MASK|\n\t\tUVD_LMI_STATUS__UMC_WRITE_CLEAN_RAW_MASK;\n\tr = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_LMI_STATUS, tmp, tmp);\n\tif (r)\n\t\treturn r;\n\n\t/* disable VCPU clock */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_VCPU_CNTL), 0,\n\t\t~(UVD_VCPU_CNTL__CLK_EN_MASK));\n\n\t/* reset LMI UMC */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\tUVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK,\n\t\t~UVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK);\n\n\t/* reset LMI */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\tUVD_SOFT_RESET__LMI_SOFT_RESET_MASK,\n\t\t~UVD_SOFT_RESET__LMI_SOFT_RESET_MASK);\n\n\t/* reset VCPU */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\tUVD_SOFT_RESET__VCPU_SOFT_RESET_MASK,\n\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\n\t/* clear status */\n\tWREG32_SOC15(VCN, 0, mmUVD_STATUS, 0);\n\n\tvcn_v2_0_enable_clock_gating(adev);\n\tvcn_v2_0_enable_static_power_gating(adev);\n\npower_off:\n\tif (adev->pm.dpm_enabled)\n\t\tamdgpu_dpm_enable_uvd(adev, false);\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_pause_dpg_mode(struct amdgpu_device *adev,\n\t\t\t\tint inst_idx, struct dpg_pause_state *new_state)\n{\n\tstruct amdgpu_ring *ring;\n\tuint32_t reg_data = 0;\n\tint ret_code;\n\n\t/* pause/unpause if state is changed */\n\tif (adev->vcn.inst[inst_idx].pause_state.fw_based != new_state->fw_based) {\n\t\tDRM_DEBUG(\"dpg pause state changed %d -> %d\",\n\t\t\tadev->vcn.inst[inst_idx].pause_state.fw_based,\tnew_state->fw_based);\n\t\treg_data = RREG32_SOC15(UVD, 0, mmUVD_DPG_PAUSE) &\n\t\t\t(~UVD_DPG_PAUSE__NJ_PAUSE_DPG_ACK_MASK);\n\n\t\tif (new_state->fw_based == VCN_DPG_STATE__PAUSE) {\n\t\t\tret_code = SOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS, 0x1,\n\t\t\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\n\t\t\tif (!ret_code) {\n\t\t\t\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\t\t\t\t/* pause DPG */\n\t\t\t\treg_data |= UVD_DPG_PAUSE__NJ_PAUSE_DPG_REQ_MASK;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_DPG_PAUSE, reg_data);\n\n\t\t\t\t/* wait for ACK */\n\t\t\t\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_DPG_PAUSE,\n\t\t\t\t\t   UVD_DPG_PAUSE__NJ_PAUSE_DPG_ACK_MASK,\n\t\t\t\t\t   UVD_DPG_PAUSE__NJ_PAUSE_DPG_ACK_MASK);\n\n\t\t\t\t/* Stall DPG before WPTR/RPTR reset */\n\t\t\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\t\t\t\t   UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK,\n\t\t\t\t\t   ~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\t\t\t\t/* Restore */\n\t\t\t\tfw_shared->multi_queue.encode_generalpurpose_queue_mode |= FW_QUEUE_RING_RESET;\n\t\t\t\tring = &adev->vcn.inst->ring_enc[0];\n\t\t\t\tring->wptr = 0;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO, ring->gpu_addr);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI, upper_32_bits(ring->gpu_addr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE, ring->ring_size / 4);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR, lower_32_bits(ring->wptr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR, lower_32_bits(ring->wptr));\n\t\t\t\tfw_shared->multi_queue.encode_generalpurpose_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\t\t\t\tfw_shared->multi_queue.encode_lowlatency_queue_mode |= FW_QUEUE_RING_RESET;\n\t\t\t\tring = &adev->vcn.inst->ring_enc[1];\n\t\t\t\tring->wptr = 0;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO2, ring->gpu_addr);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI2, upper_32_bits(ring->gpu_addr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE2, ring->ring_size / 4);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR2, lower_32_bits(ring->wptr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2, lower_32_bits(ring->wptr));\n\t\t\t\tfw_shared->multi_queue.encode_lowlatency_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\t\t\t\tfw_shared->multi_queue.decode_queue_mode |= FW_QUEUE_RING_RESET;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR,\n\t\t\t\t\t   RREG32_SOC15(UVD, 0, mmUVD_SCRATCH2) & 0x7FFFFFFF);\n\t\t\t\tfw_shared->multi_queue.decode_queue_mode &= ~FW_QUEUE_RING_RESET;\n\t\t\t\t/* Unstall DPG */\n\t\t\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\t\t\t\t   0, ~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\n\t\t\t\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS,\n\t\t\t\t\t   UVD_PGFSM_CONFIG__UVDM_UVDU_PWR_ON,\n\t\t\t\t\t   UVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\t\t\t}\n\t\t} else {\n\t\t\t/* unpause dpg, no need to wait */\n\t\t\treg_data &= ~UVD_DPG_PAUSE__NJ_PAUSE_DPG_REQ_MASK;\n\t\t\tWREG32_SOC15(UVD, 0, mmUVD_DPG_PAUSE, reg_data);\n\t\t}\n\t\tadev->vcn.inst[inst_idx].pause_state.fw_based = new_state->fw_based;\n\t}\n\n\treturn 0;\n}\n\nstatic bool vcn_v2_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn (RREG32_SOC15(VCN, 0, mmUVD_STATUS) == UVD_STATUS__IDLE);\n}\n\nstatic int vcn_v2_0_wait_for_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret;\n\n\tret = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_STATUS, UVD_STATUS__IDLE,\n\t\tUVD_STATUS__IDLE);\n\n\treturn ret;\n}\n\nstatic int vcn_v2_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool enable = (state == AMD_CG_STATE_GATE);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tif (enable) {\n\t\t/* wait for STATUS to clear */\n\t\tif (!vcn_v2_0_is_idle(handle))\n\t\t\treturn -EBUSY;\n\t\tvcn_v2_0_enable_clock_gating(adev);\n\t} else {\n\t\t/* disable HW gating and enable Sw gating */\n\t\tvcn_v2_0_disable_clock_gating(adev);\n\t}\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_dec_ring_get_rptr - get read pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware read pointer\n */\nstatic uint64_t vcn_v2_0_dec_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\treturn RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR);\n}\n\n/**\n * vcn_v2_0_dec_ring_get_wptr - get write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware write pointer\n */\nstatic uint64_t vcn_v2_0_dec_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell)\n\t\treturn *ring->wptr_cpu_addr;\n\telse\n\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR);\n}\n\n/**\n * vcn_v2_0_dec_ring_set_wptr - set write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Commits the write pointer to the hardware\n */\nstatic void vcn_v2_0_dec_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG)\n\t\tWREG32_SOC15(UVD, 0, mmUVD_SCRATCH2,\n\t\t\tlower_32_bits(ring->wptr) | 0x80000000);\n\n\tif (ring->use_doorbell) {\n\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t} else {\n\t\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR, lower_32_bits(ring->wptr));\n\t}\n}\n\n/**\n * vcn_v2_0_dec_ring_insert_start - insert a start command\n *\n * @ring: amdgpu_ring pointer\n *\n * Write a start command to the ring.\n */\nvoid vcn_v2_0_dec_ring_insert_start(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_PACKET_START << 1));\n}\n\n/**\n * vcn_v2_0_dec_ring_insert_end - insert a end command\n *\n * @ring: amdgpu_ring pointer\n *\n * Write a end command to the ring.\n */\nvoid vcn_v2_0_dec_ring_insert_end(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_PACKET_END << 1));\n}\n\n/**\n * vcn_v2_0_dec_ring_insert_nop - insert a nop command\n *\n * @ring: amdgpu_ring pointer\n * @count: the number of NOP packets to insert\n *\n * Write a nop command to the ring.\n */\nvoid vcn_v2_0_dec_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tint i;\n\n\tWARN_ON(ring->wptr % 2 || count % 2);\n\n\tfor (i = 0; i < count / 2; i++) {\n\t\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.nop, 0));\n\t\tamdgpu_ring_write(ring, 0);\n\t}\n}\n\n/**\n * vcn_v2_0_dec_ring_emit_fence - emit an fence & trap command\n *\n * @ring: amdgpu_ring pointer\n * @addr: address\n * @seq: sequence number\n * @flags: fence related flags\n *\n * Write a fence and a trap command to the ring.\n */\nvoid vcn_v2_0_dec_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\tunsigned flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.context_id, 0));\n\tamdgpu_ring_write(ring, seq);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, addr & 0xffffffff);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, upper_32_bits(addr) & 0xff);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_FENCE << 1));\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, 0);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, 0);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_TRAP << 1));\n}\n\n/**\n * vcn_v2_0_dec_ring_emit_ib - execute indirect buffer\n *\n * @ring: amdgpu_ring pointer\n * @job: job to retrieve vmid from\n * @ib: indirect buffer to execute\n * @flags: unused\n *\n * Write ring commands to execute the indirect buffer\n */\nvoid vcn_v2_0_dec_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t       struct amdgpu_job *job,\n\t\t\t       struct amdgpu_ib *ib,\n\t\t\t       uint32_t flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.ib_vmid, 0));\n\tamdgpu_ring_write(ring, vmid);\n\n\tamdgpu_ring_write(ring,\tPACKET0(adev->vcn.internal.ib_bar_low, 0));\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring,\tPACKET0(adev->vcn.internal.ib_bar_high, 0));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring,\tPACKET0(adev->vcn.internal.ib_size, 0));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\nvoid vcn_v2_0_dec_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\tuint32_t val, uint32_t mask)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, reg << 2);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, val);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.gp_scratch8, 0));\n\tamdgpu_ring_write(ring, mask);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_REG_READ_COND_WAIT << 1));\n}\n\nvoid vcn_v2_0_dec_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\tunsigned vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\tuint32_t data0, data1, mask;\n\n\tpd_addr = amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t/* wait for register write */\n\tdata0 = hub->ctx0_ptb_addr_lo32 + vmid * hub->ctx_addr_distance;\n\tdata1 = lower_32_bits(pd_addr);\n\tmask = 0xffffffff;\n\tvcn_v2_0_dec_ring_emit_reg_wait(ring, data0, data1, mask);\n}\n\nvoid vcn_v2_0_dec_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\tuint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, reg << 2);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, val);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_WRITE_REG << 1));\n}\n\n/**\n * vcn_v2_0_enc_ring_get_rptr - get enc read pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware enc read pointer\n */\nstatic uint64_t vcn_v2_0_enc_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring == &adev->vcn.inst->ring_enc[0])\n\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_RPTR);\n\telse\n\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_RPTR2);\n}\n\n /**\n * vcn_v2_0_enc_ring_get_wptr - get enc write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware enc write pointer\n */\nstatic uint64_t vcn_v2_0_enc_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring == &adev->vcn.inst->ring_enc[0]) {\n\t\tif (ring->use_doorbell)\n\t\t\treturn *ring->wptr_cpu_addr;\n\t\telse\n\t\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR);\n\t} else {\n\t\tif (ring->use_doorbell)\n\t\t\treturn *ring->wptr_cpu_addr;\n\t\telse\n\t\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2);\n\t}\n}\n\n /**\n * vcn_v2_0_enc_ring_set_wptr - set enc write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Commits the enc write pointer to the hardware\n */\nstatic void vcn_v2_0_enc_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring == &adev->vcn.inst->ring_enc[0]) {\n\t\tif (ring->use_doorbell) {\n\t\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t\t} else {\n\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR, lower_32_bits(ring->wptr));\n\t\t}\n\t} else {\n\t\tif (ring->use_doorbell) {\n\t\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t\t} else {\n\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2, lower_32_bits(ring->wptr));\n\t\t}\n\t}\n}\n\n/**\n * vcn_v2_0_enc_ring_emit_fence - emit an enc fence & trap command\n *\n * @ring: amdgpu_ring pointer\n * @addr: address\n * @seq: sequence number\n * @flags: fence related flags\n *\n * Write enc a fence and a trap command to the ring.\n */\nvoid vcn_v2_0_enc_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,\n\t\t\t\tu64 seq, unsigned flags)\n{\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_FENCE);\n\tamdgpu_ring_write(ring, addr);\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, seq);\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_TRAP);\n}\n\nvoid vcn_v2_0_enc_ring_insert_end(struct amdgpu_ring *ring)\n{\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_END);\n}\n\n/**\n * vcn_v2_0_enc_ring_emit_ib - enc execute indirect buffer\n *\n * @ring: amdgpu_ring pointer\n * @job: job to retrive vmid from\n * @ib: indirect buffer to execute\n * @flags: unused\n *\n * Write enc ring commands to execute the indirect buffer\n */\nvoid vcn_v2_0_enc_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t       struct amdgpu_job *job,\n\t\t\t       struct amdgpu_ib *ib,\n\t\t\t       uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_IB);\n\tamdgpu_ring_write(ring, vmid);\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\nvoid vcn_v2_0_enc_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\tuint32_t val, uint32_t mask)\n{\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_REG_WAIT);\n\tamdgpu_ring_write(ring, reg << 2);\n\tamdgpu_ring_write(ring, mask);\n\tamdgpu_ring_write(ring, val);\n}\n\nvoid vcn_v2_0_enc_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\tunsigned int vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\n\tpd_addr = amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t/* wait for reg writes */\n\tvcn_v2_0_enc_ring_emit_reg_wait(ring, hub->ctx0_ptb_addr_lo32 +\n\t\t\t\t\tvmid * hub->ctx_addr_distance,\n\t\t\t\t\tlower_32_bits(pd_addr), 0xffffffff);\n}\n\nvoid vcn_v2_0_enc_ring_emit_wreg(struct amdgpu_ring *ring, uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_REG_WRITE);\n\tamdgpu_ring_write(ring,\treg << 2);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic int vcn_v2_0_set_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\treturn 0;\n}\n\nstatic int vcn_v2_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"IH: VCN TRAP\\n\");\n\n\tswitch (entry->src_id) {\n\tcase VCN_2_0__SRCID__UVD_SYSTEM_MESSAGE_INTERRUPT:\n\t\tamdgpu_fence_process(&adev->vcn.inst->ring_dec);\n\t\tbreak;\n\tcase VCN_2_0__SRCID__UVD_ENC_GENERAL_PURPOSE:\n\t\tamdgpu_fence_process(&adev->vcn.inst->ring_enc[0]);\n\t\tbreak;\n\tcase VCN_2_0__SRCID__UVD_ENC_LOW_LATENCY:\n\t\tamdgpu_fence_process(&adev->vcn.inst->ring_enc[1]);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unhandled interrupt: %d %d\\n\",\n\t\t\t  entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint vcn_v2_0_dec_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t tmp = 0;\n\tunsigned i;\n\tint r;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tWREG32(adev->vcn.inst[ring->me].external.scratch9, 0xCAFEDEAD);\n\tr = amdgpu_ring_alloc(ring, 4);\n\tif (r)\n\t\treturn r;\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_PACKET_START << 1));\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.scratch9, 0));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32(adev->vcn.inst[ring->me].external.scratch9);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\n\treturn r;\n}\n\n\nstatic int vcn_v2_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\t/* This doesn't actually powergate the VCN block.\n\t * That's done in the dpm code via the SMC.  This\n\t * just re-inits the block as necessary.  The actual\n\t * gating still happens in the dpm code.  We should\n\t * revisit this when there is a cleaner line between\n\t * the smc and the hw blocks\n\t */\n\tint ret;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tadev->vcn.cur_state = AMD_PG_STATE_UNGATE;\n\t\treturn 0;\n\t}\n\n\tif (state == adev->vcn.cur_state)\n\t\treturn 0;\n\n\tif (state == AMD_PG_STATE_GATE)\n\t\tret = vcn_v2_0_stop(adev);\n\telse\n\t\tret = vcn_v2_0_start(adev);\n\n\tif (!ret)\n\t\tadev->vcn.cur_state = state;\n\treturn ret;\n}\n\nstatic int vcn_v2_0_start_mmsch(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_mm_table *table)\n{\n\tuint32_t data = 0, loop;\n\tuint64_t addr = table->gpu_addr;\n\tstruct mmsch_v2_0_init_header *header;\n\tuint32_t size;\n\tint i;\n\n\theader = (struct mmsch_v2_0_init_header *)table->cpu_addr;\n\tsize = header->header_size + header->vcn_table_size;\n\n\t/* 1, write to vce_mmsch_vf_ctx_addr_lo/hi register with GPU mc addr\n\t * of memory descriptor location\n\t */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_CTX_ADDR_LO, lower_32_bits(addr));\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_CTX_ADDR_HI, upper_32_bits(addr));\n\n\t/* 2, update vmid of descriptor */\n\tdata = RREG32_SOC15(UVD, 0, mmMMSCH_VF_VMID);\n\tdata &= ~MMSCH_VF_VMID__VF_CTX_VMID_MASK;\n\t/* use domain0 for MM scheduler */\n\tdata |= (0 << MMSCH_VF_VMID__VF_CTX_VMID__SHIFT);\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_VMID, data);\n\n\t/* 3, notify mmsch about the size of this descriptor */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_CTX_SIZE, size);\n\n\t/* 4, set resp to zero */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_RESP, 0);\n\n\tadev->vcn.inst->ring_dec.wptr = 0;\n\tadev->vcn.inst->ring_dec.wptr_old = 0;\n\tvcn_v2_0_dec_ring_set_wptr(&adev->vcn.inst->ring_dec);\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tadev->vcn.inst->ring_enc[i].wptr = 0;\n\t\tadev->vcn.inst->ring_enc[i].wptr_old = 0;\n\t\tvcn_v2_0_enc_ring_set_wptr(&adev->vcn.inst->ring_enc[i]);\n\t}\n\n\t/* 5, kick off the initialization and wait until\n\t * VCE_MMSCH_VF_MAILBOX_RESP becomes non-zero\n\t */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_HOST, 0x10000001);\n\n\tdata = RREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_RESP);\n\tloop = 1000;\n\twhile ((data & 0x10000002) != 0x10000002) {\n\t\tudelay(10);\n\t\tdata = RREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_RESP);\n\t\tloop--;\n\t\tif (!loop)\n\t\t\tbreak;\n\t}\n\n\tif (!loop) {\n\t\tDRM_ERROR(\"failed to init MMSCH, \" \\\n\t\t\t\"mmMMSCH_VF_MAILBOX_RESP = 0x%08x\\n\", data);\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_start_sriov(struct amdgpu_device *adev)\n{\n\tint r;\n\tuint32_t tmp;\n\tstruct amdgpu_ring *ring;\n\tuint32_t offset, size;\n\tuint32_t table_size = 0;\n\tstruct mmsch_v2_0_cmd_direct_write direct_wt = { {0} };\n\tstruct mmsch_v2_0_cmd_direct_read_modify_write direct_rd_mod_wt = { {0} };\n\tstruct mmsch_v2_0_cmd_end end = { {0} };\n\tstruct mmsch_v2_0_init_header *header;\n\tuint32_t *init_table = adev->virt.mm_table.cpu_addr;\n\tuint8_t i = 0;\n\n\theader = (struct mmsch_v2_0_init_header *)init_table;\n\tdirect_wt.cmd_header.command_type = MMSCH_COMMAND__DIRECT_REG_WRITE;\n\tdirect_rd_mod_wt.cmd_header.command_type =\n\t\tMMSCH_COMMAND__DIRECT_REG_READ_MODIFY_WRITE;\n\tend.cmd_header.command_type = MMSCH_COMMAND__END;\n\n\tif (header->vcn_table_offset == 0 && header->vcn_table_size == 0) {\n\t\theader->version = MMSCH_VERSION;\n\t\theader->header_size = sizeof(struct mmsch_v2_0_init_header) >> 2;\n\n\t\theader->vcn_table_offset = header->header_size;\n\n\t\tinit_table += header->vcn_table_offset;\n\n\t\tsize = AMDGPU_GPU_PAGE_ALIGN(adev->vcn.fw[0]->size + 4);\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_RD_MOD_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_STATUS),\n\t\t\t0xFFFFFFFF, 0x00000004);\n\n\t\t/* mc resume*/\n\t\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_lo);\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_hi);\n\t\t\toffset = 0;\n\t\t} else {\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr));\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr));\n\t\t\toffset = size;\n\t\t}\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_OFFSET0),\n\t\t\t0);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_SIZE0),\n\t\t\tsize);\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_OFFSET1),\n\t\t\t0);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_SIZE1),\n\t\t\tAMDGPU_VCN_STACK_SIZE);\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE2_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset +\n\t\t\t\tAMDGPU_VCN_STACK_SIZE));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE2_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset +\n\t\t\t\tAMDGPU_VCN_STACK_SIZE));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_OFFSET2),\n\t\t\t0);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_SIZE2),\n\t\t\tAMDGPU_VCN_CONTEXT_SIZE);\n\n\t\tfor (r = 0; r < adev->vcn.num_enc_rings; ++r) {\n\t\t\tring = &adev->vcn.inst->ring_enc[r];\n\t\t\tring->wptr = 0;\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RB_BASE_LO),\n\t\t\t\tlower_32_bits(ring->gpu_addr));\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RB_BASE_HI),\n\t\t\t\tupper_32_bits(ring->gpu_addr));\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RB_SIZE),\n\t\t\t\tring->ring_size / 4);\n\t\t}\n\n\t\tring = &adev->vcn.inst->ring_dec;\n\t\tring->wptr = 0;\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_RBC_RB_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(ring->gpu_addr));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_RBC_RB_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(ring->gpu_addr));\n\t\t/* force RBC into idle state */\n\t\ttmp = order_base_2(ring->ring_size);\n\t\ttmp = REG_SET_FIELD(0, UVD_RBC_RB_CNTL, RB_BUFSZ, tmp);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_BLKSZ, 1);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_FETCH, 1);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_UPDATE, 1);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_RPTR_WR_EN, 1);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RBC_RB_CNTL), tmp);\n\n\t\t/* add end packet */\n\t\ttmp = sizeof(struct mmsch_v2_0_cmd_end);\n\t\tmemcpy((void *)init_table, &end, tmp);\n\t\ttable_size += (tmp / 4);\n\t\theader->vcn_table_size = table_size;\n\n\t}\n\treturn vcn_v2_0_start_mmsch(adev, &adev->virt.mm_table);\n}\n\nstatic void vcn_v2_0_print_ip_state(void *handle, struct drm_printer *p)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, j;\n\tuint32_t reg_count = ARRAY_SIZE(vcn_reg_list_2_0);\n\tuint32_t inst_off, is_powered;\n\n\tif (!adev->vcn.ip_dump)\n\t\treturn;\n\n\tdrm_printf(p, \"num_instances:%d\\n\", adev->vcn.num_vcn_inst);\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\tif (adev->vcn.harvest_config & (1 << i)) {\n\t\t\tdrm_printf(p, \"\\nHarvested Instance:VCN%d Skipping dump\\n\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinst_off = i * reg_count;\n\t\tis_powered = (adev->vcn.ip_dump[inst_off] &\n\t\t\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK) != 1;\n\n\t\tif (is_powered) {\n\t\t\tdrm_printf(p, \"\\nActive Instance:VCN%d\\n\", i);\n\t\t\tfor (j = 0; j < reg_count; j++)\n\t\t\t\tdrm_printf(p, \"%-50s \\t 0x%08x\\n\", vcn_reg_list_2_0[j].reg_name,\n\t\t\t\t\t   adev->vcn.ip_dump[inst_off + j]);\n\t\t} else {\n\t\t\tdrm_printf(p, \"\\nInactive Instance:VCN%d\\n\", i);\n\t\t}\n\t}\n}\n\nstatic void vcn_v2_0_dump_ip_state(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, j;\n\tbool is_powered;\n\tuint32_t inst_off;\n\tuint32_t reg_count = ARRAY_SIZE(vcn_reg_list_2_0);\n\n\tif (!adev->vcn.ip_dump)\n\t\treturn;\n\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\tif (adev->vcn.harvest_config & (1 << i))\n\t\t\tcontinue;\n\n\t\tinst_off = i * reg_count;\n\t\t/* mmUVD_POWER_STATUS is always readable and is first element of the array */\n\t\tadev->vcn.ip_dump[inst_off] = RREG32_SOC15(VCN, i, mmUVD_POWER_STATUS);\n\t\tis_powered = (adev->vcn.ip_dump[inst_off] &\n\t\t\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK) != 1;\n\n\t\tif (is_powered)\n\t\t\tfor (j = 1; j < reg_count; j++)\n\t\t\t\tadev->vcn.ip_dump[inst_off + j] =\n\t\t\t\t\tRREG32(SOC15_REG_ENTRY_OFFSET_INST(vcn_reg_list_2_0[j], i));\n\t}\n}\n\nstatic const struct amd_ip_funcs vcn_v2_0_ip_funcs = {\n\t.name = \"vcn_v2_0\",\n\t.early_init = vcn_v2_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = vcn_v2_0_sw_init,\n\t.sw_fini = vcn_v2_0_sw_fini,\n\t.hw_init = vcn_v2_0_hw_init,\n\t.hw_fini = vcn_v2_0_hw_fini,\n\t.suspend = vcn_v2_0_suspend,\n\t.resume = vcn_v2_0_resume,\n\t.is_idle = vcn_v2_0_is_idle,\n\t.wait_for_idle = vcn_v2_0_wait_for_idle,\n\t.check_soft_reset = NULL,\n\t.pre_soft_reset = NULL,\n\t.soft_reset = NULL,\n\t.post_soft_reset = NULL,\n\t.set_clockgating_state = vcn_v2_0_set_clockgating_state,\n\t.set_powergating_state = vcn_v2_0_set_powergating_state,\n\t.dump_ip_state = vcn_v2_0_dump_ip_state,\n\t.print_ip_state = vcn_v2_0_print_ip_state,\n};\n\nstatic const struct amdgpu_ring_funcs vcn_v2_0_dec_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCN_DEC,\n\t.align_mask = 0xf,\n\t.secure_submission_supported = true,\n\t.get_rptr = vcn_v2_0_dec_ring_get_rptr,\n\t.get_wptr = vcn_v2_0_dec_ring_get_wptr,\n\t.set_wptr = vcn_v2_0_dec_ring_set_wptr,\n\t.emit_frame_size =\n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 6 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 8 +\n\t\t8 + /* vcn_v2_0_dec_ring_emit_vm_flush */\n\t\t14 + 14 + /* vcn_v2_0_dec_ring_emit_fence x2 vm fence */\n\t\t6,\n\t.emit_ib_size = 8, /* vcn_v2_0_dec_ring_emit_ib */\n\t.emit_ib = vcn_v2_0_dec_ring_emit_ib,\n\t.emit_fence = vcn_v2_0_dec_ring_emit_fence,\n\t.emit_vm_flush = vcn_v2_0_dec_ring_emit_vm_flush,\n\t.test_ring = vcn_v2_0_dec_ring_test_ring,\n\t.test_ib = amdgpu_vcn_dec_ring_test_ib,\n\t.insert_nop = vcn_v2_0_dec_ring_insert_nop,\n\t.insert_start = vcn_v2_0_dec_ring_insert_start,\n\t.insert_end = vcn_v2_0_dec_ring_insert_end,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vcn_ring_begin_use,\n\t.end_use = amdgpu_vcn_ring_end_use,\n\t.emit_wreg = vcn_v2_0_dec_ring_emit_wreg,\n\t.emit_reg_wait = vcn_v2_0_dec_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic const struct amdgpu_ring_funcs vcn_v2_0_enc_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCN_ENC,\n\t.align_mask = 0x3f,\n\t.nop = VCN_ENC_CMD_NO_OP,\n\t.get_rptr = vcn_v2_0_enc_ring_get_rptr,\n\t.get_wptr = vcn_v2_0_enc_ring_get_wptr,\n\t.set_wptr = vcn_v2_0_enc_ring_set_wptr,\n\t.emit_frame_size =\n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 4 +\n\t\t4 + /* vcn_v2_0_enc_ring_emit_vm_flush */\n\t\t5 + 5 + /* vcn_v2_0_enc_ring_emit_fence x2 vm fence */\n\t\t1, /* vcn_v2_0_enc_ring_insert_end */\n\t.emit_ib_size = 5, /* vcn_v2_0_enc_ring_emit_ib */\n\t.emit_ib = vcn_v2_0_enc_ring_emit_ib,\n\t.emit_fence = vcn_v2_0_enc_ring_emit_fence,\n\t.emit_vm_flush = vcn_v2_0_enc_ring_emit_vm_flush,\n\t.test_ring = amdgpu_vcn_enc_ring_test_ring,\n\t.test_ib = amdgpu_vcn_enc_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.insert_end = vcn_v2_0_enc_ring_insert_end,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vcn_ring_begin_use,\n\t.end_use = amdgpu_vcn_ring_end_use,\n\t.emit_wreg = vcn_v2_0_enc_ring_emit_wreg,\n\t.emit_reg_wait = vcn_v2_0_enc_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic void vcn_v2_0_set_dec_ring_funcs(struct amdgpu_device *adev)\n{\n\tadev->vcn.inst->ring_dec.funcs = &vcn_v2_0_dec_ring_vm_funcs;\n}\n\nstatic void vcn_v2_0_set_enc_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i)\n\t\tadev->vcn.inst->ring_enc[i].funcs = &vcn_v2_0_enc_ring_vm_funcs;\n}\n\nstatic const struct amdgpu_irq_src_funcs vcn_v2_0_irq_funcs = {\n\t.set = vcn_v2_0_set_interrupt_state,\n\t.process = vcn_v2_0_process_interrupt,\n};\n\nstatic void vcn_v2_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->vcn.inst->irq.num_types = adev->vcn.num_enc_rings + 1;\n\tadev->vcn.inst->irq.funcs = &vcn_v2_0_irq_funcs;\n}\n\nconst struct amdgpu_ip_block_version vcn_v2_0_ip_block =\n{\n\t\t.type = AMD_IP_BLOCK_TYPE_VCN,\n\t\t.major = 2,\n\t\t.minor = 0,\n\t\t.rev = 0,\n\t\t.funcs = &vcn_v2_0_ip_funcs,\n};\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf4params-bug.txt", "bug_report_text": "CID: 487621\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: RISKY_PROGRAMMING_PRACTICE\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_acpi_enumerate_xcc\nLine: 1092\n\nProblem:\nUnbounded string operation using sprintf can cause buffer overflow\n\nAbstract:\nThe code uses sprintf to write to buffer 'hid' without size checking. The sprintf \noperation could write beyond the buffer boundaries if the formatted output string \nexceeds the destination buffer size. This could corrupt adjacent memory and lead \nto system instability or security vulnerabilities.\n\nDetails:\nThe sprintf function is used to format a string containing \"AMD\" concatenated with \na number. Since sprintf performs no bounds checking, if AMD_XCC_HID_START + id \nproduces a large number, the resulting string could overflow the fixed-size 'hid' \nbuffer. This occurs in the GPU driver's ACPI enumeration code which runs with \nkernel privileges.\n\nFix:\nReplace sprintf with snprintf and specify the buffer size:\n  snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id)\n", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf4params-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..10dfa3a37333 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -1089,7 +1089,7 @@ static int amdgpu_acpi_enumerate_xcc(void)\n        xa_init(&numa_info_xa);\n\n        for (id = 0; id < AMD_XCC_MAX_HID; id++) {\n-               sprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n+               snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n                acpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n                /* These ACPI objects are expected to be in sequential order. If\n                 * one is not found, no need to check the rest.", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c", "line_number": 1092, "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger âstopâ, âenableâ and âstartâ at D0, Driver Operational.\n\t * SBIOS trigger âstopâ at D3, Driver Not Operational.\n\t * SBIOS trigger âstopâ and âdisableâ at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/resource-leak0-bug.txt", "bug_report_text": "Issue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 236\n\nDescription:\nA memory resource allocated using kvmalloc_array() may not be freed if an error occurs during execution.\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(). \nHowever, if an error occurs after this allocation but before the function successfully completes, the allocated memory may not be freed, resulting in a resource leak.\n\n", "diff_path": "dataset/raw_data/bugs/dev-set/resource-leak0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -233,6 +233,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                size *= sizeof(uint32_t);\n                if (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c", "line_number": 236, "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\t/* Only a single BO list is allowed to simplify handling. */\n\t\t\tif (p->bo_list)\n\t\t\t\tret = -EINVAL;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t\tp->jobs[i]->enforce_isolation = p->adev->enforce_isolation[fpriv->xcp_id];\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(adev, vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf0-bug.txt", "bug_report_text": "CID: 1234567\nType: Buffer overflow\nCategory: BUFFER_OVERFLOW\nClassification: Bad use of string function\nSeverity: High\nCertainty: Absolute\nStatus: New\nFunction: amdgpu_atombios_i2c_init\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nIssue:\nUnbounded sprintf can cause buffer overflow. The code uses sprintf() without size limits to write into buffer 'stmp', which could lead to buffer overflow if the formatted string exceeds the buffer size.\n\nDescription:\nThe function amdgpu_atombios_i2c_init() uses sprintf() to format a hexadecimal value into a string buffer 'stmp' without checking if the resulting string will fit in the destination buffer. This could lead to a buffer overflow if i2c.i2c_id generates a string longer than the size of stmp.\n\nUse snprintf instead", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf0-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..a6245ec89453 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp) \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c", "line_number": 150, "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/null-ptr-deref1-bug.txt", "bug_report_text": "File:       drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction:   amdgpu_gem_object_create\nLine: 116\nType:       Null Pointer Dereference\nCategory:   Error Handling Issues\nCID:        12000\nImpact:     High\n\nDescription:\n------------\nThe function dereferences the pointer adev and resv without first checking if they are NULL. \nIf either pointer is NULL, dereferencing them (e.g. adev->dev or resv->lock) will cause a crash.\n\n", "diff_path": "dataset/raw_data/bugs/dev-set/null-ptr-deref1-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -101,6 +101,8 @@ int amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n                             struct dma_resv *resv,\n                             struct drm_gem_object **obj, int8_t xcp_id_plus1)\n {\n+       if (!adev || !resv)\n+           return VM_FAULT_SIGSEGV;\n        struct amdgpu_bo *bo;\n        struct amdgpu_bo_user *ubo;\n        struct amdgpu_bo_param bp;", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c", "line_number": 116, "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\t/* Only a single BO list is allowed to simplify handling. */\n\t\t\tif (p->bo_list)\n\t\t\t\tret = -EINVAL;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t\tp->jobs[i]->enforce_isolation = p->adev->enforce_isolation[fpriv->xcp_id];\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(adev, vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/buffer-overflow2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow\n\nCID: 1024\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/atombios_dp.c\nFunction: amdgpu_atombios_dp_get_dpcd\nLine: 359\n\nDescription:\nA potential buffer overflow vulnerability was present in the function amdgpu_atombios_dp_get_dpcd.\nThe issue arises from the call to memcpy(dig_connector->dpcd, msg, DP_DPCD_SIZE);, \nwhere the size of DP_DPCD_SIZE is not validated against the size of the destination buffer dig_connector->dpcd. \nIf DP_DPCD_SIZE exceeds the size of dig_connector->dpcd, it could result in memory corruption.", "diff_path": "dataset/raw_data/bugs/dev-set/buffer-overflow2-diff.txt", "diff_text": "diff --git a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/atombios_dp.c b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/atombios_dp.c\nindex 622634c..3a2a16f 100644\n--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/atombios_dp.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/atombios_dp.c\n@@ -356,6 +356,9 @@ int amdgpu_atombios_dp_get_dpcd(struct amdgpu_connector *amdgpu_connector)\n        ret = drm_dp_dpcd_read(&amdgpu_connector->ddc_bus->aux, DP_DPCD_REV,\n                               msg, DP_DPCD_SIZE);\n        if (ret == DP_DPCD_SIZE) {\n+               if (DP_DPCD_SIZE > sizeof(dig_connector->dpcd)) {\n+                       return;\n+               }\n                memcpy(dig_connector->dpcd, msg, DP_DPCD_SIZE);\n\n                DRM_DEBUG_KMS(\"DPCD: %*ph\\n\", (int)sizeof(dig_connector->dpcd),", "source_code_path": "drivers/gpu/drm/amd/amdgpu/atombios_dp.c", "line_number": 359, "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n\n#include <drm/amdgpu_drm.h>\n#include <drm/display/drm_dp_helper.h>\n\n#include \"amdgpu.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"atombios_dp.h\"\n#include \"amdgpu_connectors.h\"\n#include \"amdgpu_atombios.h\"\n\n/* move these to drm_dp_helper.c/h */\n#define DP_LINK_CONFIGURATION_SIZE 9\n#define DP_DPCD_SIZE DP_RECEIVER_CAP_SIZE\n\nstatic char *voltage_names[] = {\n\t\"0.4V\", \"0.6V\", \"0.8V\", \"1.2V\"\n};\nstatic char *pre_emph_names[] = {\n\t\"0dB\", \"3.5dB\", \"6dB\", \"9.5dB\"\n};\n\n/***** amdgpu AUX functions *****/\n\nunion aux_channel_transaction {\n\tPROCESS_AUX_CHANNEL_TRANSACTION_PS_ALLOCATION v1;\n\tPROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS_V2 v2;\n};\n\nstatic int amdgpu_atombios_dp_process_aux_ch(struct amdgpu_i2c_chan *chan,\n\t\t\t\t      u8 *send, int send_bytes,\n\t\t\t\t      u8 *recv, int recv_size,\n\t\t\t\t      u8 delay, u8 *ack)\n{\n\tstruct drm_device *dev = chan->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion aux_channel_transaction args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ProcessAuxChannelTransaction);\n\tunsigned char *base;\n\tint recv_bytes;\n\tint r = 0;\n\n\tmemset(&args, 0, sizeof(args));\n\n\tmutex_lock(&chan->mutex);\n\n\tbase = (unsigned char *)(adev->mode_info.atom_context->scratch + 1);\n\n\tamdgpu_atombios_copy_swap(base, send, send_bytes, true);\n\n\targs.v2.lpAuxRequest = cpu_to_le16((u16)(0 + 4));\n\targs.v2.lpDataOut = cpu_to_le16((u16)(16 + 4));\n\targs.v2.ucDataOutLen = 0;\n\targs.v2.ucChannelID = chan->rec.i2c_id;\n\targs.v2.ucDelay = delay / 10;\n\targs.v2.ucHPD_ID = chan->rec.hpd;\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args, sizeof(args));\n\n\t*ack = args.v2.ucReplyStatus;\n\n\t/* timeout */\n\tif (args.v2.ucReplyStatus == 1) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto done;\n\t}\n\n\t/* flags not zero */\n\tif (args.v2.ucReplyStatus == 2) {\n\t\tDRM_DEBUG_KMS(\"dp_aux_ch flags not zero\\n\");\n\t\tr = -EIO;\n\t\tgoto done;\n\t}\n\n\t/* error */\n\tif (args.v2.ucReplyStatus == 3) {\n\t\tDRM_DEBUG_KMS(\"dp_aux_ch error\\n\");\n\t\tr = -EIO;\n\t\tgoto done;\n\t}\n\n\trecv_bytes = args.v1.ucDataOutLen;\n\tif (recv_bytes > recv_size)\n\t\trecv_bytes = recv_size;\n\n\tif (recv && recv_size)\n\t\tamdgpu_atombios_copy_swap(recv, base + 16, recv_bytes, false);\n\n\tr = recv_bytes;\ndone:\n\tmutex_unlock(&chan->mutex);\n\n\treturn r;\n}\n\n#define BARE_ADDRESS_SIZE 3\n#define HEADER_SIZE (BARE_ADDRESS_SIZE + 1)\n\nstatic ssize_t\namdgpu_atombios_dp_aux_transfer(struct drm_dp_aux *aux, struct drm_dp_aux_msg *msg)\n{\n\tstruct amdgpu_i2c_chan *chan =\n\t\tcontainer_of(aux, struct amdgpu_i2c_chan, aux);\n\tint ret;\n\tu8 tx_buf[20];\n\tsize_t tx_size;\n\tu8 ack, delay = 0;\n\n\tif (WARN_ON(msg->size > 16))\n\t\treturn -E2BIG;\n\n\ttx_buf[0] = msg->address & 0xff;\n\ttx_buf[1] = msg->address >> 8;\n\ttx_buf[2] = (msg->request << 4) |\n\t\t((msg->address >> 16) & 0xf);\n\ttx_buf[3] = msg->size ? (msg->size - 1) : 0;\n\n\tswitch (msg->request & ~DP_AUX_I2C_MOT) {\n\tcase DP_AUX_NATIVE_WRITE:\n\tcase DP_AUX_I2C_WRITE:\n\t\t/* tx_size needs to be 4 even for bare address packets since the atom\n\t\t * table needs the info in tx_buf[3].\n\t\t */\n\t\ttx_size = HEADER_SIZE + msg->size;\n\t\tif (msg->size == 0)\n\t\t\ttx_buf[3] |= BARE_ADDRESS_SIZE << 4;\n\t\telse\n\t\t\ttx_buf[3] |= tx_size << 4;\n\t\tmemcpy(tx_buf + HEADER_SIZE, msg->buffer, msg->size);\n\t\tret = amdgpu_atombios_dp_process_aux_ch(chan,\n\t\t\t\t\t\t tx_buf, tx_size, NULL, 0, delay, &ack);\n\t\tif (ret >= 0)\n\t\t\t/* Return payload size. */\n\t\t\tret = msg->size;\n\t\tbreak;\n\tcase DP_AUX_NATIVE_READ:\n\tcase DP_AUX_I2C_READ:\n\t\t/* tx_size needs to be 4 even for bare address packets since the atom\n\t\t * table needs the info in tx_buf[3].\n\t\t */\n\t\ttx_size = HEADER_SIZE;\n\t\tif (msg->size == 0)\n\t\t\ttx_buf[3] |= BARE_ADDRESS_SIZE << 4;\n\t\telse\n\t\t\ttx_buf[3] |= tx_size << 4;\n\t\tret = amdgpu_atombios_dp_process_aux_ch(chan,\n\t\t\t\t\t\t tx_buf, tx_size, msg->buffer, msg->size, delay, &ack);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret >= 0)\n\t\tmsg->reply = ack >> 4;\n\n\treturn ret;\n}\n\nvoid amdgpu_atombios_dp_aux_init(struct amdgpu_connector *amdgpu_connector)\n{\n\tamdgpu_connector->ddc_bus->rec.hpd = amdgpu_connector->hpd.hpd;\n\tamdgpu_connector->ddc_bus->aux.transfer = amdgpu_atombios_dp_aux_transfer;\n\tamdgpu_connector->ddc_bus->aux.drm_dev = amdgpu_connector->base.dev;\n\n\tdrm_dp_aux_init(&amdgpu_connector->ddc_bus->aux);\n\tamdgpu_connector->ddc_bus->has_aux = true;\n}\n\n/***** general DP utility functions *****/\n\n#define DP_VOLTAGE_MAX         DP_TRAIN_VOLTAGE_SWING_LEVEL_3\n#define DP_PRE_EMPHASIS_MAX    DP_TRAIN_PRE_EMPH_LEVEL_3\n\nstatic void amdgpu_atombios_dp_get_adjust_train(const u8 link_status[DP_LINK_STATUS_SIZE],\n\t\t\t\t\t\tint lane_count,\n\t\t\t\t\t\tu8 train_set[4])\n{\n\tu8 v = 0;\n\tu8 p = 0;\n\tint lane;\n\n\tfor (lane = 0; lane < lane_count; lane++) {\n\t\tu8 this_v = drm_dp_get_adjust_request_voltage(link_status, lane);\n\t\tu8 this_p = drm_dp_get_adjust_request_pre_emphasis(link_status, lane);\n\n\t\tDRM_DEBUG_KMS(\"requested signal parameters: lane %d voltage %s pre_emph %s\\n\",\n\t\t\t  lane,\n\t\t\t  voltage_names[this_v >> DP_TRAIN_VOLTAGE_SWING_SHIFT],\n\t\t\t  pre_emph_names[this_p >> DP_TRAIN_PRE_EMPHASIS_SHIFT]);\n\n\t\tif (this_v > v)\n\t\t\tv = this_v;\n\t\tif (this_p > p)\n\t\t\tp = this_p;\n\t}\n\n\tif (v >= DP_VOLTAGE_MAX)\n\t\tv |= DP_TRAIN_MAX_SWING_REACHED;\n\n\tif (p >= DP_PRE_EMPHASIS_MAX)\n\t\tp |= DP_TRAIN_MAX_PRE_EMPHASIS_REACHED;\n\n\tDRM_DEBUG_KMS(\"using signal parameters: voltage %s pre_emph %s\\n\",\n\t\t  voltage_names[(v & DP_TRAIN_VOLTAGE_SWING_MASK) >> DP_TRAIN_VOLTAGE_SWING_SHIFT],\n\t\t  pre_emph_names[(p & DP_TRAIN_PRE_EMPHASIS_MASK) >> DP_TRAIN_PRE_EMPHASIS_SHIFT]);\n\n\tfor (lane = 0; lane < 4; lane++)\n\t\ttrain_set[lane] = v | p;\n}\n\n/* convert bits per color to bits per pixel */\n/* get bpc from the EDID */\nstatic unsigned amdgpu_atombios_dp_convert_bpc_to_bpp(int bpc)\n{\n\tif (bpc == 0)\n\t\treturn 24;\n\telse\n\t\treturn bpc * 3;\n}\n\n/***** amdgpu specific DP functions *****/\n\nstatic int amdgpu_atombios_dp_get_dp_link_config(struct drm_connector *connector,\n\t\t\t\t\t\t const u8 dpcd[DP_DPCD_SIZE],\n\t\t\t\t\t\t unsigned pix_clock,\n\t\t\t\t\t\t unsigned *dp_lanes, unsigned *dp_rate)\n{\n\tunsigned bpp =\n\t\tamdgpu_atombios_dp_convert_bpc_to_bpp(amdgpu_connector_get_monitor_bpc(connector));\n\tstatic const unsigned link_rates[3] = { 162000, 270000, 540000 };\n\tunsigned max_link_rate = drm_dp_max_link_rate(dpcd);\n\tunsigned max_lane_num = drm_dp_max_lane_count(dpcd);\n\tunsigned lane_num, i, max_pix_clock;\n\n\tif (amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector) ==\n\t    ENCODER_OBJECT_ID_NUTMEG) {\n\t\tfor (lane_num = 1; lane_num <= max_lane_num; lane_num <<= 1) {\n\t\t\tmax_pix_clock = (lane_num * 270000 * 8) / bpp;\n\t\t\tif (max_pix_clock >= pix_clock) {\n\t\t\t\t*dp_lanes = lane_num;\n\t\t\t\t*dp_rate = 270000;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < ARRAY_SIZE(link_rates) && link_rates[i] <= max_link_rate; i++) {\n\t\t\tfor (lane_num = 1; lane_num <= max_lane_num; lane_num <<= 1) {\n\t\t\t\tmax_pix_clock = (lane_num * link_rates[i] * 8) / bpp;\n\t\t\t\tif (max_pix_clock >= pix_clock) {\n\t\t\t\t\t*dp_lanes = lane_num;\n\t\t\t\t\t*dp_rate = link_rates[i];\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic u8 amdgpu_atombios_dp_encoder_service(struct amdgpu_device *adev,\n\t\t\t\t      int action, int dp_clock,\n\t\t\t\t      u8 ucconfig, u8 lane_num)\n{\n\tDP_ENCODER_SERVICE_PARAMETERS args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DPEncoderService);\n\n\tmemset(&args, 0, sizeof(args));\n\targs.ucLinkClock = dp_clock / 10;\n\targs.ucConfig = ucconfig;\n\targs.ucAction = action;\n\targs.ucLaneNum = lane_num;\n\targs.ucStatus = 0;\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args, sizeof(args));\n\treturn args.ucStatus;\n}\n\nu8 amdgpu_atombios_dp_get_sinktype(struct amdgpu_connector *amdgpu_connector)\n{\n\tstruct drm_device *dev = amdgpu_connector->base.dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\n\treturn amdgpu_atombios_dp_encoder_service(adev, ATOM_DP_ACTION_GET_SINK_TYPE, 0,\n\t\t\t\t\t   amdgpu_connector->ddc_bus->rec.i2c_id, 0);\n}\n\nstatic void amdgpu_atombios_dp_probe_oui(struct amdgpu_connector *amdgpu_connector)\n{\n\tstruct amdgpu_connector_atom_dig *dig_connector = amdgpu_connector->con_priv;\n\tu8 buf[3];\n\n\tif (!(dig_connector->dpcd[DP_DOWN_STREAM_PORT_COUNT] & DP_OUI_SUPPORT))\n\t\treturn;\n\n\tif (drm_dp_dpcd_read(&amdgpu_connector->ddc_bus->aux, DP_SINK_OUI, buf, 3) == 3)\n\t\tDRM_DEBUG_KMS(\"Sink OUI: %02hx%02hx%02hx\\n\",\n\t\t\t      buf[0], buf[1], buf[2]);\n\n\tif (drm_dp_dpcd_read(&amdgpu_connector->ddc_bus->aux, DP_BRANCH_OUI, buf, 3) == 3)\n\t\tDRM_DEBUG_KMS(\"Branch OUI: %02hx%02hx%02hx\\n\",\n\t\t\t      buf[0], buf[1], buf[2]);\n}\n\nstatic void amdgpu_atombios_dp_ds_ports(struct amdgpu_connector *amdgpu_connector)\n{\n\tstruct amdgpu_connector_atom_dig *dig_connector = amdgpu_connector->con_priv;\n\tint ret;\n\n\tif (dig_connector->dpcd[DP_DPCD_REV] > 0x10) {\n\t\tret = drm_dp_dpcd_read(&amdgpu_connector->ddc_bus->aux,\n\t\t\t\t       DP_DOWNSTREAM_PORT_0,\n\t\t\t\t       dig_connector->downstream_ports,\n\t\t\t\t       DP_MAX_DOWNSTREAM_PORTS);\n\t\tif (ret)\n\t\t\tmemset(dig_connector->downstream_ports, 0,\n\t\t\t       DP_MAX_DOWNSTREAM_PORTS);\n\t}\n}\n\nint amdgpu_atombios_dp_get_dpcd(struct amdgpu_connector *amdgpu_connector)\n{\n\tstruct amdgpu_connector_atom_dig *dig_connector = amdgpu_connector->con_priv;\n\tu8 msg[DP_DPCD_SIZE];\n\tint ret;\n\n\tret = drm_dp_dpcd_read(&amdgpu_connector->ddc_bus->aux, DP_DPCD_REV,\n\t\t\t       msg, DP_DPCD_SIZE);\n\tif (ret == DP_DPCD_SIZE) {\n\t\tmemcpy(dig_connector->dpcd, msg, DP_DPCD_SIZE);\n\n\t\tDRM_DEBUG_KMS(\"DPCD: %*ph\\n\", (int)sizeof(dig_connector->dpcd),\n\t\t\t      dig_connector->dpcd);\n\n\t\tamdgpu_atombios_dp_probe_oui(amdgpu_connector);\n\t\tamdgpu_atombios_dp_ds_ports(amdgpu_connector);\n\t\treturn 0;\n\t}\n\n\tdig_connector->dpcd[0] = 0;\n\treturn -EINVAL;\n}\n\nint amdgpu_atombios_dp_get_panel_mode(struct drm_encoder *encoder,\n\t\t\t       struct drm_connector *connector)\n{\n\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\tint panel_mode = DP_PANEL_MODE_EXTERNAL_DP_MODE;\n\tu16 dp_bridge = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);\n\tu8 tmp;\n\n\tif (!amdgpu_connector->con_priv)\n\t\treturn panel_mode;\n\n\tif (dp_bridge != ENCODER_OBJECT_ID_NONE) {\n\t\t/* DP bridge chips */\n\t\tif (drm_dp_dpcd_readb(&amdgpu_connector->ddc_bus->aux,\n\t\t\t\t      DP_EDP_CONFIGURATION_CAP, &tmp) == 1) {\n\t\t\tif (tmp & 1)\n\t\t\t\tpanel_mode = DP_PANEL_MODE_INTERNAL_DP2_MODE;\n\t\t\telse if ((dp_bridge == ENCODER_OBJECT_ID_NUTMEG) ||\n\t\t\t\t (dp_bridge == ENCODER_OBJECT_ID_TRAVIS))\n\t\t\t\tpanel_mode = DP_PANEL_MODE_INTERNAL_DP1_MODE;\n\t\t\telse\n\t\t\t\tpanel_mode = DP_PANEL_MODE_EXTERNAL_DP_MODE;\n\t\t}\n\t} else if (connector->connector_type == DRM_MODE_CONNECTOR_eDP) {\n\t\t/* eDP */\n\t\tif (drm_dp_dpcd_readb(&amdgpu_connector->ddc_bus->aux,\n\t\t\t\t      DP_EDP_CONFIGURATION_CAP, &tmp) == 1) {\n\t\t\tif (tmp & 1)\n\t\t\t\tpanel_mode = DP_PANEL_MODE_INTERNAL_DP2_MODE;\n\t\t}\n\t}\n\n\treturn panel_mode;\n}\n\nvoid amdgpu_atombios_dp_set_link_config(struct drm_connector *connector,\n\t\t\t\t const struct drm_display_mode *mode)\n{\n\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\tstruct amdgpu_connector_atom_dig *dig_connector;\n\tint ret;\n\n\tif (!amdgpu_connector->con_priv)\n\t\treturn;\n\tdig_connector = amdgpu_connector->con_priv;\n\n\tif ((dig_connector->dp_sink_type == CONNECTOR_OBJECT_ID_DISPLAYPORT) ||\n\t    (dig_connector->dp_sink_type == CONNECTOR_OBJECT_ID_eDP)) {\n\t\tret = amdgpu_atombios_dp_get_dp_link_config(connector, dig_connector->dpcd,\n\t\t\t\t\t\t\t    mode->clock,\n\t\t\t\t\t\t\t    &dig_connector->dp_lane_count,\n\t\t\t\t\t\t\t    &dig_connector->dp_clock);\n\t\tif (ret) {\n\t\t\tdig_connector->dp_clock = 0;\n\t\t\tdig_connector->dp_lane_count = 0;\n\t\t}\n\t}\n}\n\nint amdgpu_atombios_dp_mode_valid_helper(struct drm_connector *connector,\n\t\t\t\t  struct drm_display_mode *mode)\n{\n\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\tstruct amdgpu_connector_atom_dig *dig_connector;\n\tunsigned dp_lanes, dp_clock;\n\tint ret;\n\n\tif (!amdgpu_connector->con_priv)\n\t\treturn MODE_CLOCK_HIGH;\n\tdig_connector = amdgpu_connector->con_priv;\n\n\tret = amdgpu_atombios_dp_get_dp_link_config(connector, dig_connector->dpcd,\n\t\t\t\t\t\t    mode->clock, &dp_lanes, &dp_clock);\n\tif (ret)\n\t\treturn MODE_CLOCK_HIGH;\n\n\tif ((dp_clock == 540000) &&\n\t    (!amdgpu_connector_is_dp12_capable(connector)))\n\t\treturn MODE_CLOCK_HIGH;\n\n\treturn MODE_OK;\n}\n\nbool amdgpu_atombios_dp_needs_link_train(struct amdgpu_connector *amdgpu_connector)\n{\n\tu8 link_status[DP_LINK_STATUS_SIZE];\n\tstruct amdgpu_connector_atom_dig *dig = amdgpu_connector->con_priv;\n\n\tif (drm_dp_dpcd_read_link_status(&amdgpu_connector->ddc_bus->aux, link_status)\n\t    <= 0)\n\t\treturn false;\n\tif (drm_dp_channel_eq_ok(link_status, dig->dp_lane_count))\n\t\treturn false;\n\treturn true;\n}\n\nvoid amdgpu_atombios_dp_set_rx_power_state(struct drm_connector *connector,\n\t\t\t\t    u8 power_state)\n{\n\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\tstruct amdgpu_connector_atom_dig *dig_connector;\n\n\tif (!amdgpu_connector->con_priv)\n\t\treturn;\n\n\tdig_connector = amdgpu_connector->con_priv;\n\n\t/* power up/down the sink */\n\tif (dig_connector->dpcd[0] >= 0x11) {\n\t\tdrm_dp_dpcd_writeb(&amdgpu_connector->ddc_bus->aux,\n\t\t\t\t   DP_SET_POWER, power_state);\n\t\tusleep_range(1000, 2000);\n\t}\n}\n\nstruct amdgpu_atombios_dp_link_train_info {\n\tstruct amdgpu_device *adev;\n\tstruct drm_encoder *encoder;\n\tstruct drm_connector *connector;\n\tint dp_clock;\n\tint dp_lane_count;\n\tbool tp3_supported;\n\tu8 dpcd[DP_RECEIVER_CAP_SIZE];\n\tu8 train_set[4];\n\tu8 link_status[DP_LINK_STATUS_SIZE];\n\tu8 tries;\n\tstruct drm_dp_aux *aux;\n};\n\nstatic void\namdgpu_atombios_dp_update_vs_emph(struct amdgpu_atombios_dp_link_train_info *dp_info)\n{\n\t/* set the initial vs/emph on the source */\n\tamdgpu_atombios_encoder_setup_dig_transmitter(dp_info->encoder,\n\t\t\t\t\t       ATOM_TRANSMITTER_ACTION_SETUP_VSEMPH,\n\t\t\t\t\t       0, dp_info->train_set[0]); /* sets all lanes at once */\n\n\t/* set the vs/emph on the sink */\n\tdrm_dp_dpcd_write(dp_info->aux, DP_TRAINING_LANE0_SET,\n\t\t\t  dp_info->train_set, dp_info->dp_lane_count);\n}\n\nstatic void\namdgpu_atombios_dp_set_tp(struct amdgpu_atombios_dp_link_train_info *dp_info, int tp)\n{\n\tint rtp = 0;\n\n\t/* set training pattern on the source */\n\tswitch (tp) {\n\tcase DP_TRAINING_PATTERN_1:\n\t\trtp = ATOM_ENCODER_CMD_DP_LINK_TRAINING_PATTERN1;\n\t\tbreak;\n\tcase DP_TRAINING_PATTERN_2:\n\t\trtp = ATOM_ENCODER_CMD_DP_LINK_TRAINING_PATTERN2;\n\t\tbreak;\n\tcase DP_TRAINING_PATTERN_3:\n\t\trtp = ATOM_ENCODER_CMD_DP_LINK_TRAINING_PATTERN3;\n\t\t\tbreak;\n\t}\n\tamdgpu_atombios_encoder_setup_dig_encoder(dp_info->encoder, rtp, 0);\n\n\t/* enable training pattern on the sink */\n\tdrm_dp_dpcd_writeb(dp_info->aux, DP_TRAINING_PATTERN_SET, tp);\n}\n\nstatic int\namdgpu_atombios_dp_link_train_init(struct amdgpu_atombios_dp_link_train_info *dp_info)\n{\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(dp_info->encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tu8 tmp;\n\n\t/* power up the sink */\n\tamdgpu_atombios_dp_set_rx_power_state(dp_info->connector, DP_SET_POWER_D0);\n\n\t/* possibly enable downspread on the sink */\n\tif (dp_info->dpcd[3] & 0x1)\n\t\tdrm_dp_dpcd_writeb(dp_info->aux,\n\t\t\t\t   DP_DOWNSPREAD_CTRL, DP_SPREAD_AMP_0_5);\n\telse\n\t\tdrm_dp_dpcd_writeb(dp_info->aux,\n\t\t\t\t   DP_DOWNSPREAD_CTRL, 0);\n\n\tif (dig->panel_mode == DP_PANEL_MODE_INTERNAL_DP2_MODE)\n\t\tdrm_dp_dpcd_writeb(dp_info->aux, DP_EDP_CONFIGURATION_SET, 1);\n\n\t/* set the lane count on the sink */\n\ttmp = dp_info->dp_lane_count;\n\tif (drm_dp_enhanced_frame_cap(dp_info->dpcd))\n\t\ttmp |= DP_LANE_COUNT_ENHANCED_FRAME_EN;\n\tdrm_dp_dpcd_writeb(dp_info->aux, DP_LANE_COUNT_SET, tmp);\n\n\t/* set the link rate on the sink */\n\ttmp = drm_dp_link_rate_to_bw_code(dp_info->dp_clock);\n\tdrm_dp_dpcd_writeb(dp_info->aux, DP_LINK_BW_SET, tmp);\n\n\t/* start training on the source */\n\tamdgpu_atombios_encoder_setup_dig_encoder(dp_info->encoder,\n\t\t\t\t\t   ATOM_ENCODER_CMD_DP_LINK_TRAINING_START, 0);\n\n\t/* disable the training pattern on the sink */\n\tdrm_dp_dpcd_writeb(dp_info->aux,\n\t\t\t   DP_TRAINING_PATTERN_SET,\n\t\t\t   DP_TRAINING_PATTERN_DISABLE);\n\n\treturn 0;\n}\n\nstatic int\namdgpu_atombios_dp_link_train_finish(struct amdgpu_atombios_dp_link_train_info *dp_info)\n{\n\tudelay(400);\n\n\t/* disable the training pattern on the sink */\n\tdrm_dp_dpcd_writeb(dp_info->aux,\n\t\t\t   DP_TRAINING_PATTERN_SET,\n\t\t\t   DP_TRAINING_PATTERN_DISABLE);\n\n\t/* disable the training pattern on the source */\n\tamdgpu_atombios_encoder_setup_dig_encoder(dp_info->encoder,\n\t\t\t\t\t   ATOM_ENCODER_CMD_DP_LINK_TRAINING_COMPLETE, 0);\n\n\treturn 0;\n}\n\nstatic int\namdgpu_atombios_dp_link_train_cr(struct amdgpu_atombios_dp_link_train_info *dp_info)\n{\n\tbool clock_recovery;\n\tu8 voltage;\n\tint i;\n\n\tamdgpu_atombios_dp_set_tp(dp_info, DP_TRAINING_PATTERN_1);\n\tmemset(dp_info->train_set, 0, 4);\n\tamdgpu_atombios_dp_update_vs_emph(dp_info);\n\n\tudelay(400);\n\n\t/* clock recovery loop */\n\tclock_recovery = false;\n\tdp_info->tries = 0;\n\tvoltage = 0xff;\n\twhile (1) {\n\t\tdrm_dp_link_train_clock_recovery_delay(dp_info->aux, dp_info->dpcd);\n\n\t\tif (drm_dp_dpcd_read_link_status(dp_info->aux,\n\t\t\t\t\t\t dp_info->link_status) <= 0) {\n\t\t\tDRM_ERROR(\"displayport link status failed\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (drm_dp_clock_recovery_ok(dp_info->link_status, dp_info->dp_lane_count)) {\n\t\t\tclock_recovery = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < dp_info->dp_lane_count; i++) {\n\t\t\tif ((dp_info->train_set[i] & DP_TRAIN_MAX_SWING_REACHED) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == dp_info->dp_lane_count) {\n\t\t\tDRM_ERROR(\"clock recovery reached max voltage\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((dp_info->train_set[0] & DP_TRAIN_VOLTAGE_SWING_MASK) == voltage) {\n\t\t\t++dp_info->tries;\n\t\t\tif (dp_info->tries == 5) {\n\t\t\t\tDRM_ERROR(\"clock recovery tried 5 times\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else\n\t\t\tdp_info->tries = 0;\n\n\t\tvoltage = dp_info->train_set[0] & DP_TRAIN_VOLTAGE_SWING_MASK;\n\n\t\t/* Compute new train_set as requested by sink */\n\t\tamdgpu_atombios_dp_get_adjust_train(dp_info->link_status, dp_info->dp_lane_count,\n\t\t\t\t\t     dp_info->train_set);\n\n\t\tamdgpu_atombios_dp_update_vs_emph(dp_info);\n\t}\n\tif (!clock_recovery) {\n\t\tDRM_ERROR(\"clock recovery failed\\n\");\n\t\treturn -1;\n\t} else {\n\t\tDRM_DEBUG_KMS(\"clock recovery at voltage %d pre-emphasis %d\\n\",\n\t\t\t  dp_info->train_set[0] & DP_TRAIN_VOLTAGE_SWING_MASK,\n\t\t\t  (dp_info->train_set[0] & DP_TRAIN_PRE_EMPHASIS_MASK) >>\n\t\t\t  DP_TRAIN_PRE_EMPHASIS_SHIFT);\n\t\treturn 0;\n\t}\n}\n\nstatic int\namdgpu_atombios_dp_link_train_ce(struct amdgpu_atombios_dp_link_train_info *dp_info)\n{\n\tbool channel_eq;\n\n\tif (dp_info->tp3_supported)\n\t\tamdgpu_atombios_dp_set_tp(dp_info, DP_TRAINING_PATTERN_3);\n\telse\n\t\tamdgpu_atombios_dp_set_tp(dp_info, DP_TRAINING_PATTERN_2);\n\n\t/* channel equalization loop */\n\tdp_info->tries = 0;\n\tchannel_eq = false;\n\twhile (1) {\n\t\tdrm_dp_link_train_channel_eq_delay(dp_info->aux, dp_info->dpcd);\n\n\t\tif (drm_dp_dpcd_read_link_status(dp_info->aux,\n\t\t\t\t\t\t dp_info->link_status) <= 0) {\n\t\t\tDRM_ERROR(\"displayport link status failed\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (drm_dp_channel_eq_ok(dp_info->link_status, dp_info->dp_lane_count)) {\n\t\t\tchannel_eq = true;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Try 5 times */\n\t\tif (dp_info->tries > 5) {\n\t\t\tDRM_ERROR(\"channel eq failed: 5 tries\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Compute new train_set as requested by sink */\n\t\tamdgpu_atombios_dp_get_adjust_train(dp_info->link_status, dp_info->dp_lane_count,\n\t\t\t\t\t     dp_info->train_set);\n\n\t\tamdgpu_atombios_dp_update_vs_emph(dp_info);\n\t\tdp_info->tries++;\n\t}\n\n\tif (!channel_eq) {\n\t\tDRM_ERROR(\"channel eq failed\\n\");\n\t\treturn -1;\n\t} else {\n\t\tDRM_DEBUG_KMS(\"channel eq at voltage %d pre-emphasis %d\\n\",\n\t\t\t  dp_info->train_set[0] & DP_TRAIN_VOLTAGE_SWING_MASK,\n\t\t\t  (dp_info->train_set[0] & DP_TRAIN_PRE_EMPHASIS_MASK)\n\t\t\t  >> DP_TRAIN_PRE_EMPHASIS_SHIFT);\n\t\treturn 0;\n\t}\n}\n\nvoid amdgpu_atombios_dp_link_train(struct drm_encoder *encoder,\n\t\t\t    struct drm_connector *connector)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_connector *amdgpu_connector;\n\tstruct amdgpu_connector_atom_dig *dig_connector;\n\tstruct amdgpu_atombios_dp_link_train_info dp_info;\n\tu8 tmp;\n\n\tif (!amdgpu_encoder->enc_priv)\n\t\treturn;\n\n\tamdgpu_connector = to_amdgpu_connector(connector);\n\tif (!amdgpu_connector->con_priv)\n\t\treturn;\n\tdig_connector = amdgpu_connector->con_priv;\n\n\tif ((dig_connector->dp_sink_type != CONNECTOR_OBJECT_ID_DISPLAYPORT) &&\n\t    (dig_connector->dp_sink_type != CONNECTOR_OBJECT_ID_eDP))\n\t\treturn;\n\n\tif (drm_dp_dpcd_readb(&amdgpu_connector->ddc_bus->aux, DP_MAX_LANE_COUNT, &tmp)\n\t    == 1) {\n\t\tif (tmp & DP_TPS3_SUPPORTED)\n\t\t\tdp_info.tp3_supported = true;\n\t\telse\n\t\t\tdp_info.tp3_supported = false;\n\t} else {\n\t\tdp_info.tp3_supported = false;\n\t}\n\n\tmemcpy(dp_info.dpcd, dig_connector->dpcd, DP_RECEIVER_CAP_SIZE);\n\tdp_info.adev = adev;\n\tdp_info.encoder = encoder;\n\tdp_info.connector = connector;\n\tdp_info.dp_lane_count = dig_connector->dp_lane_count;\n\tdp_info.dp_clock = dig_connector->dp_clock;\n\tdp_info.aux = &amdgpu_connector->ddc_bus->aux;\n\n\tif (amdgpu_atombios_dp_link_train_init(&dp_info))\n\t\tgoto done;\n\tif (amdgpu_atombios_dp_link_train_cr(&dp_info))\n\t\tgoto done;\n\tif (amdgpu_atombios_dp_link_train_ce(&dp_info))\n\t\tgoto done;\ndone:\n\tif (amdgpu_atombios_dp_link_train_finish(&dp_info))\n\t\treturn;\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/null-returns0-bug.txt", "bug_report_text": "CID: 10000\nClassification: NULL_RETURNS\nFunction: aldebaran_mode2_prepare_hwcontext \nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 108\n\nDescription:\nThe function 'aldebaran_mode2_prepare_hwcontext' dereferences 'reset_ctl->handle' without first \nchecking if 'reset_ctl' or 'reset_ctl->handle' is NULL. This could lead to a null pointer dereference \nif either 'reset_ctl' or 'reset_ctl->handle' is NULL when the function is called.\n\n\n", "diff_path": "dataset/raw_data/bugs/dev-set/null-returns0-diff.txt", "diff_text": "diff --git a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/aldebaran.c b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/aldebaran.c\nindex b0f95a7..35f4479 100644\n--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -105,6 +105,9 @@ aldebaran_mode2_prepare_hwcontext(struct amdgpu_reset_control *reset_ctl,\n                                  struct amdgpu_reset_context *reset_context)\n {\n        int r = 0;\n+       if (!reset->ctl || !reset_ctl->handle) {\n+               return NULL;\n+       }\n        struct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n        dev_dbg(adev->dev, \"Aldebaran prepare hw context\\n\");", "source_code_path": "drivers/gpu/drm/amd/amdgpu/aldebaran.c", "line_number": 108, "code": "/*\n * Copyright 2021 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include \"aldebaran.h\"\n#include \"amdgpu_reset.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dpm.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_psp.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic bool aldebaran_is_mode2_default(struct amdgpu_reset_control *reset_ctl)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tif ((amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t     adev->gmc.xgmi.connected_to_cpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct amdgpu_reset_handler *\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n\t\telse\n\t\t\treset_context->method = amdgpu_asic_reset_method(adev);\n\t}\n\n\tif (reset_context->method != AMD_RESET_METHOD_NONE) {\n\t\tdev_dbg(adev->dev, \"Getting reset handler for method %d\\n\",\n\t\t\treset_context->method);\n\t\tfor_each_handler(i, handler, reset_ctl) {\n\t\t\tif (handler->reset_method == reset_context->method)\n\t\t\t\treturn handler;\n\t\t}\n\t}\n\n\tdev_dbg(adev->dev, \"Reset handler not found!\\n\");\n\n\treturn NULL;\n}\n\nstatic int aldebaran_mode2_suspend_ip(struct amdgpu_device *adev)\n{\n\tint r, i;\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int\naldebaran_mode2_prepare_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tint r = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tdev_dbg(adev->dev, \"Aldebaran prepare hw context\\n\");\n\t/* Don't suspend on bare metal if we are not going to HW reset the ASIC */\n\tif (!amdgpu_sriov_vf(adev))\n\t\tr = aldebaran_mode2_suspend_ip(adev);\n\n\treturn r;\n}\n\nstatic void aldebaran_async_reset(struct work_struct *work)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_reset_control *reset_ctl =\n\t\tcontainer_of(work, struct amdgpu_reset_control, reset_work);\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tfor_each_handler(i, handler, reset_ctl)\t{\n\t\tif (handler->reset_method == reset_ctl->active_reset) {\n\t\t\tdev_dbg(adev->dev, \"Resetting device\\n\");\n\t\t\thandler->do_reset(adev);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int aldebaran_mode2_reset(struct amdgpu_device *adev)\n{\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\tadev->asic_reset_res = amdgpu_dpm_mode2_reset(adev);\n\treturn adev->asic_reset_res;\n}\n\nstatic int\naldebaran_mode2_perform_reset(struct amdgpu_reset_control *reset_ctl,\n\t\t\t      struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tint r = 0;\n\n\tdev_dbg(adev->dev, \"aldebaran perform hw reset\\n\");\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_lock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_MODE2;\n\t}\n\t/*\n\t * Mode2 reset doesn't need any sync between nodes in XGMI hive, instead launch\n\t * them together so that they can be completed asynchronously on multiple nodes\n\t */\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t/* For XGMI run all resets in parallel to speed up the process */\n\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\tif (!queue_work(system_unbound_wq,\n\t\t\t\t\t&tmp_adev->reset_cntl->reset_work))\n\t\t\t\tr = -EALREADY;\n\t\t} else\n\t\t\tr = aldebaran_mode2_reset(tmp_adev);\n\t\tif (r) {\n\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t\tr, adev_to_drm(tmp_adev)->unique);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* For XGMI wait for all resets to complete before proceed */\n\tif (!r) {\n\t\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\tflush_work(&tmp_adev->reset_cntl->reset_work);\n\t\t\t\tr = tmp_adev->asic_reset_res;\n\t\t\t\tif (r)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_unlock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_NONE;\n\t}\n\n\treturn r;\n}\n\nstatic int aldebaran_mode2_restore_ip(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_firmware_info *ucode_list[AMDGPU_UCODE_ID_MAXIMUM];\n\tstruct amdgpu_firmware_info *ucode;\n\tstruct amdgpu_ip_block *cmn_block;\n\tint ucode_count = 0;\n\tint i, r;\n\n\tdev_dbg(adev->dev, \"Reloading ucodes after reset\\n\");\n\tfor (i = 0; i < adev->firmware.max_ucodes; i++) {\n\t\tucode = &adev->firmware.ucode[i];\n\t\tif (!ucode->fw)\n\t\t\tcontinue;\n\t\tswitch (ucode->ucode_id) {\n\t\tcase AMDGPU_UCODE_ID_SDMA0:\n\t\tcase AMDGPU_UCODE_ID_SDMA1:\n\t\tcase AMDGPU_UCODE_ID_SDMA2:\n\t\tcase AMDGPU_UCODE_ID_SDMA3:\n\t\tcase AMDGPU_UCODE_ID_SDMA4:\n\t\tcase AMDGPU_UCODE_ID_SDMA5:\n\t\tcase AMDGPU_UCODE_ID_SDMA6:\n\t\tcase AMDGPU_UCODE_ID_SDMA7:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1_JT:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_G:\n\t\t\tucode_list[ucode_count++] = ucode;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Reinit NBIF block */\n\tcmn_block =\n\t\tamdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_COMMON);\n\tif (unlikely(!cmn_block)) {\n\t\tdev_err(adev->dev, \"Failed to get BIF handle\\n\");\n\t\treturn -EINVAL;\n\t}\n\tr = cmn_block->version->funcs->resume(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Reinit GFXHUB */\n\tadev->gfxhub.funcs->init(adev);\n\tr = adev->gfxhub.funcs->gart_enable(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFXHUB gart reenable failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Reload GFX firmware */\n\tr = psp_load_fw_list(&adev->psp, ucode_list, ucode_count);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFX ucode load failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Resume RLC, FW needs RLC alive to complete reset process */\n\tadev->gfx.rlc.funcs->resume(adev);\n\n\t/* Wait for FW reset event complete */\n\tr = amdgpu_dpm_wait_for_event(adev, SMU_EVENT_RESET_COMPLETE, 0);\n\tif (r) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Failed to get response from firmware after reset\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_COMMON))\n\t\t\tcontinue;\n\n\t\tif (adev->ip_blocks[i].version->funcs->late_init) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->late_init(\n\t\t\t\t(void *)adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\"late_init of IP block <%s> failed %d after reset\\n\",\n\t\t\t\t\tadev->ip_blocks[i].version->funcs->name,\n\t\t\t\t\tr);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tadev->ip_blocks[i].status.late_initialized = true;\n\t}\n\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_GATE);\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_GATE);\n\n\treturn r;\n}\n\nstatic int\naldebaran_mode2_restore_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tstruct amdgpu_ras *con;\n\tint r;\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(reset_context->reset_req_dev, MP1_HWIP, 0) ==\n\t\t    IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tdev_info(tmp_adev->dev,\n\t\t\t \"GPU reset succeeded, trying to resume\\n\");\n\t\tr = aldebaran_mode2_restore_ip(tmp_adev);\n\t\tif (r)\n\t\t\tgoto end;\n\n\t\t/*\n\t\t * Add this ASIC as tracked as reset was already\n\t\t * complete successfully.\n\t\t */\n\t\tamdgpu_register_gpu_instance(tmp_adev);\n\n\t\t/* Resume RAS, ecc_irq */\n\t\tcon = amdgpu_ras_get_context(tmp_adev);\n\t\tif (!amdgpu_sriov_vf(tmp_adev) && con) {\n\t\t\tif (tmp_adev->sdma.ras &&\n\t\t\t\ttmp_adev->sdma.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->sdma.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->sdma.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"SDMA failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (tmp_adev->gfx.ras &&\n\t\t\t\ttmp_adev->gfx.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->gfx.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->gfx.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"GFX failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_ras_resume(tmp_adev);\n\n\t\t/* Update PSP FW topology after reset */\n\t\tif (reset_context->hive &&\n\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\tr = amdgpu_xgmi_update_topology(reset_context->hive,\n\t\t\t\t\t\t\ttmp_adev);\n\n\t\tif (!r) {\n\t\t\tamdgpu_irq_gpu_reset_resume_helper(tmp_adev);\n\n\t\t\tr = amdgpu_ib_ring_tests(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\"ib ring test failed (%d).\\n\", r);\n\t\t\t\tr = -EAGAIN;\n\t\t\t\ttmp_adev->asic_reset_res = r;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\t}\n\nend:\n\treturn r;\n}\n\nstatic struct amdgpu_reset_handler aldebaran_mode2_handler = {\n\t.reset_method\t\t= AMD_RESET_METHOD_MODE2,\n\t.prepare_env\t\t= NULL,\n\t.prepare_hwcontext\t= aldebaran_mode2_prepare_hwcontext,\n\t.perform_reset\t\t= aldebaran_mode2_perform_reset,\n\t.restore_hwcontext\t= aldebaran_mode2_restore_hwcontext,\n\t.restore_env\t\t= NULL,\n\t.do_reset\t\t= aldebaran_mode2_reset,\n};\n\nstatic struct amdgpu_reset_handler\n\t*aldebaran_rst_handlers[AMDGPU_RESET_MAX_HANDLERS] = {\n\t\t&aldebaran_mode2_handler,\n\t};\n\nint aldebaran_reset_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_reset_control *reset_ctl;\n\n\treset_ctl = kzalloc(sizeof(*reset_ctl), GFP_KERNEL);\n\tif (!reset_ctl)\n\t\treturn -ENOMEM;\n\n\treset_ctl->handle = adev;\n\treset_ctl->async_reset = aldebaran_async_reset;\n\treset_ctl->active_reset = AMD_RESET_METHOD_NONE;\n\treset_ctl->get_reset_handler = aldebaran_get_reset_handler;\n\n\tINIT_WORK(&reset_ctl->reset_work, reset_ctl->async_reset);\n\t/* Only mode2 is handled through reset control now */\n\treset_ctl->reset_handlers = &aldebaran_rst_handlers;\n\n\tadev->reset_cntl = reset_ctl;\n\n\treturn 0;\n}\n\nint aldebaran_reset_fini(struct amdgpu_device *adev)\n{\n\tkfree(adev->reset_cntl);\n\tadev->reset_cntl = NULL;\n\treturn 0;\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/null-ptr-deref0-bug.txt", "bug_report_text": "File:       drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction:   amdgpu_cs_job_idx\nLine: 80\nType:       Null Pointer Dereference\nCategory:   Error Handling Issues\nCID:        12000\nImpact:     High\n\nDescription:\n------------\nA potential null pointer dereference issue has been identified in the `amdgpu_cs_job_idx` function. \nIf the `amdgpu_cs_parser *p` or `drm_amdgpu_cs_chunk_ib *chunk_ib` is NULL, accessing members of these \nstructures without validation may lead to undefined behavior and potential system instability.\n", "diff_path": "dataset/raw_data/bugs/dev-set/null-ptr-deref0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -73,6 +73,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n static int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n                             struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n {\n+       if (p == NULL || chunk_ib == NULL)\n+           return -EINVAL;\n        struct drm_sched_entity *entity;\n        unsigned int i;\n        int r;", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c", "line_number": 80, "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\t/* Only a single BO list is allowed to simplify handling. */\n\t\t\tif (p->bo_list)\n\t\t\t\tret = -EINVAL;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t\tp->jobs[i]->enforce_isolation = p->adev->enforce_isolation[fpriv->xcp_id];\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(adev, vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/buffer-overflow-memcpy0-bug.txt", "bug_report_text": "Type: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_get_notification_params\nLine: 303\n\nDescription: Potential buffer overflow in memcpy() operation\n\nThere is a potential buffer overflow in the call to memcpy(). The size argument\nto memcpy() is derived from untrusted input (info->buffer.pointer) and may exceed\nthe size of the destination buffer.\n", "diff_path": "dataset/raw_data/bugs/dev-set/buffer-overflow-memcpy0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -300,6 +300,10 @@ static int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n\n        memset(&params, 0, sizeof(params));\n        size = min(sizeof(params), size);\n+       size_t safe_size = min(sizeof(params), size);\n+       if (safe_size > info->buffer.length) {\n+               return -EINVAL;\n+       }\n        memcpy(&params, info->buffer.pointer, size);\n\n        DRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c", "line_number": 303, "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger âstopâ, âenableâ and âstartâ at D0, Driver Operational.\n\t * SBIOS trigger âstopâ at D3, Driver Not Operational.\n\t * SBIOS trigger âstopâ and âdisableâ at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/memory-leak-free0-bug.txt", "bug_report_text": "Issue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c\nLine: 547\n\nDescription:\nThe function create_sg_table allocates memory for buffer sg using but does not free this memory in all code paths. The allocated memory is returned to the caller without being freed.\n\n", "diff_path": "dataset/raw_data/bugs/dev-set/memory-leak-free0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c\n@@ -544,6 +544,10 @@ static struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n #ifdef CONFIG_NEED_SG_DMA_LENGTH\n        sg->sgl->dma_length = size;\n #endif\n+    if (!sg) {\n+        pr_warn(\"sg table buffer is NULL\");\n+        return NULL;\n+    }\n     return sg;\n}", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c", "line_number": 547, "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2014-2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include <linux/dma-buf.h>\n#include <linux/list.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_object.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dma_buf.h\"\n#include <uapi/linux/kfd_ioctl.h>\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_smi_events.h\"\n\n/* Userptr restore delay, just long enough to allow consecutive VM\n * changes to accumulate\n */\n#define AMDGPU_USERPTR_RESTORE_DELAY_MS 1\n#define AMDGPU_RESERVE_MEM_LIMIT\t\t\t(3UL << 29)\n\n/*\n * Align VRAM availability to 2MB to avoid fragmentation caused by 4K allocations in the tail 2MB\n * BO chunk\n */\n#define VRAM_AVAILABLITY_ALIGN (1 << 21)\n\n/* Impose limit on how much memory KFD can use */\nstatic struct {\n\tuint64_t max_system_mem_limit;\n\tuint64_t max_ttm_mem_limit;\n\tint64_t system_mem_used;\n\tint64_t ttm_mem_used;\n\tspinlock_t mem_limit_lock;\n} kfd_mem_limit;\n\nstatic const char * const domain_bit_to_string[] = {\n\t\t\"CPU\",\n\t\t\"GTT\",\n\t\t\"VRAM\",\n\t\t\"GDS\",\n\t\t\"GWS\",\n\t\t\"OA\"\n};\n\n#define domain_string(domain) domain_bit_to_string[ffs(domain)-1]\n\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work);\n\nstatic bool kfd_mem_is_attached(struct amdgpu_vm *avm,\n\t\tstruct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list)\n\t\tif (entry->bo_va->base.vm == avm)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * reuse_dmamap() - Check whether adev can share the original\n * userptr BO\n *\n * If both adev and bo_adev are in direct mapping or\n * in the same iommu group, they can share the original BO.\n *\n * @adev: Device to which can or cannot share the original BO\n * @bo_adev: Device to which allocated BO belongs to\n *\n * Return: returns true if adev can share original userptr BO,\n * false otherwise.\n */\nstatic bool reuse_dmamap(struct amdgpu_device *adev, struct amdgpu_device *bo_adev)\n{\n\treturn (adev->ram_is_direct_mapped && bo_adev->ram_is_direct_mapped) ||\n\t\t\t(adev->dev->iommu_group == bo_adev->dev->iommu_group);\n}\n\n/* Set memory usage limits. Current, limits are\n *  System (TTM + userptr) memory - 15/16th System RAM\n *  TTM memory - 3/8th System RAM\n */\nvoid amdgpu_amdkfd_gpuvm_init_mem_limits(void)\n{\n\tstruct sysinfo si;\n\tuint64_t mem;\n\n\tif (kfd_mem_limit.max_system_mem_limit)\n\t\treturn;\n\n\tsi_meminfo(&si);\n\tmem = si.totalram - si.totalhigh;\n\tmem *= si.mem_unit;\n\n\tspin_lock_init(&kfd_mem_limit.mem_limit_lock);\n\tkfd_mem_limit.max_system_mem_limit = mem - (mem >> 6);\n\tif (kfd_mem_limit.max_system_mem_limit < 2 * AMDGPU_RESERVE_MEM_LIMIT)\n\t\tkfd_mem_limit.max_system_mem_limit >>= 1;\n\telse\n\t\tkfd_mem_limit.max_system_mem_limit -= AMDGPU_RESERVE_MEM_LIMIT;\n\n\tkfd_mem_limit.max_ttm_mem_limit = ttm_tt_pages_limit() << PAGE_SHIFT;\n\tpr_debug(\"Kernel memory limit %lluM, TTM limit %lluM\\n\",\n\t\t(kfd_mem_limit.max_system_mem_limit >> 20),\n\t\t(kfd_mem_limit.max_ttm_mem_limit >> 20));\n}\n\nvoid amdgpu_amdkfd_reserve_system_mem(uint64_t size)\n{\n\tkfd_mem_limit.system_mem_used += size;\n}\n\n/* Estimate page table size needed to represent a given memory size\n *\n * With 4KB pages, we need one 8 byte PTE for each 4KB of memory\n * (factor 512, >> 9). With 2MB pages, we need one 8 byte PTE for 2MB\n * of memory (factor 256K, >> 18). ROCm user mode tries to optimize\n * for 2MB pages for TLB efficiency. However, small allocations and\n * fragmented system memory still need some 4KB pages. We choose a\n * compromise that should work in most cases without reserving too\n * much memory for page tables unnecessarily (factor 16K, >> 14).\n */\n\n#define ESTIMATE_PT_SIZE(mem_size) max(((mem_size) >> 14), AMDGPU_VM_RESERVED_VRAM)\n\n/**\n * amdgpu_amdkfd_reserve_mem_limit() - Decrease available memory by size\n * of buffer.\n *\n * @adev: Device to which allocated BO belongs to\n * @size: Size of buffer, in bytes, encapsulated by B0. This should be\n * equivalent to amdgpu_bo_size(BO)\n * @alloc_flag: Flag used in allocating a BO as noted above\n * @xcp_id: xcp_id is used to get xcp from xcp manager, one xcp is\n * managed as one compute node in driver for app\n *\n * Return:\n *\treturns -ENOMEM in case of error, ZERO otherwise\n */\nint amdgpu_amdkfd_reserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tsize_t system_mem_needed, ttm_mem_needed, vram_needed;\n\tint ret = 0;\n\tuint64_t vram_size = 0;\n\n\tsystem_mem_needed = 0;\n\tttm_mem_needed = 0;\n\tvram_needed = 0;\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tsystem_mem_needed = size;\n\t\tttm_mem_needed = size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\t/*\n\t\t * Conservatively round up the allocation requirement to 2 MB\n\t\t * to avoid fragmentation caused by 4K allocations in the tail\n\t\t * 2M BO chunk.\n\t\t */\n\t\tvram_needed = size;\n\t\t/*\n\t\t * For GFX 9.4.3, get the VRAM size from XCP structs\n\t\t */\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\treturn -EINVAL;\n\n\t\tvram_size = KFD_XCP_MEMORY_SIZE(adev, xcp_id);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tsystem_mem_needed = size;\n\t\t\tttm_mem_needed = size;\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tsystem_mem_needed = size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (kfd_mem_limit.system_mem_used + system_mem_needed >\n\t    kfd_mem_limit.max_system_mem_limit)\n\t\tpr_debug(\"Set no_system_mem_limit=1 if using shared memory\\n\");\n\n\tif ((kfd_mem_limit.system_mem_used + system_mem_needed >\n\t     kfd_mem_limit.max_system_mem_limit && !no_system_mem_limit) ||\n\t    (kfd_mem_limit.ttm_mem_used + ttm_mem_needed >\n\t     kfd_mem_limit.max_ttm_mem_limit) ||\n\t    (adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] + vram_needed >\n\t     vram_size - reserved_for_pt - reserved_for_ras - atomic64_read(&adev->vram_pin_size))) {\n\t\tret = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\t/* Update memory accounting by decreasing available system\n\t * memory, TTM memory and GPU memory as computed above\n\t */\n\tWARN_ONCE(vram_needed && !adev,\n\t\t  \"adev reference can't be null when vram is used\");\n\tif (adev && xcp_id >= 0) {\n\t\tadev->kfd.vram_used[xcp_id] += vram_needed;\n\t\tadev->kfd.vram_used_aligned[xcp_id] +=\n\t\t\t\t(adev->flags & AMD_IS_APU) ?\n\t\t\t\tvram_needed :\n\t\t\t\tALIGN(vram_needed, VRAM_AVAILABLITY_ALIGN);\n\t}\n\tkfd_mem_limit.system_mem_used += system_mem_needed;\n\tkfd_mem_limit.ttm_mem_used += ttm_mem_needed;\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\treturn ret;\n}\n\nvoid amdgpu_amdkfd_unreserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tWARN_ONCE(!adev,\n\t\t\t  \"adev reference can't be null when alloc mem flags vram is set\");\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\tgoto release;\n\n\t\tif (adev) {\n\t\t\tadev->kfd.vram_used[xcp_id] -= size;\n\t\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -= size;\n\t\t\t\tkfd_mem_limit.system_mem_used -= size;\n\t\t\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t\t\t} else {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -=\n\t\t\t\t\tALIGN(size, VRAM_AVAILABLITY_ALIGN);\n\t\t\t}\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\tgoto release;\n\t}\n\tWARN_ONCE(adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] < 0,\n\t\t  \"KFD VRAM memory accounting unbalanced for xcp: %d\", xcp_id);\n\tWARN_ONCE(kfd_mem_limit.ttm_mem_used < 0,\n\t\t  \"KFD TTM memory accounting unbalanced\");\n\tWARN_ONCE(kfd_mem_limit.system_mem_used < 0,\n\t\t  \"KFD system memory accounting unbalanced\");\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n}\n\nvoid amdgpu_amdkfd_release_notify(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tu32 alloc_flags = bo->kfd_bo->alloc_flags;\n\tu64 size = amdgpu_bo_size(bo);\n\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, size, alloc_flags,\n\t\t\t\t\t  bo->xcp_id);\n\n\tkfree(bo->kfd_bo);\n}\n\n/**\n * create_dmamap_sg_bo() - Creates a amdgpu_bo object to reflect information\n * about USERPTR or DOOREBELL or MMIO BO.\n *\n * @adev: Device for which dmamap BO is being created\n * @mem: BO of peer device that is being DMA mapped. Provides parameters\n *\t in building the dmamap BO\n * @bo_out: Output parameter updated with handle of dmamap BO\n */\nstatic int\ncreate_dmamap_sg_bo(struct amdgpu_device *adev,\n\t\t struct kgd_mem *mem, struct amdgpu_bo **bo_out)\n{\n\tstruct drm_gem_object *gem_obj;\n\tint ret;\n\tuint64_t flags = 0;\n\n\tret = amdgpu_bo_reserve(mem->bo, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR)\n\t\tflags |= mem->bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t\t\tAMDGPU_GEM_CREATE_UNCACHED);\n\n\tret = amdgpu_gem_object_create(adev, mem->bo->tbo.base.size, 1,\n\t\t\tAMDGPU_GEM_DOMAIN_CPU, AMDGPU_GEM_CREATE_PREEMPTIBLE | flags,\n\t\t\tttm_bo_type_sg, mem->bo->tbo.base.resv, &gem_obj, 0);\n\n\tamdgpu_bo_unreserve(mem->bo);\n\n\tif (ret) {\n\t\tpr_err(\"Error in creating DMA mappable SG BO on domain: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\n\t*bo_out = gem_to_amdgpu_bo(gem_obj);\n\t(*bo_out)->parent = amdgpu_bo_ref(mem->bo);\n\treturn ret;\n}\n\n/* amdgpu_amdkfd_remove_eviction_fence - Removes eviction fence from BO's\n *  reservation object.\n *\n * @bo: [IN] Remove eviction fence(s) from this BO\n * @ef: [IN] This eviction fence is removed if it\n *  is present in the shared list.\n *\n * NOTE: Must be called with BO reserved i.e. bo->tbo.resv->lock held.\n */\nstatic int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tstruct amdgpu_amdkfd_fence *ef)\n{\n\tstruct dma_fence *replacement;\n\n\tif (!ef)\n\t\treturn -EINVAL;\n\n\t/* TODO: Instead of block before we should use the fence of the page\n\t * table update and TLB flush here directly.\n\t */\n\treplacement = dma_fence_get_stub();\n\tdma_resv_replace_fences(bo->tbo.base.resv, ef->base.context,\n\t\t\t\treplacement, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_fence_put(replacement);\n\treturn 0;\n}\n\nint amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo *root = bo;\n\tstruct amdgpu_vm_bo_base *vm_bo;\n\tstruct amdgpu_vm *vm;\n\tstruct amdkfd_process_info *info;\n\tstruct amdgpu_amdkfd_fence *ef;\n\tint ret;\n\n\t/* we can always get vm_bo from root PD bo.*/\n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tvm_bo = root->vm_bo;\n\tif (!vm_bo)\n\t\treturn 0;\n\n\tvm = vm_bo->vm;\n\tif (!vm)\n\t\treturn 0;\n\n\tinfo = vm->process_info;\n\tif (!info || !info->eviction_fence)\n\t\treturn 0;\n\n\tef = container_of(dma_fence_get(&info->eviction_fence->base),\n\t\t\tstruct amdgpu_amdkfd_fence, base);\n\n\tBUG_ON(!dma_resv_trylock(bo->tbo.base.resv));\n\tret = amdgpu_amdkfd_remove_eviction_fence(bo, ef);\n\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tdma_fence_put(&ef->base);\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,\n\t\t\t\t     bool wait)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint ret;\n\n\tif (WARN(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm),\n\t\t \"Called with userptr BO\"))\n\t\treturn -EINVAL;\n\n\t/* bo has been pinned, not need validate it */\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto validate_fail;\n\tif (wait)\n\t\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\nvalidate_fail:\n\treturn ret;\n}\n\nint amdgpu_amdkfd_bo_validate_and_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tuint32_t domain,\n\t\t\t\t\tstruct dma_fence *fence)\n{\n\tint ret = amdgpu_bo_reserve(bo, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = amdgpu_amdkfd_bo_validate(bo, domain, true);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tret = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tdma_resv_add_fence(bo->tbo.base.resv, fence,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\nunreserve_out:\n\tamdgpu_bo_unreserve(bo);\n\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_validate_vm_bo(void *_unused, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_amdkfd_bo_validate(bo, bo->allowed_domains, false);\n}\n\n/* vm_validate_pt_pd_bos - Validate page table and directory BOs\n *\n * Page directories are not updated here because huge page handling\n * during page table updates can invalidate page directory entries\n * again. Page directories are only updated after updating page\n * tables.\n */\nstatic int vm_validate_pt_pd_bos(struct amdgpu_vm *vm,\n\t\t\t\t struct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_validate(adev, vm, ticket,\n\t\t\t\t amdgpu_amdkfd_validate_vm_bo, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to validate PT BOs\\n\");\n\t\treturn ret;\n\t}\n\n\tvm->pd_phys_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\n\treturn 0;\n}\n\nstatic int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_sync_fence(sync, vm->last_update);\n}\n\nstatic uint64_t get_pte_flags(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tuint32_t mapping_flags = AMDGPU_VM_PAGE_READABLE |\n\t\t\t\t AMDGPU_VM_MTYPE_DEFAULT;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\treturn amdgpu_gem_va_map_flags(adev, mapping_flags);\n}\n\n/**\n * create_sg_table() - Create an sg_table for a contiguous DMA addr range\n * @addr: The starting address to point to\n * @size: Size of memory area in bytes being pointed to\n *\n * Allocates an instance of sg_table and initializes it to point to memory\n * area specified by input parameters. The address used to build is assumed\n * to be DMA mapped, if needed.\n *\n * DOORBELL or MMIO BOs use only one scatterlist node in their sg_table\n * because they are physically contiguous.\n *\n * Return: Initialized instance of SG Table or NULL\n */\nstatic struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n{\n\tstruct sg_table *sg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\n\tif (!sg)\n\t\treturn NULL;\n\tif (sg_alloc_table(sg, 1, GFP_KERNEL)) {\n\t\tkfree(sg);\n\t\treturn NULL;\n\t}\n\tsg_dma_address(sg->sgl) = addr;\n\tsg->sgl->length = size;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\tsg->sgl->dma_length = size;\n#endif\n\treturn sg;\n}\n\nstatic int\nkfd_mem_dmamap_userptr(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *src_ttm = mem->bo->tbo.ttm;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tint ret;\n\n\tif (WARN_ON(ttm->num_pages != src_ttm->num_pages))\n\t\treturn -EINVAL;\n\n\tttm->sg = kmalloc(sizeof(*ttm->sg), GFP_KERNEL);\n\tif (unlikely(!ttm->sg))\n\t\treturn -ENOMEM;\n\n\t/* Same sequence as in amdgpu_ttm_tt_pin_userptr */\n\tret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,\n\t\t\t\t\tttm->num_pages, 0,\n\t\t\t\t\t(u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\tret = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (unlikely(ret))\n\t\tgoto release_sg;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto unmap_sg;\n\n\treturn 0;\n\nunmap_sg:\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\nrelease_sg:\n\tpr_err(\"DMA map userptr failed: %d\\n\", ret);\n\tsg_free_table(ttm->sg);\nfree_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tint ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n/**\n * kfd_mem_dmamap_sg_bo() - Create DMA mapped sg_table to access DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * An access request from the device that owns DOORBELL does not require DMA mapping.\n * This is because the request doesn't go through PCIe root complex i.e. it instead\n * loops back. The need to DMA map arises only when accessing peer device's DOORBELL\n *\n * In contrast, all access requests for MMIO need to be DMA mapped without regard to\n * device ownership. This is because access requests for MMIO go through PCIe root\n * complex.\n *\n * This is accomplished in two steps:\n *   - Obtain DMA mapped address of DOORBELL or MMIO memory that could be used\n *         in updating requesting device's page table\n *   - Signal TTM to mark memory pointed to by requesting device's BO as GPU\n *         accessible. This allows an update of requesting device's page table\n *         with entries associated with DOOREBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *   - Mapping of DOORBELL or MMIO BO of same or peer device\n *   - Validating an evicted DOOREBELL or MMIO BO on device seeking access\n *\n * Return: ZERO if successful, NON-ZERO otherwise\n */\nstatic int\nkfd_mem_dmamap_sg_bo(struct kgd_mem *mem,\n\t\t     struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\tdma_addr_t dma_addr;\n\tbool mmio;\n\tint ret;\n\n\t/* Expect SG Table of dmapmap BO to be NULL */\n\tmmio = (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);\n\tif (unlikely(ttm->sg)) {\n\t\tpr_err(\"SG Table of %d BO for peer device is UNEXPECTEDLY NON-NULL\", mmio);\n\t\treturn -EINVAL;\n\t}\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_addr = mem->bo->tbo.sg->sgl->dma_address;\n\tpr_debug(\"%d BO size: %d\\n\", mmio, mem->bo->tbo.sg->sgl->length);\n\tpr_debug(\"%d BO address before DMA mapping: %llx\\n\", mmio, dma_addr);\n\tdma_addr = dma_map_resource(adev->dev, dma_addr,\n\t\t\tmem->bo->tbo.sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tret = dma_mapping_error(adev->dev, dma_addr);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tpr_debug(\"%d BO address after DMA mapping: %llx\\n\", mmio, dma_addr);\n\n\tttm->sg = create_sg_table(dma_addr, mem->bo->tbo.sg->sgl->length);\n\tif (unlikely(!ttm->sg)) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\treturn ret;\n\nfree_sg:\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\nunmap_sg:\n\tdma_unmap_resource(adev->dev, dma_addr, mem->bo->tbo.sg->sgl->length,\n\t\t\t   dir, DMA_ATTR_SKIP_CPU_SYNC);\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_attachment(struct kgd_mem *mem,\n\t\t\t  struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\treturn 0;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\treturn kfd_mem_dmamap_userptr(mem, attachment);\n\tcase KFD_MEM_ATT_DMABUF:\n\t\treturn kfd_mem_dmamap_dmabuf(attachment);\n\tcase KFD_MEM_ATT_SG:\n\t\treturn kfd_mem_dmamap_sg_bo(mem, attachment);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nkfd_mem_dmaunmap_userptr(struct kgd_mem *mem,\n\t\t\t struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = false};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\n\tif (unlikely(!ttm->sg))\n\t\treturn;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\t/* This is a no-op. We don't want to trigger eviction fences when\n\t * unmapping DMABufs. Therefore the invalidation (moving to system\n\t * domain) is done in kfd_mem_dmamap_dmabuf.\n\t */\n}\n\n/**\n * kfd_mem_dmaunmap_sg_bo() - Free DMA mapped sg_table of DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * The method performs following steps:\n *   - Signal TTM to mark memory pointed to by BO as GPU inaccessible\n *   - Free SG Table that is used to encapsulate DMA mapped memory of\n *          peer device's DOORBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *     UNMapping of DOORBELL or MMIO BO on a device having access to its memory\n *     Eviction of DOOREBELL or MMIO BO on device having access to its memory\n *\n * Return: void\n */\nstatic void\nkfd_mem_dmaunmap_sg_bo(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\n\tif (unlikely(!ttm->sg)) {\n\t\tpr_debug(\"SG Table of BO is NULL\");\n\t\treturn;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_unmap_resource(adev->dev, ttm->sg->sgl->dma_address,\n\t\t\tttm->sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\tbo->tbo.sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_attachment(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\tbreak;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\tkfd_mem_dmaunmap_userptr(mem, attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_DMABUF:\n\t\tkfd_mem_dmaunmap_dmabuf(attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_SG:\n\t\tkfd_mem_dmaunmap_sg_bo(mem, attachment);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic int kfd_mem_export_dmabuf(struct kgd_mem *mem)\n{\n\tif (!mem->dmabuf) {\n\t\tstruct amdgpu_device *bo_adev;\n\t\tstruct dma_buf *dmabuf;\n\n\t\tbo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\t\tdmabuf = drm_gem_prime_handle_to_dmabuf(&bo_adev->ddev, bo_adev->kfd.client.file,\n\t\t\t\t\t       mem->gem_handle,\n\t\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\t\t       DRM_RDWR : 0);\n\t\tif (IS_ERR(dmabuf))\n\t\t\treturn PTR_ERR(dmabuf);\n\t\tmem->dmabuf = dmabuf;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nkfd_mem_attach_dmabuf(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\t      struct amdgpu_bo **bo)\n{\n\tstruct drm_gem_object *gobj;\n\tint ret;\n\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tgobj = amdgpu_gem_prime_import(adev_to_drm(adev), mem->dmabuf);\n\tif (IS_ERR(gobj))\n\t\treturn PTR_ERR(gobj);\n\n\t*bo = gem_to_amdgpu_bo(gobj);\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\treturn 0;\n}\n\n/* kfd_mem_attach - Add a BO to a VM\n *\n * Everything that needs to bo done only once when a BO is first added\n * to a VM. It can later be mapped and unmapped many times without\n * repeating these steps.\n *\n * 0. Create BO for DMA mapping, if needed\n * 1. Allocate and initialize BO VA entry data structure\n * 2. Add BO to the VM\n * 3. Determine ASIC-specific PTE flags\n * 4. Alloc page tables and directories if needed\n * 4a.  Validate new page tables and directories\n */\nstatic int kfd_mem_attach(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tstruct amdgpu_vm *vm, bool is_aql)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tuint64_t va = mem->va;\n\tstruct kfd_mem_attachment *attachment[2] = {NULL, NULL};\n\tstruct amdgpu_bo *bo[2] = {NULL, NULL};\n\tstruct amdgpu_bo_va *bo_va;\n\tbool same_hive = false;\n\tint i, ret;\n\n\tif (!va) {\n\t\tpr_err(\"Invalid VA when adding BO to VM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Determine access to VRAM, MMIO and DOORBELL BOs of peer devices\n\t *\n\t * The access path of MMIO and DOORBELL BOs of is always over PCIe.\n\t * In contrast the access path of VRAM BOs depens upon the type of\n\t * link that connects the peer device. Access over PCIe is allowed\n\t * if peer device has large BAR. In contrast, access over xGMI is\n\t * allowed for both small and large BAR configurations of peer device\n\t */\n\tif ((adev != bo_adev && !(adev->flags & AMD_IS_APU)) &&\n\t    ((mem->domain == AMDGPU_GEM_DOMAIN_VRAM) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tif (mem->domain == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\tsame_hive = amdgpu_xgmi_same_hive(adev, bo_adev);\n\t\tif (!same_hive && !amdgpu_device_is_peer_accessible(bo_adev, adev))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i <= is_aql; i++) {\n\t\tattachment[i] = kzalloc(sizeof(*attachment[i]), GFP_KERNEL);\n\t\tif (unlikely(!attachment[i])) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tpr_debug(\"\\t add VA 0x%llx - 0x%llx to vm %p\\n\", va,\n\t\t\t va + bo_size, vm);\n\n\t\tif ((adev == bo_adev && !(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) ||\n\t\t    (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && reuse_dmamap(adev, bo_adev)) ||\n\t\t    (mem->domain == AMDGPU_GEM_DOMAIN_GTT && reuse_dmamap(adev, bo_adev)) ||\n\t\t    same_hive) {\n\t\t\t/* Mappings on the local GPU, or VRAM mappings in the\n\t\t\t * local hive, or userptr, or GTT mapping can reuse dma map\n\t\t\t * address space share the original BO\n\t\t\t */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = mem->bo;\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (i > 0) {\n\t\t\t/* Multiple mappings on the same GPU share the BO */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = bo[0];\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\t\t/* Create an SG BO to DMA-map userptrs on other GPUs */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_USERPTR;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Handle DOORBELL BOs of peer devices and MMIO BOs of local and peer devices */\n\t\t} else if (mem->bo->tbo.type == ttm_bo_type_sg) {\n\t\t\tWARN_ONCE(!(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL ||\n\t\t\t\t    mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP),\n\t\t\t\t  \"Handing invalid SG BO in ATTACH request\");\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SG;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Enable acces to GTT and VRAM BOs of peer devices */\n\t\t} else if (mem->domain == AMDGPU_GEM_DOMAIN_GTT ||\n\t\t\t   mem->domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tattachment[i]->type = KFD_MEM_ATT_DMABUF;\n\t\t\tret = kfd_mem_attach_dmabuf(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t\tpr_debug(\"Employ DMABUF mechanism to enable peer GPU access\\n\");\n\t\t} else {\n\t\t\tWARN_ONCE(true, \"Handling invalid ATTACH request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\t/* Add BO to VM internal data structures */\n\t\tret = amdgpu_bo_reserve(bo[i], false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Unable to reserve BO during memory attach\");\n\t\t\tgoto unwind;\n\t\t}\n\t\tbo_va = amdgpu_vm_bo_find(vm, bo[i]);\n\t\tif (!bo_va)\n\t\t\tbo_va = amdgpu_vm_bo_add(adev, vm, bo[i]);\n\t\telse\n\t\t\t++bo_va->ref_count;\n\t\tattachment[i]->bo_va = bo_va;\n\t\tamdgpu_bo_unreserve(bo[i]);\n\t\tif (unlikely(!attachment[i]->bo_va)) {\n\t\t\tret = -ENOMEM;\n\t\t\tpr_err(\"Failed to add BO object to VM. ret == %d\\n\",\n\t\t\t       ret);\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->va = va;\n\t\tattachment[i]->pte_flags = get_pte_flags(adev, mem);\n\t\tattachment[i]->adev = adev;\n\t\tlist_add(&attachment[i]->list, &mem->attachments);\n\n\t\tva += bo_size;\n\t}\n\n\treturn 0;\n\nunwind:\n\tfor (; i >= 0; i--) {\n\t\tif (!attachment[i])\n\t\t\tcontinue;\n\t\tif (attachment[i]->bo_va) {\n\t\t\tamdgpu_bo_reserve(bo[i], true);\n\t\t\tif (--attachment[i]->bo_va->ref_count == 0)\n\t\t\t\tamdgpu_vm_bo_del(adev, attachment[i]->bo_va);\n\t\t\tamdgpu_bo_unreserve(bo[i]);\n\t\t\tlist_del(&attachment[i]->list);\n\t\t}\n\t\tif (bo[i])\n\t\t\tdrm_gem_object_put(&bo[i]->tbo.base);\n\t\tkfree(attachment[i]);\n\t}\n\treturn ret;\n}\n\nstatic void kfd_mem_detach(struct kfd_mem_attachment *attachment)\n{\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\n\tpr_debug(\"\\t remove VA 0x%llx in entry %p\\n\",\n\t\t\tattachment->va, attachment);\n\tif (--attachment->bo_va->ref_count == 0)\n\t\tamdgpu_vm_bo_del(attachment->adev, attachment->bo_va);\n\tdrm_gem_object_put(&bo->tbo.base);\n\tlist_del(&attachment->list);\n\tkfree(attachment);\n}\n\nstatic void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,\n\t\t\t\tstruct amdkfd_process_info *process_info,\n\t\t\t\tbool userptr)\n{\n\tmutex_lock(&process_info->lock);\n\tif (userptr)\n\t\tlist_add_tail(&mem->validate_list,\n\t\t\t      &process_info->userptr_valid_list);\n\telse\n\t\tlist_add_tail(&mem->validate_list, &process_info->kfd_bo_list);\n\tmutex_unlock(&process_info->lock);\n}\n\nstatic void remove_kgd_mem_from_kfd_bo_list(struct kgd_mem *mem,\n\t\tstruct amdkfd_process_info *process_info)\n{\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n}\n\n/* Initializes user pages. It registers the MMU notifier and validates\n * the userptr BO in the GTT domain.\n *\n * The BO must already be on the userptr_valid_list. Otherwise an\n * eviction and restore may happen that leaves the new BO unmapped\n * with the user mode queues running.\n *\n * Takes the process_info->lock to protect against concurrent restore\n * workers.\n *\n * Returns 0 for success, negative errno for errors.\n */\nstatic int init_user_pages(struct kgd_mem *mem, uint64_t user_addr,\n\t\t\t   bool criu_resume)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct hmm_range *range;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->lock);\n\n\tret = amdgpu_ttm_tt_set_userptr(&bo->tbo, user_addr, 0);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to set userptr: %d\\n\", __func__, ret);\n\t\tgoto out;\n\t}\n\n\tret = amdgpu_hmm_register(bo, user_addr);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register MMU notifier: %d\\n\",\n\t\t       __func__, ret);\n\t\tgoto out;\n\t}\n\n\tif (criu_resume) {\n\t\t/*\n\t\t * During a CRIU restore operation, the userptr buffer objects\n\t\t * will be validated in the restore_userptr_work worker at a\n\t\t * later stage when it is scheduled by another ioctl called by\n\t\t * CRIU master process for the target pid for restore.\n\t\t */\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tmem->invalid++;\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t\tmutex_unlock(&process_info->lock);\n\t\treturn 0;\n\t}\n\n\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);\n\tif (ret) {\n\t\tif (ret == -EAGAIN)\n\t\t\tpr_debug(\"Failed to get user pages, try again\\n\");\n\t\telse\n\t\t\tpr_err(\"%s: Failed to get user pages: %d\\n\", __func__, ret);\n\t\tgoto unregister_out;\n\t}\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to reserve BO\\n\", __func__);\n\t\tgoto release_out;\n\t}\n\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\tamdgpu_bo_unreserve(bo);\n\nrelease_out:\n\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\nunregister_out:\n\tif (ret)\n\t\tamdgpu_hmm_unregister(bo);\nout:\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\n/* Reserving a BO and its page table BOs must happen atomically to\n * avoid deadlocks. Some operations update multiple VMs at once. Track\n * all the reservation info in a context structure. Optionally a sync\n * object can track VM updates.\n */\nstruct bo_vm_reservation_context {\n\t/* DRM execution context for the reservation */\n\tstruct drm_exec exec;\n\t/* Number of VMs reserved */\n\tunsigned int n_vms;\n\t/* Pointer to sync object */\n\tstruct amdgpu_sync *sync;\n};\n\nenum bo_vm_match {\n\tBO_VM_NOT_MAPPED = 0,\t/* Match VMs where a BO is not mapped */\n\tBO_VM_MAPPED,\t\t/* Match VMs where a BO is mapped     */\n\tBO_VM_ALL,\t\t/* Match all VMs a BO was added to    */\n};\n\n/**\n * reserve_bo_and_vm - reserve a BO and a VM unconditionally.\n * @mem: KFD BO structure.\n * @vm: the VM to reserve.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n */\nstatic int reserve_bo_and_vm(struct kgd_mem *mem,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct bo_vm_reservation_context *ctx)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tWARN_ON(!vm);\n\n\tctx->n_vms = 1;\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tret = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * reserve_bo_and_cond_vms - reserve a BO and some VMs conditionally\n * @mem: KFD BO structure.\n * @vm: the VM to reserve. If NULL, then all VMs associated with the BO\n * is used. Otherwise, a single VM associated with the BO.\n * @map_type: the mapping status that will be used to filter the VMs.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n *\n * Returns 0 for success, negative for failure.\n */\nstatic int reserve_bo_and_cond_vms(struct kgd_mem *mem,\n\t\t\t\tstruct amdgpu_vm *vm, enum bo_vm_match map_type,\n\t\t\t\tstruct bo_vm_reservation_context *ctx)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tctx->n_vms = 0;\n\t\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\t\tif ((vm && vm != entry->bo_va->base.vm) ||\n\t\t\t\t(entry->is_mapped != map_type\n\t\t\t\t&& map_type != BO_VM_ALL))\n\t\t\t\tcontinue;\n\n\t\t\tret = amdgpu_vm_lock_pd(entry->bo_va->base.vm,\n\t\t\t\t\t\t&ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto error;\n\t\t\t++ctx->n_vms;\n\t\t}\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * unreserve_bo_and_vms - Unreserve BO and VMs from a reservation context\n * @ctx: Reservation context to unreserve\n * @wait: Optionally wait for a sync object representing pending VM updates\n * @intr: Whether the wait is interruptible\n *\n * Also frees any resources allocated in\n * reserve_bo_and_(cond_)vm(s). Returns the status from\n * amdgpu_sync_wait.\n */\nstatic int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,\n\t\t\t\t bool wait, bool intr)\n{\n\tint ret = 0;\n\n\tif (wait)\n\t\tret = amdgpu_sync_wait(ctx->sync, intr);\n\n\tdrm_exec_fini(&ctx->exec);\n\tctx->sync = NULL;\n\treturn ret;\n}\n\nstatic int unmap_bo_from_gpuvm(struct kgd_mem *mem,\n\t\t\t\tstruct kfd_mem_attachment *entry,\n\t\t\t\tstruct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\n\tif (bo_va->queue_refcount) {\n\t\tpr_debug(\"bo_va->queue_refcount %d\\n\", bo_va->queue_refcount);\n\t\treturn -EBUSY;\n\t}\n\n\tamdgpu_vm_bo_unmap(adev, bo_va, entry->va);\n\n\tamdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);\n\n\tamdgpu_sync_fence(sync, bo_va->last_pt_update);\n\n\treturn 0;\n}\n\nstatic int update_gpuvm_pte(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *entry,\n\t\t\t    struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tint ret;\n\n\tret = kfd_mem_dmamap_attachment(mem, entry);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Update the page tables  */\n\tret = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (ret) {\n\t\tpr_err(\"amdgpu_vm_bo_update failed\\n\");\n\t\treturn ret;\n\t}\n\n\treturn amdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int map_bo_to_gpuvm(struct kgd_mem *mem,\n\t\t\t   struct kfd_mem_attachment *entry,\n\t\t\t   struct amdgpu_sync *sync,\n\t\t\t   bool no_update_pte)\n{\n\tint ret;\n\n\t/* Set virtual address for the allocation */\n\tret = amdgpu_vm_bo_map(entry->adev, entry->bo_va, entry->va, 0,\n\t\t\t       amdgpu_bo_size(entry->bo_va->base.bo),\n\t\t\t       entry->pte_flags);\n\tif (ret) {\n\t\tpr_err(\"Failed to map VA 0x%llx in vm. ret %d\\n\",\n\t\t\t\tentry->va, ret);\n\t\treturn ret;\n\t}\n\n\tif (no_update_pte)\n\t\treturn 0;\n\n\tret = update_gpuvm_pte(mem, entry, sync);\n\tif (ret) {\n\t\tpr_err(\"update_gpuvm_pte() failed\\n\");\n\t\tgoto update_gpuvm_pte_failed;\n\t}\n\n\treturn 0;\n\nupdate_gpuvm_pte_failed:\n\tunmap_bo_from_gpuvm(mem, entry, sync);\n\tkfd_mem_dmaunmap_attachment(mem, entry);\n\treturn ret;\n}\n\nstatic int process_validate_vms(struct amdkfd_process_info *process_info,\n\t\t\t\tstruct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_validate_pt_pd_bos(peer_vm, ticket);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_sync_pds_resv(struct amdkfd_process_info *process_info,\n\t\t\t\t struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *pd = peer_vm->root.bo;\n\n\t\tret = amdgpu_sync_resv(NULL, sync, pd->tbo.base.resv,\n\t\t\t\t       AMDGPU_SYNC_NE_OWNER,\n\t\t\t\t       AMDGPU_FENCE_OWNER_KFD);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_update_pds(struct amdkfd_process_info *process_info,\n\t\t\t      struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_update_pds(peer_vm, sync);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,\n\t\t       struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *info = NULL;\n\tint ret;\n\n\tif (!*process_info) {\n\t\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tmutex_init(&info->lock);\n\t\tmutex_init(&info->notifier_lock);\n\t\tINIT_LIST_HEAD(&info->vm_list_head);\n\t\tINIT_LIST_HEAD(&info->kfd_bo_list);\n\t\tINIT_LIST_HEAD(&info->userptr_valid_list);\n\t\tINIT_LIST_HEAD(&info->userptr_inval_list);\n\n\t\tinfo->eviction_fence =\n\t\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t\t   current->mm,\n\t\t\t\t\t\t   NULL);\n\t\tif (!info->eviction_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto create_evict_fence_fail;\n\t\t}\n\n\t\tinfo->pid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\t\tINIT_DELAYED_WORK(&info->restore_userptr_work,\n\t\t\t\t  amdgpu_amdkfd_restore_userptr_worker);\n\n\t\t*process_info = info;\n\t}\n\n\tvm->process_info = *process_info;\n\n\t/* Validate page directory and attach eviction fence */\n\tret = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (ret)\n\t\tgoto reserve_pd_fail;\n\tret = vm_validate_pt_pd_bos(vm, NULL);\n\tif (ret) {\n\t\tpr_err(\"validate_pt_pd_bos() failed\\n\");\n\t\tgoto validate_pd_fail;\n\t}\n\tret = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t  AMDGPU_FENCE_OWNER_KFD, false);\n\tif (ret)\n\t\tgoto wait_pd_fail;\n\tret = dma_resv_reserve_fences(vm->root.bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(vm->root.bo->tbo.base.resv,\n\t\t\t   &vm->process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\n\t/* Update process info */\n\tmutex_lock(&vm->process_info->lock);\n\tlist_add_tail(&vm->vm_list_node,\n\t\t\t&(vm->process_info->vm_list_head));\n\tvm->process_info->n_vms++;\n\n\t*ef = dma_fence_get(&vm->process_info->eviction_fence->base);\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn 0;\n\nreserve_shared_fail:\nwait_pd_fail:\nvalidate_pd_fail:\n\tamdgpu_bo_unreserve(vm->root.bo);\nreserve_pd_fail:\n\tvm->process_info = NULL;\n\tif (info) {\n\t\tdma_fence_put(&info->eviction_fence->base);\n\t\t*process_info = NULL;\n\t\tput_pid(info->pid);\ncreate_evict_fence_fail:\n\t\tmutex_destroy(&info->lock);\n\t\tmutex_destroy(&info->notifier_lock);\n\t\tkfree(info);\n\t}\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_pin_bo() - Pins a BO using following criteria\n * @bo: Handle of buffer object being pinned\n * @domain: Domain into which BO should be pinned\n *\n *   - USERPTR BOs are UNPINNABLE and will return error\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count incremented. It is valid to PIN a BO multiple times\n *\n * Return: ZERO if successful in pinning, Non-Zero in case of error.\n */\nstatic int amdgpu_amdkfd_gpuvm_pin_bo(struct amdgpu_bo *bo, u32 domain)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {\n\t\t/*\n\t\t * If bo is not contiguous on VRAM, move to system memory first to ensure\n\t\t * we can get contiguous VRAM space after evicting other BOs.\n\t\t */\n\t\tif (!(bo->tbo.resource->placement & TTM_PL_FLAG_CONTIGUOUS)) {\n\t\t\tstruct ttm_operation_ctx ctx = { true, false };\n\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_debug(\"validate bo 0x%p to GTT failed %d\\n\", &bo->tbo, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n\tif (ret)\n\t\tpr_err(\"Error in Pinning BO to domain: %d\\n\", domain);\n\n\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\nout:\n\tamdgpu_bo_unreserve(bo);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_unpin_bo() - Unpins BO using following criteria\n * @bo: Handle of buffer object being unpinned\n *\n *   - Is a illegal request for USERPTR BOs and is ignored\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count decremented. Calls to UNPIN must balance calls to PIN\n */\nstatic void amdgpu_amdkfd_gpuvm_unpin_bo(struct amdgpu_bo *bo)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn;\n\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_set_vm_pasid(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_vm *avm, u32 pasid)\n\n{\n\tint ret;\n\n\t/* Free the original amdgpu allocated pasid,\n\t * will be replaced with kfd allocated pasid.\n\t */\n\tif (avm->pasid) {\n\t\tamdgpu_pasid_free(avm->pasid);\n\t\tamdgpu_vm_set_pasid(adev, avm, 0);\n\t}\n\n\tret = amdgpu_vm_set_pasid(adev, avm, pasid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_acquire_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *avm,\n\t\t\t\t\t   void **process_info,\n\t\t\t\t\t   struct dma_fence **ef)\n{\n\tint ret;\n\n\t/* Already a compute VM? */\n\tif (avm->process_info)\n\t\treturn -EINVAL;\n\n\t/* Convert VM into a compute VM */\n\tret = amdgpu_vm_make_compute(adev, avm);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Initialize KFD part of the VM and process info */\n\tret = init_kfd_vm(avm, process_info, ef);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_vm_set_task_info(avm);\n\n\treturn 0;\n}\n\nvoid amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm)\n{\n\tstruct amdkfd_process_info *process_info = vm->process_info;\n\n\tif (!process_info)\n\t\treturn;\n\n\t/* Update process info */\n\tmutex_lock(&process_info->lock);\n\tprocess_info->n_vms--;\n\tlist_del(&vm->vm_list_node);\n\tmutex_unlock(&process_info->lock);\n\n\tvm->process_info = NULL;\n\n\t/* Release per-process resources when last compute VM is destroyed */\n\tif (!process_info->n_vms) {\n\t\tWARN_ON(!list_empty(&process_info->kfd_bo_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_valid_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_inval_list));\n\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tcancel_delayed_work_sync(&process_info->restore_userptr_work);\n\t\tput_pid(process_info->pid);\n\t\tmutex_destroy(&process_info->lock);\n\t\tmutex_destroy(&process_info->notifier_lock);\n\t\tkfree(process_info);\n\t}\n}\n\nvoid amdgpu_amdkfd_gpuvm_release_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t    void *drm_priv)\n{\n\tstruct amdgpu_vm *avm;\n\n\tif (WARN_ON(!adev || !drm_priv))\n\t\treturn;\n\n\tavm = drm_priv_to_vm(drm_priv);\n\n\tpr_debug(\"Releasing process vm %p\\n\", avm);\n\n\t/* The original pasid of amdgpu vm has already been\n\t * released during making a amdgpu vm to a compute vm\n\t * The current pasid is managed by kfd and will be\n\t * released on kfd process destroy. Set amdgpu pasid\n\t * to 0 to avoid duplicate release.\n\t */\n\tamdgpu_vm_release_compute(adev, avm);\n}\n\nuint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *pd = avm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\n\tif (adev->asic_type < CHIP_VEGA10)\n\t\treturn avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;\n\treturn avm->pd_phys_addr;\n}\n\nvoid amdgpu_amdkfd_block_mmu_notifications(void *p)\n{\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tWRITE_ONCE(pinfo->block_mmu_notifications, true);\n\tmutex_unlock(&pinfo->lock);\n}\n\nint amdgpu_amdkfd_criu_resume(void *p)\n{\n\tint ret = 0;\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tpr_debug(\"scheduling work\\n\");\n\tmutex_lock(&pinfo->notifier_lock);\n\tpinfo->evicted_bos++;\n\tmutex_unlock(&pinfo->notifier_lock);\n\tif (!READ_ONCE(pinfo->block_mmu_notifications)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tWRITE_ONCE(pinfo->block_mmu_notifications, false);\n\tqueue_delayed_work(system_freezable_wq,\n\t\t\t   &pinfo->restore_userptr_work, 0);\n\nout_unlock:\n\tmutex_unlock(&pinfo->lock);\n\treturn ret;\n}\n\nsize_t amdgpu_amdkfd_get_available_memory(struct amdgpu_device *adev,\n\t\t\t\t\t  uint8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tssize_t available;\n\tuint64_t vram_available, system_mem_available, ttm_mem_available;\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tvram_available = KFD_XCP_MEMORY_SIZE(adev, xcp_id)\n\t\t- adev->kfd.vram_used_aligned[xcp_id]\n\t\t- atomic64_read(&adev->vram_pin_size)\n\t\t- reserved_for_pt\n\t\t- reserved_for_ras;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tsystem_mem_available = no_system_mem_limit ?\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit :\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit -\n\t\t\t\t\tkfd_mem_limit.system_mem_used;\n\n\t\tttm_mem_available = kfd_mem_limit.max_ttm_mem_limit -\n\t\t\t\tkfd_mem_limit.ttm_mem_used;\n\n\t\tavailable = min3(system_mem_available, ttm_mem_available,\n\t\t\t\t vram_available);\n\t\tavailable = ALIGN_DOWN(available, PAGE_SIZE);\n\t} else {\n\t\tavailable = ALIGN_DOWN(vram_available, VRAM_AVAILABLITY_ALIGN);\n\t}\n\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\n\treturn available;\n}\n\nint amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, uint64_t va, uint64_t size,\n\t\tvoid *drm_priv, struct kgd_mem **mem,\n\t\tuint64_t *offset, uint32_t flags, bool criu_resume)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_fpriv *fpriv = container_of(avm, struct amdgpu_fpriv, vm);\n\tenum ttm_bo_type bo_type = ttm_bo_type_device;\n\tstruct sg_table *sg = NULL;\n\tuint64_t user_addr = 0;\n\tstruct amdgpu_bo *bo;\n\tstruct drm_gem_object *gobj = NULL;\n\tu32 domain, alloc_domain;\n\tuint64_t aligned_size;\n\tint8_t xcp_id = -1;\n\tu64 alloc_flags;\n\tint ret;\n\n\t/*\n\t * Check on which domain to allocate BO\n\t */\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_flags = 0;\n\t\t} else {\n\t\t\talloc_flags = AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\t\t\talloc_flags |= (flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) ?\n\t\t\tAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED : 0;\n\n\t\t\t/* For contiguous VRAM allocation */\n\t\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_CONTIGUOUS)\n\t\t\t\talloc_flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\t\t}\n\t\txcp_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ?\n\t\t\t\t\t0 : fpriv->xcp_id;\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_flags = 0;\n\t} else {\n\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_domain = AMDGPU_GEM_DOMAIN_CPU;\n\t\talloc_flags = AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\tif (!offset || !*offset)\n\t\t\t\treturn -EINVAL;\n\t\t\tuser_addr = untagged_addr(*offset);\n\t\t} else if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\t\tbo_type = ttm_bo_type_sg;\n\t\t\tif (size > UINT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\tsg = create_sg_table(*offset, size);\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_EXT_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_EXT_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_UNCACHED)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_UNCACHED;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\t(*mem)->aql_queue = !!(flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);\n\n\t/* Workaround for AQL queue wraparound bug. Map the same\n\t * memory twice. That means we only actually allocate half\n\t * the memory.\n\t */\n\tif ((*mem)->aql_queue)\n\t\tsize >>= 1;\n\taligned_size = PAGE_ALIGN(size);\n\n\t(*mem)->alloc_flags = flags;\n\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\tret = amdgpu_amdkfd_reserve_mem_limit(adev, aligned_size, flags,\n\t\t\t\t\t      xcp_id);\n\tif (ret) {\n\t\tpr_debug(\"Insufficient memory\\n\");\n\t\tgoto err_reserve_limit;\n\t}\n\n\tpr_debug(\"\\tcreate BO VA 0x%llx size 0x%llx domain %s xcp_id %d\\n\",\n\t\t va, (*mem)->aql_queue ? size << 1 : size,\n\t\t domain_string(alloc_domain), xcp_id);\n\n\tret = amdgpu_gem_object_create(adev, aligned_size, 1, alloc_domain, alloc_flags,\n\t\t\t\t       bo_type, NULL, &gobj, xcp_id + 1);\n\tif (ret) {\n\t\tpr_debug(\"Failed to create BO on domain %s. ret %d\\n\",\n\t\t\t domain_string(alloc_domain), ret);\n\t\tgoto err_bo_create;\n\t}\n\tret = drm_vma_node_allow(&gobj->vma_node, drm_priv);\n\tif (ret) {\n\t\tpr_debug(\"Failed to allow vma node access. ret %d\\n\", ret);\n\t\tgoto err_node_allow;\n\t}\n\tret = drm_gem_handle_create(adev->kfd.client.file, gobj, &(*mem)->gem_handle);\n\tif (ret)\n\t\tgoto err_gem_handle_create;\n\tbo = gem_to_amdgpu_bo(gobj);\n\tif (bo_type == ttm_bo_type_sg) {\n\t\tbo->tbo.sg = sg;\n\t\tbo->tbo.ttm->sg = sg;\n\t}\n\tbo->kfd_bo = *mem;\n\t(*mem)->bo = bo;\n\tif (user_addr)\n\t\tbo->flags |= AMDGPU_AMDKFD_CREATE_USERPTR_BO;\n\n\t(*mem)->va = va;\n\t(*mem)->domain = domain;\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);\n\n\tif (user_addr) {\n\t\tpr_debug(\"creating userptr BO for user_addr = %llx\\n\", user_addr);\n\t\tret = init_user_pages(*mem, user_addr, criu_resume);\n\t\tif (ret)\n\t\t\tgoto allocate_init_user_pages_failed;\n\t} else  if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tret = amdgpu_amdkfd_gpuvm_pin_bo(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (ret) {\n\t\t\tpr_err(\"Pinning MMIO/DOORBELL BO during ALLOC FAILED\\n\");\n\t\t\tgoto err_pin_bo;\n\t\t}\n\t\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t} else {\n\t\tmutex_lock(&avm->process_info->lock);\n\t\tif (avm->process_info->eviction_fence &&\n\t\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\t\tmutex_unlock(&avm->process_info->lock);\n\t\tif (ret)\n\t\t\tgoto err_validate_bo;\n\t}\n\n\tif (offset)\n\t\t*offset = amdgpu_bo_mmap_offset(bo);\n\n\treturn 0;\n\nallocate_init_user_pages_failed:\nerr_pin_bo:\nerr_validate_bo:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_gem_handle_delete(adev->kfd.client.file, (*mem)->gem_handle);\nerr_gem_handle_create:\n\tdrm_vma_node_revoke(&gobj->vma_node, drm_priv);\nerr_node_allow:\n\t/* Don't unreserve system mem limit twice */\n\tgoto err_reserve_limit;\nerr_bo_create:\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, aligned_size, flags, xcp_id);\nerr_reserve_limit:\n\tamdgpu_sync_free(&(*mem)->sync);\n\tmutex_destroy(&(*mem)->lock);\n\tif (gobj)\n\t\tdrm_gem_object_put(gobj);\n\telse\n\t\tkfree(*mem);\nerr:\n\tif (sg) {\n\t\tsg_free_table(sg);\n\t\tkfree(sg);\n\t}\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_free_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv,\n\t\tuint64_t *size)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tbool use_release_notifier = (mem->bo->kfd_bo == mem);\n\tstruct kfd_mem_attachment *entry, *tmp;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned int mapped_to_gpu_memory;\n\tint ret;\n\tbool is_imported = false;\n\n\tmutex_lock(&mem->lock);\n\n\t/* Unpin MMIO/DOORBELL BO's that were pinned during allocation */\n\tif (mem->alloc_flags &\n\t    (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t     KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tamdgpu_amdkfd_gpuvm_unpin_bo(mem->bo);\n\t}\n\n\tmapped_to_gpu_memory = mem->mapped_to_gpu_memory;\n\tis_imported = mem->is_imported;\n\tmutex_unlock(&mem->lock);\n\t/* lock is not needed after this, since mem is unused and will\n\t * be freed anyway\n\t */\n\n\tif (mapped_to_gpu_memory > 0) {\n\t\tpr_debug(\"BO VA 0x%llx size 0x%lx is still mapped.\\n\",\n\t\t\t\tmem->va, bo_size);\n\t\treturn -EBUSY;\n\t}\n\n\t/* Make sure restore workers don't access the BO any more */\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n\n\t/* Cleanup user pages and MMU notifiers */\n\tif (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\tamdgpu_hmm_unregister(mem->bo);\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tamdgpu_ttm_tt_discard_user_pages(mem->bo->tbo.ttm, mem->range);\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t}\n\n\tret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\tprocess_info->eviction_fence);\n\tpr_debug(\"Release VA 0x%llx - 0x%llx\\n\", mem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue));\n\n\t/* Remove from VM internal data structures */\n\tlist_for_each_entry_safe(entry, tmp, &mem->attachments, list) {\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t\tkfd_mem_detach(entry);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\t/* Free the sync object */\n\tamdgpu_sync_free(&mem->sync);\n\n\t/* If the SG is not NULL, it's one we created for a doorbell or mmio\n\t * remap BO. We need to free it.\n\t */\n\tif (mem->bo->tbo.sg) {\n\t\tsg_free_table(mem->bo->tbo.sg);\n\t\tkfree(mem->bo->tbo.sg);\n\t}\n\n\t/* Update the size of the BO being freed if it was allocated from\n\t * VRAM and is not imported. For APP APU VRAM allocations are done\n\t * in GTT domain\n\t */\n\tif (size) {\n\t\tif (!is_imported &&\n\t\t   (mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM ||\n\t\t   ((adev->flags & AMD_IS_APU) &&\n\t\t    mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT)))\n\t\t\t*size = bo_size;\n\t\telse\n\t\t\t*size = 0;\n\t}\n\n\t/* Free the BO*/\n\tdrm_vma_node_revoke(&mem->bo->tbo.base.vma_node, drm_priv);\n\tdrm_gem_handle_delete(adev->kfd.client.file, mem->gem_handle);\n\tif (mem->dmabuf) {\n\t\tdma_buf_put(mem->dmabuf);\n\t\tmem->dmabuf = NULL;\n\t}\n\tmutex_destroy(&mem->lock);\n\n\t/* If this releases the last reference, it will end up calling\n\t * amdgpu_amdkfd_release_notify and kfree the mem struct. That's why\n\t * this needs to be the last call here.\n\t */\n\tdrm_gem_object_put(&mem->bo->tbo.base);\n\n\t/*\n\t * For kgd_mem allocated in amdgpu_amdkfd_gpuvm_import_dmabuf(),\n\t * explicitly free it here.\n\t */\n\tif (!use_release_notifier)\n\t\tkfree(mem);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tvoid *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tint ret;\n\tstruct amdgpu_bo *bo;\n\tuint32_t domain;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned long bo_size;\n\tbool is_invalid_userptr = false;\n\n\tbo = mem->bo;\n\tif (!bo) {\n\t\tpr_err(\"Invalid BO when mapping memory to GPU\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Make sure restore is not running concurrently. Since we\n\t * don't map invalid userptr BOs, we rely on the next restore\n\t * worker to do the mapping\n\t */\n\tmutex_lock(&mem->process_info->lock);\n\n\t/* Lock notifier lock. If we find an invalid userptr BO, we can be\n\t * sure that the MMU notifier is no longer running\n\t * concurrently and the queues are actually stopped\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tmutex_lock(&mem->process_info->notifier_lock);\n\t\tis_invalid_userptr = !!mem->invalid;\n\t\tmutex_unlock(&mem->process_info->notifier_lock);\n\t}\n\n\tmutex_lock(&mem->lock);\n\n\tdomain = mem->domain;\n\tbo_size = bo->tbo.base.size;\n\n\tpr_debug(\"Map VA 0x%llx - 0x%llx to vm %p domain %s\\n\",\n\t\t\tmem->va,\n\t\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\t\tavm, domain_string(domain));\n\n\tif (!kfd_mem_is_attached(avm, mem)) {\n\t\tret = kfd_mem_attach(adev, mem, avm, mem->aql_queue);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = reserve_bo_and_vm(mem, avm, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\t/* Userptr can be marked as \"not invalid\", but not actually be\n\t * validated yet (still in the system domain). In that case\n\t * the queues are still stopped and we can leave mapping for\n\t * the next restore worker\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_SYSTEM)\n\t\tis_invalid_userptr = true;\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto out_unreserve;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t map VA 0x%llx - 0x%llx in entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = map_bo_to_gpuvm(mem, entry, ctx.sync,\n\t\t\t\t      is_invalid_userptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map bo to gpuvm\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tret = vm_update_pds(avm, ctx.sync);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to update page directories\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tentry->is_mapped = true;\n\t\tmem->mapped_to_gpu_memory++;\n\t\tpr_debug(\"\\t INC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\tgoto out;\n\nout_unreserve:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->process_info->lock);\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_dmaunmap_mem(struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_vm *vm;\n\tint ret;\n\n\tvm = drm_priv_to_vm(drm_priv);\n\n\tmutex_lock(&mem->lock);\n\n\tret = amdgpu_bo_reserve(mem->bo, true);\n\tif (ret)\n\t\tgoto out;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != vm)\n\t\t\tcontinue;\n\t\tif (entry->bo_va->base.bo->tbo.ttm &&\n\t\t    !entry->bo_va->base.bo->tbo.ttm->sg)\n\t\t\tcontinue;\n\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t}\n\n\tamdgpu_bo_unreserve(mem->bo);\nout:\n\tmutex_unlock(&mem->lock);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\n\tret = reserve_bo_and_cond_vms(mem, avm, BO_VM_MAPPED, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\t/* If no VMs were reserved, it means the BO wasn't actually mapped */\n\tif (ctx.n_vms == 0) {\n\t\tret = -EINVAL;\n\t\tgoto unreserve_out;\n\t}\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto unreserve_out;\n\n\tpr_debug(\"Unmap VA 0x%llx - 0x%llx from vm %p\\n\",\n\t\tmem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\tavm);\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || !entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t unmap VA 0x%llx - 0x%llx from entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = unmap_bo_from_gpuvm(mem, entry, ctx.sync);\n\t\tif (ret)\n\t\t\tgoto unreserve_out;\n\n\t\tentry->is_mapped = false;\n\n\t\tmem->mapped_to_gpu_memory--;\n\t\tpr_debug(\"\\t DEC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\nunreserve_out:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_sync_memory(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tmutex_lock(&mem->lock);\n\tamdgpu_sync_clone(&mem->sync, &sync);\n\tmutex_unlock(&mem->lock);\n\n\tret = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_map_gtt_bo_to_gart - Map BO to GART and increment reference count\n * @bo: Buffer object to be mapped\n * @bo_gart: Return bo reference\n *\n * Before return, bo reference count is incremented. To release the reference and unpin/\n * unmap the BO, call amdgpu_amdkfd_free_gtt_mem.\n */\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo, struct amdgpu_bo **bo_gart)\n{\n\tint ret;\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto err_reserve_bo_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto err_pin_bo_failed;\n\t}\n\n\tret = amdgpu_ttm_alloc_gart(&bo->tbo);\n\tif (ret) {\n\t\tpr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\n\t\tgoto err_map_bo_gart_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, bo->vm_bo->vm->process_info->eviction_fence);\n\n\tamdgpu_bo_unreserve(bo);\n\n\t*bo_gart = amdgpu_bo_ref(bo);\n\n\treturn 0;\n\nerr_map_bo_gart_failed:\n\tamdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\n\tamdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Map a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be mapped for CPU access\n * @kptr[out]: pointer in kernel CPU address space\n * @size[out]: size of the buffer\n *\n * Pins the BO and maps it for kernel CPU access. The eviction fence is removed\n * from the BO, since pinned BOs cannot be evicted. The bo must remain on the\n * validate_list, so the GPU mapping can be restored after a page table was\n * evicted.\n *\n * Return: 0 on success, error code on failure\n */\nint amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_mem *mem,\n\t\t\t\t\t     void **kptr, uint64_t *size)\n{\n\tint ret;\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tpr_err(\"userptr can't be mapped to kernel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&mem->process_info->lock);\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto bo_reserve_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto pin_failed;\n\t}\n\n\tret = amdgpu_bo_kmap(bo, kptr);\n\tif (ret) {\n\t\tpr_err(\"Failed to map bo to kernel. ret %d\\n\", ret);\n\t\tgoto kmap_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, mem->process_info->eviction_fence);\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tmutex_unlock(&mem->process_info->lock);\n\treturn 0;\n\nkmap_failed:\n\tamdgpu_bo_unpin(bo);\npin_failed:\n\tamdgpu_bo_unreserve(bo);\nbo_reserve_failed:\n\tmutex_unlock(&mem->process_info->lock);\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Unmap a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be unmapped for CPU access\n *\n * Removes the kernel CPU mapping and unpins the BO. It does not restore the\n * eviction fence, so this function should only be used for cleanup before the\n * BO is destroyed.\n */\nvoid amdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(struct kgd_mem *mem)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct kfd_vm_fault_info *mem)\n{\n\tif (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {\n\t\t*mem = *adev->gmc.vm_fault_info;\n\t\tmb(); /* make sure read happened */\n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\t}\n\treturn 0;\n}\n\nstatic int import_obj_create(struct amdgpu_device *adev,\n\t\t\t     struct dma_buf *dma_buf,\n\t\t\t     struct drm_gem_object *obj,\n\t\t\t     uint64_t va, void *drm_priv,\n\t\t\t     struct kgd_mem **mem, uint64_t *size,\n\t\t\t     uint64_t *mmap_offset)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tbo = gem_to_amdgpu_bo(obj);\n\tif (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT)))\n\t\t/* Only VRAM and GTT BOs are supported */\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tret = drm_vma_node_allow(&obj->vma_node, drm_priv);\n\tif (ret)\n\t\tgoto err_free_mem;\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tif (mmap_offset)\n\t\t*mmap_offset = amdgpu_bo_mmap_offset(bo);\n\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\n\t(*mem)->alloc_flags =\n\t\t((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?\n\t\tKFD_IOC_ALLOC_MEM_FLAGS_VRAM : KFD_IOC_ALLOC_MEM_FLAGS_GTT)\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\n\tget_dma_buf(dma_buf);\n\t(*mem)->dmabuf = dma_buf;\n\t(*mem)->bo = bo;\n\t(*mem)->va = va;\n\t(*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&\n\t\t\t !(adev->flags & AMD_IS_APU) ?\n\t\t\t AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;\n\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\t(*mem)->is_imported = true;\n\n\tmutex_lock(&avm->process_info->lock);\n\tif (avm->process_info->eviction_fence &&\n\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, (*mem)->domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\tmutex_unlock(&avm->process_info->lock);\n\tif (ret)\n\t\tgoto err_remove_mem;\n\n\treturn 0;\n\nerr_remove_mem:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_vma_node_revoke(&obj->vma_node, drm_priv);\nerr_free_mem:\n\tkfree(*mem);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n\t\t\t\t\t uint64_t va, void *drm_priv,\n\t\t\t\t\t struct kgd_mem **mem, uint64_t *size,\n\t\t\t\t\t uint64_t *mmap_offset)\n{\n\tstruct drm_gem_object *obj;\n\tuint32_t handle;\n\tint ret;\n\n\tret = drm_gem_prime_fd_to_handle(&adev->ddev, adev->kfd.client.file, fd,\n\t\t\t\t\t &handle);\n\tif (ret)\n\t\treturn ret;\n\tobj = drm_gem_object_lookup(adev->kfd.client.file, handle);\n\tif (!obj) {\n\t\tret = -EINVAL;\n\t\tgoto err_release_handle;\n\t}\n\n\tret = import_obj_create(adev, obj->dma_buf, obj, va, drm_priv, mem, size,\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\nerr_release_handle:\n\tdrm_gem_handle_delete(adev->kfd.client.file, handle);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_mem *mem,\n\t\t\t\t      struct dma_buf **dma_buf)\n{\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\tgoto out;\n\n\tget_dma_buf(mem->dmabuf);\n\t*dma_buf = mem->dmabuf;\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\n/* Evict a userptr BO by stopping the queues if necessary\n *\n * Runs in MMU notifier, may be in RECLAIM_FS context. This means it\n * cannot do any memory allocations, and cannot take any locks that\n * are held elsewhere while allocating memory.\n *\n * It doesn't do anything to the BO itself. The real work happens in\n * restore, where we get updated page addresses. This function only\n * ensures that GPU access to the BO is stopped.\n */\nint amdgpu_amdkfd_evict_userptr(struct mmu_interval_notifier *mni,\n\t\t\t\tunsigned long cur_seq, struct kgd_mem *mem)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tint r = 0;\n\n\t/* Do not process MMU notifications during CRIU restore until\n\t * KFD_CRIU_OP_RESUME IOCTL is received\n\t */\n\tif (READ_ONCE(process_info->block_mmu_notifications))\n\t\treturn 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmem->invalid++;\n\tif (++process_info->evicted_bos == 1) {\n\t\t/* First eviction, stop the queues */\n\t\tr = kgd2kfd_quiesce_mm(mni->mm,\n\t\t\t\t       KFD_QUEUE_EVICTION_TRIGGER_USERPTR);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to quiesce KFD\\n\");\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\t}\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn r;\n}\n\n/* Update invalid userptr BOs\n *\n * Moves invalidated (evicted) userptr BOs from userptr_valid_list to\n * userptr_inval_list and updates user pages for all BOs that have\n * been invalidated since their last update.\n */\nstatic int update_invalid_user_pages(struct amdkfd_process_info *process_info,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tuint32_t invalid;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\n\t/* Move all invalidated BOs to the userptr_inval_list */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_valid_list,\n\t\t\t\t validate_list)\n\t\tif (mem->invalid)\n\t\t\tlist_move_tail(&mem->validate_list,\n\t\t\t\t       &process_info->userptr_inval_list);\n\n\t/* Go through userptr_inval_list and update any invalid user_pages */\n\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t    validate_list) {\n\t\tinvalid = mem->invalid;\n\t\tif (!invalid)\n\t\t\t/* BO hasn't been invalidated since the last\n\t\t\t * revalidation attempt. Keep its page list.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\tbo = mem->bo;\n\n\t\tamdgpu_ttm_tt_discard_user_pages(bo->tbo.ttm, mem->range);\n\t\tmem->range = NULL;\n\n\t\t/* BO reservations and getting user pages (hmm_range_fault)\n\t\t * must happen outside the notifier lock\n\t\t */\n\t\tmutex_unlock(&process_info->notifier_lock);\n\n\t\t/* Move the BO to system (CPU) domain if necessary to unmap\n\t\t * and free the SG table\n\t\t */\n\t\tif (bo->tbo.resource->mem_type != TTM_PL_SYSTEM) {\n\t\t\tif (amdgpu_bo_reserve(bo, true))\n\t\t\t\treturn -EAGAIN;\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Failed to invalidate userptr BO\\n\",\n\t\t\t\t       __func__);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\t/* Get updated user pages */\n\t\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t   &mem->range);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed %d to get user pages\\n\", ret);\n\n\t\t\t/* Return -EFAULT bad address error as success. It will\n\t\t\t * fail later with a VM fault if the GPU tries to access\n\t\t\t * it. Better than hanging indefinitely with stalled\n\t\t\t * user mode queues.\n\t\t\t *\n\t\t\t * Return other error -EBUSY or -ENOMEM to retry restore\n\t\t\t */\n\t\t\tif (ret != -EFAULT)\n\t\t\t\treturn ret;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\tmutex_lock(&process_info->notifier_lock);\n\n\t\t/* Mark the BO as valid unless it was invalidated\n\t\t * again concurrently.\n\t\t */\n\t\tif (mem->invalid != invalid) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t /* set mem valid if mem has hmm range associated */\n\t\tif (mem->range)\n\t\t\tmem->invalid = 0;\n\t}\n\nunlock_out:\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn ret;\n}\n\n/* Validate invalid userptr BOs\n *\n * Validates BOs on the userptr_inval_list. Also updates GPUVM page tables\n * with new page addresses and waits for the page table updates to complete.\n */\nstatic int validate_invalid_user_pages(struct amdkfd_process_info *process_info)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0, 0);\n\t/* Reserve all BOs and page tables for validation */\n\tdrm_exec_until_all_locked(&exec) {\n\t\t/* Reserve all the page directories */\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\t/* Reserve the userptr_inval_list entries to resv_list */\n\t\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\tret = process_validate_vms(process_info, NULL);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\t/* Validate BOs and update GPUVM page tables */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tbo = mem->bo;\n\n\t\t/* Validate the BO if we got user pages */\n\t\tif (bo->tbo.ttm->pages[0]) {\n\t\t\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\n\t\t/* Update mapping. If the BO was not validated\n\t\t * (because we couldn't get user pages), this will\n\t\t * clear the page table entries, which will result in\n\t\t * VM faults if the GPU tries to access the invalid\n\t\t * memory.\n\t\t */\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: update PTE failed\\n\", __func__);\n\t\t\t\t/* make sure this gets validated again */\n\t\t\t\tmutex_lock(&process_info->notifier_lock);\n\t\t\t\tmem->invalid++;\n\t\t\t\tmutex_unlock(&process_info->notifier_lock);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync);\n\nunreserve_out:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_wait(&sync, false);\n\tamdgpu_sync_free(&sync);\n\n\treturn ret;\n}\n\n/* Confirm that all user pages are valid while holding the notifier lock\n *\n * Moves valid BOs from the userptr_inval_list back to userptr_val_list.\n */\nstatic int confirm_valid_user_pages_locked(struct amdkfd_process_info *process_info)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tbool valid;\n\n\t\t/* keep mem without hmm range at userptr_inval_list */\n\t\tif (!mem->range)\n\t\t\tcontinue;\n\n\t\t/* Only check mem with hmm range associated */\n\t\tvalid = amdgpu_ttm_tt_get_user_pages_done(\n\t\t\t\t\tmem->bo->tbo.ttm, mem->range);\n\n\t\tmem->range = NULL;\n\t\tif (!valid) {\n\t\t\tWARN(!mem->invalid, \"Invalid BO not marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem->invalid) {\n\t\t\tWARN(1, \"Valid BO is marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move_tail(&mem->validate_list,\n\t\t\t       &process_info->userptr_valid_list);\n\t}\n\n\treturn ret;\n}\n\n/* Worker callback to restore evicted userptr BOs\n *\n * Tries to update and validate all userptr BOs. If successful and no\n * concurrent evictions happened, the queues are restarted. Otherwise,\n * reschedule for another attempt later.\n */\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info =\n\t\tcontainer_of(dwork, struct amdkfd_process_info,\n\t\t\t     restore_userptr_work);\n\tstruct task_struct *usertask;\n\tstruct mm_struct *mm;\n\tuint32_t evicted_bos;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tevicted_bos = process_info->evicted_bos;\n\tmutex_unlock(&process_info->notifier_lock);\n\tif (!evicted_bos)\n\t\treturn;\n\n\t/* Reference task and mm in case of concurrent process termination */\n\tusertask = get_pid_task(process_info->pid, PIDTYPE_PID);\n\tif (!usertask)\n\t\treturn;\n\tmm = get_task_mm(usertask);\n\tif (!mm) {\n\t\tput_task_struct(usertask);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\n\tif (update_invalid_user_pages(process_info, mm))\n\t\tgoto unlock_out;\n\t/* userptr_inval_list can be empty if all evicted userptr BOs\n\t * have been freed. In that case there is nothing to validate\n\t * and we can just restart the queues.\n\t */\n\tif (!list_empty(&process_info->userptr_inval_list)) {\n\t\tif (validate_invalid_user_pages(process_info))\n\t\t\tgoto unlock_out;\n\t}\n\t/* Final check for concurrent evicton and atomic update. If\n\t * another eviction happens after successful update, it will\n\t * be a first eviction that calls quiesce_mm. The eviction\n\t * reference counting inside KFD will handle this case.\n\t */\n\tmutex_lock(&process_info->notifier_lock);\n\tif (process_info->evicted_bos != evicted_bos)\n\t\tgoto unlock_notifier_out;\n\n\tif (confirm_valid_user_pages_locked(process_info)) {\n\t\tWARN(1, \"User pages unexpectedly invalid\");\n\t\tgoto unlock_notifier_out;\n\t}\n\n\tprocess_info->evicted_bos = evicted_bos = 0;\n\n\tif (kgd2kfd_resume_mm(mm)) {\n\t\tpr_err(\"%s: Failed to resume KFD\\n\", __func__);\n\t\t/* No recovery from this failure. Probably the CP is\n\t\t * hanging. No point trying again.\n\t\t */\n\t}\n\nunlock_notifier_out:\n\tmutex_unlock(&process_info->notifier_lock);\nunlock_out:\n\tmutex_unlock(&process_info->lock);\n\n\t/* If validation failed, reschedule another attempt */\n\tif (evicted_bos) {\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n\tput_task_struct(usertask);\n}\n\nstatic void replace_eviction_fence(struct dma_fence __rcu **ef,\n\t\t\t\t   struct dma_fence *new_ef)\n{\n\tstruct dma_fence *old_ef = rcu_replace_pointer(*ef, new_ef, true\n\t\t/* protected by process_info->lock */);\n\n\t/* If we're replacing an unsignaled eviction fence, that fence will\n\t * never be signaled, and if anyone is still waiting on that fence,\n\t * they will hang forever. This should never happen. We should only\n\t * replace the fence in restore_work that only gets scheduled after\n\t * eviction work signaled the fence.\n\t */\n\tWARN_ONCE(!dma_fence_is_signaled(old_ef),\n\t\t  \"Replacing unsignaled eviction fence\");\n\tdma_fence_put(old_ef);\n}\n\n/** amdgpu_amdkfd_gpuvm_restore_process_bos - Restore all BOs for the given\n *   KFD process identified by process_info\n *\n * @process_info: amdkfd_process_info of the KFD process\n *\n * After memory eviction, restore thread calls this function. The function\n * should be called when the Process is still valid. BO restore involves -\n *\n * 1.  Release old eviction fence and create new one\n * 2.  Get two copies of PD BO list from all the VMs. Keep one copy as pd_list.\n * 3   Use the second PD list and kfd_bo_list to create a list (ctx.list) of\n *     BOs that need to be reserved.\n * 4.  Reserve all the BOs\n * 5.  Validate of PD and PT BOs.\n * 6.  Validate all KFD BOs using kfd_bo_list and Map them and add new fence\n * 7.  Add fence to all PD and PT BOs.\n * 8.  Unreserve all BOs\n */\nint amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct dma_fence __rcu **ef)\n{\n\tstruct amdkfd_process_info *process_info = info;\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem;\n\tstruct list_head duplicate_save;\n\tstruct amdgpu_sync sync_obj;\n\tunsigned long failed_size = 0;\n\tunsigned long total_size = 0;\n\tstruct drm_exec exec;\n\tint ret;\n\n\tINIT_LIST_HEAD(&duplicate_save);\n\n\tmutex_lock(&process_info->lock);\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"Locking VM PD failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Reserve all BOs and page tables/directory. Add all BOs from\n\t\t * kfd_bo_list to ctx.list\n\t\t */\n\t\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"drm_exec_prepare_obj failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tamdgpu_sync_create(&sync_obj);\n\n\t/* Validate BOs managed by KFD */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\n\t\tstruct amdgpu_bo *bo = mem->bo;\n\t\tuint32_t domain = mem->domain;\n\t\tstruct dma_resv_iter cursor;\n\t\tstruct dma_fence *fence;\n\n\t\ttotal_size += amdgpu_bo_size(bo);\n\n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: Validate BOs failed\\n\");\n\t\t\tfailed_size += amdgpu_bo_size(bo);\n\t\t\tret = amdgpu_amdkfd_bo_validate(bo,\n\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT, false);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tdma_resv_for_each_fence(&cursor, bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_KERNEL, fence) {\n\t\t\tret = amdgpu_sync_fence(&sync_obj, fence);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Sync BO fence failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (failed_size)\n\t\tpr_debug(\"0x%lx/0x%lx in system\\n\", failed_size, total_size);\n\n\t/* Validate PDs, PTs and evicted DMABuf imports last. Otherwise BO\n\t * validations above would invalidate DMABuf imports again.\n\t */\n\tret = process_validate_vms(process_info, &exec.ticket);\n\tif (ret) {\n\t\tpr_debug(\"Validating VMs failed, ret: %d\\n\", ret);\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Update mappings managed by KFD. */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync_obj);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: update PTE failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update mappings not managed by KFD */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\tvm_list_node) {\n\t\tstruct amdgpu_device *adev = amdgpu_ttm_adev(\n\t\t\tpeer_vm->root.bo->tbo.bdev);\n\n\t\tret = amdgpu_vm_handle_moved(adev, peer_vm, &exec.ticket);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: handle moved failed. Try again\\n\");\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: update PDs failed. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Sync with fences on all the page tables. They implicitly depend on any\n\t * move fences from amdgpu_vm_handle_moved above.\n\t */\n\tret = process_sync_pds_resv(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: Failed to sync to PD BO moving fence. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Wait for validate and PT updates to finish */\n\tamdgpu_sync_wait(&sync_obj, false);\n\n\t/* The old eviction fence may be unsignaled if restore happens\n\t * after a GPU reset or suspend/resume. Keep the old fence in that\n\t * case. Otherwise release the old eviction fence and create new\n\t * one, because fence only goes from unsignaled to signaled once\n\t * and cannot be reused. Use context and mm from the old fence.\n\t *\n\t * If an old eviction fence signals after this check, that's OK.\n\t * Anyone signaling an eviction fence must stop the queues first\n\t * and schedule another restore worker.\n\t */\n\tif (dma_fence_is_signaled(&process_info->eviction_fence->base)) {\n\t\tstruct amdgpu_amdkfd_fence *new_fence =\n\t\t\tamdgpu_amdkfd_fence_create(\n\t\t\t\tprocess_info->eviction_fence->base.context,\n\t\t\t\tprocess_info->eviction_fence->mm,\n\t\t\t\tNULL);\n\n\t\tif (!new_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tprocess_info->eviction_fence = new_fence;\n\t\treplace_eviction_fence(ef, dma_fence_get(&new_fence->base));\n\t} else {\n\t\tWARN_ONCE(*ef != &process_info->eviction_fence->base,\n\t\t\t  \"KFD eviction fence doesn't match KGD process_info\");\n\t}\n\n\t/* Attach new eviction fence to all BOs except pinned ones */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list, validate_list) {\n\t\tif (mem->bo->tbo.pin_count)\n\t\t\tcontinue;\n\n\t\tdma_resv_add_fence(mem->bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\t/* Attach eviction fence to PD / PT BOs and DMABuf imports */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *bo = peer_vm->root.bo;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\nvalidate_map_fail:\n\tamdgpu_sync_free(&sync_obj);\nttm_reserve_fail:\n\tdrm_exec_fini(&exec);\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_add_gws_to_process(void *info, void *gws, struct kgd_mem **mem)\n{\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct amdgpu_bo *gws_bo = (struct amdgpu_bo *)gws;\n\tint ret;\n\n\tif (!info || !gws)\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&(*mem)->lock);\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\t(*mem)->bo = amdgpu_bo_ref(gws_bo);\n\t(*mem)->domain = AMDGPU_GEM_DOMAIN_GWS;\n\t(*mem)->process_info = process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\n\t/* Validate gws bo the first time it is added to process */\n\tmutex_lock(&(*mem)->process_info->lock);\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\tgoto bo_reservation_failure;\n\t}\n\n\tret = amdgpu_amdkfd_bo_validate(gws_bo, AMDGPU_GEM_DOMAIN_GWS, true);\n\tif (ret) {\n\t\tpr_err(\"GWS BO validate failed %d\\n\", ret);\n\t\tgoto bo_validation_failure;\n\t}\n\t/* GWS resource is shared b/t amdgpu and amdkfd\n\t * Add process eviction fence to bo so they can\n\t * evict each other.\n\t */\n\tret = dma_resv_reserve_fences(gws_bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(gws_bo->tbo.base.resv,\n\t\t\t   &process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(gws_bo);\n\tmutex_unlock(&(*mem)->process_info->lock);\n\n\treturn ret;\n\nreserve_shared_fail:\nbo_validation_failure:\n\tamdgpu_bo_unreserve(gws_bo);\nbo_reservation_failure:\n\tmutex_unlock(&(*mem)->process_info->lock);\n\tamdgpu_sync_free(&(*mem)->sync);\n\tremove_kgd_mem_from_kfd_bo_list(*mem, process_info);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&(*mem)->lock);\n\tkfree(*mem);\n\t*mem = NULL;\n\treturn ret;\n}\n\nint amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)\n{\n\tint ret;\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\tstruct amdgpu_bo *gws_bo = kgd_mem->bo;\n\n\t/* Remove BO from process's validate list so restore worker won't touch\n\t * it anymore\n\t */\n\tremove_kgd_mem_from_kfd_bo_list(kgd_mem, process_info);\n\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\t//TODO add BO back to validate_list?\n\t\treturn ret;\n\t}\n\tamdgpu_amdkfd_remove_eviction_fence(gws_bo,\n\t\t\tprocess_info->eviction_fence);\n\tamdgpu_bo_unreserve(gws_bo);\n\tamdgpu_sync_free(&kgd_mem->sync);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&kgd_mem->lock);\n\tkfree(mem);\n\treturn 0;\n}\n\n/* Returns GPU-specific tiling mode information */\nint amdgpu_amdkfd_get_tile_config(struct amdgpu_device *adev,\n\t\t\t\tstruct tile_config *config)\n{\n\tconfig->gb_addr_config = adev->gfx.config.gb_addr_config;\n\tconfig->tile_config_ptr = adev->gfx.config.tile_mode_array;\n\tconfig->num_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.tile_mode_array);\n\tconfig->macro_tile_config_ptr =\n\t\t\tadev->gfx.config.macrotile_mode_array;\n\tconfig->num_macro_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.macrotile_mode_array);\n\n\t/* Those values are not set from GFX9 onwards */\n\tconfig->num_banks = adev->gfx.config.num_banks;\n\tconfig->num_ranks = adev->gfx.config.num_ranks;\n\n\treturn 0;\n}\n\nbool amdgpu_amdkfd_bo_mapped_to_dev(void *drm_priv, struct kgd_mem *mem)\n{\n\tstruct amdgpu_vm *vm = drm_priv_to_vm(drm_priv);\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->is_mapped && entry->bo_va->base.vm == vm)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_kfd_mem_limits(struct seq_file *m, void *data)\n{\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tseq_printf(m, \"System mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.system_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_system_mem_limit >> 20));\n\tseq_printf(m, \"TTM mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.ttm_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_ttm_mem_limit >> 20));\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\treturn 0;\n}\n\n#endif\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/buffer-overflow1-bug.txt", "bug_report_text": "Integer Overflow in vi_read_bios_from_rom\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/vi.c\nLine: 651\nLocation: vi_read_bios_from_rom function\nDescription\nAn integer overflow risk has been identified in the vi_read_bios_from_rom function. \nThe variable length_dw, defined as a u8, is assigned the result of an ALIGN macro operation, \nwhich can yield values exceeding 255 when certain length_bytes inputs are provided.\n\nu32 length_dw = ALIGN(length_bytes, 4);\n\n", "diff_path": "dataset/raw_data/bugs/dev-set/buffer-overflow1-diff.txt", "diff_text": "diff --git a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/vi.c b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/vi.c\nindex d28d6b9..00851a4 100644\n--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/vi.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/vi.c\n@@ -648,7 +648,7 @@ static bool vi_read_bios_from_rom(struct amdgpu_device *adev,\n                return false;\n\n        dw_ptr = (u32 *)bios;\n-       u8 length_dw = ALIGN(length_bytes, 4);\n+       u32 length_dw = ALIGN(length_bytes, 4);\n        /* take the smc lock since we are using the smc index */\n        spin_lock_irqsave(&adev->smc_idx_lock, flags);\n        /* set rom index to 0 */", "source_code_path": "drivers/gpu/drm/amd/amdgpu/vi.c", "line_number": 651, "code": "/*\n * Copyright 2014 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/slab.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"amdgpu_ucode.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n\n#include \"gmc/gmc_8_1_d.h\"\n#include \"gmc/gmc_8_1_sh_mask.h\"\n\n#include \"oss/oss_3_0_d.h\"\n#include \"oss/oss_3_0_sh_mask.h\"\n\n#include \"bif/bif_5_0_d.h\"\n#include \"bif/bif_5_0_sh_mask.h\"\n\n#include \"gca/gfx_8_0_d.h\"\n#include \"gca/gfx_8_0_sh_mask.h\"\n\n#include \"smu/smu_7_1_1_d.h\"\n#include \"smu/smu_7_1_1_sh_mask.h\"\n\n#include \"uvd/uvd_5_0_d.h\"\n#include \"uvd/uvd_5_0_sh_mask.h\"\n\n#include \"vce/vce_3_0_d.h\"\n#include \"vce/vce_3_0_sh_mask.h\"\n\n#include \"dce/dce_10_0_d.h\"\n#include \"dce/dce_10_0_sh_mask.h\"\n\n#include \"vid.h\"\n#include \"vi.h\"\n#include \"gmc_v8_0.h\"\n#include \"gmc_v7_0.h\"\n#include \"gfx_v8_0.h\"\n#include \"sdma_v2_4.h\"\n#include \"sdma_v3_0.h\"\n#include \"dce_v10_0.h\"\n#include \"dce_v11_0.h\"\n#include \"iceland_ih.h\"\n#include \"tonga_ih.h\"\n#include \"cz_ih.h\"\n#include \"uvd_v5_0.h\"\n#include \"uvd_v6_0.h\"\n#include \"vce_v3_0.h\"\n#if defined(CONFIG_DRM_AMD_ACP)\n#include \"amdgpu_acp.h\"\n#endif\n#include \"amdgpu_vkms.h\"\n#include \"mxgpu_vi.h\"\n#include \"amdgpu_dm.h\"\n\n#define ixPCIE_LC_L1_PM_SUBSTATE\t0x100100C6\n#define PCIE_LC_L1_PM_SUBSTATE__LC_L1_SUBSTATES_OVERRIDE_EN_MASK\t0x00000001L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_2_OVERRIDE_MASK\t0x00000002L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_1_OVERRIDE_MASK\t0x00000004L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_2_OVERRIDE_MASK\t\t0x00000008L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_1_OVERRIDE_MASK\t\t0x00000010L\n#define ixPCIE_L1_PM_SUB_CNTL\t0x378\n#define PCIE_L1_PM_SUB_CNTL__ASPM_L1_2_EN_MASK\t0x00000004L\n#define PCIE_L1_PM_SUB_CNTL__ASPM_L1_1_EN_MASK\t0x00000008L\n#define PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_2_EN_MASK\t0x00000001L\n#define PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_1_EN_MASK\t0x00000002L\n#define PCIE_LC_CNTL6__LC_L1_POWERDOWN_MASK\t\t0x00200000L\n#define LINK_CAP\t0x64\n#define PCIE_LINK_CAP__CLOCK_POWER_MANAGEMENT_MASK\t0x00040000L\n#define ixCPM_CONTROL\t0x1400118\n#define ixPCIE_LC_CNTL7\t0x100100BC\n#define PCIE_LC_CNTL7__LC_L1_SIDEBAND_CLKREQ_PDWN_EN_MASK\t0x00000400L\n#define PCIE_LC_CNTL__LC_L0S_INACTIVITY_DEFAULT\t0x00000007\n#define PCIE_LC_CNTL__LC_L1_INACTIVITY_DEFAULT\t0x00000009\n#define CPM_CONTROL__CLKREQb_UNGATE_TXCLK_ENABLE_MASK\t0x01000000L\n#define PCIE_L1_PM_SUB_CNTL\t0x378\n#define ASIC_IS_P22(asic_type, rid)\t((asic_type >= CHIP_POLARIS10) && \\\n\t\t\t\t\t\t\t\t\t(asic_type <= CHIP_POLARIS12) && \\\n\t\t\t\t\t\t\t\t\t(rid >= 0x6E))\n/* Topaz */\nstatic const struct amdgpu_video_codecs topaz_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* Tonga, CZ, ST, Fiji */\nstatic const struct amdgpu_video_codec_info tonga_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tonga_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tonga_video_codecs_encode_array),\n\t.codec_array = tonga_video_codecs_encode_array,\n};\n\n/* Polaris */\nstatic const struct amdgpu_video_codec_info polaris_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs polaris_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(polaris_video_codecs_encode_array),\n\t.codec_array = polaris_video_codecs_encode_array,\n};\n\n/* Topaz */\nstatic const struct amdgpu_video_codecs topaz_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* Tonga */\nstatic const struct amdgpu_video_codec_info tonga_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 52,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tonga_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tonga_video_codecs_decode_array),\n\t.codec_array = tonga_video_codecs_decode_array,\n};\n\n/* CZ, ST, Fiji, Polaris */\nstatic const struct amdgpu_video_codec_info cz_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 52,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 4,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 186,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_JPEG,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs cz_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(cz_video_codecs_decode_array),\n\t.codec_array = cz_video_codecs_decode_array,\n};\n\nstatic int vi_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tif (encode)\n\t\t\t*codecs = &topaz_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &topaz_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_TONGA:\n\t\tif (encode)\n\t\t\t*codecs = &tonga_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tonga_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tif (encode)\n\t\t\t*codecs = &polaris_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &cz_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_FIJI:\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tif (encode)\n\t\t\t*codecs = &tonga_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &cz_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/*\n * Indirect registers accessor\n */\nstatic u32 vi_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32_NO_KIQ(mmPCIE_INDEX, reg);\n\t(void)RREG32_NO_KIQ(mmPCIE_INDEX);\n\tr = RREG32_NO_KIQ(mmPCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32_NO_KIQ(mmPCIE_INDEX, reg);\n\t(void)RREG32_NO_KIQ(mmPCIE_INDEX);\n\tWREG32_NO_KIQ(mmPCIE_DATA, v);\n\t(void)RREG32_NO_KIQ(mmPCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 vi_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32_NO_KIQ(mmSMC_IND_INDEX_11, (reg));\n\tr = RREG32_NO_KIQ(mmSMC_IND_DATA_11);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32_NO_KIQ(mmSMC_IND_INDEX_11, (reg));\n\tWREG32_NO_KIQ(mmSMC_IND_DATA_11, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\n/* smu_8_0_d.h */\n#define mmMP0PUB_IND_INDEX                                                      0x180\n#define mmMP0PUB_IND_DATA                                                       0x181\n\nstatic u32 cz_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(mmMP0PUB_IND_INDEX, (reg));\n\tr = RREG32(mmMP0PUB_IND_DATA);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void cz_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(mmMP0PUB_IND_INDEX, (reg));\n\tWREG32(mmMP0PUB_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 vi_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic u32 vi_didt_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->didt_idx_lock, flags);\n\tWREG32(mmDIDT_IND_INDEX, (reg));\n\tr = RREG32(mmDIDT_IND_DATA);\n\tspin_unlock_irqrestore(&adev->didt_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_didt_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->didt_idx_lock, flags);\n\tWREG32(mmDIDT_IND_INDEX, (reg));\n\tWREG32(mmDIDT_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->didt_idx_lock, flags);\n}\n\nstatic u32 vi_gc_cac_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->gc_cac_idx_lock, flags);\n\tWREG32(mmGC_CAC_IND_INDEX, (reg));\n\tr = RREG32(mmGC_CAC_IND_DATA);\n\tspin_unlock_irqrestore(&adev->gc_cac_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_gc_cac_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->gc_cac_idx_lock, flags);\n\tWREG32(mmGC_CAC_IND_INDEX, (reg));\n\tWREG32(mmGC_CAC_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->gc_cac_idx_lock, flags);\n}\n\n\nstatic const u32 tonga_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, 0xC060000C,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 fiji_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, 0xC060000C,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 iceland_mgcg_cgcg_init[] =\n{\n\tmmPCIE_INDEX, 0xffffffff, ixPCIE_CNTL2,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, ixCGTT_ROM_CLK_CTRL0,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 cz_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 stoney_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xffffffff, 0x00000104,\n\tmmHDP_HOST_PATH_CNTL, 0xffffffff, 0x0f000027,\n};\n\nstatic void vi_init_golden_registers(struct amdgpu_device *adev)\n{\n\t/* Some of the registers might be dependent on GRBM_GFX_INDEX */\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\txgpu_vi_init_golden_registers(adev);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn;\n\t}\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ticeland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(iceland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tfiji_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(fiji_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttonga_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tonga_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tcz_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(cz_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tstoney_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(stoney_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\tdefault:\n\t\tbreak;\n\t}\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n/**\n * vi_get_xclk - get the xclk\n *\n * @adev: amdgpu_device pointer\n *\n * Returns the reference clock used by the gfx engine\n * (VI).\n */\nstatic u32 vi_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_STONEY:\n\t\t\t/* vbios says 48Mhz, but the actual freq is 100Mhz */\n\t\t\treturn 10000;\n\t\tdefault:\n\t\t\treturn reference_clock;\n\t\t}\n\t}\n\n\ttmp = RREG32_SMC(ixCG_CLKPIN_CNTL_2);\n\tif (REG_GET_FIELD(tmp, CG_CLKPIN_CNTL_2, MUX_TCLK_TO_XCLK))\n\t\treturn 1000;\n\n\ttmp = RREG32_SMC(ixCG_CLKPIN_CNTL);\n\tif (REG_GET_FIELD(tmp, CG_CLKPIN_CNTL, XTALIN_DIVIDE))\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\n/**\n * vi_srbm_select - select specific register instances\n *\n * @adev: amdgpu_device pointer\n * @me: selected ME (micro engine)\n * @pipe: pipe\n * @queue: queue\n * @vmid: VMID\n *\n * Switches the currently active registers instances.  Some\n * registers are instanced per VMID, others are instanced per\n * me/pipe/queue combination.\n */\nvoid vi_srbm_select(struct amdgpu_device *adev,\n\t\t     u32 me, u32 pipe, u32 queue, u32 vmid)\n{\n\tu32 srbm_gfx_cntl = 0;\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, PIPEID, pipe);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, MEID, me);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, VMID, vmid);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, QUEUEID, queue);\n\tWREG32(mmSRBM_GFX_CNTL, srbm_gfx_cntl);\n}\n\nstatic bool vi_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(mmBUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(mmD1VGA_CONTROL);\n\t\td2vga_control = RREG32(mmD2VGA_CONTROL);\n\t\tvga_render_control = RREG32(mmVGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32_SMC(ixROM_CNTL);\n\n\t/* enable the rom */\n\tWREG32(mmBUS_CNTL, (bus_cntl & ~BUS_CNTL__BIOS_ROM_DIS_MASK));\n\tif (adev->mode_info.num_crtc) {\n\t\t/* Disable VGA mode */\n\t\tWREG32(mmD1VGA_CONTROL,\n\t\t       (d1vga_control & ~(D1VGA_CONTROL__D1VGA_MODE_ENABLE_MASK |\n\t\t\t\t\t  D1VGA_CONTROL__D1VGA_TIMING_SELECT_MASK)));\n\t\tWREG32(mmD2VGA_CONTROL,\n\t\t       (d2vga_control & ~(D2VGA_CONTROL__D2VGA_MODE_ENABLE_MASK |\n\t\t\t\t\t  D2VGA_CONTROL__D2VGA_TIMING_SELECT_MASK)));\n\t\tWREG32(mmVGA_RENDER_CONTROL,\n\t\t       (vga_render_control & ~VGA_RENDER_CONTROL__VGA_VSTATUS_CNTL_MASK));\n\t}\n\tWREG32_SMC(ixROM_CNTL, rom_cntl | ROM_CNTL__SCK_OVERWRITE_MASK);\n\n\tr = amdgpu_read_bios(adev);\n\n\t/* restore regs */\n\tWREG32(mmBUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(mmD1VGA_CONTROL, d1vga_control);\n\t\tWREG32(mmD2VGA_CONTROL, d2vga_control);\n\t\tWREG32(mmVGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32_SMC(ixROM_CNTL, rom_cntl);\n\treturn r;\n}\n\nstatic bool vi_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tunsigned long flags;\n\tu32 i;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tu8 length_dw = ALIGN(length_bytes, 4);\n\t/* take the smc lock since we are using the smc index */\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\t/* set rom index to 0 */\n\tWREG32(mmSMC_IND_INDEX_11, ixROM_INDEX);\n\tWREG32(mmSMC_IND_DATA_11, 0);\n\t/* set index to data for continous read */\n\tWREG32(mmSMC_IND_INDEX_11, ixROM_DATA);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmSMC_IND_DATA_11);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\n\treturn true;\n}\n\nstatic const struct amdgpu_allowed_register_entry vi_allowed_read_registers[] = {\n\t{mmGRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmGRBM_STATUS_SE2},\n\t{mmGRBM_STATUS_SE3},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{mmSRBM_STATUS3},\n\t{mmSDMA0_STATUS_REG + SDMA0_REGISTER_OFFSET},\n\t{mmSDMA0_STATUS_REG + SDMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{mmCP_CPF_BUSY_STAT},\n\t{mmCP_CPF_STALLED_STAT1},\n\t{mmCP_CPF_STATUS},\n\t{mmCP_CPC_BUSY_STAT},\n\t{mmCP_CPC_STALLED_STAT1},\n\t{mmCP_CPC_STATUS},\n\t{mmGB_ADDR_CONFIG},\n\t{mmMC_ARB_RAMCFG},\n\t{mmGB_TILE_MODE0},\n\t{mmGB_TILE_MODE1},\n\t{mmGB_TILE_MODE2},\n\t{mmGB_TILE_MODE3},\n\t{mmGB_TILE_MODE4},\n\t{mmGB_TILE_MODE5},\n\t{mmGB_TILE_MODE6},\n\t{mmGB_TILE_MODE7},\n\t{mmGB_TILE_MODE8},\n\t{mmGB_TILE_MODE9},\n\t{mmGB_TILE_MODE10},\n\t{mmGB_TILE_MODE11},\n\t{mmGB_TILE_MODE12},\n\t{mmGB_TILE_MODE13},\n\t{mmGB_TILE_MODE14},\n\t{mmGB_TILE_MODE15},\n\t{mmGB_TILE_MODE16},\n\t{mmGB_TILE_MODE17},\n\t{mmGB_TILE_MODE18},\n\t{mmGB_TILE_MODE19},\n\t{mmGB_TILE_MODE20},\n\t{mmGB_TILE_MODE21},\n\t{mmGB_TILE_MODE22},\n\t{mmGB_TILE_MODE23},\n\t{mmGB_TILE_MODE24},\n\t{mmGB_TILE_MODE25},\n\t{mmGB_TILE_MODE26},\n\t{mmGB_TILE_MODE27},\n\t{mmGB_TILE_MODE28},\n\t{mmGB_TILE_MODE29},\n\t{mmGB_TILE_MODE30},\n\t{mmGB_TILE_MODE31},\n\t{mmGB_MACROTILE_MODE0},\n\t{mmGB_MACROTILE_MODE1},\n\t{mmGB_MACROTILE_MODE2},\n\t{mmGB_MACROTILE_MODE3},\n\t{mmGB_MACROTILE_MODE4},\n\t{mmGB_MACROTILE_MODE5},\n\t{mmGB_MACROTILE_MODE6},\n\t{mmGB_MACROTILE_MODE7},\n\t{mmGB_MACROTILE_MODE8},\n\t{mmGB_MACROTILE_MODE9},\n\t{mmGB_MACROTILE_MODE10},\n\t{mmGB_MACROTILE_MODE11},\n\t{mmGB_MACROTILE_MODE12},\n\t{mmGB_MACROTILE_MODE13},\n\t{mmGB_MACROTILE_MODE14},\n\t{mmGB_MACROTILE_MODE15},\n\t{mmCC_RB_BACKEND_DISABLE, true},\n\t{mmGC_USER_RB_BACKEND_DISABLE, true},\n\t{mmGB_BACKEND_MAP, false},\n\t{mmPA_SC_RASTER_CONFIG, true},\n\t{mmPA_SC_RASTER_CONFIG_1, true},\n};\n\nstatic uint32_t vi_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\tcase mmPA_SC_RASTER_CONFIG_1:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config_1;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tcase mmGB_MACROTILE_MODE0:\n\t\tcase mmGB_MACROTILE_MODE1:\n\t\tcase mmGB_MACROTILE_MODE2:\n\t\tcase mmGB_MACROTILE_MODE3:\n\t\tcase mmGB_MACROTILE_MODE4:\n\t\tcase mmGB_MACROTILE_MODE5:\n\t\tcase mmGB_MACROTILE_MODE6:\n\t\tcase mmGB_MACROTILE_MODE7:\n\t\tcase mmGB_MACROTILE_MODE8:\n\t\tcase mmGB_MACROTILE_MODE9:\n\t\tcase mmGB_MACROTILE_MODE10:\n\t\tcase mmGB_MACROTILE_MODE11:\n\t\tcase mmGB_MACROTILE_MODE12:\n\t\tcase mmGB_MACROTILE_MODE13:\n\t\tcase mmGB_MACROTILE_MODE14:\n\t\tcase mmGB_MACROTILE_MODE15:\n\t\t\tidx = (reg_offset - mmGB_MACROTILE_MODE0);\n\t\t\treturn adev->gfx.config.macrotile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\n\nstatic int vi_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t    u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(vi_allowed_read_registers); i++) {\n\t\tbool indexed = vi_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != vi_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = vi_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\n/**\n * vi_asic_pci_config_reset - soft reset GPU\n *\n * @adev: amdgpu_device pointer\n *\n * Use PCI Config method to reset the GPU.\n *\n * Returns 0 for success.\n */\nstatic int vi_asic_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\t/* reset */\n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t/* wait for asic to come out of reset */\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t/* enable BM */\n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic int vi_asic_supports_baco(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\t\treturn amdgpu_dpm_is_baco_supported(adev);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic enum amd_reset_method\nvi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tint baco_reset;\n\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_LEGACY ||\n\t    amdgpu_reset_method == AMD_RESET_METHOD_BACO)\n\t\treturn amdgpu_reset_method;\n\n\tif (amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t\t  amdgpu_reset_method);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\t\tbaco_reset = amdgpu_dpm_is_baco_supported(adev);\n\t\tbreak;\n\tdefault:\n\t\tbaco_reset = 0;\n\t\tbreak;\n\t}\n\n\tif (baco_reset)\n\t\treturn AMD_RESET_METHOD_BACO;\n\telse\n\t\treturn AMD_RESET_METHOD_LEGACY;\n}\n\n/**\n * vi_asic_reset - soft reset GPU\n *\n * @adev: amdgpu_device pointer\n *\n * Look up which blocks are hung and attempt\n * to reset them.\n * Returns 0 for success.\n */\nstatic int vi_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t/* APUs don't have full asic reset */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tif (vi_asic_reset_method(adev) == AMD_RESET_METHOD_BACO) {\n\t\tdev_info(adev->dev, \"BACO reset\\n\");\n\t\tr = amdgpu_dpm_baco_reset(adev);\n\t} else {\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = vi_asic_pci_config_reset(adev);\n\t}\n\n\treturn r;\n}\n\nstatic u32 vi_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic int vi_set_uvd_clock(struct amdgpu_device *adev, u32 clock,\n\t\t\tu32 cntl_reg, u32 status_reg)\n{\n\tint r, i;\n\tstruct atom_clock_dividers dividers;\n\tuint32_t tmp;\n\n\tr = amdgpu_atombios_get_clock_dividers(adev,\n\t\t\t\t\t       COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\n\t\t\t\t\t       clock, false, &dividers);\n\tif (r)\n\t\treturn r;\n\n\ttmp = RREG32_SMC(cntl_reg);\n\n\tif (adev->flags & AMD_IS_APU)\n\t\ttmp &= ~CG_DCLK_CNTL__DCLK_DIVIDER_MASK;\n\telse\n\t\ttmp &= ~(CG_DCLK_CNTL__DCLK_DIR_CNTL_EN_MASK |\n\t\t\t\tCG_DCLK_CNTL__DCLK_DIVIDER_MASK);\n\ttmp |= dividers.post_divider;\n\tWREG32_SMC(cntl_reg, tmp);\n\n\tfor (i = 0; i < 100; i++) {\n\t\ttmp = RREG32_SMC(status_reg);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tif (tmp & 0x10000)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (tmp & CG_DCLK_STATUS__DCLK_STATUS_MASK)\n\t\t\t\tbreak;\n\t\t}\n\t\tmdelay(10);\n\t}\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\treturn 0;\n}\n\n#define ixGNB_CLK1_DFS_CNTL 0xD82200F0\n#define ixGNB_CLK1_STATUS   0xD822010C\n#define ixGNB_CLK2_DFS_CNTL 0xD8220110\n#define ixGNB_CLK2_STATUS   0xD822012C\n#define ixGNB_CLK3_DFS_CNTL 0xD8220130\n#define ixGNB_CLK3_STATUS   0xD822014C\n\nstatic int vi_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tint r;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tr = vi_set_uvd_clock(adev, vclk, ixGNB_CLK2_DFS_CNTL, ixGNB_CLK2_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = vi_set_uvd_clock(adev, dclk, ixGNB_CLK1_DFS_CNTL, ixGNB_CLK1_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\t} else {\n\t\tr = vi_set_uvd_clock(adev, vclk, ixCG_VCLK_CNTL, ixCG_VCLK_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = vi_set_uvd_clock(adev, dclk, ixCG_DCLK_CNTL, ixCG_DCLK_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int vi_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tint r, i;\n\tstruct atom_clock_dividers dividers;\n\tu32 tmp;\n\tu32 reg_ctrl;\n\tu32 reg_status;\n\tu32 status_mask;\n\tu32 reg_mask;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\treg_ctrl = ixGNB_CLK3_DFS_CNTL;\n\t\treg_status = ixGNB_CLK3_STATUS;\n\t\tstatus_mask = 0x00010000;\n\t\treg_mask = CG_ECLK_CNTL__ECLK_DIVIDER_MASK;\n\t} else {\n\t\treg_ctrl = ixCG_ECLK_CNTL;\n\t\treg_status = ixCG_ECLK_STATUS;\n\t\tstatus_mask = CG_ECLK_STATUS__ECLK_STATUS_MASK;\n\t\treg_mask = CG_ECLK_CNTL__ECLK_DIR_CNTL_EN_MASK | CG_ECLK_CNTL__ECLK_DIVIDER_MASK;\n\t}\n\n\tr = amdgpu_atombios_get_clock_dividers(adev,\n\t\t\t\t\t       COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\n\t\t\t\t\t       ecclk, false, &dividers);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < 100; i++) {\n\t\tif (RREG32_SMC(reg_status) & status_mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\n\ttmp = RREG32_SMC(reg_ctrl);\n\ttmp &= ~reg_mask;\n\ttmp |= dividers.post_divider;\n\tWREG32_SMC(reg_ctrl, tmp);\n\n\tfor (i = 0; i < 100; i++) {\n\t\tif (RREG32_SMC(reg_status) & status_mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\n\treturn 0;\n}\n\nstatic void vi_enable_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\tdata |= PCIE_LC_CNTL__LC_L0S_INACTIVITY_DEFAULT <<\n\t\t\tPCIE_LC_CNTL__LC_L0S_INACTIVITY__SHIFT;\n\tdata |= PCIE_LC_CNTL__LC_L1_INACTIVITY_DEFAULT <<\n\t\t\tPCIE_LC_CNTL__LC_L1_INACTIVITY__SHIFT;\n\tdata &= ~PCIE_LC_CNTL__LC_PMI_TO_L1_DIS_MASK;\n\tdata |= PCIE_LC_CNTL__LC_DELAY_L1_EXIT_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n}\n\nstatic void vi_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, data1, orig;\n\tbool bL1SS = false;\n\tbool bClkReqSupport = true;\n\n\tif (!amdgpu_device_should_use_aspm(adev))\n\t\treturn;\n\n\tif (adev->asic_type < CHIP_POLARIS10)\n\t\treturn;\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\tdata &= ~PCIE_LC_CNTL__LC_L1_INACTIVITY_MASK;\n\tdata &= ~PCIE_LC_CNTL__LC_L0S_INACTIVITY_MASK;\n\tdata |= PCIE_LC_CNTL__LC_PMI_TO_L1_DIS_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_N_FTS_CNTL);\n\tdata &= ~PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS_MASK;\n\tdata |= 0x0024 << PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS__SHIFT;\n\tdata |= PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS_OVERRIDE_EN_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL3);\n\tdata |= PCIE_LC_CNTL3__LC_GO_TO_RECOVERY_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_P_CNTL);\n\tdata |= PCIE_P_CNTL__P_IGNORE_EDB_ERR_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_P_CNTL, data);\n\n\tdata = RREG32_PCIE(ixPCIE_LC_L1_PM_SUBSTATE);\n\tpci_read_config_dword(adev->pdev, PCIE_L1_PM_SUB_CNTL, &data1);\n\tif (data & PCIE_LC_L1_PM_SUBSTATE__LC_L1_SUBSTATES_OVERRIDE_EN_MASK &&\n\t    (data & (PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_2_OVERRIDE_MASK |\n\t\t    PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_1_OVERRIDE_MASK |\n\t\t\tPCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_2_OVERRIDE_MASK |\n\t\t\tPCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_1_OVERRIDE_MASK))) {\n\t\tbL1SS = true;\n\t} else if (data1 & (PCIE_L1_PM_SUB_CNTL__ASPM_L1_2_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__ASPM_L1_1_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_2_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_1_EN_MASK)) {\n\t\tbL1SS = true;\n\t}\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL6);\n\tdata |= PCIE_LC_CNTL6__LC_L1_POWERDOWN_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL6, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_LINK_WIDTH_CNTL);\n\tdata |= PCIE_LC_LINK_WIDTH_CNTL__LC_DYN_LANES_PWR_STATE_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_LINK_WIDTH_CNTL, data);\n\n\tpci_read_config_dword(adev->pdev, LINK_CAP, &data);\n\tif (!(data & PCIE_LINK_CAP__CLOCK_POWER_MANAGEMENT_MASK))\n\t\tbClkReqSupport = false;\n\n\tif (bClkReqSupport) {\n\t\torig = data = RREG32_SMC(ixTHM_CLK_CNTL);\n\t\tdata &= ~(THM_CLK_CNTL__CMON_CLK_SEL_MASK | THM_CLK_CNTL__TMON_CLK_SEL_MASK);\n\t\tdata |= (1 << THM_CLK_CNTL__CMON_CLK_SEL__SHIFT) |\n\t\t\t\t(1 << THM_CLK_CNTL__TMON_CLK_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixTHM_CLK_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixMISC_CLK_CTRL);\n\t\tdata &= ~(MISC_CLK_CTRL__DEEP_SLEEP_CLK_SEL_MASK |\n\t\t\tMISC_CLK_CTRL__ZCLK_SEL_MASK | MISC_CLK_CTRL__DFT_SMS_PG_CLK_SEL_MASK);\n\t\tdata |= (1 << MISC_CLK_CTRL__DEEP_SLEEP_CLK_SEL__SHIFT) |\n\t\t\t\t(1 << MISC_CLK_CTRL__ZCLK_SEL__SHIFT);\n\t\tdata |= (0x20 << MISC_CLK_CTRL__DFT_SMS_PG_CLK_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixMISC_CLK_CTRL, data);\n\n\t\torig = data = RREG32_SMC(ixCG_CLKPIN_CNTL);\n\t\tdata |= CG_CLKPIN_CNTL__XTALIN_DIVIDE_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixCG_CLKPIN_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixCG_CLKPIN_CNTL_2);\n\t\tdata |= CG_CLKPIN_CNTL_2__ENABLE_XCLK_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixCG_CLKPIN_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixMPLL_BYPASSCLK_SEL);\n\t\tdata &= ~MPLL_BYPASSCLK_SEL__MPLL_CLKOUT_SEL_MASK;\n\t\tdata |= (4 << MPLL_BYPASSCLK_SEL__MPLL_CLKOUT_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixMPLL_BYPASSCLK_SEL, data);\n\n\t\torig = data = RREG32_PCIE(ixCPM_CONTROL);\n\t\tdata |= (CPM_CONTROL__REFCLK_XSTCLK_ENABLE_MASK |\n\t\t\t\tCPM_CONTROL__CLKREQb_UNGATE_TXCLK_ENABLE_MASK);\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixCPM_CONTROL, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_CONFIG_CNTL);\n\t\tdata &= ~PCIE_CONFIG_CNTL__DYN_CLK_LATENCY_MASK;\n\t\tdata |= (0xE << PCIE_CONFIG_CNTL__DYN_CLK_LATENCY__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_CONFIG_CNTL, data);\n\n\t\torig = data = RREG32(mmBIF_CLK_CTRL);\n\t\tdata |= BIF_CLK_CTRL__BIF_XSTCLK_READY_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32(mmBIF_CLK_CTRL, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL7);\n\t\tdata |= PCIE_LC_CNTL7__LC_L1_SIDEBAND_CLKREQ_PDWN_EN_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL7, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_HW_DEBUG);\n\t\tdata |= PCIE_HW_DEBUG__HW_01_DEBUG_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_HW_DEBUG, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL2);\n\t\tdata |= PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L23_MASK;\n\t\tdata |= PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L1_MASK;\n\t\tif (bL1SS)\n\t\t\tdata &= ~PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L1_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL2, data);\n\n\t}\n\n\tvi_enable_aspm(adev);\n\n\tdata = RREG32_PCIE(ixPCIE_LC_N_FTS_CNTL);\n\tdata1 = RREG32_PCIE(ixPCIE_LC_STATUS1);\n\tif (((data & PCIE_LC_N_FTS_CNTL__LC_N_FTS_MASK) == PCIE_LC_N_FTS_CNTL__LC_N_FTS_MASK) &&\n\t    data1 & PCIE_LC_STATUS1__LC_REVERSE_XMIT_MASK &&\n\t    data1 & PCIE_LC_STATUS1__LC_REVERSE_RCVR_MASK) {\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\t\tdata &= ~PCIE_LC_CNTL__LC_L0S_INACTIVITY_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n\t}\n\n\tif ((adev->asic_type == CHIP_POLARIS12 &&\n\t    !(ASICID_IS_P23(adev->pdev->device, adev->pdev->revision))) ||\n\t    ASIC_IS_P22(adev->asic_type, adev->external_rev_id)) {\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_TRAINING_CNTL);\n\t\tdata &= ~PCIE_LC_TRAINING_CNTL__LC_DISABLE_TRAINING_BIT_ARCH_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_TRAINING_CNTL, data);\n\t}\n}\n\nstatic void vi_enable_doorbell_aperture(struct amdgpu_device *adev,\n\t\t\t\t\tbool enable)\n{\n\tu32 tmp;\n\n\t/* not necessary on CZ */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\ttmp = RREG32(mmBIF_DOORBELL_APER_EN);\n\tif (enable)\n\t\ttmp = REG_SET_FIELD(tmp, BIF_DOORBELL_APER_EN, BIF_DOORBELL_APER_EN, 1);\n\telse\n\t\ttmp = REG_SET_FIELD(tmp, BIF_DOORBELL_APER_EN, BIF_DOORBELL_APER_EN, 0);\n\n\tWREG32(mmBIF_DOORBELL_APER_EN, tmp);\n}\n\n#define ATI_REV_ID_FUSE_MACRO__ADDRESS      0xC0014044\n#define ATI_REV_ID_FUSE_MACRO__SHIFT        9\n#define ATI_REV_ID_FUSE_MACRO__MASK         0x00001E00\n\nstatic uint32_t vi_get_rev_id(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn (RREG32_SMC(ATI_REV_ID_FUSE_MACRO__ADDRESS) & ATI_REV_ID_FUSE_MACRO__MASK)\n\t\t\t>> ATI_REV_ID_FUSE_MACRO__SHIFT;\n\telse\n\t\treturn (RREG32(mmPCIE_EFUSE4) & PCIE_EFUSE4__STRAP_BIF_ATI_REV_ID_MASK)\n\t\t\t>> PCIE_EFUSE4__STRAP_BIF_ATI_REV_ID__SHIFT;\n}\n\nstatic void vi_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void vi_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool vi_need_full_reset(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\t/* CZ has hang issues with full reset at the moment */\n\t\treturn false;\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\t\t/* XXX: soft reset should work on fiji and tonga */\n\t\treturn true;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\tdefault:\n\t\t/* change this when we support soft reset */\n\t\treturn true;\n\t}\n}\n\nstatic void vi_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t/* This reports 0 on APUs, so return to avoid writing/reading registers\n\t * that may or may not be different from their GPU counterparts\n\t */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t/* Set the 2 events that we wish to watch, defined above */\n\t/* Reg 40 is # received msgs, Reg 104 is # of posted requests sent */\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t/* Write to enable desired perf counters */\n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t/* Zero out and enable the perf counters\n\t * Write 0x5:\n\t * Bit 0 = Start all counters(1)\n\t * Bit 2 = Global counter reset enable(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t/* Load the shadow and disable the perf counters\n\t * Write 0x2:\n\t * Bit 0 = Stop counters(0)\n\t * Bit 1 = Load the shadow counters(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t/* Read register values to get any >32bit overflow */\n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t/* Get the values and add the overflow */\n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t vi_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t/* Get the number of NAKs received and generated */\n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t/* Add the total number of NAKs, i.e the number of replays */\n\treturn (nak_r + nak_g);\n}\n\nstatic bool vi_need_reset_on_init(struct amdgpu_device *adev)\n{\n\tu32 clock_cntl, pc;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t/* check if the SMC is already running */\n\tclock_cntl = RREG32_SMC(ixSMC_SYSCON_CLOCK_CNTL_0);\n\tpc = RREG32_SMC(ixSMC_PC_C);\n\tif ((0 == REG_GET_FIELD(clock_cntl, SMC_SYSCON_CLOCK_CNTL_0, ck_disable)) &&\n\t    (0x20100 <= pc))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void vi_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs vi_asic_funcs =\n{\n\t.read_disabled_bios = &vi_read_disabled_bios,\n\t.read_bios_from_rom = &vi_read_bios_from_rom,\n\t.read_register = &vi_read_register,\n\t.reset = &vi_asic_reset,\n\t.reset_method = &vi_asic_reset_method,\n\t.get_xclk = &vi_get_xclk,\n\t.set_uvd_clocks = &vi_set_uvd_clocks,\n\t.set_vce_clocks = &vi_set_vce_clocks,\n\t.get_config_memsize = &vi_get_config_memsize,\n\t.flush_hdp = &vi_flush_hdp,\n\t.invalidate_hdp = &vi_invalidate_hdp,\n\t.need_full_reset = &vi_need_full_reset,\n\t.init_doorbell_index = &legacy_doorbell_index_init,\n\t.get_pcie_usage = &vi_get_pcie_usage,\n\t.need_reset_on_init = &vi_need_reset_on_init,\n\t.get_pcie_replay_count = &vi_get_pcie_replay_count,\n\t.supports_baco = &vi_asic_supports_baco,\n\t.pre_asic_init = &vi_pre_asic_init,\n\t.query_video_codecs = &vi_query_video_codecs,\n};\n\n#define CZ_REV_BRISTOL(rev)\t \\\n\t((rev >= 0xC8 && rev <= 0xCE) || (rev >= 0xE1 && rev <= 0xE6))\n\nstatic int vi_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tadev->smc_rreg = &cz_smc_rreg;\n\t\tadev->smc_wreg = &cz_smc_wreg;\n\t} else {\n\t\tadev->smc_rreg = &vi_smc_rreg;\n\t\tadev->smc_wreg = &vi_smc_wreg;\n\t}\n\tadev->pcie_rreg = &vi_pcie_rreg;\n\tadev->pcie_wreg = &vi_pcie_wreg;\n\tadev->uvd_ctx_rreg = &vi_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = &vi_uvd_ctx_wreg;\n\tadev->didt_rreg = &vi_didt_rreg;\n\tadev->didt_wreg = &vi_didt_wreg;\n\tadev->gc_cac_rreg = &vi_gc_cac_rreg;\n\tadev->gc_cac_wreg = &vi_gc_cac_wreg;\n\n\tadev->asic_funcs = &vi_asic_funcs;\n\n\tadev->rev_id = vi_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tadev->cg_flags = 0;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 0x1;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x3c;\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x14;\n\t\tbreak;\n\tcase CHIP_POLARIS11:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x5A;\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x50;\n\t\tbreak;\n\tcase CHIP_POLARIS12:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x64;\n\t\tbreak;\n\tcase CHIP_VEGAM:\n\t\tadev->cg_flags = 0;\n\t\t\t/*AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;*/\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x6E;\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\t/* rev0 hardware requires workarounds to support PG */\n\t\tadev->pg_flags = 0;\n\t\tif (adev->rev_id != 0x00 || CZ_REV_BRISTOL(adev->pdev->revision)) {\n\t\t\tadev->pg_flags |= AMD_PG_SUPPORT_GFX_SMG |\n\t\t\t\tAMD_PG_SUPPORT_GFX_PIPELINE |\n\t\t\t\tAMD_PG_SUPPORT_CP |\n\t\t\t\tAMD_PG_SUPPORT_UVD |\n\t\t\t\tAMD_PG_SUPPORT_VCE;\n\t\t}\n\t\tadev->external_rev_id = adev->rev_id + 0x1;\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = AMD_PG_SUPPORT_GFX_PG |\n\t\t\tAMD_PG_SUPPORT_GFX_SMG |\n\t\t\tAMD_PG_SUPPORT_GFX_PIPELINE |\n\t\t\tAMD_PG_SUPPORT_CP |\n\t\t\tAMD_PG_SUPPORT_UVD |\n\t\t\tAMD_PG_SUPPORT_VCE;\n\t\tadev->external_rev_id = adev->rev_id + 0x61;\n\t\tbreak;\n\tdefault:\n\t\t/* FIXME: not supported yet */\n\t\treturn -EINVAL;\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_init_setting(adev);\n\t\txgpu_vi_mailbox_set_irq_funcs(adev);\n\t}\n\n\treturn 0;\n}\n\nstatic int vi_common_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_get_irq(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_sw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_add_irq_id(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int vi_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t/* move the golden regs per IP block */\n\tvi_init_golden_registers(adev);\n\t/* enable aspm */\n\tvi_program_aspm(adev);\n\t/* enable the doorbell aperture */\n\tvi_enable_doorbell_aperture(adev, true);\n\n\treturn 0;\n}\n\nstatic int vi_common_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t/* enable the doorbell aperture */\n\tvi_enable_doorbell_aperture(adev, false);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_put_irq(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn vi_common_hw_fini(adev);\n}\n\nstatic int vi_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn vi_common_hw_init(adev);\n}\n\nstatic bool vi_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int vi_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int vi_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic void vi_update_bif_medium_grain_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t\t\t   bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32_PCIE(ixPCIE_CNTL2);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS))\n\t\tdata |= PCIE_CNTL2__SLV_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__MST_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__REPLAY_MEM_LS_EN_MASK;\n\telse\n\t\tdata &= ~(PCIE_CNTL2__SLV_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__MST_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__REPLAY_MEM_LS_EN_MASK);\n\n\tif (temp != data)\n\t\tWREG32_PCIE(ixPCIE_CNTL2, data);\n}\n\nstatic void vi_update_hdp_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t    bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(mmHDP_HOST_PATH_CNTL);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG))\n\t\tdata &= ~HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK;\n\telse\n\t\tdata |= HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK;\n\n\tif (temp != data)\n\t\tWREG32(mmHDP_HOST_PATH_CNTL, data);\n}\n\nstatic void vi_update_hdp_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t      bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(mmHDP_MEM_POWER_LS);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS))\n\t\tdata |= HDP_MEM_POWER_LS__LS_ENABLE_MASK;\n\telse\n\t\tdata &= ~HDP_MEM_POWER_LS__LS_ENABLE_MASK;\n\n\tif (temp != data)\n\t\tWREG32(mmHDP_MEM_POWER_LS, data);\n}\n\nstatic void vi_update_drm_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t      bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(0x157a);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_DRM_LS))\n\t\tdata |= 1;\n\telse\n\t\tdata &= ~1;\n\n\tif (temp != data)\n\t\tWREG32(0x157a, data);\n}\n\n\nstatic void vi_update_rom_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t    bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32_SMC(ixCGTT_ROM_CLK_CTRL0);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_ROM_MGCG))\n\t\tdata &= ~(CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK |\n\t\t\t\tCGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE1_MASK);\n\telse\n\t\tdata |= CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK |\n\t\t\t\tCGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE1_MASK;\n\n\tif (temp != data)\n\t\tWREG32_SMC(ixCGTT_ROM_CLK_CTRL0, data);\n}\n\nstatic int vi_common_set_clockgating_state_by_smu(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tuint32_t msg_id, pp_state = 0;\n\tuint32_t pp_support_state = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_MC_LS | AMD_CG_SUPPORT_MC_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_MC_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_MC_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_MC,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_SDMA_LS | AMD_CG_SUPPORT_SDMA_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_SDMA_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_SDMA,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_HDP_LS | AMD_CG_SUPPORT_HDP_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_HDP,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS) {\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_LS;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_BIF,\n\t\t\t       PP_STATE_SUPPORT_LS,\n\t\t\t        pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\tif (adev->cg_flags & AMD_CG_SUPPORT_BIF_MGCG) {\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_CG;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_BIF,\n\t\t\t       PP_STATE_SUPPORT_CG,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_DRM_LS) {\n\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_LS;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_DRM,\n\t\t\t       PP_STATE_SUPPORT_LS,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_ROM_MGCG) {\n\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_CG;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_ROM,\n\t\t\t       PP_STATE_SUPPORT_CG,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\treturn 0;\n}\n\nstatic int vi_common_set_clockgating_state(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\t\tvi_update_bif_medium_grain_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_rom_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tvi_update_bif_medium_grain_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_drm_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tbreak;\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tvi_common_set_clockgating_state_by_smu(adev, state);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int vi_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic void vi_common_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\t*flags = 0;\n\n\t/* AMD_CG_SUPPORT_BIF_LS */\n\tdata = RREG32_PCIE(ixPCIE_CNTL2);\n\tif (data & PCIE_CNTL2__SLV_MEM_LS_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_BIF_LS;\n\n\t/* AMD_CG_SUPPORT_HDP_LS */\n\tdata = RREG32(mmHDP_MEM_POWER_LS);\n\tif (data & HDP_MEM_POWER_LS__LS_ENABLE_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_HDP_LS;\n\n\t/* AMD_CG_SUPPORT_HDP_MGCG */\n\tdata = RREG32(mmHDP_HOST_PATH_CNTL);\n\tif (!(data & HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_HDP_MGCG;\n\n\t/* AMD_CG_SUPPORT_ROM_MGCG */\n\tdata = RREG32_SMC(ixCGTT_ROM_CLK_CTRL0);\n\tif (!(data & CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_ROM_MGCG;\n}\n\nstatic const struct amd_ip_funcs vi_common_ip_funcs = {\n\t.name = \"vi_common\",\n\t.early_init = vi_common_early_init,\n\t.late_init = vi_common_late_init,\n\t.sw_init = vi_common_sw_init,\n\t.sw_fini = vi_common_sw_fini,\n\t.hw_init = vi_common_hw_init,\n\t.hw_fini = vi_common_hw_fini,\n\t.suspend = vi_common_suspend,\n\t.resume = vi_common_resume,\n\t.is_idle = vi_common_is_idle,\n\t.wait_for_idle = vi_common_wait_for_idle,\n\t.soft_reset = vi_common_soft_reset,\n\t.set_clockgating_state = vi_common_set_clockgating_state,\n\t.set_powergating_state = vi_common_set_powergating_state,\n\t.get_clockgating_state = vi_common_get_clockgating_state,\n\t.dump_ip_state = NULL,\n\t.print_ip_state = NULL,\n};\n\nstatic const struct amdgpu_ip_block_version vi_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &vi_common_ip_funcs,\n};\n\nvoid vi_set_virt_ops(struct amdgpu_device *adev)\n{\n\tadev->virt.ops = &xgpu_vi_virt_ops;\n}\n\nint vi_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tamdgpu_device_set_sriov_virtual_display(adev);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\t/* topaz has no DCE, UVD, VCE */\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v7_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &iceland_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v2_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_5_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v10_1_ip_block);\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_0_ip_block);\n\t\t\tamdgpu_device_ip_block_add(adev, &vce_v3_0_ip_block);\n\t\t}\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v10_0_ip_block);\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tamdgpu_device_ip_block_add(adev, &uvd_v5_0_ip_block);\n\t\t\tamdgpu_device_ip_block_add(adev, &vce_v3_0_ip_block);\n\t\t}\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_2_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_3_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_4_ip_block);\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &cz_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_1_ip_block);\n#if defined(CONFIG_DRM_AMD_ACP)\n\t\tamdgpu_device_ip_block_add(adev, &acp_ip_block);\n#endif\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &cz_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_2_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_4_ip_block);\n#if defined(CONFIG_DRM_AMD_ACP)\n\t\tamdgpu_device_ip_block_add(adev, &acp_ip_block);\n#endif\n\t\tbreak;\n\tdefault:\n\t\t/* FIXME: not supported yet */\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid legacy_doorbell_index_init(struct amdgpu_device *adev)\n{\n\tadev->doorbell_index.kiq = AMDGPU_DOORBELL_KIQ;\n\tadev->doorbell_index.mec_ring0 = AMDGPU_DOORBELL_MEC_RING0;\n\tadev->doorbell_index.mec_ring1 = AMDGPU_DOORBELL_MEC_RING1;\n\tadev->doorbell_index.mec_ring2 = AMDGPU_DOORBELL_MEC_RING2;\n\tadev->doorbell_index.mec_ring3 = AMDGPU_DOORBELL_MEC_RING3;\n\tadev->doorbell_index.mec_ring4 = AMDGPU_DOORBELL_MEC_RING4;\n\tadev->doorbell_index.mec_ring5 = AMDGPU_DOORBELL_MEC_RING5;\n\tadev->doorbell_index.mec_ring6 = AMDGPU_DOORBELL_MEC_RING6;\n\tadev->doorbell_index.mec_ring7 = AMDGPU_DOORBELL_MEC_RING7;\n\tadev->doorbell_index.gfx_ring0 = AMDGPU_DOORBELL_GFX_RING0;\n\tadev->doorbell_index.sdma_engine[0] = AMDGPU_DOORBELL_sDMA_ENGINE0;\n\tadev->doorbell_index.sdma_engine[1] = AMDGPU_DOORBELL_sDMA_ENGINE1;\n\tadev->doorbell_index.ih = AMDGPU_DOORBELL_IH;\n\tadev->doorbell_index.max_assignment = AMDGPU_DOORBELL_MAX_ASSIGNMENT;\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/buffer-overflow0-bug.txt", "bug_report_text": "Issue: Lack of Boundary Checks\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/vi.c\nLine: 659\n\nDescription:\nThe function vi_read_bios_from_rom lacks proper boundary checks when reading data into the bios buffer. \nThe function calculates the number of dwords to read based on the input length_bytes, but it doesn't verify \nif this calculated length exceeds the actual allocated size of the bios buffer. \nThis can lead to a buffer overflow, potentially causing memory corruption or security vulnerabilities.\n\n", "diff_path": "dataset/raw_data/bugs/dev-set/buffer-overflow0-diff.txt", "diff_text": "--- a/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/vi.c\n+++ b/coverity/dataset/raw_data/code/drivers/gpu/drm/amd/amdgpu/vi.c\n@@ -656,6 +656,8 @@ static bool vi_read_bios_from_rom(struct amdgpu_device *adev,\n        WREG32(mmSMC_IND_DATA_11, 0);\n        /* set index to data for continous read */\n        WREG32(mmSMC_IND_INDEX_11, ixROM_DATA);\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        for (i = 0; i < length_dw; i++)\n                dw_ptr[i] = RREG32(mmSMC_IND_DATA_11);\n        spin_unlock_irqrestore(&adev->smc_idx_lock, flags);", "source_code_path": "drivers/gpu/drm/amd/amdgpu/vi.c", "line_number": 659, "code": "/*\n * Copyright 2014 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/slab.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"amdgpu_ucode.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n\n#include \"gmc/gmc_8_1_d.h\"\n#include \"gmc/gmc_8_1_sh_mask.h\"\n\n#include \"oss/oss_3_0_d.h\"\n#include \"oss/oss_3_0_sh_mask.h\"\n\n#include \"bif/bif_5_0_d.h\"\n#include \"bif/bif_5_0_sh_mask.h\"\n\n#include \"gca/gfx_8_0_d.h\"\n#include \"gca/gfx_8_0_sh_mask.h\"\n\n#include \"smu/smu_7_1_1_d.h\"\n#include \"smu/smu_7_1_1_sh_mask.h\"\n\n#include \"uvd/uvd_5_0_d.h\"\n#include \"uvd/uvd_5_0_sh_mask.h\"\n\n#include \"vce/vce_3_0_d.h\"\n#include \"vce/vce_3_0_sh_mask.h\"\n\n#include \"dce/dce_10_0_d.h\"\n#include \"dce/dce_10_0_sh_mask.h\"\n\n#include \"vid.h\"\n#include \"vi.h\"\n#include \"gmc_v8_0.h\"\n#include \"gmc_v7_0.h\"\n#include \"gfx_v8_0.h\"\n#include \"sdma_v2_4.h\"\n#include \"sdma_v3_0.h\"\n#include \"dce_v10_0.h\"\n#include \"dce_v11_0.h\"\n#include \"iceland_ih.h\"\n#include \"tonga_ih.h\"\n#include \"cz_ih.h\"\n#include \"uvd_v5_0.h\"\n#include \"uvd_v6_0.h\"\n#include \"vce_v3_0.h\"\n#if defined(CONFIG_DRM_AMD_ACP)\n#include \"amdgpu_acp.h\"\n#endif\n#include \"amdgpu_vkms.h\"\n#include \"mxgpu_vi.h\"\n#include \"amdgpu_dm.h\"\n\n#define ixPCIE_LC_L1_PM_SUBSTATE\t0x100100C6\n#define PCIE_LC_L1_PM_SUBSTATE__LC_L1_SUBSTATES_OVERRIDE_EN_MASK\t0x00000001L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_2_OVERRIDE_MASK\t0x00000002L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_1_OVERRIDE_MASK\t0x00000004L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_2_OVERRIDE_MASK\t\t0x00000008L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_1_OVERRIDE_MASK\t\t0x00000010L\n#define ixPCIE_L1_PM_SUB_CNTL\t0x378\n#define PCIE_L1_PM_SUB_CNTL__ASPM_L1_2_EN_MASK\t0x00000004L\n#define PCIE_L1_PM_SUB_CNTL__ASPM_L1_1_EN_MASK\t0x00000008L\n#define PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_2_EN_MASK\t0x00000001L\n#define PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_1_EN_MASK\t0x00000002L\n#define PCIE_LC_CNTL6__LC_L1_POWERDOWN_MASK\t\t0x00200000L\n#define LINK_CAP\t0x64\n#define PCIE_LINK_CAP__CLOCK_POWER_MANAGEMENT_MASK\t0x00040000L\n#define ixCPM_CONTROL\t0x1400118\n#define ixPCIE_LC_CNTL7\t0x100100BC\n#define PCIE_LC_CNTL7__LC_L1_SIDEBAND_CLKREQ_PDWN_EN_MASK\t0x00000400L\n#define PCIE_LC_CNTL__LC_L0S_INACTIVITY_DEFAULT\t0x00000007\n#define PCIE_LC_CNTL__LC_L1_INACTIVITY_DEFAULT\t0x00000009\n#define CPM_CONTROL__CLKREQb_UNGATE_TXCLK_ENABLE_MASK\t0x01000000L\n#define PCIE_L1_PM_SUB_CNTL\t0x378\n#define ASIC_IS_P22(asic_type, rid)\t((asic_type >= CHIP_POLARIS10) && \\\n\t\t\t\t\t\t\t\t\t(asic_type <= CHIP_POLARIS12) && \\\n\t\t\t\t\t\t\t\t\t(rid >= 0x6E))\n/* Topaz */\nstatic const struct amdgpu_video_codecs topaz_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* Tonga, CZ, ST, Fiji */\nstatic const struct amdgpu_video_codec_info tonga_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tonga_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tonga_video_codecs_encode_array),\n\t.codec_array = tonga_video_codecs_encode_array,\n};\n\n/* Polaris */\nstatic const struct amdgpu_video_codec_info polaris_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs polaris_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(polaris_video_codecs_encode_array),\n\t.codec_array = polaris_video_codecs_encode_array,\n};\n\n/* Topaz */\nstatic const struct amdgpu_video_codecs topaz_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* Tonga */\nstatic const struct amdgpu_video_codec_info tonga_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 52,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tonga_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tonga_video_codecs_decode_array),\n\t.codec_array = tonga_video_codecs_decode_array,\n};\n\n/* CZ, ST, Fiji, Polaris */\nstatic const struct amdgpu_video_codec_info cz_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 52,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 4,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 186,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_JPEG,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs cz_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(cz_video_codecs_decode_array),\n\t.codec_array = cz_video_codecs_decode_array,\n};\n\nstatic int vi_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tif (encode)\n\t\t\t*codecs = &topaz_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &topaz_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_TONGA:\n\t\tif (encode)\n\t\t\t*codecs = &tonga_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tonga_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tif (encode)\n\t\t\t*codecs = &polaris_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &cz_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_FIJI:\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tif (encode)\n\t\t\t*codecs = &tonga_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &cz_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n/*\n * Indirect registers accessor\n */\nstatic u32 vi_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32_NO_KIQ(mmPCIE_INDEX, reg);\n\t(void)RREG32_NO_KIQ(mmPCIE_INDEX);\n\tr = RREG32_NO_KIQ(mmPCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32_NO_KIQ(mmPCIE_INDEX, reg);\n\t(void)RREG32_NO_KIQ(mmPCIE_INDEX);\n\tWREG32_NO_KIQ(mmPCIE_DATA, v);\n\t(void)RREG32_NO_KIQ(mmPCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 vi_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32_NO_KIQ(mmSMC_IND_INDEX_11, (reg));\n\tr = RREG32_NO_KIQ(mmSMC_IND_DATA_11);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32_NO_KIQ(mmSMC_IND_INDEX_11, (reg));\n\tWREG32_NO_KIQ(mmSMC_IND_DATA_11, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\n/* smu_8_0_d.h */\n#define mmMP0PUB_IND_INDEX                                                      0x180\n#define mmMP0PUB_IND_DATA                                                       0x181\n\nstatic u32 cz_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(mmMP0PUB_IND_INDEX, (reg));\n\tr = RREG32(mmMP0PUB_IND_DATA);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void cz_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(mmMP0PUB_IND_INDEX, (reg));\n\tWREG32(mmMP0PUB_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 vi_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic u32 vi_didt_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->didt_idx_lock, flags);\n\tWREG32(mmDIDT_IND_INDEX, (reg));\n\tr = RREG32(mmDIDT_IND_DATA);\n\tspin_unlock_irqrestore(&adev->didt_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_didt_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->didt_idx_lock, flags);\n\tWREG32(mmDIDT_IND_INDEX, (reg));\n\tWREG32(mmDIDT_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->didt_idx_lock, flags);\n}\n\nstatic u32 vi_gc_cac_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->gc_cac_idx_lock, flags);\n\tWREG32(mmGC_CAC_IND_INDEX, (reg));\n\tr = RREG32(mmGC_CAC_IND_DATA);\n\tspin_unlock_irqrestore(&adev->gc_cac_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_gc_cac_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->gc_cac_idx_lock, flags);\n\tWREG32(mmGC_CAC_IND_INDEX, (reg));\n\tWREG32(mmGC_CAC_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->gc_cac_idx_lock, flags);\n}\n\n\nstatic const u32 tonga_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, 0xC060000C,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 fiji_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, 0xC060000C,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 iceland_mgcg_cgcg_init[] =\n{\n\tmmPCIE_INDEX, 0xffffffff, ixPCIE_CNTL2,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, ixCGTT_ROM_CLK_CTRL0,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 cz_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 stoney_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xffffffff, 0x00000104,\n\tmmHDP_HOST_PATH_CNTL, 0xffffffff, 0x0f000027,\n};\n\nstatic void vi_init_golden_registers(struct amdgpu_device *adev)\n{\n\t/* Some of the registers might be dependent on GRBM_GFX_INDEX */\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\txgpu_vi_init_golden_registers(adev);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn;\n\t}\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ticeland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(iceland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tfiji_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(fiji_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttonga_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tonga_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tcz_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(cz_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tstoney_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(stoney_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\tdefault:\n\t\tbreak;\n\t}\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n/**\n * vi_get_xclk - get the xclk\n *\n * @adev: amdgpu_device pointer\n *\n * Returns the reference clock used by the gfx engine\n * (VI).\n */\nstatic u32 vi_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_STONEY:\n\t\t\t/* vbios says 48Mhz, but the actual freq is 100Mhz */\n\t\t\treturn 10000;\n\t\tdefault:\n\t\t\treturn reference_clock;\n\t\t}\n\t}\n\n\ttmp = RREG32_SMC(ixCG_CLKPIN_CNTL_2);\n\tif (REG_GET_FIELD(tmp, CG_CLKPIN_CNTL_2, MUX_TCLK_TO_XCLK))\n\t\treturn 1000;\n\n\ttmp = RREG32_SMC(ixCG_CLKPIN_CNTL);\n\tif (REG_GET_FIELD(tmp, CG_CLKPIN_CNTL, XTALIN_DIVIDE))\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\n/**\n * vi_srbm_select - select specific register instances\n *\n * @adev: amdgpu_device pointer\n * @me: selected ME (micro engine)\n * @pipe: pipe\n * @queue: queue\n * @vmid: VMID\n *\n * Switches the currently active registers instances.  Some\n * registers are instanced per VMID, others are instanced per\n * me/pipe/queue combination.\n */\nvoid vi_srbm_select(struct amdgpu_device *adev,\n\t\t     u32 me, u32 pipe, u32 queue, u32 vmid)\n{\n\tu32 srbm_gfx_cntl = 0;\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, PIPEID, pipe);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, MEID, me);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, VMID, vmid);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, QUEUEID, queue);\n\tWREG32(mmSRBM_GFX_CNTL, srbm_gfx_cntl);\n}\n\nstatic bool vi_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(mmBUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(mmD1VGA_CONTROL);\n\t\td2vga_control = RREG32(mmD2VGA_CONTROL);\n\t\tvga_render_control = RREG32(mmVGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32_SMC(ixROM_CNTL);\n\n\t/* enable the rom */\n\tWREG32(mmBUS_CNTL, (bus_cntl & ~BUS_CNTL__BIOS_ROM_DIS_MASK));\n\tif (adev->mode_info.num_crtc) {\n\t\t/* Disable VGA mode */\n\t\tWREG32(mmD1VGA_CONTROL,\n\t\t       (d1vga_control & ~(D1VGA_CONTROL__D1VGA_MODE_ENABLE_MASK |\n\t\t\t\t\t  D1VGA_CONTROL__D1VGA_TIMING_SELECT_MASK)));\n\t\tWREG32(mmD2VGA_CONTROL,\n\t\t       (d2vga_control & ~(D2VGA_CONTROL__D2VGA_MODE_ENABLE_MASK |\n\t\t\t\t\t  D2VGA_CONTROL__D2VGA_TIMING_SELECT_MASK)));\n\t\tWREG32(mmVGA_RENDER_CONTROL,\n\t\t       (vga_render_control & ~VGA_RENDER_CONTROL__VGA_VSTATUS_CNTL_MASK));\n\t}\n\tWREG32_SMC(ixROM_CNTL, rom_cntl | ROM_CNTL__SCK_OVERWRITE_MASK);\n\n\tr = amdgpu_read_bios(adev);\n\n\t/* restore regs */\n\tWREG32(mmBUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(mmD1VGA_CONTROL, d1vga_control);\n\t\tWREG32(mmD2VGA_CONTROL, d2vga_control);\n\t\tWREG32(mmVGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32_SMC(ixROM_CNTL, rom_cntl);\n\treturn r;\n}\n\nstatic bool vi_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tunsigned long flags;\n\tu32 i;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tu8 length_dw = ALIGN(length_bytes, 4);\n\t/* take the smc lock since we are using the smc index */\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\t/* set rom index to 0 */\n\tWREG32(mmSMC_IND_INDEX_11, ixROM_INDEX);\n\tWREG32(mmSMC_IND_DATA_11, 0);\n\t/* set index to data for continous read */\n\tWREG32(mmSMC_IND_INDEX_11, ixROM_DATA);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmSMC_IND_DATA_11);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\n\treturn true;\n}\n\nstatic const struct amdgpu_allowed_register_entry vi_allowed_read_registers[] = {\n\t{mmGRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmGRBM_STATUS_SE2},\n\t{mmGRBM_STATUS_SE3},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{mmSRBM_STATUS3},\n\t{mmSDMA0_STATUS_REG + SDMA0_REGISTER_OFFSET},\n\t{mmSDMA0_STATUS_REG + SDMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{mmCP_CPF_BUSY_STAT},\n\t{mmCP_CPF_STALLED_STAT1},\n\t{mmCP_CPF_STATUS},\n\t{mmCP_CPC_BUSY_STAT},\n\t{mmCP_CPC_STALLED_STAT1},\n\t{mmCP_CPC_STATUS},\n\t{mmGB_ADDR_CONFIG},\n\t{mmMC_ARB_RAMCFG},\n\t{mmGB_TILE_MODE0},\n\t{mmGB_TILE_MODE1},\n\t{mmGB_TILE_MODE2},\n\t{mmGB_TILE_MODE3},\n\t{mmGB_TILE_MODE4},\n\t{mmGB_TILE_MODE5},\n\t{mmGB_TILE_MODE6},\n\t{mmGB_TILE_MODE7},\n\t{mmGB_TILE_MODE8},\n\t{mmGB_TILE_MODE9},\n\t{mmGB_TILE_MODE10},\n\t{mmGB_TILE_MODE11},\n\t{mmGB_TILE_MODE12},\n\t{mmGB_TILE_MODE13},\n\t{mmGB_TILE_MODE14},\n\t{mmGB_TILE_MODE15},\n\t{mmGB_TILE_MODE16},\n\t{mmGB_TILE_MODE17},\n\t{mmGB_TILE_MODE18},\n\t{mmGB_TILE_MODE19},\n\t{mmGB_TILE_MODE20},\n\t{mmGB_TILE_MODE21},\n\t{mmGB_TILE_MODE22},\n\t{mmGB_TILE_MODE23},\n\t{mmGB_TILE_MODE24},\n\t{mmGB_TILE_MODE25},\n\t{mmGB_TILE_MODE26},\n\t{mmGB_TILE_MODE27},\n\t{mmGB_TILE_MODE28},\n\t{mmGB_TILE_MODE29},\n\t{mmGB_TILE_MODE30},\n\t{mmGB_TILE_MODE31},\n\t{mmGB_MACROTILE_MODE0},\n\t{mmGB_MACROTILE_MODE1},\n\t{mmGB_MACROTILE_MODE2},\n\t{mmGB_MACROTILE_MODE3},\n\t{mmGB_MACROTILE_MODE4},\n\t{mmGB_MACROTILE_MODE5},\n\t{mmGB_MACROTILE_MODE6},\n\t{mmGB_MACROTILE_MODE7},\n\t{mmGB_MACROTILE_MODE8},\n\t{mmGB_MACROTILE_MODE9},\n\t{mmGB_MACROTILE_MODE10},\n\t{mmGB_MACROTILE_MODE11},\n\t{mmGB_MACROTILE_MODE12},\n\t{mmGB_MACROTILE_MODE13},\n\t{mmGB_MACROTILE_MODE14},\n\t{mmGB_MACROTILE_MODE15},\n\t{mmCC_RB_BACKEND_DISABLE, true},\n\t{mmGC_USER_RB_BACKEND_DISABLE, true},\n\t{mmGB_BACKEND_MAP, false},\n\t{mmPA_SC_RASTER_CONFIG, true},\n\t{mmPA_SC_RASTER_CONFIG_1, true},\n};\n\nstatic uint32_t vi_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\tcase mmPA_SC_RASTER_CONFIG_1:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config_1;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tcase mmGB_MACROTILE_MODE0:\n\t\tcase mmGB_MACROTILE_MODE1:\n\t\tcase mmGB_MACROTILE_MODE2:\n\t\tcase mmGB_MACROTILE_MODE3:\n\t\tcase mmGB_MACROTILE_MODE4:\n\t\tcase mmGB_MACROTILE_MODE5:\n\t\tcase mmGB_MACROTILE_MODE6:\n\t\tcase mmGB_MACROTILE_MODE7:\n\t\tcase mmGB_MACROTILE_MODE8:\n\t\tcase mmGB_MACROTILE_MODE9:\n\t\tcase mmGB_MACROTILE_MODE10:\n\t\tcase mmGB_MACROTILE_MODE11:\n\t\tcase mmGB_MACROTILE_MODE12:\n\t\tcase mmGB_MACROTILE_MODE13:\n\t\tcase mmGB_MACROTILE_MODE14:\n\t\tcase mmGB_MACROTILE_MODE15:\n\t\t\tidx = (reg_offset - mmGB_MACROTILE_MODE0);\n\t\t\treturn adev->gfx.config.macrotile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\n\nstatic int vi_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t    u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(vi_allowed_read_registers); i++) {\n\t\tbool indexed = vi_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != vi_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = vi_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\n/**\n * vi_asic_pci_config_reset - soft reset GPU\n *\n * @adev: amdgpu_device pointer\n *\n * Use PCI Config method to reset the GPU.\n *\n * Returns 0 for success.\n */\nstatic int vi_asic_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\t/* reset */\n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t/* wait for asic to come out of reset */\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t/* enable BM */\n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic int vi_asic_supports_baco(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\t\treturn amdgpu_dpm_is_baco_supported(adev);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic enum amd_reset_method\nvi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tint baco_reset;\n\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_LEGACY ||\n\t    amdgpu_reset_method == AMD_RESET_METHOD_BACO)\n\t\treturn amdgpu_reset_method;\n\n\tif (amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t\t  amdgpu_reset_method);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\t\tbaco_reset = amdgpu_dpm_is_baco_supported(adev);\n\t\tbreak;\n\tdefault:\n\t\tbaco_reset = 0;\n\t\tbreak;\n\t}\n\n\tif (baco_reset)\n\t\treturn AMD_RESET_METHOD_BACO;\n\telse\n\t\treturn AMD_RESET_METHOD_LEGACY;\n}\n\n/**\n * vi_asic_reset - soft reset GPU\n *\n * @adev: amdgpu_device pointer\n *\n * Look up which blocks are hung and attempt\n * to reset them.\n * Returns 0 for success.\n */\nstatic int vi_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t/* APUs don't have full asic reset */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tif (vi_asic_reset_method(adev) == AMD_RESET_METHOD_BACO) {\n\t\tdev_info(adev->dev, \"BACO reset\\n\");\n\t\tr = amdgpu_dpm_baco_reset(adev);\n\t} else {\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = vi_asic_pci_config_reset(adev);\n\t}\n\n\treturn r;\n}\n\nstatic u32 vi_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic int vi_set_uvd_clock(struct amdgpu_device *adev, u32 clock,\n\t\t\tu32 cntl_reg, u32 status_reg)\n{\n\tint r, i;\n\tstruct atom_clock_dividers dividers;\n\tuint32_t tmp;\n\n\tr = amdgpu_atombios_get_clock_dividers(adev,\n\t\t\t\t\t       COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\n\t\t\t\t\t       clock, false, &dividers);\n\tif (r)\n\t\treturn r;\n\n\ttmp = RREG32_SMC(cntl_reg);\n\n\tif (adev->flags & AMD_IS_APU)\n\t\ttmp &= ~CG_DCLK_CNTL__DCLK_DIVIDER_MASK;\n\telse\n\t\ttmp &= ~(CG_DCLK_CNTL__DCLK_DIR_CNTL_EN_MASK |\n\t\t\t\tCG_DCLK_CNTL__DCLK_DIVIDER_MASK);\n\ttmp |= dividers.post_divider;\n\tWREG32_SMC(cntl_reg, tmp);\n\n\tfor (i = 0; i < 100; i++) {\n\t\ttmp = RREG32_SMC(status_reg);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tif (tmp & 0x10000)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (tmp & CG_DCLK_STATUS__DCLK_STATUS_MASK)\n\t\t\t\tbreak;\n\t\t}\n\t\tmdelay(10);\n\t}\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\treturn 0;\n}\n\n#define ixGNB_CLK1_DFS_CNTL 0xD82200F0\n#define ixGNB_CLK1_STATUS   0xD822010C\n#define ixGNB_CLK2_DFS_CNTL 0xD8220110\n#define ixGNB_CLK2_STATUS   0xD822012C\n#define ixGNB_CLK3_DFS_CNTL 0xD8220130\n#define ixGNB_CLK3_STATUS   0xD822014C\n\nstatic int vi_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tint r;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tr = vi_set_uvd_clock(adev, vclk, ixGNB_CLK2_DFS_CNTL, ixGNB_CLK2_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = vi_set_uvd_clock(adev, dclk, ixGNB_CLK1_DFS_CNTL, ixGNB_CLK1_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\t} else {\n\t\tr = vi_set_uvd_clock(adev, vclk, ixCG_VCLK_CNTL, ixCG_VCLK_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = vi_set_uvd_clock(adev, dclk, ixCG_DCLK_CNTL, ixCG_DCLK_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int vi_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tint r, i;\n\tstruct atom_clock_dividers dividers;\n\tu32 tmp;\n\tu32 reg_ctrl;\n\tu32 reg_status;\n\tu32 status_mask;\n\tu32 reg_mask;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\treg_ctrl = ixGNB_CLK3_DFS_CNTL;\n\t\treg_status = ixGNB_CLK3_STATUS;\n\t\tstatus_mask = 0x00010000;\n\t\treg_mask = CG_ECLK_CNTL__ECLK_DIVIDER_MASK;\n\t} else {\n\t\treg_ctrl = ixCG_ECLK_CNTL;\n\t\treg_status = ixCG_ECLK_STATUS;\n\t\tstatus_mask = CG_ECLK_STATUS__ECLK_STATUS_MASK;\n\t\treg_mask = CG_ECLK_CNTL__ECLK_DIR_CNTL_EN_MASK | CG_ECLK_CNTL__ECLK_DIVIDER_MASK;\n\t}\n\n\tr = amdgpu_atombios_get_clock_dividers(adev,\n\t\t\t\t\t       COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\n\t\t\t\t\t       ecclk, false, &dividers);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < 100; i++) {\n\t\tif (RREG32_SMC(reg_status) & status_mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\n\ttmp = RREG32_SMC(reg_ctrl);\n\ttmp &= ~reg_mask;\n\ttmp |= dividers.post_divider;\n\tWREG32_SMC(reg_ctrl, tmp);\n\n\tfor (i = 0; i < 100; i++) {\n\t\tif (RREG32_SMC(reg_status) & status_mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\n\treturn 0;\n}\n\nstatic void vi_enable_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\tdata |= PCIE_LC_CNTL__LC_L0S_INACTIVITY_DEFAULT <<\n\t\t\tPCIE_LC_CNTL__LC_L0S_INACTIVITY__SHIFT;\n\tdata |= PCIE_LC_CNTL__LC_L1_INACTIVITY_DEFAULT <<\n\t\t\tPCIE_LC_CNTL__LC_L1_INACTIVITY__SHIFT;\n\tdata &= ~PCIE_LC_CNTL__LC_PMI_TO_L1_DIS_MASK;\n\tdata |= PCIE_LC_CNTL__LC_DELAY_L1_EXIT_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n}\n\nstatic void vi_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, data1, orig;\n\tbool bL1SS = false;\n\tbool bClkReqSupport = true;\n\n\tif (!amdgpu_device_should_use_aspm(adev))\n\t\treturn;\n\n\tif (adev->asic_type < CHIP_POLARIS10)\n\t\treturn;\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\tdata &= ~PCIE_LC_CNTL__LC_L1_INACTIVITY_MASK;\n\tdata &= ~PCIE_LC_CNTL__LC_L0S_INACTIVITY_MASK;\n\tdata |= PCIE_LC_CNTL__LC_PMI_TO_L1_DIS_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_N_FTS_CNTL);\n\tdata &= ~PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS_MASK;\n\tdata |= 0x0024 << PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS__SHIFT;\n\tdata |= PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS_OVERRIDE_EN_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL3);\n\tdata |= PCIE_LC_CNTL3__LC_GO_TO_RECOVERY_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_P_CNTL);\n\tdata |= PCIE_P_CNTL__P_IGNORE_EDB_ERR_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_P_CNTL, data);\n\n\tdata = RREG32_PCIE(ixPCIE_LC_L1_PM_SUBSTATE);\n\tpci_read_config_dword(adev->pdev, PCIE_L1_PM_SUB_CNTL, &data1);\n\tif (data & PCIE_LC_L1_PM_SUBSTATE__LC_L1_SUBSTATES_OVERRIDE_EN_MASK &&\n\t    (data & (PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_2_OVERRIDE_MASK |\n\t\t    PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_1_OVERRIDE_MASK |\n\t\t\tPCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_2_OVERRIDE_MASK |\n\t\t\tPCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_1_OVERRIDE_MASK))) {\n\t\tbL1SS = true;\n\t} else if (data1 & (PCIE_L1_PM_SUB_CNTL__ASPM_L1_2_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__ASPM_L1_1_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_2_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_1_EN_MASK)) {\n\t\tbL1SS = true;\n\t}\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL6);\n\tdata |= PCIE_LC_CNTL6__LC_L1_POWERDOWN_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL6, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_LINK_WIDTH_CNTL);\n\tdata |= PCIE_LC_LINK_WIDTH_CNTL__LC_DYN_LANES_PWR_STATE_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_LINK_WIDTH_CNTL, data);\n\n\tpci_read_config_dword(adev->pdev, LINK_CAP, &data);\n\tif (!(data & PCIE_LINK_CAP__CLOCK_POWER_MANAGEMENT_MASK))\n\t\tbClkReqSupport = false;\n\n\tif (bClkReqSupport) {\n\t\torig = data = RREG32_SMC(ixTHM_CLK_CNTL);\n\t\tdata &= ~(THM_CLK_CNTL__CMON_CLK_SEL_MASK | THM_CLK_CNTL__TMON_CLK_SEL_MASK);\n\t\tdata |= (1 << THM_CLK_CNTL__CMON_CLK_SEL__SHIFT) |\n\t\t\t\t(1 << THM_CLK_CNTL__TMON_CLK_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixTHM_CLK_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixMISC_CLK_CTRL);\n\t\tdata &= ~(MISC_CLK_CTRL__DEEP_SLEEP_CLK_SEL_MASK |\n\t\t\tMISC_CLK_CTRL__ZCLK_SEL_MASK | MISC_CLK_CTRL__DFT_SMS_PG_CLK_SEL_MASK);\n\t\tdata |= (1 << MISC_CLK_CTRL__DEEP_SLEEP_CLK_SEL__SHIFT) |\n\t\t\t\t(1 << MISC_CLK_CTRL__ZCLK_SEL__SHIFT);\n\t\tdata |= (0x20 << MISC_CLK_CTRL__DFT_SMS_PG_CLK_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixMISC_CLK_CTRL, data);\n\n\t\torig = data = RREG32_SMC(ixCG_CLKPIN_CNTL);\n\t\tdata |= CG_CLKPIN_CNTL__XTALIN_DIVIDE_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixCG_CLKPIN_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixCG_CLKPIN_CNTL_2);\n\t\tdata |= CG_CLKPIN_CNTL_2__ENABLE_XCLK_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixCG_CLKPIN_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixMPLL_BYPASSCLK_SEL);\n\t\tdata &= ~MPLL_BYPASSCLK_SEL__MPLL_CLKOUT_SEL_MASK;\n\t\tdata |= (4 << MPLL_BYPASSCLK_SEL__MPLL_CLKOUT_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixMPLL_BYPASSCLK_SEL, data);\n\n\t\torig = data = RREG32_PCIE(ixCPM_CONTROL);\n\t\tdata |= (CPM_CONTROL__REFCLK_XSTCLK_ENABLE_MASK |\n\t\t\t\tCPM_CONTROL__CLKREQb_UNGATE_TXCLK_ENABLE_MASK);\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixCPM_CONTROL, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_CONFIG_CNTL);\n\t\tdata &= ~PCIE_CONFIG_CNTL__DYN_CLK_LATENCY_MASK;\n\t\tdata |= (0xE << PCIE_CONFIG_CNTL__DYN_CLK_LATENCY__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_CONFIG_CNTL, data);\n\n\t\torig = data = RREG32(mmBIF_CLK_CTRL);\n\t\tdata |= BIF_CLK_CTRL__BIF_XSTCLK_READY_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32(mmBIF_CLK_CTRL, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL7);\n\t\tdata |= PCIE_LC_CNTL7__LC_L1_SIDEBAND_CLKREQ_PDWN_EN_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL7, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_HW_DEBUG);\n\t\tdata |= PCIE_HW_DEBUG__HW_01_DEBUG_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_HW_DEBUG, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL2);\n\t\tdata |= PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L23_MASK;\n\t\tdata |= PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L1_MASK;\n\t\tif (bL1SS)\n\t\t\tdata &= ~PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L1_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL2, data);\n\n\t}\n\n\tvi_enable_aspm(adev);\n\n\tdata = RREG32_PCIE(ixPCIE_LC_N_FTS_CNTL);\n\tdata1 = RREG32_PCIE(ixPCIE_LC_STATUS1);\n\tif (((data & PCIE_LC_N_FTS_CNTL__LC_N_FTS_MASK) == PCIE_LC_N_FTS_CNTL__LC_N_FTS_MASK) &&\n\t    data1 & PCIE_LC_STATUS1__LC_REVERSE_XMIT_MASK &&\n\t    data1 & PCIE_LC_STATUS1__LC_REVERSE_RCVR_MASK) {\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\t\tdata &= ~PCIE_LC_CNTL__LC_L0S_INACTIVITY_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n\t}\n\n\tif ((adev->asic_type == CHIP_POLARIS12 &&\n\t    !(ASICID_IS_P23(adev->pdev->device, adev->pdev->revision))) ||\n\t    ASIC_IS_P22(adev->asic_type, adev->external_rev_id)) {\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_TRAINING_CNTL);\n\t\tdata &= ~PCIE_LC_TRAINING_CNTL__LC_DISABLE_TRAINING_BIT_ARCH_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_TRAINING_CNTL, data);\n\t}\n}\n\nstatic void vi_enable_doorbell_aperture(struct amdgpu_device *adev,\n\t\t\t\t\tbool enable)\n{\n\tu32 tmp;\n\n\t/* not necessary on CZ */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\ttmp = RREG32(mmBIF_DOORBELL_APER_EN);\n\tif (enable)\n\t\ttmp = REG_SET_FIELD(tmp, BIF_DOORBELL_APER_EN, BIF_DOORBELL_APER_EN, 1);\n\telse\n\t\ttmp = REG_SET_FIELD(tmp, BIF_DOORBELL_APER_EN, BIF_DOORBELL_APER_EN, 0);\n\n\tWREG32(mmBIF_DOORBELL_APER_EN, tmp);\n}\n\n#define ATI_REV_ID_FUSE_MACRO__ADDRESS      0xC0014044\n#define ATI_REV_ID_FUSE_MACRO__SHIFT        9\n#define ATI_REV_ID_FUSE_MACRO__MASK         0x00001E00\n\nstatic uint32_t vi_get_rev_id(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn (RREG32_SMC(ATI_REV_ID_FUSE_MACRO__ADDRESS) & ATI_REV_ID_FUSE_MACRO__MASK)\n\t\t\t>> ATI_REV_ID_FUSE_MACRO__SHIFT;\n\telse\n\t\treturn (RREG32(mmPCIE_EFUSE4) & PCIE_EFUSE4__STRAP_BIF_ATI_REV_ID_MASK)\n\t\t\t>> PCIE_EFUSE4__STRAP_BIF_ATI_REV_ID__SHIFT;\n}\n\nstatic void vi_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void vi_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool vi_need_full_reset(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\t/* CZ has hang issues with full reset at the moment */\n\t\treturn false;\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\t\t/* XXX: soft reset should work on fiji and tonga */\n\t\treturn true;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\tdefault:\n\t\t/* change this when we support soft reset */\n\t\treturn true;\n\t}\n}\n\nstatic void vi_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t/* This reports 0 on APUs, so return to avoid writing/reading registers\n\t * that may or may not be different from their GPU counterparts\n\t */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t/* Set the 2 events that we wish to watch, defined above */\n\t/* Reg 40 is # received msgs, Reg 104 is # of posted requests sent */\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t/* Write to enable desired perf counters */\n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t/* Zero out and enable the perf counters\n\t * Write 0x5:\n\t * Bit 0 = Start all counters(1)\n\t * Bit 2 = Global counter reset enable(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t/* Load the shadow and disable the perf counters\n\t * Write 0x2:\n\t * Bit 0 = Stop counters(0)\n\t * Bit 1 = Load the shadow counters(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t/* Read register values to get any >32bit overflow */\n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t/* Get the values and add the overflow */\n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t vi_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t/* Get the number of NAKs received and generated */\n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t/* Add the total number of NAKs, i.e the number of replays */\n\treturn (nak_r + nak_g);\n}\n\nstatic bool vi_need_reset_on_init(struct amdgpu_device *adev)\n{\n\tu32 clock_cntl, pc;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t/* check if the SMC is already running */\n\tclock_cntl = RREG32_SMC(ixSMC_SYSCON_CLOCK_CNTL_0);\n\tpc = RREG32_SMC(ixSMC_PC_C);\n\tif ((0 == REG_GET_FIELD(clock_cntl, SMC_SYSCON_CLOCK_CNTL_0, ck_disable)) &&\n\t    (0x20100 <= pc))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void vi_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs vi_asic_funcs =\n{\n\t.read_disabled_bios = &vi_read_disabled_bios,\n\t.read_bios_from_rom = &vi_read_bios_from_rom,\n\t.read_register = &vi_read_register,\n\t.reset = &vi_asic_reset,\n\t.reset_method = &vi_asic_reset_method,\n\t.get_xclk = &vi_get_xclk,\n\t.set_uvd_clocks = &vi_set_uvd_clocks,\n\t.set_vce_clocks = &vi_set_vce_clocks,\n\t.get_config_memsize = &vi_get_config_memsize,\n\t.flush_hdp = &vi_flush_hdp,\n\t.invalidate_hdp = &vi_invalidate_hdp,\n\t.need_full_reset = &vi_need_full_reset,\n\t.init_doorbell_index = &legacy_doorbell_index_init,\n\t.get_pcie_usage = &vi_get_pcie_usage,\n\t.need_reset_on_init = &vi_need_reset_on_init,\n\t.get_pcie_replay_count = &vi_get_pcie_replay_count,\n\t.supports_baco = &vi_asic_supports_baco,\n\t.pre_asic_init = &vi_pre_asic_init,\n\t.query_video_codecs = &vi_query_video_codecs,\n};\n\n#define CZ_REV_BRISTOL(rev)\t \\\n\t((rev >= 0xC8 && rev <= 0xCE) || (rev >= 0xE1 && rev <= 0xE6))\n\nstatic int vi_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tadev->smc_rreg = &cz_smc_rreg;\n\t\tadev->smc_wreg = &cz_smc_wreg;\n\t} else {\n\t\tadev->smc_rreg = &vi_smc_rreg;\n\t\tadev->smc_wreg = &vi_smc_wreg;\n\t}\n\tadev->pcie_rreg = &vi_pcie_rreg;\n\tadev->pcie_wreg = &vi_pcie_wreg;\n\tadev->uvd_ctx_rreg = &vi_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = &vi_uvd_ctx_wreg;\n\tadev->didt_rreg = &vi_didt_rreg;\n\tadev->didt_wreg = &vi_didt_wreg;\n\tadev->gc_cac_rreg = &vi_gc_cac_rreg;\n\tadev->gc_cac_wreg = &vi_gc_cac_wreg;\n\n\tadev->asic_funcs = &vi_asic_funcs;\n\n\tadev->rev_id = vi_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tadev->cg_flags = 0;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 0x1;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x3c;\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x14;\n\t\tbreak;\n\tcase CHIP_POLARIS11:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x5A;\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x50;\n\t\tbreak;\n\tcase CHIP_POLARIS12:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x64;\n\t\tbreak;\n\tcase CHIP_VEGAM:\n\t\tadev->cg_flags = 0;\n\t\t\t/*AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;*/\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x6E;\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\t/* rev0 hardware requires workarounds to support PG */\n\t\tadev->pg_flags = 0;\n\t\tif (adev->rev_id != 0x00 || CZ_REV_BRISTOL(adev->pdev->revision)) {\n\t\t\tadev->pg_flags |= AMD_PG_SUPPORT_GFX_SMG |\n\t\t\t\tAMD_PG_SUPPORT_GFX_PIPELINE |\n\t\t\t\tAMD_PG_SUPPORT_CP |\n\t\t\t\tAMD_PG_SUPPORT_UVD |\n\t\t\t\tAMD_PG_SUPPORT_VCE;\n\t\t}\n\t\tadev->external_rev_id = adev->rev_id + 0x1;\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = AMD_PG_SUPPORT_GFX_PG |\n\t\t\tAMD_PG_SUPPORT_GFX_SMG |\n\t\t\tAMD_PG_SUPPORT_GFX_PIPELINE |\n\t\t\tAMD_PG_SUPPORT_CP |\n\t\t\tAMD_PG_SUPPORT_UVD |\n\t\t\tAMD_PG_SUPPORT_VCE;\n\t\tadev->external_rev_id = adev->rev_id + 0x61;\n\t\tbreak;\n\tdefault:\n\t\t/* FIXME: not supported yet */\n\t\treturn -EINVAL;\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_init_setting(adev);\n\t\txgpu_vi_mailbox_set_irq_funcs(adev);\n\t}\n\n\treturn 0;\n}\n\nstatic int vi_common_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_get_irq(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_sw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_add_irq_id(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int vi_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t/* move the golden regs per IP block */\n\tvi_init_golden_registers(adev);\n\t/* enable aspm */\n\tvi_program_aspm(adev);\n\t/* enable the doorbell aperture */\n\tvi_enable_doorbell_aperture(adev, true);\n\n\treturn 0;\n}\n\nstatic int vi_common_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t/* enable the doorbell aperture */\n\tvi_enable_doorbell_aperture(adev, false);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_put_irq(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn vi_common_hw_fini(adev);\n}\n\nstatic int vi_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn vi_common_hw_init(adev);\n}\n\nstatic bool vi_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int vi_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int vi_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic void vi_update_bif_medium_grain_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t\t\t   bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32_PCIE(ixPCIE_CNTL2);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS))\n\t\tdata |= PCIE_CNTL2__SLV_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__MST_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__REPLAY_MEM_LS_EN_MASK;\n\telse\n\t\tdata &= ~(PCIE_CNTL2__SLV_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__MST_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__REPLAY_MEM_LS_EN_MASK);\n\n\tif (temp != data)\n\t\tWREG32_PCIE(ixPCIE_CNTL2, data);\n}\n\nstatic void vi_update_hdp_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t    bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(mmHDP_HOST_PATH_CNTL);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG))\n\t\tdata &= ~HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK;\n\telse\n\t\tdata |= HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK;\n\n\tif (temp != data)\n\t\tWREG32(mmHDP_HOST_PATH_CNTL, data);\n}\n\nstatic void vi_update_hdp_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t      bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(mmHDP_MEM_POWER_LS);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS))\n\t\tdata |= HDP_MEM_POWER_LS__LS_ENABLE_MASK;\n\telse\n\t\tdata &= ~HDP_MEM_POWER_LS__LS_ENABLE_MASK;\n\n\tif (temp != data)\n\t\tWREG32(mmHDP_MEM_POWER_LS, data);\n}\n\nstatic void vi_update_drm_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t      bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(0x157a);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_DRM_LS))\n\t\tdata |= 1;\n\telse\n\t\tdata &= ~1;\n\n\tif (temp != data)\n\t\tWREG32(0x157a, data);\n}\n\n\nstatic void vi_update_rom_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t    bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32_SMC(ixCGTT_ROM_CLK_CTRL0);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_ROM_MGCG))\n\t\tdata &= ~(CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK |\n\t\t\t\tCGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE1_MASK);\n\telse\n\t\tdata |= CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK |\n\t\t\t\tCGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE1_MASK;\n\n\tif (temp != data)\n\t\tWREG32_SMC(ixCGTT_ROM_CLK_CTRL0, data);\n}\n\nstatic int vi_common_set_clockgating_state_by_smu(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tuint32_t msg_id, pp_state = 0;\n\tuint32_t pp_support_state = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_MC_LS | AMD_CG_SUPPORT_MC_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_MC_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_MC_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_MC,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_SDMA_LS | AMD_CG_SUPPORT_SDMA_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_SDMA_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_SDMA,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_HDP_LS | AMD_CG_SUPPORT_HDP_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_HDP,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS) {\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_LS;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_BIF,\n\t\t\t       PP_STATE_SUPPORT_LS,\n\t\t\t        pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\tif (adev->cg_flags & AMD_CG_SUPPORT_BIF_MGCG) {\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_CG;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_BIF,\n\t\t\t       PP_STATE_SUPPORT_CG,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_DRM_LS) {\n\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_LS;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_DRM,\n\t\t\t       PP_STATE_SUPPORT_LS,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_ROM_MGCG) {\n\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_CG;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_ROM,\n\t\t\t       PP_STATE_SUPPORT_CG,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\treturn 0;\n}\n\nstatic int vi_common_set_clockgating_state(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\t\tvi_update_bif_medium_grain_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_rom_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tvi_update_bif_medium_grain_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_drm_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tbreak;\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tvi_common_set_clockgating_state_by_smu(adev, state);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int vi_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic void vi_common_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\t*flags = 0;\n\n\t/* AMD_CG_SUPPORT_BIF_LS */\n\tdata = RREG32_PCIE(ixPCIE_CNTL2);\n\tif (data & PCIE_CNTL2__SLV_MEM_LS_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_BIF_LS;\n\n\t/* AMD_CG_SUPPORT_HDP_LS */\n\tdata = RREG32(mmHDP_MEM_POWER_LS);\n\tif (data & HDP_MEM_POWER_LS__LS_ENABLE_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_HDP_LS;\n\n\t/* AMD_CG_SUPPORT_HDP_MGCG */\n\tdata = RREG32(mmHDP_HOST_PATH_CNTL);\n\tif (!(data & HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_HDP_MGCG;\n\n\t/* AMD_CG_SUPPORT_ROM_MGCG */\n\tdata = RREG32_SMC(ixCGTT_ROM_CLK_CTRL0);\n\tif (!(data & CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_ROM_MGCG;\n}\n\nstatic const struct amd_ip_funcs vi_common_ip_funcs = {\n\t.name = \"vi_common\",\n\t.early_init = vi_common_early_init,\n\t.late_init = vi_common_late_init,\n\t.sw_init = vi_common_sw_init,\n\t.sw_fini = vi_common_sw_fini,\n\t.hw_init = vi_common_hw_init,\n\t.hw_fini = vi_common_hw_fini,\n\t.suspend = vi_common_suspend,\n\t.resume = vi_common_resume,\n\t.is_idle = vi_common_is_idle,\n\t.wait_for_idle = vi_common_wait_for_idle,\n\t.soft_reset = vi_common_soft_reset,\n\t.set_clockgating_state = vi_common_set_clockgating_state,\n\t.set_powergating_state = vi_common_set_powergating_state,\n\t.get_clockgating_state = vi_common_get_clockgating_state,\n\t.dump_ip_state = NULL,\n\t.print_ip_state = NULL,\n};\n\nstatic const struct amdgpu_ip_block_version vi_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &vi_common_ip_funcs,\n};\n\nvoid vi_set_virt_ops(struct amdgpu_device *adev)\n{\n\tadev->virt.ops = &xgpu_vi_virt_ops;\n}\n\nint vi_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tamdgpu_device_set_sriov_virtual_display(adev);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\t/* topaz has no DCE, UVD, VCE */\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v7_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &iceland_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v2_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_5_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v10_1_ip_block);\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_0_ip_block);\n\t\t\tamdgpu_device_ip_block_add(adev, &vce_v3_0_ip_block);\n\t\t}\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v10_0_ip_block);\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tamdgpu_device_ip_block_add(adev, &uvd_v5_0_ip_block);\n\t\t\tamdgpu_device_ip_block_add(adev, &vce_v3_0_ip_block);\n\t\t}\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_2_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_3_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_4_ip_block);\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &cz_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_1_ip_block);\n#if defined(CONFIG_DRM_AMD_ACP)\n\t\tamdgpu_device_ip_block_add(adev, &acp_ip_block);\n#endif\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &cz_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_2_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_4_ip_block);\n#if defined(CONFIG_DRM_AMD_ACP)\n\t\tamdgpu_device_ip_block_add(adev, &acp_ip_block);\n#endif\n\t\tbreak;\n\tdefault:\n\t\t/* FIXME: not supported yet */\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid legacy_doorbell_index_init(struct amdgpu_device *adev)\n{\n\tadev->doorbell_index.kiq = AMDGPU_DOORBELL_KIQ;\n\tadev->doorbell_index.mec_ring0 = AMDGPU_DOORBELL_MEC_RING0;\n\tadev->doorbell_index.mec_ring1 = AMDGPU_DOORBELL_MEC_RING1;\n\tadev->doorbell_index.mec_ring2 = AMDGPU_DOORBELL_MEC_RING2;\n\tadev->doorbell_index.mec_ring3 = AMDGPU_DOORBELL_MEC_RING3;\n\tadev->doorbell_index.mec_ring4 = AMDGPU_DOORBELL_MEC_RING4;\n\tadev->doorbell_index.mec_ring5 = AMDGPU_DOORBELL_MEC_RING5;\n\tadev->doorbell_index.mec_ring6 = AMDGPU_DOORBELL_MEC_RING6;\n\tadev->doorbell_index.mec_ring7 = AMDGPU_DOORBELL_MEC_RING7;\n\tadev->doorbell_index.gfx_ring0 = AMDGPU_DOORBELL_GFX_RING0;\n\tadev->doorbell_index.sdma_engine[0] = AMDGPU_DOORBELL_sDMA_ENGINE0;\n\tadev->doorbell_index.sdma_engine[1] = AMDGPU_DOORBELL_sDMA_ENGINE1;\n\tadev->doorbell_index.ih = AMDGPU_DOORBELL_IH;\n\tadev->doorbell_index.max_assignment = AMDGPU_DOORBELL_MAX_ASSIGNMENT;\n}\n"}
