{"bug_report_path": "si-bug.txt", "bug_report_text": "Issue: Lack of Boundary Checks\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/si.c\nLine: 1324\n\nDescription:\nThe function si_read_bios_from_rom lacks proper boundary checks when reading data into the bios buffer. The function calculates the number of dwords to read based on the input length_bytes, but it doesn't verify if this calculated length exceeds the actual allocated size of the bios buffer. This can lead to a buffer overflow, potentially causing memory corruption or security vulnerabilities.\n\n", "diff_path": "si-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n", "code": "/*\n * Copyright 2015 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/firmware.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n#include \"si_dpm.h\"\n#include \"sid.h\"\n#include \"si_ih.h\"\n#include \"gfx_v6_0.h\"\n#include \"gmc_v6_0.h\"\n#include \"si_dma.h\"\n#include \"dce_v6_0.h\"\n#include \"si.h\"\n#include \"uvd_v3_1.h\"\n#include \"amdgpu_vkms.h\"\n#include \"gca/gfx_6_0_d.h\"\n#include \"oss/oss_1_0_d.h\"\n#include \"oss/oss_1_0_sh_mask.h\"\n#include \"gmc/gmc_6_0_d.h\"\n#include \"dce/dce_6_0_d.h\"\n#include \"uvd/uvd_4_0_d.h\"\n#include \"bif/bif_3_0_d.h\"\n#include \"bif/bif_3_0_sh_mask.h\"\n\n#include \"amdgpu_dm.h\"\n\nstatic const u32 tahiti_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000000c0, 0x00800040,\n\t0x360c, 0x000000c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_ARB_WTM_CNTL_RD, 0x00000003, 0x000007ff,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x00000200, 0x000002fb,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x0000543b,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0xa9210876,\n\tmmVGT_FIFO_DEPTHS, 0xffffffff, 0x000fff40,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_CONTEXT0_CNTL, 0x20000000, 0x20fffed8,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 tahiti_golden_registers2[] =\n{\n\tmmMCIF_MEM_CONTROL, 0x00000001, 0x00000001,\n};\n\nstatic const u32 tahiti_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n\tmmUVD_CGC_GATE, 0x00000008, 0x00000000,\n};\n\nstatic const u32 pitcairn_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f7,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x32761054,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 pitcairn_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601004,\n\t0x311f, 0xffffffff, 0x10102020,\n\t0x3122, 0xffffffff, 0x01000020,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000a4,\n};\n\nstatic const u32 verde_pg_init[] =\n{\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x40000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x200010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x7007,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x300010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x400000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x100010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x120200,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x500010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x1e1e16,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x600010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x171f1e,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x700010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x9ff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x0,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10000800,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1000051e,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x8,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x80500,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x12,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x9050c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xb052c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1053e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10546,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x30,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xa054e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1055f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3f,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10567,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x42,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1056f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x45,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10572,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x48,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20575,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x190801,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x67,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1082a,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x6a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1b082d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x87,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x310851,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xba,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x891,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x893,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbe,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20895,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc2,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20899,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc6,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2089d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xca,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xcc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a3,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xce,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x308a5,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xd3,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x6d08cd,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x142,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2000095a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x144,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x301f095b,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x165,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc094d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x173,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf096d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x184,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x15097f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x19b,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc0998,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1a9,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x409a7,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1af,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xcdc,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1b1,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x800,\n\tmmGMCON_RENG_EXECUTE, 0xffffffff, 0x6c9b2000,\n\tmmGMCON_MISC2, 0xfc00, 0x2000,\n\tmmGMCON_MISC3, 0xffffffff, 0xfc0,\n\tmmMC_PMG_AUTO_CFG, 0x00000100, 0x100,\n};\n\nstatic const u32 verde_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x033f1005,\n\t0x311f, 0xffffffff, 0x10808020,\n\t0x3122, 0xffffffff, 0x00800008,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00001000,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x80010014,\n};\n\nstatic const u32 verde_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x0000124a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x00000003,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00001032,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 oland_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000082,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f3,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\n};\n\nstatic const u32 oland_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n};\n\nstatic const u32 hainan_golden_registers[] =\n{\n\t0x17bc, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\t0x031e, 0x00000080, 0x00000000,\n\t0x3430, 0xff000fff, 0x00000100,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x3630, 0xff000fff, 0x00000100,\n\t0x360c, 0x000300c0, 0x00800040,\n\t0x16ec, 0x000000f0, 0x00000070,\n\t0x16f0, 0x00200000, 0x50100000,\n\t0x1c0c, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000000,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x03e00000, 0x03600000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f1,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 hainan_golden_registers2[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x2011003,\n};\n\nstatic const u32 tahiti_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\t0x2476, 0xffffffff, 0x00070006,\n\t0x2477, 0xffffffff, 0x00090008,\n\t0x2478, 0xffffffff, 0x0000000c,\n\t0x2479, 0xffffffff, 0x000b000a,\n\t0x247a, 0xffffffff, 0x000e000d,\n\t0x247b, 0xffffffff, 0x00080007,\n\t0x247c, 0xffffffff, 0x000a0009,\n\t0x247d, 0xffffffff, 0x0000000d,\n\t0x247e, 0xffffffff, 0x000c000b,\n\t0x247f, 0xffffffff, 0x000f000e,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\t0x2499, 0xffffffff, 0x000e000d,\n\t0x249a, 0xffffffff, 0x0010000f,\n\t0x249b, 0xffffffff, 0x00000013,\n\t0x249c, 0xffffffff, 0x00120011,\n\t0x249d, 0xffffffff, 0x00150014,\n\t0x249e, 0xffffffff, 0x000f000e,\n\t0x249f, 0xffffffff, 0x00110010,\n\t0x24a0, 0xffffffff, 0x00000014,\n\t0x24a1, 0xffffffff, 0x00130012,\n\t0x24a2, 0xffffffff, 0x00160015,\n\t0x24a3, 0xffffffff, 0x0010000f,\n\t0x24a4, 0xffffffff, 0x00120011,\n\t0x24a5, 0xffffffff, 0x00000015,\n\t0x24a6, 0xffffffff, 0x00140013,\n\t0x24a7, 0xffffffff, 0x00170016,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\nstatic const u32 pitcairn_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 verde_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 oland_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 hainan_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\t0x0409, 0xffffffff, 0x00000100,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\n/* XXX: update when we support VCE */\n#if 0\n/* tahiti, pitcarin, verde */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_encode_array),\n\t.codec_array = tahiti_video_codecs_encode_array,\n};\n#else\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n#endif\n/* oland and hainan don't support encode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* tahiti, pitcarin, verde, oland */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 41,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_decode_array),\n\t.codec_array = tahiti_video_codecs_decode_array,\n};\n\n/* hainan doesn't support decode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\nstatic int si_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tif (encode)\n\t\t\t*codecs = &tahiti_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_OLAND:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_HAINAN:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &hainan_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic u32 si_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tr = RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tWREG32(AMDGPU_PCIE_DATA, v);\n\t(void)RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_pciep_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tr = RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pciep_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tWREG32(PCIE_PORT_DATA, (v));\n\t(void)RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tr = RREG32(SMC_IND_DATA_0);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tWREG32(SMC_IND_DATA_0, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 si_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic struct amdgpu_allowed_register_entry si_allowed_read_registers[] = {\n\t{GRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{DMA_STATUS_REG + DMA0_REGISTER_OFFSET},\n\t{DMA_STATUS_REG + DMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{GB_ADDR_CONFIG},\n\t{MC_ARB_RAMCFG},\n\t{GB_TILE_MODE0},\n\t{GB_TILE_MODE1},\n\t{GB_TILE_MODE2},\n\t{GB_TILE_MODE3},\n\t{GB_TILE_MODE4},\n\t{GB_TILE_MODE5},\n\t{GB_TILE_MODE6},\n\t{GB_TILE_MODE7},\n\t{GB_TILE_MODE8},\n\t{GB_TILE_MODE9},\n\t{GB_TILE_MODE10},\n\t{GB_TILE_MODE11},\n\t{GB_TILE_MODE12},\n\t{GB_TILE_MODE13},\n\t{GB_TILE_MODE14},\n\t{GB_TILE_MODE15},\n\t{GB_TILE_MODE16},\n\t{GB_TILE_MODE17},\n\t{GB_TILE_MODE18},\n\t{GB_TILE_MODE19},\n\t{GB_TILE_MODE20},\n\t{GB_TILE_MODE21},\n\t{GB_TILE_MODE22},\n\t{GB_TILE_MODE23},\n\t{GB_TILE_MODE24},\n\t{GB_TILE_MODE25},\n\t{GB_TILE_MODE26},\n\t{GB_TILE_MODE27},\n\t{GB_TILE_MODE28},\n\t{GB_TILE_MODE29},\n\t{GB_TILE_MODE30},\n\t{GB_TILE_MODE31},\n\t{CC_RB_BACKEND_DISABLE, true},\n\t{GC_USER_RB_BACKEND_DISABLE, true},\n\t{PA_SC_RASTER_CONFIG, true},\n};\n\nstatic uint32_t si_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\nstatic int si_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t     u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(si_allowed_read_registers); i++) {\n\t\tbool indexed = si_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != si_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = si_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic bool si_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(R600_BUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(AVIVO_D1VGA_CONTROL);\n\t\td2vga_control = RREG32(AVIVO_D2VGA_CONTROL);\n\t\tvga_render_control = RREG32(VGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32(R600_ROM_CNTL);\n\n\t/* enable the rom */\n\tWREG32(R600_BUS_CNTL, (bus_cntl & ~R600_BIOS_ROM_DIS));\n\tif (adev->mode_info.num_crtc) {\n\t\t/* Disable VGA mode */\n\t\tWREG32(AVIVO_D1VGA_CONTROL,\n\t\t       (d1vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(AVIVO_D2VGA_CONTROL,\n\t\t       (d2vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(VGA_RENDER_CONTROL,\n\t\t       (vga_render_control & C_000300_VGA_VSTATUS_CNTL));\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl | R600_SCK_OVERWRITE);\n\n\tr = amdgpu_read_bios(adev);\n\n\t/* restore regs */\n\tWREG32(R600_BUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(AVIVO_D1VGA_CONTROL, d1vga_control);\n\t\tWREG32(AVIVO_D2VGA_CONTROL, d2vga_control);\n\t\tWREG32(VGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl);\n\treturn r;\n}\n\n#define mmROM_INDEX 0x2A\n#define mmROM_DATA  0x2B\n\nstatic bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\nstatic void si_set_clk_bypass_mode(struct amdgpu_device *adev)\n{\n\tu32 tmp, i;\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_BYPASS_EN;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp |= SPLL_CTLREQ_CHG;\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(SPLL_STATUS) & SPLL_CHG_STATUS)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp &= ~(SPLL_CTLREQ_CHG | SCLK_MUX_UPDATE);\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\ttmp = RREG32(MPLL_CNTL_MODE);\n\ttmp &= ~MPLL_MCLK_SEL;\n\tWREG32(MPLL_CNTL_MODE, tmp);\n}\n\nstatic void si_spll_powerdown(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp |= SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_RESET;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_SLEEP;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp &= ~SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n}\n\nstatic int si_gpu_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t/* set mclk/sclk to bypass */\n\tsi_set_clk_bypass_mode(adev);\n\t/* powerdown spll */\n\tsi_spll_powerdown(adev);\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\t/* reset */\n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t/* wait for asic to come out of reset */\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t/* enable BM */\n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic int si_asic_supports_baco(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\nstatic enum amd_reset_method\nsi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_PCI)\n\t\treturn amdgpu_reset_method;\n\telse if (amdgpu_reset_method != AMD_RESET_METHOD_LEGACY &&\n\t\t amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t amdgpu_reset_method);\n\n\treturn AMD_RESET_METHOD_LEGACY;\n}\n\nstatic int si_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tswitch (si_asic_reset_method(adev)) {\n\tcase AMD_RESET_METHOD_PCI:\n\t\tdev_info(adev->dev, \"PCI reset\\n\");\n\t\tr = amdgpu_device_pci_reset(adev);\n\t\tbreak;\n\tdefault:\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = si_gpu_pci_config_reset(adev);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic u32 si_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic void si_vga_set_state(struct amdgpu_device *adev, bool state)\n{\n\tuint32_t temp;\n\n\ttemp = RREG32(CONFIG_CNTL);\n\tif (!state) {\n\t\ttemp &= ~(1<<0);\n\t\ttemp |= (1<<1);\n\t} else {\n\t\ttemp &= ~(1<<1);\n\t}\n\tWREG32(CONFIG_CNTL, temp);\n}\n\nstatic u32 si_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL_2);\n\tif (tmp & MUX_TCLK_TO_XCLK)\n\t\treturn TCLK;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL);\n\tif (tmp & XTALIN_DIVIDE)\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\nstatic void si_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void si_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool si_need_full_reset(struct amdgpu_device *adev)\n{\n\t/* change this when we support soft reset */\n\treturn true;\n}\n\nstatic bool si_need_reset_on_init(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n\nstatic int si_get_pcie_lanes(struct amdgpu_device *adev)\n{\n\tu32 link_width_cntl;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\n\tswitch ((link_width_cntl & LC_LINK_WIDTH_RD_MASK) >> LC_LINK_WIDTH_RD_SHIFT) {\n\tcase LC_LINK_WIDTH_X1:\n\t\treturn 1;\n\tcase LC_LINK_WIDTH_X2:\n\t\treturn 2;\n\tcase LC_LINK_WIDTH_X4:\n\t\treturn 4;\n\tcase LC_LINK_WIDTH_X8:\n\t\treturn 8;\n\tcase LC_LINK_WIDTH_X0:\n\tcase LC_LINK_WIDTH_X16:\n\tdefault:\n\t\treturn 16;\n\t}\n}\n\nstatic void si_set_pcie_lanes(struct amdgpu_device *adev, int lanes)\n{\n\tu32 link_width_cntl, mask;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tswitch (lanes) {\n\tcase 0:\n\t\tmask = LC_LINK_WIDTH_X0;\n\t\tbreak;\n\tcase 1:\n\t\tmask = LC_LINK_WIDTH_X1;\n\t\tbreak;\n\tcase 2:\n\t\tmask = LC_LINK_WIDTH_X2;\n\t\tbreak;\n\tcase 4:\n\t\tmask = LC_LINK_WIDTH_X4;\n\t\tbreak;\n\tcase 8:\n\t\tmask = LC_LINK_WIDTH_X8;\n\t\tbreak;\n\tcase 16:\n\t\tmask = LC_LINK_WIDTH_X16;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"invalid pcie lane request: %d\\n\", lanes);\n\t\treturn;\n\t}\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\tlink_width_cntl &= ~LC_LINK_WIDTH_MASK;\n\tlink_width_cntl |= mask << LC_LINK_WIDTH_SHIFT;\n\tlink_width_cntl |= (LC_RECONFIG_NOW |\n\t\t\t    LC_RECONFIG_ARC_MISSING_ESCAPE);\n\n\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, link_width_cntl);\n}\n\nstatic void si_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t/* This reports 0 on APUs, so return to avoid writing/reading registers\n\t * that may or may not be different from their GPU counterparts\n\t */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t/* Set the 2 events that we wish to watch, defined above */\n\t/* Reg 40 is # received msgs, Reg 104 is # of posted requests sent */\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t/* Write to enable desired perf counters */\n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t/* Zero out and enable the perf counters\n\t * Write 0x5:\n\t * Bit 0 = Start all counters(1)\n\t * Bit 2 = Global counter reset enable(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t/* Load the shadow and disable the perf counters\n\t * Write 0x2:\n\t * Bit 0 = Stop counters(0)\n\t * Bit 1 = Load the shadow counters(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t/* Read register values to get any >32bit overflow */\n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t/* Get the values and add the overflow */\n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t si_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t/* Get the number of NAKs received and generated */\n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t/* Add the total number of NAKs, i.e the number of replays */\n\treturn (nak_r + nak_g);\n}\n\nstatic int si_uvd_send_upll_ctlreq(struct amdgpu_device *adev,\n\t\t\t\t   unsigned cg_upll_func_cntl)\n{\n\tunsigned i;\n\n\t/* Make sure UPLL_CTLREQ is deasserted */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32(cg_upll_func_cntl) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned si_uvd_calc_upll_post_div(unsigned vco_freq,\n\t\t\t\t\t  unsigned target_freq,\n\t\t\t\t\t  unsigned pd_min,\n\t\t\t\t\t  unsigned pd_even)\n{\n\tunsigned post_div = vco_freq / target_freq;\n\n\t/* Adjust to post divider minimum value */\n\tif (post_div < pd_min)\n\t\tpost_div = pd_min;\n\n\t/* We alway need a frequency less than or equal the target */\n\tif ((vco_freq / post_div) > target_freq)\n\t\tpost_div += 1;\n\n\t/* Post dividers above a certain value must be even */\n\tif (post_div > pd_even && post_div % 2)\n\t\tpost_div += 1;\n\n\treturn post_div;\n}\n\n/**\n * si_calc_upll_dividers - calc UPLL clock dividers\n *\n * @adev: amdgpu_device pointer\n * @vclk: wanted VCLK\n * @dclk: wanted DCLK\n * @vco_min: minimum VCO frequency\n * @vco_max: maximum VCO frequency\n * @fb_factor: factor to multiply vco freq with\n * @fb_mask: limit and bitmask for feedback divider\n * @pd_min: post divider minimum\n * @pd_max: post divider maximum\n * @pd_even: post divider must be even above this value\n * @optimal_fb_div: resulting feedback divider\n * @optimal_vclk_div: resulting vclk post divider\n * @optimal_dclk_div: resulting dclk post divider\n *\n * Calculate dividers for UVDs UPLL (except APUs).\n * Returns zero on success; -EINVAL on error.\n */\nstatic int si_calc_upll_dividers(struct amdgpu_device *adev,\n\t\t\t\t unsigned vclk, unsigned dclk,\n\t\t\t\t unsigned vco_min, unsigned vco_max,\n\t\t\t\t unsigned fb_factor, unsigned fb_mask,\n\t\t\t\t unsigned pd_min, unsigned pd_max,\n\t\t\t\t unsigned pd_even,\n\t\t\t\t unsigned *optimal_fb_div,\n\t\t\t\t unsigned *optimal_vclk_div,\n\t\t\t\t unsigned *optimal_dclk_div)\n{\n\tunsigned vco_freq, ref_freq = adev->clock.spll.reference_freq;\n\n\t/* Start off with something large */\n\tunsigned optimal_score = ~0;\n\n\t/* Loop through vco from low to high */\n\tvco_min = max(max(vco_min, vclk), dclk);\n\tfor (vco_freq = vco_min; vco_freq <= vco_max; vco_freq += 100) {\n\t\tuint64_t fb_div = (uint64_t)vco_freq * fb_factor;\n\t\tunsigned vclk_div, dclk_div, score;\n\n\t\tdo_div(fb_div, ref_freq);\n\n\t\t/* fb div out of range ? */\n\t\tif (fb_div > fb_mask)\n\t\t\tbreak; /* It can oly get worse */\n\n\t\tfb_div &= fb_mask;\n\n\t\t/* Calc vclk divider with current vco freq */\n\t\tvclk_div = si_uvd_calc_upll_post_div(vco_freq, vclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (vclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc dclk divider with current vco freq */\n\t\tdclk_div = si_uvd_calc_upll_post_div(vco_freq, dclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (dclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc score with current vco freq */\n\t\tscore = vclk - (vco_freq / vclk_div) + dclk - (vco_freq / dclk_div);\n\n\t\t/* Determine if this vco setting is better than current optimal settings */\n\t\tif (score < optimal_score) {\n\t\t\t*optimal_fb_div = fb_div;\n\t\t\t*optimal_vclk_div = vclk_div;\n\t\t\t*optimal_dclk_div = dclk_div;\n\t\t\toptimal_score = score;\n\t\t\tif (optimal_score == 0)\n\t\t\t\tbreak; /* It can't get better than this */\n\t\t}\n\t}\n\n\t/* Did we found a valid setup ? */\n\tif (optimal_score == ~0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int si_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tunsigned fb_div = 0, vclk_div = 0, dclk_div = 0;\n\tint r;\n\n\t/* Bypass vclk and dclk with bclk */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(1) | DCLK_SRC_SEL(1),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_BYPASS_EN_MASK, ~UPLL_BYPASS_EN_MASK);\n\n\tif (!vclk || !dclk) {\n\t\t/* Keep the Bypass mode */\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, vclk, dclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &vclk_div, &dclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_VCO_MODE_MASK, ~UPLL_VCO_MODE_MASK);\n\n\t/* Disable sleep mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_SLEEP_MASK);\n\n\t/* Deassert UPLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert UPLL_RESET again */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_RESET_MASK, ~UPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_P(CG_UPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_3, UPLL_FB_DIV(fb_div), ~UPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_REF_DIV_MASK);\n\n\tif (fb_div < 307200)\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4, 0, ~UPLL_SPARE_ISPARE9);\n\telse\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4,\n\t\t\t UPLL_SPARE_ISPARE9,\n\t\t\t ~UPLL_SPARE_ISPARE9);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t UPLL_PDIV_A(vclk_div) | UPLL_PDIV_B(dclk_div),\n\t\t ~(UPLL_PDIV_A_MASK | UPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_BYPASS_EN_MASK);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(2) | DCLK_SRC_SEL(2),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic int si_vce_send_vcepll_ctlreq(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t/* Make sure VCEPLL_CTLREQ is deasserted */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32_SMC(CG_VCEPLL_FUNC_CNTL) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tunsigned fb_div = 0, evclk_div = 0, ecclk_div = 0;\n\tint r;\n\n\t/* Bypass evclk and ecclk with bclk */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(1) | ECCLK_SRC_SEL(1),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_BYPASS_EN_MASK,\n\t\t     ~VCEPLL_BYPASS_EN_MASK);\n\n\tif (!evclk || !ecclk) {\n\t\t/* Keep the Bypass mode, put PLL to sleep */\n\t\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t\t     ~VCEPLL_SLEEP_MASK);\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, evclk, ecclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &evclk_div, &ecclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_VCO_MODE_MASK,\n\t\t     ~VCEPLL_VCO_MODE_MASK);\n\n\t/* Toggle VCEPLL_SLEEP to 1 then back to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t     ~VCEPLL_SLEEP_MASK);\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_SLEEP_MASK);\n\n\t/* Deassert VCEPLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert VCEPLL_RESET again */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_RESET_MASK, ~VCEPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_SMC_P(CG_VCEPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_3,\n\t\t     VCEPLL_FB_DIV(fb_div),\n\t\t     ~VCEPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_REF_DIV_MASK);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     VCEPLL_PDIV_A(evclk_div) | VCEPLL_PDIV_B(ecclk_div),\n\t\t     ~(VCEPLL_PDIV_A_MASK | VCEPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_BYPASS_EN_MASK);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(16) | ECCLK_SRC_SEL(16),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic void si_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs si_asic_funcs =\n{\n\t.read_disabled_bios = &si_read_disabled_bios,\n\t.read_bios_from_rom = &si_read_bios_from_rom,\n\t.read_register = &si_read_register,\n\t.reset = &si_asic_reset,\n\t.reset_method = &si_asic_reset_method,\n\t.set_vga_state = &si_vga_set_state,\n\t.get_xclk = &si_get_xclk,\n\t.set_uvd_clocks = &si_set_uvd_clocks,\n\t.set_vce_clocks = &si_set_vce_clocks,\n\t.get_pcie_lanes = &si_get_pcie_lanes,\n\t.set_pcie_lanes = &si_set_pcie_lanes,\n\t.get_config_memsize = &si_get_config_memsize,\n\t.flush_hdp = &si_flush_hdp,\n\t.invalidate_hdp = &si_invalidate_hdp,\n\t.need_full_reset = &si_need_full_reset,\n\t.get_pcie_usage = &si_get_pcie_usage,\n\t.need_reset_on_init = &si_need_reset_on_init,\n\t.get_pcie_replay_count = &si_get_pcie_replay_count,\n\t.supports_baco = &si_asic_supports_baco,\n\t.pre_asic_init = &si_pre_asic_init,\n\t.query_video_codecs = &si_query_video_codecs,\n};\n\nstatic uint32_t si_get_rev_id(struct amdgpu_device *adev)\n{\n\treturn (RREG32(CC_DRM_ID_STRAPS) & CC_DRM_ID_STRAPS__ATI_REV_ID_MASK)\n\t\t>> CC_DRM_ID_STRAPS__ATI_REV_ID__SHIFT;\n}\n\nstatic int si_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->smc_rreg = &si_smc_rreg;\n\tadev->smc_wreg = &si_smc_wreg;\n\tadev->pcie_rreg = &si_pcie_rreg;\n\tadev->pcie_wreg = &si_pcie_wreg;\n\tadev->pciep_rreg = &si_pciep_rreg;\n\tadev->pciep_wreg = &si_pciep_wreg;\n\tadev->uvd_ctx_rreg = si_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = si_uvd_ctx_wreg;\n\tadev->didt_rreg = NULL;\n\tadev->didt_wreg = NULL;\n\n\tadev->asic_funcs = &si_asic_funcs;\n\n\tadev->rev_id = si_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = (adev->rev_id == 0) ? 1 :\n\t\t\t\t\t(adev->rev_id == 1) ? 5 : 6;\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 20;\n\t\tbreak;\n\n\tcase CHIP_VERDE:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\t//???\n\t\tadev->external_rev_id = adev->rev_id + 40;\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 60;\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 70;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_common_sw_init(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\n\nstatic void si_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers2));\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_VERDE:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_pg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_pg_init));\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers2));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_mgcg_cgcg_init));\n\t\tbreak;\n\n\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void si_pcie_gen3_enable(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *root = adev->pdev->bus->self;\n\tu32 speed_cntl, current_data_rate;\n\tint i;\n\tu16 tmp16;\n\n\tif (pci_is_root_bus(adev->pdev->bus))\n\t\treturn;\n\n\tif (amdgpu_pcie_gen2 == 0)\n\t\treturn;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tif (!(adev->pm.pcie_gen_mask & (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\tCAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)))\n\t\treturn;\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tcurrent_data_rate = (speed_cntl & LC_CURRENT_DATA_RATE_MASK) >>\n\t\tLC_CURRENT_DATA_RATE_SHIFT;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate == 2) {\n\t\t\tDRM_INFO(\"PCIE gen 3 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 3 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t} else if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2) {\n\t\tif (current_data_rate == 1) {\n\t\t\tDRM_INFO(\"PCIE gen 2 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 2 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t}\n\n\tif (!pci_is_pcie(root) || !pci_is_pcie(adev->pdev))\n\t\treturn;\n\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate != 2) {\n\t\t\tu16 bridge_cfg, gpu_cfg;\n\t\t\tu16 bridge_cfg2, gpu_cfg2;\n\t\t\tu32 max_lw, current_lw, tmp;\n\n\t\t\tpcie_capability_set_word(root, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\t\t\tpcie_capability_set_word(adev->pdev, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\n\t\t\ttmp = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tmax_lw = (tmp & LC_DETECTED_LINK_WIDTH_MASK) >> LC_DETECTED_LINK_WIDTH_SHIFT;\n\t\t\tcurrent_lw = (tmp & LC_OPERATING_LINK_WIDTH_MASK) >> LC_OPERATING_LINK_WIDTH_SHIFT;\n\n\t\t\tif (current_lw < max_lw) {\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\t\tif (tmp & LC_RENEGOTIATION_SUPPORT) {\n\t\t\t\t\ttmp &= ~(LC_LINK_WIDTH_MASK | LC_UPCONFIGURE_DIS);\n\t\t\t\t\ttmp |= (max_lw << LC_LINK_WIDTH_SHIFT);\n\t\t\t\t\ttmp |= LC_UPCONFIGURE_SUPPORT | LC_RENEGOTIATE_EN | LC_RECONFIG_NOW;\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, tmp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (i = 0; i < 10; i++) {\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_DEVSTA,\n\t\t\t\t\t\t\t  &tmp16);\n\t\t\t\tif (tmp16 & PCI_EXP_DEVSTA_TRPND)\n\t\t\t\t\tbreak;\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &bridge_cfg);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &gpu_cfg);\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &bridge_cfg2);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &gpu_cfg2);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_REDO_EQ;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\tmdelay(100);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   bridge_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   gpu_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   bridge_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   gpu_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp &= ~LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\t\t\t}\n\t\t}\n\t}\n\n\tspeed_cntl |= LC_FORCE_EN_SW_SPEED_CHANGE | LC_FORCE_DIS_HW_SPEED_CHANGE;\n\tspeed_cntl &= ~LC_FORCE_DIS_SW_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\ttmp16 = 0;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_8_0GT; /* gen3 */\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_5_0GT; /* gen2 */\n\telse\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_2_5GT; /* gen1 */\n\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t   PCI_EXP_LNKCTL2_TLS, tmp16);\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tspeed_cntl |= LC_INITIATE_LINK_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\t\tif ((speed_cntl & LC_INITIATE_LINK_SPEED_CHANGE) == 0)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n}\n\nstatic inline u32 si_pif_phy0_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY0_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy0_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY0_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic inline u32 si_pif_phy1_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY1_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy1_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY1_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\nstatic void si_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\tbool disable_l0s = false, disable_l1 = false, disable_plloff_in_l1 = false;\n\tbool disable_clkreq = false;\n\n\tif (!amdgpu_device_should_use_aspm(adev))\n\t\treturn;\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\tdata &= ~LC_XMIT_N_FTS_MASK;\n\tdata |= LC_XMIT_N_FTS(0x24) | LC_XMIT_N_FTS_OVERRIDE_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL3);\n\tdata |= LC_GO_TO_RECOVERY;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(PCIE_P_CNTL);\n\tdata |= P_IGNORE_EDB_ERR;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_P_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\tdata &= ~(LC_L0S_INACTIVITY_MASK | LC_L1_INACTIVITY_MASK);\n\tdata |= LC_PMI_TO_L1_DIS;\n\tif (!disable_l0s)\n\t\tdata |= LC_L0S_INACTIVITY(7);\n\n\tif (!disable_l1) {\n\t\tdata |= LC_L1_INACTIVITY(7);\n\t\tdata &= ~LC_PMI_TO_L1_DIS;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\n\t\tif (!disable_plloff_in_l1) {\n\t\t\tbool clk_req_support;\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\tif ((adev->asic_type != CHIP_OLAND) && (adev->asic_type != CHIP_HAINAN)) {\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_3, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_3, data);\n\t\t\t}\n\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\tdata &= ~LC_DYN_LANES_PWR_STATE_MASK;\n\t\t\tdata |= LC_DYN_LANES_PWR_STATE(3);\n\t\t\tif (orig != data)\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_CNTL, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_CNTL, data);\n\n\t\t\tif (!disable_clkreq &&\n\t\t\t    !pci_is_root_bus(adev->pdev->bus)) {\n\t\t\t\tstruct pci_dev *root = adev->pdev->bus->self;\n\t\t\t\tu32 lnkcap;\n\n\t\t\t\tclk_req_support = false;\n\t\t\t\tpcie_capability_read_dword(root, PCI_EXP_LNKCAP, &lnkcap);\n\t\t\t\tif (lnkcap & PCI_EXP_LNKCAP_CLKPM)\n\t\t\t\t\tclk_req_support = true;\n\t\t\t} else {\n\t\t\t\tclk_req_support = false;\n\t\t\t}\n\n\t\t\tif (clk_req_support) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL2);\n\t\t\t\tdata |= LC_ALLOW_PDWN_IN_L1 | LC_ALLOW_PDWN_IN_L23;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL2, data);\n\n\t\t\t\torig = data = RREG32(THM_CLK_CNTL);\n\t\t\t\tdata &= ~(CMON_CLK_SEL_MASK | TMON_CLK_SEL_MASK);\n\t\t\t\tdata |= CMON_CLK_SEL(1) | TMON_CLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(THM_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(MISC_CLK_CNTL);\n\t\t\t\tdata &= ~(DEEP_SLEEP_CLK_SEL_MASK | ZCLK_SEL_MASK);\n\t\t\t\tdata |= DEEP_SLEEP_CLK_SEL(1) | ZCLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MISC_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL);\n\t\t\t\tdata &= ~BCLK_AS_XCLK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL_2);\n\t\t\t\tdata &= ~FORCE_BIF_REFCLK_EN;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL_2, data);\n\n\t\t\t\torig = data = RREG32(MPLL_BYPASSCLK_SEL);\n\t\t\t\tdata &= ~MPLL_CLKOUT_SEL_MASK;\n\t\t\t\tdata |= MPLL_CLKOUT_SEL(4);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MPLL_BYPASSCLK_SEL, data);\n\n\t\t\t\torig = data = RREG32(SPLL_CNTL_MODE);\n\t\t\t\tdata &= ~SPLL_REFCLK_SEL_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(SPLL_CNTL_MODE, data);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t}\n\n\torig = data = RREG32_PCIE(PCIE_CNTL2);\n\tdata |= SLV_MEM_LS_EN | MST_MEM_LS_EN | REPLAY_MEM_LS_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_CNTL2, data);\n\n\tif (!disable_l0s) {\n\t\tdata = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\t\tif((data & LC_N_FTS_MASK) == LC_N_FTS_MASK) {\n\t\t\tdata = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tif ((data & LC_REVERSE_XMIT) && (data & LC_REVERSE_RCVR)) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\t\t\t\tdata &= ~LC_L0S_INACTIVITY_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void si_fix_pci_max_read_req_size(struct amdgpu_device *adev)\n{\n\tint readrq;\n\tu16 v;\n\n\treadrq = pcie_get_readrq(adev->pdev);\n\tv = ffs(readrq) - 8;\n\tif ((v == 0) || (v == 6) || (v == 7))\n\t\tpcie_set_readrq(adev->pdev, 512);\n}\n\nstatic int si_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsi_fix_pci_max_read_req_size(adev);\n\tsi_init_golden_registers(adev);\n\tsi_pcie_gen3_enable(adev);\n\tsi_program_aspm(adev);\n\n\treturn 0;\n}\n\nstatic int si_common_hw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_fini(adev);\n}\n\nstatic int si_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_init(adev);\n}\n\nstatic bool si_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int si_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_clockgating_state(void *handle,\n\t\t\t\t\t    enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs si_common_ip_funcs = {\n\t.name = \"si_common\",\n\t.early_init = si_common_early_init,\n\t.late_init = NULL,\n\t.sw_init = si_common_sw_init,\n\t.sw_fini = si_common_sw_fini,\n\t.hw_init = si_common_hw_init,\n\t.hw_fini = si_common_hw_fini,\n\t.suspend = si_common_suspend,\n\t.resume = si_common_resume,\n\t.is_idle = si_common_is_idle,\n\t.wait_for_idle = si_common_wait_for_idle,\n\t.soft_reset = si_common_soft_reset,\n\t.set_clockgating_state = si_common_set_clockgating_state,\n\t.set_powergating_state = si_common_set_powergating_state,\n\t.dump_ip_state = NULL,\n\t.print_ip_state = NULL,\n};\n\nstatic const struct amdgpu_ip_block_version si_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &si_common_ip_funcs,\n};\n\nint si_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn 0;\n}\n\n", "source_code_path": "si.c", "line_number": 1324, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: si.c\nLine: 1324\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Lack of Boundary Checks\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/si.c\nLine: 1324\n\nDescription:\nThe function si_read_bios_from_rom lacks proper boundary checks when reading data into the bios buffer. The function calculates the number of dwords to read based on the input length_bytes, but it doesn't verify if this calculated length exceeds the actual allocated size of the bios buffer. This can lead to a buffer overflow, potentially causing memory corruption or security vulnerabilities.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "si-bug.txt", "bug_report_text": "**Issue:** Lack of Boundary Checks **Type:** BUFFER_OVERFLOW **File:** drivers/gpu/drm/amd/amdgpu/si.c **Line:** 1324\n\n**Description:**\nThe `si_read_bios_from_rom` function neglects to validate the data transfer size, allowing the risk of exceeding the buffer's capacity and potentially causing a buffer overflow, which can lead to memory corruption or security breaches.", "diff_path": "si-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n", "code": "/*\n * Copyright 2015 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/firmware.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n#include \"si_dpm.h\"\n#include \"sid.h\"\n#include \"si_ih.h\"\n#include \"gfx_v6_0.h\"\n#include \"gmc_v6_0.h\"\n#include \"si_dma.h\"\n#include \"dce_v6_0.h\"\n#include \"si.h\"\n#include \"uvd_v3_1.h\"\n#include \"amdgpu_vkms.h\"\n#include \"gca/gfx_6_0_d.h\"\n#include \"oss/oss_1_0_d.h\"\n#include \"oss/oss_1_0_sh_mask.h\"\n#include \"gmc/gmc_6_0_d.h\"\n#include \"dce/dce_6_0_d.h\"\n#include \"uvd/uvd_4_0_d.h\"\n#include \"bif/bif_3_0_d.h\"\n#include \"bif/bif_3_0_sh_mask.h\"\n\n#include \"amdgpu_dm.h\"\n\nstatic const u32 tahiti_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000000c0, 0x00800040,\n\t0x360c, 0x000000c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_ARB_WTM_CNTL_RD, 0x00000003, 0x000007ff,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x00000200, 0x000002fb,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x0000543b,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0xa9210876,\n\tmmVGT_FIFO_DEPTHS, 0xffffffff, 0x000fff40,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_CONTEXT0_CNTL, 0x20000000, 0x20fffed8,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 tahiti_golden_registers2[] =\n{\n\tmmMCIF_MEM_CONTROL, 0x00000001, 0x00000001,\n};\n\nstatic const u32 tahiti_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n\tmmUVD_CGC_GATE, 0x00000008, 0x00000000,\n};\n\nstatic const u32 pitcairn_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f7,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x32761054,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 pitcairn_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601004,\n\t0x311f, 0xffffffff, 0x10102020,\n\t0x3122, 0xffffffff, 0x01000020,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000a4,\n};\n\nstatic const u32 verde_pg_init[] =\n{\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x40000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x200010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x7007,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x300010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x400000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x100010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x120200,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x500010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x1e1e16,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x600010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x171f1e,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x700010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x9ff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x0,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10000800,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1000051e,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x8,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x80500,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x12,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x9050c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xb052c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1053e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10546,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x30,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xa054e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1055f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3f,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10567,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x42,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1056f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x45,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10572,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x48,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20575,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x190801,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x67,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1082a,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x6a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1b082d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x87,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x310851,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xba,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x891,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x893,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbe,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20895,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc2,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20899,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc6,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2089d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xca,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xcc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a3,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xce,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x308a5,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xd3,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x6d08cd,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x142,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2000095a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x144,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x301f095b,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x165,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc094d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x173,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf096d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x184,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x15097f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x19b,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc0998,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1a9,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x409a7,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1af,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xcdc,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1b1,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x800,\n\tmmGMCON_RENG_EXECUTE, 0xffffffff, 0x6c9b2000,\n\tmmGMCON_MISC2, 0xfc00, 0x2000,\n\tmmGMCON_MISC3, 0xffffffff, 0xfc0,\n\tmmMC_PMG_AUTO_CFG, 0x00000100, 0x100,\n};\n\nstatic const u32 verde_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x033f1005,\n\t0x311f, 0xffffffff, 0x10808020,\n\t0x3122, 0xffffffff, 0x00800008,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00001000,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x80010014,\n};\n\nstatic const u32 verde_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x0000124a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x00000003,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00001032,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 oland_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000082,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f3,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\n};\n\nstatic const u32 oland_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n};\n\nstatic const u32 hainan_golden_registers[] =\n{\n\t0x17bc, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\t0x031e, 0x00000080, 0x00000000,\n\t0x3430, 0xff000fff, 0x00000100,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x3630, 0xff000fff, 0x00000100,\n\t0x360c, 0x000300c0, 0x00800040,\n\t0x16ec, 0x000000f0, 0x00000070,\n\t0x16f0, 0x00200000, 0x50100000,\n\t0x1c0c, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000000,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x03e00000, 0x03600000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f1,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 hainan_golden_registers2[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x2011003,\n};\n\nstatic const u32 tahiti_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\t0x2476, 0xffffffff, 0x00070006,\n\t0x2477, 0xffffffff, 0x00090008,\n\t0x2478, 0xffffffff, 0x0000000c,\n\t0x2479, 0xffffffff, 0x000b000a,\n\t0x247a, 0xffffffff, 0x000e000d,\n\t0x247b, 0xffffffff, 0x00080007,\n\t0x247c, 0xffffffff, 0x000a0009,\n\t0x247d, 0xffffffff, 0x0000000d,\n\t0x247e, 0xffffffff, 0x000c000b,\n\t0x247f, 0xffffffff, 0x000f000e,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\t0x2499, 0xffffffff, 0x000e000d,\n\t0x249a, 0xffffffff, 0x0010000f,\n\t0x249b, 0xffffffff, 0x00000013,\n\t0x249c, 0xffffffff, 0x00120011,\n\t0x249d, 0xffffffff, 0x00150014,\n\t0x249e, 0xffffffff, 0x000f000e,\n\t0x249f, 0xffffffff, 0x00110010,\n\t0x24a0, 0xffffffff, 0x00000014,\n\t0x24a1, 0xffffffff, 0x00130012,\n\t0x24a2, 0xffffffff, 0x00160015,\n\t0x24a3, 0xffffffff, 0x0010000f,\n\t0x24a4, 0xffffffff, 0x00120011,\n\t0x24a5, 0xffffffff, 0x00000015,\n\t0x24a6, 0xffffffff, 0x00140013,\n\t0x24a7, 0xffffffff, 0x00170016,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\nstatic const u32 pitcairn_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 verde_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 oland_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 hainan_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\t0x0409, 0xffffffff, 0x00000100,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\n/* XXX: update when we support VCE */\n#if 0\n/* tahiti, pitcarin, verde */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_encode_array),\n\t.codec_array = tahiti_video_codecs_encode_array,\n};\n#else\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n#endif\n/* oland and hainan don't support encode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* tahiti, pitcarin, verde, oland */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 41,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_decode_array),\n\t.codec_array = tahiti_video_codecs_decode_array,\n};\n\n/* hainan doesn't support decode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\nstatic int si_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tif (encode)\n\t\t\t*codecs = &tahiti_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_OLAND:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_HAINAN:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &hainan_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic u32 si_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tr = RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tWREG32(AMDGPU_PCIE_DATA, v);\n\t(void)RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_pciep_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tr = RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pciep_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tWREG32(PCIE_PORT_DATA, (v));\n\t(void)RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tr = RREG32(SMC_IND_DATA_0);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tWREG32(SMC_IND_DATA_0, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 si_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic struct amdgpu_allowed_register_entry si_allowed_read_registers[] = {\n\t{GRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{DMA_STATUS_REG + DMA0_REGISTER_OFFSET},\n\t{DMA_STATUS_REG + DMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{GB_ADDR_CONFIG},\n\t{MC_ARB_RAMCFG},\n\t{GB_TILE_MODE0},\n\t{GB_TILE_MODE1},\n\t{GB_TILE_MODE2},\n\t{GB_TILE_MODE3},\n\t{GB_TILE_MODE4},\n\t{GB_TILE_MODE5},\n\t{GB_TILE_MODE6},\n\t{GB_TILE_MODE7},\n\t{GB_TILE_MODE8},\n\t{GB_TILE_MODE9},\n\t{GB_TILE_MODE10},\n\t{GB_TILE_MODE11},\n\t{GB_TILE_MODE12},\n\t{GB_TILE_MODE13},\n\t{GB_TILE_MODE14},\n\t{GB_TILE_MODE15},\n\t{GB_TILE_MODE16},\n\t{GB_TILE_MODE17},\n\t{GB_TILE_MODE18},\n\t{GB_TILE_MODE19},\n\t{GB_TILE_MODE20},\n\t{GB_TILE_MODE21},\n\t{GB_TILE_MODE22},\n\t{GB_TILE_MODE23},\n\t{GB_TILE_MODE24},\n\t{GB_TILE_MODE25},\n\t{GB_TILE_MODE26},\n\t{GB_TILE_MODE27},\n\t{GB_TILE_MODE28},\n\t{GB_TILE_MODE29},\n\t{GB_TILE_MODE30},\n\t{GB_TILE_MODE31},\n\t{CC_RB_BACKEND_DISABLE, true},\n\t{GC_USER_RB_BACKEND_DISABLE, true},\n\t{PA_SC_RASTER_CONFIG, true},\n};\n\nstatic uint32_t si_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\nstatic int si_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t     u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(si_allowed_read_registers); i++) {\n\t\tbool indexed = si_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != si_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = si_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic bool si_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(R600_BUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(AVIVO_D1VGA_CONTROL);\n\t\td2vga_control = RREG32(AVIVO_D2VGA_CONTROL);\n\t\tvga_render_control = RREG32(VGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32(R600_ROM_CNTL);\n\n\t/* enable the rom */\n\tWREG32(R600_BUS_CNTL, (bus_cntl & ~R600_BIOS_ROM_DIS));\n\tif (adev->mode_info.num_crtc) {\n\t\t/* Disable VGA mode */\n\t\tWREG32(AVIVO_D1VGA_CONTROL,\n\t\t       (d1vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(AVIVO_D2VGA_CONTROL,\n\t\t       (d2vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(VGA_RENDER_CONTROL,\n\t\t       (vga_render_control & C_000300_VGA_VSTATUS_CNTL));\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl | R600_SCK_OVERWRITE);\n\n\tr = amdgpu_read_bios(adev);\n\n\t/* restore regs */\n\tWREG32(R600_BUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(AVIVO_D1VGA_CONTROL, d1vga_control);\n\t\tWREG32(AVIVO_D2VGA_CONTROL, d2vga_control);\n\t\tWREG32(VGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl);\n\treturn r;\n}\n\n#define mmROM_INDEX 0x2A\n#define mmROM_DATA  0x2B\n\nstatic bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\nstatic void si_set_clk_bypass_mode(struct amdgpu_device *adev)\n{\n\tu32 tmp, i;\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_BYPASS_EN;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp |= SPLL_CTLREQ_CHG;\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(SPLL_STATUS) & SPLL_CHG_STATUS)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp &= ~(SPLL_CTLREQ_CHG | SCLK_MUX_UPDATE);\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\ttmp = RREG32(MPLL_CNTL_MODE);\n\ttmp &= ~MPLL_MCLK_SEL;\n\tWREG32(MPLL_CNTL_MODE, tmp);\n}\n\nstatic void si_spll_powerdown(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp |= SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_RESET;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_SLEEP;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp &= ~SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n}\n\nstatic int si_gpu_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t/* set mclk/sclk to bypass */\n\tsi_set_clk_bypass_mode(adev);\n\t/* powerdown spll */\n\tsi_spll_powerdown(adev);\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\t/* reset */\n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t/* wait for asic to come out of reset */\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t/* enable BM */\n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic int si_asic_supports_baco(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\nstatic enum amd_reset_method\nsi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_PCI)\n\t\treturn amdgpu_reset_method;\n\telse if (amdgpu_reset_method != AMD_RESET_METHOD_LEGACY &&\n\t\t amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t amdgpu_reset_method);\n\n\treturn AMD_RESET_METHOD_LEGACY;\n}\n\nstatic int si_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tswitch (si_asic_reset_method(adev)) {\n\tcase AMD_RESET_METHOD_PCI:\n\t\tdev_info(adev->dev, \"PCI reset\\n\");\n\t\tr = amdgpu_device_pci_reset(adev);\n\t\tbreak;\n\tdefault:\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = si_gpu_pci_config_reset(adev);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic u32 si_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic void si_vga_set_state(struct amdgpu_device *adev, bool state)\n{\n\tuint32_t temp;\n\n\ttemp = RREG32(CONFIG_CNTL);\n\tif (!state) {\n\t\ttemp &= ~(1<<0);\n\t\ttemp |= (1<<1);\n\t} else {\n\t\ttemp &= ~(1<<1);\n\t}\n\tWREG32(CONFIG_CNTL, temp);\n}\n\nstatic u32 si_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL_2);\n\tif (tmp & MUX_TCLK_TO_XCLK)\n\t\treturn TCLK;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL);\n\tif (tmp & XTALIN_DIVIDE)\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\nstatic void si_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void si_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool si_need_full_reset(struct amdgpu_device *adev)\n{\n\t/* change this when we support soft reset */\n\treturn true;\n}\n\nstatic bool si_need_reset_on_init(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n\nstatic int si_get_pcie_lanes(struct amdgpu_device *adev)\n{\n\tu32 link_width_cntl;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\n\tswitch ((link_width_cntl & LC_LINK_WIDTH_RD_MASK) >> LC_LINK_WIDTH_RD_SHIFT) {\n\tcase LC_LINK_WIDTH_X1:\n\t\treturn 1;\n\tcase LC_LINK_WIDTH_X2:\n\t\treturn 2;\n\tcase LC_LINK_WIDTH_X4:\n\t\treturn 4;\n\tcase LC_LINK_WIDTH_X8:\n\t\treturn 8;\n\tcase LC_LINK_WIDTH_X0:\n\tcase LC_LINK_WIDTH_X16:\n\tdefault:\n\t\treturn 16;\n\t}\n}\n\nstatic void si_set_pcie_lanes(struct amdgpu_device *adev, int lanes)\n{\n\tu32 link_width_cntl, mask;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tswitch (lanes) {\n\tcase 0:\n\t\tmask = LC_LINK_WIDTH_X0;\n\t\tbreak;\n\tcase 1:\n\t\tmask = LC_LINK_WIDTH_X1;\n\t\tbreak;\n\tcase 2:\n\t\tmask = LC_LINK_WIDTH_X2;\n\t\tbreak;\n\tcase 4:\n\t\tmask = LC_LINK_WIDTH_X4;\n\t\tbreak;\n\tcase 8:\n\t\tmask = LC_LINK_WIDTH_X8;\n\t\tbreak;\n\tcase 16:\n\t\tmask = LC_LINK_WIDTH_X16;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"invalid pcie lane request: %d\\n\", lanes);\n\t\treturn;\n\t}\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\tlink_width_cntl &= ~LC_LINK_WIDTH_MASK;\n\tlink_width_cntl |= mask << LC_LINK_WIDTH_SHIFT;\n\tlink_width_cntl |= (LC_RECONFIG_NOW |\n\t\t\t    LC_RECONFIG_ARC_MISSING_ESCAPE);\n\n\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, link_width_cntl);\n}\n\nstatic void si_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t/* This reports 0 on APUs, so return to avoid writing/reading registers\n\t * that may or may not be different from their GPU counterparts\n\t */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t/* Set the 2 events that we wish to watch, defined above */\n\t/* Reg 40 is # received msgs, Reg 104 is # of posted requests sent */\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t/* Write to enable desired perf counters */\n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t/* Zero out and enable the perf counters\n\t * Write 0x5:\n\t * Bit 0 = Start all counters(1)\n\t * Bit 2 = Global counter reset enable(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t/* Load the shadow and disable the perf counters\n\t * Write 0x2:\n\t * Bit 0 = Stop counters(0)\n\t * Bit 1 = Load the shadow counters(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t/* Read register values to get any >32bit overflow */\n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t/* Get the values and add the overflow */\n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t si_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t/* Get the number of NAKs received and generated */\n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t/* Add the total number of NAKs, i.e the number of replays */\n\treturn (nak_r + nak_g);\n}\n\nstatic int si_uvd_send_upll_ctlreq(struct amdgpu_device *adev,\n\t\t\t\t   unsigned cg_upll_func_cntl)\n{\n\tunsigned i;\n\n\t/* Make sure UPLL_CTLREQ is deasserted */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32(cg_upll_func_cntl) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned si_uvd_calc_upll_post_div(unsigned vco_freq,\n\t\t\t\t\t  unsigned target_freq,\n\t\t\t\t\t  unsigned pd_min,\n\t\t\t\t\t  unsigned pd_even)\n{\n\tunsigned post_div = vco_freq / target_freq;\n\n\t/* Adjust to post divider minimum value */\n\tif (post_div < pd_min)\n\t\tpost_div = pd_min;\n\n\t/* We alway need a frequency less than or equal the target */\n\tif ((vco_freq / post_div) > target_freq)\n\t\tpost_div += 1;\n\n\t/* Post dividers above a certain value must be even */\n\tif (post_div > pd_even && post_div % 2)\n\t\tpost_div += 1;\n\n\treturn post_div;\n}\n\n/**\n * si_calc_upll_dividers - calc UPLL clock dividers\n *\n * @adev: amdgpu_device pointer\n * @vclk: wanted VCLK\n * @dclk: wanted DCLK\n * @vco_min: minimum VCO frequency\n * @vco_max: maximum VCO frequency\n * @fb_factor: factor to multiply vco freq with\n * @fb_mask: limit and bitmask for feedback divider\n * @pd_min: post divider minimum\n * @pd_max: post divider maximum\n * @pd_even: post divider must be even above this value\n * @optimal_fb_div: resulting feedback divider\n * @optimal_vclk_div: resulting vclk post divider\n * @optimal_dclk_div: resulting dclk post divider\n *\n * Calculate dividers for UVDs UPLL (except APUs).\n * Returns zero on success; -EINVAL on error.\n */\nstatic int si_calc_upll_dividers(struct amdgpu_device *adev,\n\t\t\t\t unsigned vclk, unsigned dclk,\n\t\t\t\t unsigned vco_min, unsigned vco_max,\n\t\t\t\t unsigned fb_factor, unsigned fb_mask,\n\t\t\t\t unsigned pd_min, unsigned pd_max,\n\t\t\t\t unsigned pd_even,\n\t\t\t\t unsigned *optimal_fb_div,\n\t\t\t\t unsigned *optimal_vclk_div,\n\t\t\t\t unsigned *optimal_dclk_div)\n{\n\tunsigned vco_freq, ref_freq = adev->clock.spll.reference_freq;\n\n\t/* Start off with something large */\n\tunsigned optimal_score = ~0;\n\n\t/* Loop through vco from low to high */\n\tvco_min = max(max(vco_min, vclk), dclk);\n\tfor (vco_freq = vco_min; vco_freq <= vco_max; vco_freq += 100) {\n\t\tuint64_t fb_div = (uint64_t)vco_freq * fb_factor;\n\t\tunsigned vclk_div, dclk_div, score;\n\n\t\tdo_div(fb_div, ref_freq);\n\n\t\t/* fb div out of range ? */\n\t\tif (fb_div > fb_mask)\n\t\t\tbreak; /* It can oly get worse */\n\n\t\tfb_div &= fb_mask;\n\n\t\t/* Calc vclk divider with current vco freq */\n\t\tvclk_div = si_uvd_calc_upll_post_div(vco_freq, vclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (vclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc dclk divider with current vco freq */\n\t\tdclk_div = si_uvd_calc_upll_post_div(vco_freq, dclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (dclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc score with current vco freq */\n\t\tscore = vclk - (vco_freq / vclk_div) + dclk - (vco_freq / dclk_div);\n\n\t\t/* Determine if this vco setting is better than current optimal settings */\n\t\tif (score < optimal_score) {\n\t\t\t*optimal_fb_div = fb_div;\n\t\t\t*optimal_vclk_div = vclk_div;\n\t\t\t*optimal_dclk_div = dclk_div;\n\t\t\toptimal_score = score;\n\t\t\tif (optimal_score == 0)\n\t\t\t\tbreak; /* It can't get better than this */\n\t\t}\n\t}\n\n\t/* Did we found a valid setup ? */\n\tif (optimal_score == ~0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int si_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tunsigned fb_div = 0, vclk_div = 0, dclk_div = 0;\n\tint r;\n\n\t/* Bypass vclk and dclk with bclk */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(1) | DCLK_SRC_SEL(1),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_BYPASS_EN_MASK, ~UPLL_BYPASS_EN_MASK);\n\n\tif (!vclk || !dclk) {\n\t\t/* Keep the Bypass mode */\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, vclk, dclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &vclk_div, &dclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_VCO_MODE_MASK, ~UPLL_VCO_MODE_MASK);\n\n\t/* Disable sleep mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_SLEEP_MASK);\n\n\t/* Deassert UPLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert UPLL_RESET again */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_RESET_MASK, ~UPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_P(CG_UPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_3, UPLL_FB_DIV(fb_div), ~UPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_REF_DIV_MASK);\n\n\tif (fb_div < 307200)\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4, 0, ~UPLL_SPARE_ISPARE9);\n\telse\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4,\n\t\t\t UPLL_SPARE_ISPARE9,\n\t\t\t ~UPLL_SPARE_ISPARE9);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t UPLL_PDIV_A(vclk_div) | UPLL_PDIV_B(dclk_div),\n\t\t ~(UPLL_PDIV_A_MASK | UPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_BYPASS_EN_MASK);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(2) | DCLK_SRC_SEL(2),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic int si_vce_send_vcepll_ctlreq(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t/* Make sure VCEPLL_CTLREQ is deasserted */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32_SMC(CG_VCEPLL_FUNC_CNTL) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tunsigned fb_div = 0, evclk_div = 0, ecclk_div = 0;\n\tint r;\n\n\t/* Bypass evclk and ecclk with bclk */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(1) | ECCLK_SRC_SEL(1),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_BYPASS_EN_MASK,\n\t\t     ~VCEPLL_BYPASS_EN_MASK);\n\n\tif (!evclk || !ecclk) {\n\t\t/* Keep the Bypass mode, put PLL to sleep */\n\t\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t\t     ~VCEPLL_SLEEP_MASK);\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, evclk, ecclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &evclk_div, &ecclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_VCO_MODE_MASK,\n\t\t     ~VCEPLL_VCO_MODE_MASK);\n\n\t/* Toggle VCEPLL_SLEEP to 1 then back to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t     ~VCEPLL_SLEEP_MASK);\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_SLEEP_MASK);\n\n\t/* Deassert VCEPLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert VCEPLL_RESET again */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_RESET_MASK, ~VCEPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_SMC_P(CG_VCEPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_3,\n\t\t     VCEPLL_FB_DIV(fb_div),\n\t\t     ~VCEPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_REF_DIV_MASK);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     VCEPLL_PDIV_A(evclk_div) | VCEPLL_PDIV_B(ecclk_div),\n\t\t     ~(VCEPLL_PDIV_A_MASK | VCEPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_BYPASS_EN_MASK);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(16) | ECCLK_SRC_SEL(16),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic void si_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs si_asic_funcs =\n{\n\t.read_disabled_bios = &si_read_disabled_bios,\n\t.read_bios_from_rom = &si_read_bios_from_rom,\n\t.read_register = &si_read_register,\n\t.reset = &si_asic_reset,\n\t.reset_method = &si_asic_reset_method,\n\t.set_vga_state = &si_vga_set_state,\n\t.get_xclk = &si_get_xclk,\n\t.set_uvd_clocks = &si_set_uvd_clocks,\n\t.set_vce_clocks = &si_set_vce_clocks,\n\t.get_pcie_lanes = &si_get_pcie_lanes,\n\t.set_pcie_lanes = &si_set_pcie_lanes,\n\t.get_config_memsize = &si_get_config_memsize,\n\t.flush_hdp = &si_flush_hdp,\n\t.invalidate_hdp = &si_invalidate_hdp,\n\t.need_full_reset = &si_need_full_reset,\n\t.get_pcie_usage = &si_get_pcie_usage,\n\t.need_reset_on_init = &si_need_reset_on_init,\n\t.get_pcie_replay_count = &si_get_pcie_replay_count,\n\t.supports_baco = &si_asic_supports_baco,\n\t.pre_asic_init = &si_pre_asic_init,\n\t.query_video_codecs = &si_query_video_codecs,\n};\n\nstatic uint32_t si_get_rev_id(struct amdgpu_device *adev)\n{\n\treturn (RREG32(CC_DRM_ID_STRAPS) & CC_DRM_ID_STRAPS__ATI_REV_ID_MASK)\n\t\t>> CC_DRM_ID_STRAPS__ATI_REV_ID__SHIFT;\n}\n\nstatic int si_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->smc_rreg = &si_smc_rreg;\n\tadev->smc_wreg = &si_smc_wreg;\n\tadev->pcie_rreg = &si_pcie_rreg;\n\tadev->pcie_wreg = &si_pcie_wreg;\n\tadev->pciep_rreg = &si_pciep_rreg;\n\tadev->pciep_wreg = &si_pciep_wreg;\n\tadev->uvd_ctx_rreg = si_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = si_uvd_ctx_wreg;\n\tadev->didt_rreg = NULL;\n\tadev->didt_wreg = NULL;\n\n\tadev->asic_funcs = &si_asic_funcs;\n\n\tadev->rev_id = si_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = (adev->rev_id == 0) ? 1 :\n\t\t\t\t\t(adev->rev_id == 1) ? 5 : 6;\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 20;\n\t\tbreak;\n\n\tcase CHIP_VERDE:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\t//???\n\t\tadev->external_rev_id = adev->rev_id + 40;\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 60;\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 70;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_common_sw_init(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\n\nstatic void si_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers2));\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_VERDE:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_pg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_pg_init));\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers2));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_mgcg_cgcg_init));\n\t\tbreak;\n\n\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void si_pcie_gen3_enable(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *root = adev->pdev->bus->self;\n\tu32 speed_cntl, current_data_rate;\n\tint i;\n\tu16 tmp16;\n\n\tif (pci_is_root_bus(adev->pdev->bus))\n\t\treturn;\n\n\tif (amdgpu_pcie_gen2 == 0)\n\t\treturn;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tif (!(adev->pm.pcie_gen_mask & (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\tCAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)))\n\t\treturn;\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tcurrent_data_rate = (speed_cntl & LC_CURRENT_DATA_RATE_MASK) >>\n\t\tLC_CURRENT_DATA_RATE_SHIFT;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate == 2) {\n\t\t\tDRM_INFO(\"PCIE gen 3 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 3 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t} else if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2) {\n\t\tif (current_data_rate == 1) {\n\t\t\tDRM_INFO(\"PCIE gen 2 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 2 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t}\n\n\tif (!pci_is_pcie(root) || !pci_is_pcie(adev->pdev))\n\t\treturn;\n\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate != 2) {\n\t\t\tu16 bridge_cfg, gpu_cfg;\n\t\t\tu16 bridge_cfg2, gpu_cfg2;\n\t\t\tu32 max_lw, current_lw, tmp;\n\n\t\t\tpcie_capability_set_word(root, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\t\t\tpcie_capability_set_word(adev->pdev, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\n\t\t\ttmp = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tmax_lw = (tmp & LC_DETECTED_LINK_WIDTH_MASK) >> LC_DETECTED_LINK_WIDTH_SHIFT;\n\t\t\tcurrent_lw = (tmp & LC_OPERATING_LINK_WIDTH_MASK) >> LC_OPERATING_LINK_WIDTH_SHIFT;\n\n\t\t\tif (current_lw < max_lw) {\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\t\tif (tmp & LC_RENEGOTIATION_SUPPORT) {\n\t\t\t\t\ttmp &= ~(LC_LINK_WIDTH_MASK | LC_UPCONFIGURE_DIS);\n\t\t\t\t\ttmp |= (max_lw << LC_LINK_WIDTH_SHIFT);\n\t\t\t\t\ttmp |= LC_UPCONFIGURE_SUPPORT | LC_RENEGOTIATE_EN | LC_RECONFIG_NOW;\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, tmp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (i = 0; i < 10; i++) {\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_DEVSTA,\n\t\t\t\t\t\t\t  &tmp16);\n\t\t\t\tif (tmp16 & PCI_EXP_DEVSTA_TRPND)\n\t\t\t\t\tbreak;\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &bridge_cfg);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &gpu_cfg);\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &bridge_cfg2);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &gpu_cfg2);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_REDO_EQ;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\tmdelay(100);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   bridge_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   gpu_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   bridge_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   gpu_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp &= ~LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\t\t\t}\n\t\t}\n\t}\n\n\tspeed_cntl |= LC_FORCE_EN_SW_SPEED_CHANGE | LC_FORCE_DIS_HW_SPEED_CHANGE;\n\tspeed_cntl &= ~LC_FORCE_DIS_SW_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\ttmp16 = 0;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_8_0GT; /* gen3 */\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_5_0GT; /* gen2 */\n\telse\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_2_5GT; /* gen1 */\n\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t   PCI_EXP_LNKCTL2_TLS, tmp16);\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tspeed_cntl |= LC_INITIATE_LINK_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\t\tif ((speed_cntl & LC_INITIATE_LINK_SPEED_CHANGE) == 0)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n}\n\nstatic inline u32 si_pif_phy0_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY0_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy0_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY0_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic inline u32 si_pif_phy1_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY1_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy1_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY1_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\nstatic void si_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\tbool disable_l0s = false, disable_l1 = false, disable_plloff_in_l1 = false;\n\tbool disable_clkreq = false;\n\n\tif (!amdgpu_device_should_use_aspm(adev))\n\t\treturn;\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\tdata &= ~LC_XMIT_N_FTS_MASK;\n\tdata |= LC_XMIT_N_FTS(0x24) | LC_XMIT_N_FTS_OVERRIDE_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL3);\n\tdata |= LC_GO_TO_RECOVERY;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(PCIE_P_CNTL);\n\tdata |= P_IGNORE_EDB_ERR;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_P_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\tdata &= ~(LC_L0S_INACTIVITY_MASK | LC_L1_INACTIVITY_MASK);\n\tdata |= LC_PMI_TO_L1_DIS;\n\tif (!disable_l0s)\n\t\tdata |= LC_L0S_INACTIVITY(7);\n\n\tif (!disable_l1) {\n\t\tdata |= LC_L1_INACTIVITY(7);\n\t\tdata &= ~LC_PMI_TO_L1_DIS;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\n\t\tif (!disable_plloff_in_l1) {\n\t\t\tbool clk_req_support;\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\tif ((adev->asic_type != CHIP_OLAND) && (adev->asic_type != CHIP_HAINAN)) {\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_3, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_3, data);\n\t\t\t}\n\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\tdata &= ~LC_DYN_LANES_PWR_STATE_MASK;\n\t\t\tdata |= LC_DYN_LANES_PWR_STATE(3);\n\t\t\tif (orig != data)\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_CNTL, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_CNTL, data);\n\n\t\t\tif (!disable_clkreq &&\n\t\t\t    !pci_is_root_bus(adev->pdev->bus)) {\n\t\t\t\tstruct pci_dev *root = adev->pdev->bus->self;\n\t\t\t\tu32 lnkcap;\n\n\t\t\t\tclk_req_support = false;\n\t\t\t\tpcie_capability_read_dword(root, PCI_EXP_LNKCAP, &lnkcap);\n\t\t\t\tif (lnkcap & PCI_EXP_LNKCAP_CLKPM)\n\t\t\t\t\tclk_req_support = true;\n\t\t\t} else {\n\t\t\t\tclk_req_support = false;\n\t\t\t}\n\n\t\t\tif (clk_req_support) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL2);\n\t\t\t\tdata |= LC_ALLOW_PDWN_IN_L1 | LC_ALLOW_PDWN_IN_L23;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL2, data);\n\n\t\t\t\torig = data = RREG32(THM_CLK_CNTL);\n\t\t\t\tdata &= ~(CMON_CLK_SEL_MASK | TMON_CLK_SEL_MASK);\n\t\t\t\tdata |= CMON_CLK_SEL(1) | TMON_CLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(THM_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(MISC_CLK_CNTL);\n\t\t\t\tdata &= ~(DEEP_SLEEP_CLK_SEL_MASK | ZCLK_SEL_MASK);\n\t\t\t\tdata |= DEEP_SLEEP_CLK_SEL(1) | ZCLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MISC_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL);\n\t\t\t\tdata &= ~BCLK_AS_XCLK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL_2);\n\t\t\t\tdata &= ~FORCE_BIF_REFCLK_EN;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL_2, data);\n\n\t\t\t\torig = data = RREG32(MPLL_BYPASSCLK_SEL);\n\t\t\t\tdata &= ~MPLL_CLKOUT_SEL_MASK;\n\t\t\t\tdata |= MPLL_CLKOUT_SEL(4);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MPLL_BYPASSCLK_SEL, data);\n\n\t\t\t\torig = data = RREG32(SPLL_CNTL_MODE);\n\t\t\t\tdata &= ~SPLL_REFCLK_SEL_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(SPLL_CNTL_MODE, data);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t}\n\n\torig = data = RREG32_PCIE(PCIE_CNTL2);\n\tdata |= SLV_MEM_LS_EN | MST_MEM_LS_EN | REPLAY_MEM_LS_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_CNTL2, data);\n\n\tif (!disable_l0s) {\n\t\tdata = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\t\tif((data & LC_N_FTS_MASK) == LC_N_FTS_MASK) {\n\t\t\tdata = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tif ((data & LC_REVERSE_XMIT) && (data & LC_REVERSE_RCVR)) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\t\t\t\tdata &= ~LC_L0S_INACTIVITY_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void si_fix_pci_max_read_req_size(struct amdgpu_device *adev)\n{\n\tint readrq;\n\tu16 v;\n\n\treadrq = pcie_get_readrq(adev->pdev);\n\tv = ffs(readrq) - 8;\n\tif ((v == 0) || (v == 6) || (v == 7))\n\t\tpcie_set_readrq(adev->pdev, 512);\n}\n\nstatic int si_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsi_fix_pci_max_read_req_size(adev);\n\tsi_init_golden_registers(adev);\n\tsi_pcie_gen3_enable(adev);\n\tsi_program_aspm(adev);\n\n\treturn 0;\n}\n\nstatic int si_common_hw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_fini(adev);\n}\n\nstatic int si_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_init(adev);\n}\n\nstatic bool si_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int si_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_clockgating_state(void *handle,\n\t\t\t\t\t    enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs si_common_ip_funcs = {\n\t.name = \"si_common\",\n\t.early_init = si_common_early_init,\n\t.late_init = NULL,\n\t.sw_init = si_common_sw_init,\n\t.sw_fini = si_common_sw_fini,\n\t.hw_init = si_common_hw_init,\n\t.hw_fini = si_common_hw_fini,\n\t.suspend = si_common_suspend,\n\t.resume = si_common_resume,\n\t.is_idle = si_common_is_idle,\n\t.wait_for_idle = si_common_wait_for_idle,\n\t.soft_reset = si_common_soft_reset,\n\t.set_clockgating_state = si_common_set_clockgating_state,\n\t.set_powergating_state = si_common_set_powergating_state,\n\t.dump_ip_state = NULL,\n\t.print_ip_state = NULL,\n};\n\nstatic const struct amdgpu_ip_block_version si_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &si_common_ip_funcs,\n};\n\nint si_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn 0;\n}\n\n", "source_code_path": "si.c", "line_number": 1324, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: si.c\nLine: 1324\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\n**Issue:** Lack of Boundary Checks **Type:** BUFFER_OVERFLOW **File:** drivers/gpu/drm/amd/amdgpu/si.c **Line:** 1324\n\n**Description:**\nThe `si_read_bios_from_rom` function neglects to validate the data transfer size, allowing the risk of exceeding the buffer's capacity and potentially causing a buffer overflow, which can lead to memory corruption or security breaches.====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "si-bug.txt", "bug_report_text": "**Issue:** Lack of Boundary Checks **Type:** BUFFER_OVERFLOW **File:** drivers/gpu/drm/amd/amdgpu/si.c **Line:** 1324\n\n**Description:**\nThe `si_read_bios_from_rom` function does not properly check the data transfer size against the allocated buffer size, leaving it vulnerable to buffer overflows that can result in memory corruption or security vulnerabilities.", "diff_path": "si-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n", "code": "/*\n * Copyright 2015 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/firmware.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n#include \"si_dpm.h\"\n#include \"sid.h\"\n#include \"si_ih.h\"\n#include \"gfx_v6_0.h\"\n#include \"gmc_v6_0.h\"\n#include \"si_dma.h\"\n#include \"dce_v6_0.h\"\n#include \"si.h\"\n#include \"uvd_v3_1.h\"\n#include \"amdgpu_vkms.h\"\n#include \"gca/gfx_6_0_d.h\"\n#include \"oss/oss_1_0_d.h\"\n#include \"oss/oss_1_0_sh_mask.h\"\n#include \"gmc/gmc_6_0_d.h\"\n#include \"dce/dce_6_0_d.h\"\n#include \"uvd/uvd_4_0_d.h\"\n#include \"bif/bif_3_0_d.h\"\n#include \"bif/bif_3_0_sh_mask.h\"\n\n#include \"amdgpu_dm.h\"\n\nstatic const u32 tahiti_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000000c0, 0x00800040,\n\t0x360c, 0x000000c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_ARB_WTM_CNTL_RD, 0x00000003, 0x000007ff,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x00000200, 0x000002fb,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x0000543b,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0xa9210876,\n\tmmVGT_FIFO_DEPTHS, 0xffffffff, 0x000fff40,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_CONTEXT0_CNTL, 0x20000000, 0x20fffed8,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 tahiti_golden_registers2[] =\n{\n\tmmMCIF_MEM_CONTROL, 0x00000001, 0x00000001,\n};\n\nstatic const u32 tahiti_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n\tmmUVD_CGC_GATE, 0x00000008, 0x00000000,\n};\n\nstatic const u32 pitcairn_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f7,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x32761054,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 pitcairn_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601004,\n\t0x311f, 0xffffffff, 0x10102020,\n\t0x3122, 0xffffffff, 0x01000020,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000a4,\n};\n\nstatic const u32 verde_pg_init[] =\n{\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x40000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x200010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x7007,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x300010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x400000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x100010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x120200,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x500010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x1e1e16,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x600010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x171f1e,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x700010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x9ff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x0,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10000800,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1000051e,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x8,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x80500,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x12,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x9050c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xb052c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1053e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10546,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x30,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xa054e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1055f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3f,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10567,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x42,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1056f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x45,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10572,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x48,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20575,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x190801,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x67,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1082a,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x6a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1b082d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x87,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x310851,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xba,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x891,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x893,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbe,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20895,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc2,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20899,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc6,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2089d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xca,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xcc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a3,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xce,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x308a5,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xd3,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x6d08cd,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x142,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2000095a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x144,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x301f095b,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x165,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc094d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x173,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf096d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x184,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x15097f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x19b,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc0998,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1a9,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x409a7,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1af,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xcdc,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1b1,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x800,\n\tmmGMCON_RENG_EXECUTE, 0xffffffff, 0x6c9b2000,\n\tmmGMCON_MISC2, 0xfc00, 0x2000,\n\tmmGMCON_MISC3, 0xffffffff, 0xfc0,\n\tmmMC_PMG_AUTO_CFG, 0x00000100, 0x100,\n};\n\nstatic const u32 verde_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x033f1005,\n\t0x311f, 0xffffffff, 0x10808020,\n\t0x3122, 0xffffffff, 0x00800008,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00001000,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x80010014,\n};\n\nstatic const u32 verde_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x0000124a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x00000003,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00001032,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 oland_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000082,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f3,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\n};\n\nstatic const u32 oland_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n};\n\nstatic const u32 hainan_golden_registers[] =\n{\n\t0x17bc, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\t0x031e, 0x00000080, 0x00000000,\n\t0x3430, 0xff000fff, 0x00000100,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x3630, 0xff000fff, 0x00000100,\n\t0x360c, 0x000300c0, 0x00800040,\n\t0x16ec, 0x000000f0, 0x00000070,\n\t0x16f0, 0x00200000, 0x50100000,\n\t0x1c0c, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000000,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x03e00000, 0x03600000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f1,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 hainan_golden_registers2[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x2011003,\n};\n\nstatic const u32 tahiti_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\t0x2476, 0xffffffff, 0x00070006,\n\t0x2477, 0xffffffff, 0x00090008,\n\t0x2478, 0xffffffff, 0x0000000c,\n\t0x2479, 0xffffffff, 0x000b000a,\n\t0x247a, 0xffffffff, 0x000e000d,\n\t0x247b, 0xffffffff, 0x00080007,\n\t0x247c, 0xffffffff, 0x000a0009,\n\t0x247d, 0xffffffff, 0x0000000d,\n\t0x247e, 0xffffffff, 0x000c000b,\n\t0x247f, 0xffffffff, 0x000f000e,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\t0x2499, 0xffffffff, 0x000e000d,\n\t0x249a, 0xffffffff, 0x0010000f,\n\t0x249b, 0xffffffff, 0x00000013,\n\t0x249c, 0xffffffff, 0x00120011,\n\t0x249d, 0xffffffff, 0x00150014,\n\t0x249e, 0xffffffff, 0x000f000e,\n\t0x249f, 0xffffffff, 0x00110010,\n\t0x24a0, 0xffffffff, 0x00000014,\n\t0x24a1, 0xffffffff, 0x00130012,\n\t0x24a2, 0xffffffff, 0x00160015,\n\t0x24a3, 0xffffffff, 0x0010000f,\n\t0x24a4, 0xffffffff, 0x00120011,\n\t0x24a5, 0xffffffff, 0x00000015,\n\t0x24a6, 0xffffffff, 0x00140013,\n\t0x24a7, 0xffffffff, 0x00170016,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\nstatic const u32 pitcairn_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 verde_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 oland_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 hainan_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\t0x0409, 0xffffffff, 0x00000100,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\n/* XXX: update when we support VCE */\n#if 0\n/* tahiti, pitcarin, verde */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_encode_array),\n\t.codec_array = tahiti_video_codecs_encode_array,\n};\n#else\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n#endif\n/* oland and hainan don't support encode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* tahiti, pitcarin, verde, oland */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 41,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_decode_array),\n\t.codec_array = tahiti_video_codecs_decode_array,\n};\n\n/* hainan doesn't support decode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\nstatic int si_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tif (encode)\n\t\t\t*codecs = &tahiti_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_OLAND:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_HAINAN:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &hainan_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic u32 si_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tr = RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tWREG32(AMDGPU_PCIE_DATA, v);\n\t(void)RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_pciep_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tr = RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pciep_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tWREG32(PCIE_PORT_DATA, (v));\n\t(void)RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tr = RREG32(SMC_IND_DATA_0);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tWREG32(SMC_IND_DATA_0, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 si_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic struct amdgpu_allowed_register_entry si_allowed_read_registers[] = {\n\t{GRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{DMA_STATUS_REG + DMA0_REGISTER_OFFSET},\n\t{DMA_STATUS_REG + DMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{GB_ADDR_CONFIG},\n\t{MC_ARB_RAMCFG},\n\t{GB_TILE_MODE0},\n\t{GB_TILE_MODE1},\n\t{GB_TILE_MODE2},\n\t{GB_TILE_MODE3},\n\t{GB_TILE_MODE4},\n\t{GB_TILE_MODE5},\n\t{GB_TILE_MODE6},\n\t{GB_TILE_MODE7},\n\t{GB_TILE_MODE8},\n\t{GB_TILE_MODE9},\n\t{GB_TILE_MODE10},\n\t{GB_TILE_MODE11},\n\t{GB_TILE_MODE12},\n\t{GB_TILE_MODE13},\n\t{GB_TILE_MODE14},\n\t{GB_TILE_MODE15},\n\t{GB_TILE_MODE16},\n\t{GB_TILE_MODE17},\n\t{GB_TILE_MODE18},\n\t{GB_TILE_MODE19},\n\t{GB_TILE_MODE20},\n\t{GB_TILE_MODE21},\n\t{GB_TILE_MODE22},\n\t{GB_TILE_MODE23},\n\t{GB_TILE_MODE24},\n\t{GB_TILE_MODE25},\n\t{GB_TILE_MODE26},\n\t{GB_TILE_MODE27},\n\t{GB_TILE_MODE28},\n\t{GB_TILE_MODE29},\n\t{GB_TILE_MODE30},\n\t{GB_TILE_MODE31},\n\t{CC_RB_BACKEND_DISABLE, true},\n\t{GC_USER_RB_BACKEND_DISABLE, true},\n\t{PA_SC_RASTER_CONFIG, true},\n};\n\nstatic uint32_t si_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\nstatic int si_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t     u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(si_allowed_read_registers); i++) {\n\t\tbool indexed = si_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != si_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = si_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic bool si_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(R600_BUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(AVIVO_D1VGA_CONTROL);\n\t\td2vga_control = RREG32(AVIVO_D2VGA_CONTROL);\n\t\tvga_render_control = RREG32(VGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32(R600_ROM_CNTL);\n\n\t/* enable the rom */\n\tWREG32(R600_BUS_CNTL, (bus_cntl & ~R600_BIOS_ROM_DIS));\n\tif (adev->mode_info.num_crtc) {\n\t\t/* Disable VGA mode */\n\t\tWREG32(AVIVO_D1VGA_CONTROL,\n\t\t       (d1vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(AVIVO_D2VGA_CONTROL,\n\t\t       (d2vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(VGA_RENDER_CONTROL,\n\t\t       (vga_render_control & C_000300_VGA_VSTATUS_CNTL));\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl | R600_SCK_OVERWRITE);\n\n\tr = amdgpu_read_bios(adev);\n\n\t/* restore regs */\n\tWREG32(R600_BUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(AVIVO_D1VGA_CONTROL, d1vga_control);\n\t\tWREG32(AVIVO_D2VGA_CONTROL, d2vga_control);\n\t\tWREG32(VGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl);\n\treturn r;\n}\n\n#define mmROM_INDEX 0x2A\n#define mmROM_DATA  0x2B\n\nstatic bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\nstatic void si_set_clk_bypass_mode(struct amdgpu_device *adev)\n{\n\tu32 tmp, i;\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_BYPASS_EN;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp |= SPLL_CTLREQ_CHG;\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(SPLL_STATUS) & SPLL_CHG_STATUS)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp &= ~(SPLL_CTLREQ_CHG | SCLK_MUX_UPDATE);\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\ttmp = RREG32(MPLL_CNTL_MODE);\n\ttmp &= ~MPLL_MCLK_SEL;\n\tWREG32(MPLL_CNTL_MODE, tmp);\n}\n\nstatic void si_spll_powerdown(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp |= SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_RESET;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_SLEEP;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp &= ~SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n}\n\nstatic int si_gpu_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t/* set mclk/sclk to bypass */\n\tsi_set_clk_bypass_mode(adev);\n\t/* powerdown spll */\n\tsi_spll_powerdown(adev);\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\t/* reset */\n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t/* wait for asic to come out of reset */\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t/* enable BM */\n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic int si_asic_supports_baco(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\nstatic enum amd_reset_method\nsi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_PCI)\n\t\treturn amdgpu_reset_method;\n\telse if (amdgpu_reset_method != AMD_RESET_METHOD_LEGACY &&\n\t\t amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t amdgpu_reset_method);\n\n\treturn AMD_RESET_METHOD_LEGACY;\n}\n\nstatic int si_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tswitch (si_asic_reset_method(adev)) {\n\tcase AMD_RESET_METHOD_PCI:\n\t\tdev_info(adev->dev, \"PCI reset\\n\");\n\t\tr = amdgpu_device_pci_reset(adev);\n\t\tbreak;\n\tdefault:\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = si_gpu_pci_config_reset(adev);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic u32 si_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic void si_vga_set_state(struct amdgpu_device *adev, bool state)\n{\n\tuint32_t temp;\n\n\ttemp = RREG32(CONFIG_CNTL);\n\tif (!state) {\n\t\ttemp &= ~(1<<0);\n\t\ttemp |= (1<<1);\n\t} else {\n\t\ttemp &= ~(1<<1);\n\t}\n\tWREG32(CONFIG_CNTL, temp);\n}\n\nstatic u32 si_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL_2);\n\tif (tmp & MUX_TCLK_TO_XCLK)\n\t\treturn TCLK;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL);\n\tif (tmp & XTALIN_DIVIDE)\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\nstatic void si_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void si_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool si_need_full_reset(struct amdgpu_device *adev)\n{\n\t/* change this when we support soft reset */\n\treturn true;\n}\n\nstatic bool si_need_reset_on_init(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n\nstatic int si_get_pcie_lanes(struct amdgpu_device *adev)\n{\n\tu32 link_width_cntl;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\n\tswitch ((link_width_cntl & LC_LINK_WIDTH_RD_MASK) >> LC_LINK_WIDTH_RD_SHIFT) {\n\tcase LC_LINK_WIDTH_X1:\n\t\treturn 1;\n\tcase LC_LINK_WIDTH_X2:\n\t\treturn 2;\n\tcase LC_LINK_WIDTH_X4:\n\t\treturn 4;\n\tcase LC_LINK_WIDTH_X8:\n\t\treturn 8;\n\tcase LC_LINK_WIDTH_X0:\n\tcase LC_LINK_WIDTH_X16:\n\tdefault:\n\t\treturn 16;\n\t}\n}\n\nstatic void si_set_pcie_lanes(struct amdgpu_device *adev, int lanes)\n{\n\tu32 link_width_cntl, mask;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tswitch (lanes) {\n\tcase 0:\n\t\tmask = LC_LINK_WIDTH_X0;\n\t\tbreak;\n\tcase 1:\n\t\tmask = LC_LINK_WIDTH_X1;\n\t\tbreak;\n\tcase 2:\n\t\tmask = LC_LINK_WIDTH_X2;\n\t\tbreak;\n\tcase 4:\n\t\tmask = LC_LINK_WIDTH_X4;\n\t\tbreak;\n\tcase 8:\n\t\tmask = LC_LINK_WIDTH_X8;\n\t\tbreak;\n\tcase 16:\n\t\tmask = LC_LINK_WIDTH_X16;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"invalid pcie lane request: %d\\n\", lanes);\n\t\treturn;\n\t}\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\tlink_width_cntl &= ~LC_LINK_WIDTH_MASK;\n\tlink_width_cntl |= mask << LC_LINK_WIDTH_SHIFT;\n\tlink_width_cntl |= (LC_RECONFIG_NOW |\n\t\t\t    LC_RECONFIG_ARC_MISSING_ESCAPE);\n\n\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, link_width_cntl);\n}\n\nstatic void si_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t/* This reports 0 on APUs, so return to avoid writing/reading registers\n\t * that may or may not be different from their GPU counterparts\n\t */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t/* Set the 2 events that we wish to watch, defined above */\n\t/* Reg 40 is # received msgs, Reg 104 is # of posted requests sent */\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t/* Write to enable desired perf counters */\n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t/* Zero out and enable the perf counters\n\t * Write 0x5:\n\t * Bit 0 = Start all counters(1)\n\t * Bit 2 = Global counter reset enable(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t/* Load the shadow and disable the perf counters\n\t * Write 0x2:\n\t * Bit 0 = Stop counters(0)\n\t * Bit 1 = Load the shadow counters(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t/* Read register values to get any >32bit overflow */\n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t/* Get the values and add the overflow */\n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t si_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t/* Get the number of NAKs received and generated */\n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t/* Add the total number of NAKs, i.e the number of replays */\n\treturn (nak_r + nak_g);\n}\n\nstatic int si_uvd_send_upll_ctlreq(struct amdgpu_device *adev,\n\t\t\t\t   unsigned cg_upll_func_cntl)\n{\n\tunsigned i;\n\n\t/* Make sure UPLL_CTLREQ is deasserted */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32(cg_upll_func_cntl) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned si_uvd_calc_upll_post_div(unsigned vco_freq,\n\t\t\t\t\t  unsigned target_freq,\n\t\t\t\t\t  unsigned pd_min,\n\t\t\t\t\t  unsigned pd_even)\n{\n\tunsigned post_div = vco_freq / target_freq;\n\n\t/* Adjust to post divider minimum value */\n\tif (post_div < pd_min)\n\t\tpost_div = pd_min;\n\n\t/* We alway need a frequency less than or equal the target */\n\tif ((vco_freq / post_div) > target_freq)\n\t\tpost_div += 1;\n\n\t/* Post dividers above a certain value must be even */\n\tif (post_div > pd_even && post_div % 2)\n\t\tpost_div += 1;\n\n\treturn post_div;\n}\n\n/**\n * si_calc_upll_dividers - calc UPLL clock dividers\n *\n * @adev: amdgpu_device pointer\n * @vclk: wanted VCLK\n * @dclk: wanted DCLK\n * @vco_min: minimum VCO frequency\n * @vco_max: maximum VCO frequency\n * @fb_factor: factor to multiply vco freq with\n * @fb_mask: limit and bitmask for feedback divider\n * @pd_min: post divider minimum\n * @pd_max: post divider maximum\n * @pd_even: post divider must be even above this value\n * @optimal_fb_div: resulting feedback divider\n * @optimal_vclk_div: resulting vclk post divider\n * @optimal_dclk_div: resulting dclk post divider\n *\n * Calculate dividers for UVDs UPLL (except APUs).\n * Returns zero on success; -EINVAL on error.\n */\nstatic int si_calc_upll_dividers(struct amdgpu_device *adev,\n\t\t\t\t unsigned vclk, unsigned dclk,\n\t\t\t\t unsigned vco_min, unsigned vco_max,\n\t\t\t\t unsigned fb_factor, unsigned fb_mask,\n\t\t\t\t unsigned pd_min, unsigned pd_max,\n\t\t\t\t unsigned pd_even,\n\t\t\t\t unsigned *optimal_fb_div,\n\t\t\t\t unsigned *optimal_vclk_div,\n\t\t\t\t unsigned *optimal_dclk_div)\n{\n\tunsigned vco_freq, ref_freq = adev->clock.spll.reference_freq;\n\n\t/* Start off with something large */\n\tunsigned optimal_score = ~0;\n\n\t/* Loop through vco from low to high */\n\tvco_min = max(max(vco_min, vclk), dclk);\n\tfor (vco_freq = vco_min; vco_freq <= vco_max; vco_freq += 100) {\n\t\tuint64_t fb_div = (uint64_t)vco_freq * fb_factor;\n\t\tunsigned vclk_div, dclk_div, score;\n\n\t\tdo_div(fb_div, ref_freq);\n\n\t\t/* fb div out of range ? */\n\t\tif (fb_div > fb_mask)\n\t\t\tbreak; /* It can oly get worse */\n\n\t\tfb_div &= fb_mask;\n\n\t\t/* Calc vclk divider with current vco freq */\n\t\tvclk_div = si_uvd_calc_upll_post_div(vco_freq, vclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (vclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc dclk divider with current vco freq */\n\t\tdclk_div = si_uvd_calc_upll_post_div(vco_freq, dclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (dclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc score with current vco freq */\n\t\tscore = vclk - (vco_freq / vclk_div) + dclk - (vco_freq / dclk_div);\n\n\t\t/* Determine if this vco setting is better than current optimal settings */\n\t\tif (score < optimal_score) {\n\t\t\t*optimal_fb_div = fb_div;\n\t\t\t*optimal_vclk_div = vclk_div;\n\t\t\t*optimal_dclk_div = dclk_div;\n\t\t\toptimal_score = score;\n\t\t\tif (optimal_score == 0)\n\t\t\t\tbreak; /* It can't get better than this */\n\t\t}\n\t}\n\n\t/* Did we found a valid setup ? */\n\tif (optimal_score == ~0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int si_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tunsigned fb_div = 0, vclk_div = 0, dclk_div = 0;\n\tint r;\n\n\t/* Bypass vclk and dclk with bclk */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(1) | DCLK_SRC_SEL(1),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_BYPASS_EN_MASK, ~UPLL_BYPASS_EN_MASK);\n\n\tif (!vclk || !dclk) {\n\t\t/* Keep the Bypass mode */\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, vclk, dclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &vclk_div, &dclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_VCO_MODE_MASK, ~UPLL_VCO_MODE_MASK);\n\n\t/* Disable sleep mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_SLEEP_MASK);\n\n\t/* Deassert UPLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert UPLL_RESET again */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_RESET_MASK, ~UPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_P(CG_UPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_3, UPLL_FB_DIV(fb_div), ~UPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_REF_DIV_MASK);\n\n\tif (fb_div < 307200)\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4, 0, ~UPLL_SPARE_ISPARE9);\n\telse\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4,\n\t\t\t UPLL_SPARE_ISPARE9,\n\t\t\t ~UPLL_SPARE_ISPARE9);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t UPLL_PDIV_A(vclk_div) | UPLL_PDIV_B(dclk_div),\n\t\t ~(UPLL_PDIV_A_MASK | UPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_BYPASS_EN_MASK);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(2) | DCLK_SRC_SEL(2),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic int si_vce_send_vcepll_ctlreq(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t/* Make sure VCEPLL_CTLREQ is deasserted */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32_SMC(CG_VCEPLL_FUNC_CNTL) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tunsigned fb_div = 0, evclk_div = 0, ecclk_div = 0;\n\tint r;\n\n\t/* Bypass evclk and ecclk with bclk */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(1) | ECCLK_SRC_SEL(1),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_BYPASS_EN_MASK,\n\t\t     ~VCEPLL_BYPASS_EN_MASK);\n\n\tif (!evclk || !ecclk) {\n\t\t/* Keep the Bypass mode, put PLL to sleep */\n\t\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t\t     ~VCEPLL_SLEEP_MASK);\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, evclk, ecclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &evclk_div, &ecclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_VCO_MODE_MASK,\n\t\t     ~VCEPLL_VCO_MODE_MASK);\n\n\t/* Toggle VCEPLL_SLEEP to 1 then back to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t     ~VCEPLL_SLEEP_MASK);\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_SLEEP_MASK);\n\n\t/* Deassert VCEPLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert VCEPLL_RESET again */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_RESET_MASK, ~VCEPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_SMC_P(CG_VCEPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_3,\n\t\t     VCEPLL_FB_DIV(fb_div),\n\t\t     ~VCEPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_REF_DIV_MASK);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     VCEPLL_PDIV_A(evclk_div) | VCEPLL_PDIV_B(ecclk_div),\n\t\t     ~(VCEPLL_PDIV_A_MASK | VCEPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_BYPASS_EN_MASK);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(16) | ECCLK_SRC_SEL(16),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic void si_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs si_asic_funcs =\n{\n\t.read_disabled_bios = &si_read_disabled_bios,\n\t.read_bios_from_rom = &si_read_bios_from_rom,\n\t.read_register = &si_read_register,\n\t.reset = &si_asic_reset,\n\t.reset_method = &si_asic_reset_method,\n\t.set_vga_state = &si_vga_set_state,\n\t.get_xclk = &si_get_xclk,\n\t.set_uvd_clocks = &si_set_uvd_clocks,\n\t.set_vce_clocks = &si_set_vce_clocks,\n\t.get_pcie_lanes = &si_get_pcie_lanes,\n\t.set_pcie_lanes = &si_set_pcie_lanes,\n\t.get_config_memsize = &si_get_config_memsize,\n\t.flush_hdp = &si_flush_hdp,\n\t.invalidate_hdp = &si_invalidate_hdp,\n\t.need_full_reset = &si_need_full_reset,\n\t.get_pcie_usage = &si_get_pcie_usage,\n\t.need_reset_on_init = &si_need_reset_on_init,\n\t.get_pcie_replay_count = &si_get_pcie_replay_count,\n\t.supports_baco = &si_asic_supports_baco,\n\t.pre_asic_init = &si_pre_asic_init,\n\t.query_video_codecs = &si_query_video_codecs,\n};\n\nstatic uint32_t si_get_rev_id(struct amdgpu_device *adev)\n{\n\treturn (RREG32(CC_DRM_ID_STRAPS) & CC_DRM_ID_STRAPS__ATI_REV_ID_MASK)\n\t\t>> CC_DRM_ID_STRAPS__ATI_REV_ID__SHIFT;\n}\n\nstatic int si_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->smc_rreg = &si_smc_rreg;\n\tadev->smc_wreg = &si_smc_wreg;\n\tadev->pcie_rreg = &si_pcie_rreg;\n\tadev->pcie_wreg = &si_pcie_wreg;\n\tadev->pciep_rreg = &si_pciep_rreg;\n\tadev->pciep_wreg = &si_pciep_wreg;\n\tadev->uvd_ctx_rreg = si_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = si_uvd_ctx_wreg;\n\tadev->didt_rreg = NULL;\n\tadev->didt_wreg = NULL;\n\n\tadev->asic_funcs = &si_asic_funcs;\n\n\tadev->rev_id = si_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = (adev->rev_id == 0) ? 1 :\n\t\t\t\t\t(adev->rev_id == 1) ? 5 : 6;\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 20;\n\t\tbreak;\n\n\tcase CHIP_VERDE:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\t//???\n\t\tadev->external_rev_id = adev->rev_id + 40;\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 60;\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 70;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_common_sw_init(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\n\nstatic void si_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers2));\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_VERDE:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_pg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_pg_init));\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers2));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_mgcg_cgcg_init));\n\t\tbreak;\n\n\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void si_pcie_gen3_enable(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *root = adev->pdev->bus->self;\n\tu32 speed_cntl, current_data_rate;\n\tint i;\n\tu16 tmp16;\n\n\tif (pci_is_root_bus(adev->pdev->bus))\n\t\treturn;\n\n\tif (amdgpu_pcie_gen2 == 0)\n\t\treturn;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tif (!(adev->pm.pcie_gen_mask & (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\tCAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)))\n\t\treturn;\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tcurrent_data_rate = (speed_cntl & LC_CURRENT_DATA_RATE_MASK) >>\n\t\tLC_CURRENT_DATA_RATE_SHIFT;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate == 2) {\n\t\t\tDRM_INFO(\"PCIE gen 3 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 3 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t} else if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2) {\n\t\tif (current_data_rate == 1) {\n\t\t\tDRM_INFO(\"PCIE gen 2 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 2 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t}\n\n\tif (!pci_is_pcie(root) || !pci_is_pcie(adev->pdev))\n\t\treturn;\n\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate != 2) {\n\t\t\tu16 bridge_cfg, gpu_cfg;\n\t\t\tu16 bridge_cfg2, gpu_cfg2;\n\t\t\tu32 max_lw, current_lw, tmp;\n\n\t\t\tpcie_capability_set_word(root, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\t\t\tpcie_capability_set_word(adev->pdev, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\n\t\t\ttmp = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tmax_lw = (tmp & LC_DETECTED_LINK_WIDTH_MASK) >> LC_DETECTED_LINK_WIDTH_SHIFT;\n\t\t\tcurrent_lw = (tmp & LC_OPERATING_LINK_WIDTH_MASK) >> LC_OPERATING_LINK_WIDTH_SHIFT;\n\n\t\t\tif (current_lw < max_lw) {\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\t\tif (tmp & LC_RENEGOTIATION_SUPPORT) {\n\t\t\t\t\ttmp &= ~(LC_LINK_WIDTH_MASK | LC_UPCONFIGURE_DIS);\n\t\t\t\t\ttmp |= (max_lw << LC_LINK_WIDTH_SHIFT);\n\t\t\t\t\ttmp |= LC_UPCONFIGURE_SUPPORT | LC_RENEGOTIATE_EN | LC_RECONFIG_NOW;\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, tmp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (i = 0; i < 10; i++) {\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_DEVSTA,\n\t\t\t\t\t\t\t  &tmp16);\n\t\t\t\tif (tmp16 & PCI_EXP_DEVSTA_TRPND)\n\t\t\t\t\tbreak;\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &bridge_cfg);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &gpu_cfg);\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &bridge_cfg2);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &gpu_cfg2);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_REDO_EQ;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\tmdelay(100);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   bridge_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   gpu_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   bridge_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   gpu_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp &= ~LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\t\t\t}\n\t\t}\n\t}\n\n\tspeed_cntl |= LC_FORCE_EN_SW_SPEED_CHANGE | LC_FORCE_DIS_HW_SPEED_CHANGE;\n\tspeed_cntl &= ~LC_FORCE_DIS_SW_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\ttmp16 = 0;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_8_0GT; /* gen3 */\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_5_0GT; /* gen2 */\n\telse\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_2_5GT; /* gen1 */\n\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t   PCI_EXP_LNKCTL2_TLS, tmp16);\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tspeed_cntl |= LC_INITIATE_LINK_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\t\tif ((speed_cntl & LC_INITIATE_LINK_SPEED_CHANGE) == 0)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n}\n\nstatic inline u32 si_pif_phy0_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY0_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy0_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY0_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic inline u32 si_pif_phy1_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY1_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy1_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY1_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\nstatic void si_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\tbool disable_l0s = false, disable_l1 = false, disable_plloff_in_l1 = false;\n\tbool disable_clkreq = false;\n\n\tif (!amdgpu_device_should_use_aspm(adev))\n\t\treturn;\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\tdata &= ~LC_XMIT_N_FTS_MASK;\n\tdata |= LC_XMIT_N_FTS(0x24) | LC_XMIT_N_FTS_OVERRIDE_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL3);\n\tdata |= LC_GO_TO_RECOVERY;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(PCIE_P_CNTL);\n\tdata |= P_IGNORE_EDB_ERR;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_P_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\tdata &= ~(LC_L0S_INACTIVITY_MASK | LC_L1_INACTIVITY_MASK);\n\tdata |= LC_PMI_TO_L1_DIS;\n\tif (!disable_l0s)\n\t\tdata |= LC_L0S_INACTIVITY(7);\n\n\tif (!disable_l1) {\n\t\tdata |= LC_L1_INACTIVITY(7);\n\t\tdata &= ~LC_PMI_TO_L1_DIS;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\n\t\tif (!disable_plloff_in_l1) {\n\t\t\tbool clk_req_support;\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\tif ((adev->asic_type != CHIP_OLAND) && (adev->asic_type != CHIP_HAINAN)) {\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_3, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_3, data);\n\t\t\t}\n\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\tdata &= ~LC_DYN_LANES_PWR_STATE_MASK;\n\t\t\tdata |= LC_DYN_LANES_PWR_STATE(3);\n\t\t\tif (orig != data)\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_CNTL, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_CNTL, data);\n\n\t\t\tif (!disable_clkreq &&\n\t\t\t    !pci_is_root_bus(adev->pdev->bus)) {\n\t\t\t\tstruct pci_dev *root = adev->pdev->bus->self;\n\t\t\t\tu32 lnkcap;\n\n\t\t\t\tclk_req_support = false;\n\t\t\t\tpcie_capability_read_dword(root, PCI_EXP_LNKCAP, &lnkcap);\n\t\t\t\tif (lnkcap & PCI_EXP_LNKCAP_CLKPM)\n\t\t\t\t\tclk_req_support = true;\n\t\t\t} else {\n\t\t\t\tclk_req_support = false;\n\t\t\t}\n\n\t\t\tif (clk_req_support) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL2);\n\t\t\t\tdata |= LC_ALLOW_PDWN_IN_L1 | LC_ALLOW_PDWN_IN_L23;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL2, data);\n\n\t\t\t\torig = data = RREG32(THM_CLK_CNTL);\n\t\t\t\tdata &= ~(CMON_CLK_SEL_MASK | TMON_CLK_SEL_MASK);\n\t\t\t\tdata |= CMON_CLK_SEL(1) | TMON_CLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(THM_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(MISC_CLK_CNTL);\n\t\t\t\tdata &= ~(DEEP_SLEEP_CLK_SEL_MASK | ZCLK_SEL_MASK);\n\t\t\t\tdata |= DEEP_SLEEP_CLK_SEL(1) | ZCLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MISC_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL);\n\t\t\t\tdata &= ~BCLK_AS_XCLK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL_2);\n\t\t\t\tdata &= ~FORCE_BIF_REFCLK_EN;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL_2, data);\n\n\t\t\t\torig = data = RREG32(MPLL_BYPASSCLK_SEL);\n\t\t\t\tdata &= ~MPLL_CLKOUT_SEL_MASK;\n\t\t\t\tdata |= MPLL_CLKOUT_SEL(4);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MPLL_BYPASSCLK_SEL, data);\n\n\t\t\t\torig = data = RREG32(SPLL_CNTL_MODE);\n\t\t\t\tdata &= ~SPLL_REFCLK_SEL_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(SPLL_CNTL_MODE, data);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t}\n\n\torig = data = RREG32_PCIE(PCIE_CNTL2);\n\tdata |= SLV_MEM_LS_EN | MST_MEM_LS_EN | REPLAY_MEM_LS_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_CNTL2, data);\n\n\tif (!disable_l0s) {\n\t\tdata = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\t\tif((data & LC_N_FTS_MASK) == LC_N_FTS_MASK) {\n\t\t\tdata = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tif ((data & LC_REVERSE_XMIT) && (data & LC_REVERSE_RCVR)) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\t\t\t\tdata &= ~LC_L0S_INACTIVITY_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void si_fix_pci_max_read_req_size(struct amdgpu_device *adev)\n{\n\tint readrq;\n\tu16 v;\n\n\treadrq = pcie_get_readrq(adev->pdev);\n\tv = ffs(readrq) - 8;\n\tif ((v == 0) || (v == 6) || (v == 7))\n\t\tpcie_set_readrq(adev->pdev, 512);\n}\n\nstatic int si_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsi_fix_pci_max_read_req_size(adev);\n\tsi_init_golden_registers(adev);\n\tsi_pcie_gen3_enable(adev);\n\tsi_program_aspm(adev);\n\n\treturn 0;\n}\n\nstatic int si_common_hw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_fini(adev);\n}\n\nstatic int si_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_init(adev);\n}\n\nstatic bool si_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int si_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_clockgating_state(void *handle,\n\t\t\t\t\t    enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs si_common_ip_funcs = {\n\t.name = \"si_common\",\n\t.early_init = si_common_early_init,\n\t.late_init = NULL,\n\t.sw_init = si_common_sw_init,\n\t.sw_fini = si_common_sw_fini,\n\t.hw_init = si_common_hw_init,\n\t.hw_fini = si_common_hw_fini,\n\t.suspend = si_common_suspend,\n\t.resume = si_common_resume,\n\t.is_idle = si_common_is_idle,\n\t.wait_for_idle = si_common_wait_for_idle,\n\t.soft_reset = si_common_soft_reset,\n\t.set_clockgating_state = si_common_set_clockgating_state,\n\t.set_powergating_state = si_common_set_powergating_state,\n\t.dump_ip_state = NULL,\n\t.print_ip_state = NULL,\n};\n\nstatic const struct amdgpu_ip_block_version si_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &si_common_ip_funcs,\n};\n\nint si_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn 0;\n}\n\n", "source_code_path": "si.c", "line_number": 1324, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: si.c\nLine: 1324\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\n**Issue:** Lack of Boundary Checks **Type:** BUFFER_OVERFLOW **File:** drivers/gpu/drm/amd/amdgpu/si.c **Line:** 1324\n\n**Description:**\nThe `si_read_bios_from_rom` function does not properly check the data transfer size against the allocated buffer size, leaving it vulnerable to buffer overflows that can result in memory corruption or security vulnerabilities.====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "si-bug.txt", "bug_report_text": "**Issue:** Lack of Boundary Checks **Type:** BUFFER_OVERFLOW **File:** drivers/gpu/drm/amd/amdgpu/si.c **Line:** 1324\n\n**Description:**\nThe `si_read_bios_from_rom` function fails to enforce a safeguard against excessive data transfer, allowing the possibility of a buffer overflow that can compromise system security and lead to memory corruption.", "diff_path": "si-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n", "code": "/*\n * Copyright 2015 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/firmware.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n#include \"si_dpm.h\"\n#include \"sid.h\"\n#include \"si_ih.h\"\n#include \"gfx_v6_0.h\"\n#include \"gmc_v6_0.h\"\n#include \"si_dma.h\"\n#include \"dce_v6_0.h\"\n#include \"si.h\"\n#include \"uvd_v3_1.h\"\n#include \"amdgpu_vkms.h\"\n#include \"gca/gfx_6_0_d.h\"\n#include \"oss/oss_1_0_d.h\"\n#include \"oss/oss_1_0_sh_mask.h\"\n#include \"gmc/gmc_6_0_d.h\"\n#include \"dce/dce_6_0_d.h\"\n#include \"uvd/uvd_4_0_d.h\"\n#include \"bif/bif_3_0_d.h\"\n#include \"bif/bif_3_0_sh_mask.h\"\n\n#include \"amdgpu_dm.h\"\n\nstatic const u32 tahiti_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000000c0, 0x00800040,\n\t0x360c, 0x000000c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_ARB_WTM_CNTL_RD, 0x00000003, 0x000007ff,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x00000200, 0x000002fb,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x0000543b,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0xa9210876,\n\tmmVGT_FIFO_DEPTHS, 0xffffffff, 0x000fff40,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_CONTEXT0_CNTL, 0x20000000, 0x20fffed8,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 tahiti_golden_registers2[] =\n{\n\tmmMCIF_MEM_CONTROL, 0x00000001, 0x00000001,\n};\n\nstatic const u32 tahiti_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n\tmmUVD_CGC_GATE, 0x00000008, 0x00000000,\n};\n\nstatic const u32 pitcairn_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x2a00126a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f7,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x32761054,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 pitcairn_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x12011003,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601004,\n\t0x311f, 0xffffffff, 0x10102020,\n\t0x3122, 0xffffffff, 0x01000020,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000a4,\n};\n\nstatic const u32 verde_pg_init[] =\n{\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x40000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x200010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x7007,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x300010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x400000,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x100010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x120200,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x500010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x1e1e16,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x600010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x171f1e,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x700010ff,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_WRITE, 0xffffffff, 0x0,\n\tmmGMCON_PGFSM_CONFIG, 0xffffffff, 0x9ff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x0,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10000800,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1000051e,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xffff,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x8,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x80500,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x12,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x9050c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xb052c,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1053e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x2d,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10546,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x30,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xa054e,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1055f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x3f,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10567,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x42,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1056f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x45,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x10572,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x48,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20575,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x4c,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x190801,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x67,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1082a,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x6a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1b082d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x87,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x310851,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xba,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x891,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x893,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xbe,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20895,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc2,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x20899,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xc6,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2089d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xca,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xcc,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x8a3,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xce,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x308a5,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0xd3,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x6d08cd,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x142,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x2000095a,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x1,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x144,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x301f095b,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x165,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc094d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x173,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xf096d,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x184,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x15097f,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x19b,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xc0998,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1a9,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x409a7,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1af,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0xcdc,\n\tmmGMCON_RENG_RAM_INDEX, 0xffffffff, 0x1b1,\n\tmmGMCON_RENG_RAM_DATA, 0xffffffff, 0x800,\n\tmmGMCON_RENG_EXECUTE, 0xffffffff, 0x6c9b2000,\n\tmmGMCON_MISC2, 0xfc00, 0x2000,\n\tmmGMCON_MISC3, 0xffffffff, 0xfc0,\n\tmmMC_PMG_AUTO_CFG, 0x00000100, 0x100,\n};\n\nstatic const u32 verde_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x033f1005,\n\t0x311f, 0xffffffff, 0x10808020,\n\t0x3122, 0xffffffff, 0x00800008,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00001000,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x80010014,\n};\n\nstatic const u32 verde_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x0000124a,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSQ_DED_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSQ_SEC_CNT, 0x01ff1f3f, 0x00000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x00000003,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00001032,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 oland_golden_registers[] =\n{\n\tmmAZALIA_SCLK_CONTROL, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\tmmDCI_CLK_CNTL, 0x00000080, 0x00000000,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x360c, 0x000300c0, 0x00800040,\n\tmmFBC_DEBUG_COMP, 0x000000f0, 0x00000070,\n\tmmFBC_MISC, 0x00200000, 0x50100000,\n\tmmDIG0_HDMI_CONTROL, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000082,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x07ffffff, 0x03000000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f3,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\n};\n\nstatic const u32 oland_golden_rlc_registers[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x02010002,\n\tmmRLC_LB_PARAMS, 0xffffffff, 0x00601005,\n\t0x311f, 0xffffffff, 0x10104040,\n\t0x3122, 0xffffffff, 0x0100000a,\n\tmmRLC_LB_CNTR_MAX, 0xffffffff, 0x00000800,\n\tmmRLC_LB_CNTL, 0xffffffff, 0x800000f4,\n};\n\nstatic const u32 hainan_golden_registers[] =\n{\n\t0x17bc, 0x00000030, 0x00000011,\n\tmmCB_HW_CONTROL, 0x00010000, 0x00018208,\n\tmmDB_DEBUG, 0xffffffff, 0x00000000,\n\tmmDB_DEBUG2, 0xf00fffff, 0x00000400,\n\tmmDB_DEBUG3, 0x0002021c, 0x00020200,\n\t0x031e, 0x00000080, 0x00000000,\n\t0x3430, 0xff000fff, 0x00000100,\n\t0x340c, 0x000300c0, 0x00800040,\n\t0x3630, 0xff000fff, 0x00000100,\n\t0x360c, 0x000300c0, 0x00800040,\n\t0x16ec, 0x000000f0, 0x00000070,\n\t0x16f0, 0x00200000, 0x50100000,\n\t0x1c0c, 0x31000311, 0x00000011,\n\tmmMC_SEQ_PMG_PG_HWCNTL, 0x00073ffe, 0x000022a2,\n\tmmMC_XPB_P2P_BAR_CFG, 0x000007ff, 0x00000000,\n\tmmPA_CL_ENHANCE, 0xf000001f, 0x00000007,\n\tmmPA_SC_FORCE_EOV_MAX_CNTS, 0xffffffff, 0x00ffffff,\n\tmmPA_SC_LINE_STIPPLE_STATE, 0x0000ff0f, 0x00000000,\n\tmmPA_SC_MODE_CNTL_1, 0x07ffffff, 0x4e000000,\n\tmmPA_SC_RASTER_CONFIG, 0x3f3f3fff, 0x00000000,\n\t0x000c, 0xffffffff, 0x0040,\n\t0x000d, 0x00000040, 0x00004040,\n\tmmSPI_CONFIG_CNTL, 0x03e00000, 0x03600000,\n\tmmSX_DEBUG_1, 0x0000007f, 0x00000020,\n\tmmTA_CNTL_AUX, 0x00010000, 0x00010000,\n\tmmTCP_ADDR_CONFIG, 0x000003ff, 0x000000f1,\n\tmmTCP_CHAN_STEER_HI, 0xffffffff, 0x00000000,\n\tmmTCP_CHAN_STEER_LO, 0xffffffff, 0x00003210,\n\tmmVGT_GS_VERTEX_REUSE, 0x0000001f, 0x00000010,\n\tmmVM_L2_CG, 0x000c0fc0, 0x000c0400,\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0xffffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n};\n\nstatic const u32 hainan_golden_registers2[] =\n{\n\tmmGB_ADDR_CONFIG, 0xffffffff, 0x2011003,\n};\n\nstatic const u32 tahiti_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\t0x2476, 0xffffffff, 0x00070006,\n\t0x2477, 0xffffffff, 0x00090008,\n\t0x2478, 0xffffffff, 0x0000000c,\n\t0x2479, 0xffffffff, 0x000b000a,\n\t0x247a, 0xffffffff, 0x000e000d,\n\t0x247b, 0xffffffff, 0x00080007,\n\t0x247c, 0xffffffff, 0x000a0009,\n\t0x247d, 0xffffffff, 0x0000000d,\n\t0x247e, 0xffffffff, 0x000c000b,\n\t0x247f, 0xffffffff, 0x000f000e,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\t0x2499, 0xffffffff, 0x000e000d,\n\t0x249a, 0xffffffff, 0x0010000f,\n\t0x249b, 0xffffffff, 0x00000013,\n\t0x249c, 0xffffffff, 0x00120011,\n\t0x249d, 0xffffffff, 0x00150014,\n\t0x249e, 0xffffffff, 0x000f000e,\n\t0x249f, 0xffffffff, 0x00110010,\n\t0x24a0, 0xffffffff, 0x00000014,\n\t0x24a1, 0xffffffff, 0x00130012,\n\t0x24a2, 0xffffffff, 0x00160015,\n\t0x24a3, 0xffffffff, 0x0010000f,\n\t0x24a4, 0xffffffff, 0x00120011,\n\t0x24a5, 0xffffffff, 0x00000015,\n\t0x24a6, 0xffffffff, 0x00140013,\n\t0x24a7, 0xffffffff, 0x00170016,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\nstatic const u32 pitcairn_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 verde_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2480, 0xffffffff, 0x00090008,\n\t0x2481, 0xffffffff, 0x000b000a,\n\t0x2482, 0xffffffff, 0x000c000f,\n\t0x2483, 0xffffffff, 0x000e000d,\n\t0x2484, 0xffffffff, 0x00110010,\n\t0x2485, 0xffffffff, 0x000a0009,\n\t0x2486, 0xffffffff, 0x000c000b,\n\t0x2487, 0xffffffff, 0x0000000f,\n\t0x2488, 0xffffffff, 0x000e000d,\n\t0x2489, 0xffffffff, 0x00110010,\n\t0x248a, 0xffffffff, 0x000b000a,\n\t0x248b, 0xffffffff, 0x000d000c,\n\t0x248c, 0xffffffff, 0x00000010,\n\t0x248d, 0xffffffff, 0x000f000e,\n\t0x248e, 0xffffffff, 0x00120011,\n\t0x248f, 0xffffffff, 0x000c000b,\n\t0x2490, 0xffffffff, 0x000e000d,\n\t0x2491, 0xffffffff, 0x00000011,\n\t0x2492, 0xffffffff, 0x0010000f,\n\t0x2493, 0xffffffff, 0x00130012,\n\t0x2494, 0xffffffff, 0x000d000c,\n\t0x2495, 0xffffffff, 0x000f000e,\n\t0x2496, 0xffffffff, 0x00100013,\n\t0x2497, 0xffffffff, 0x00120011,\n\t0x2498, 0xffffffff, 0x00150014,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 oland_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\tmmXDMA_CLOCK_GATING_CNTL, 0xffffffff, 0x00000100,\n\tmmXDMA_MEM_POWER_CNTL, 0x00000101, 0x00000000,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\t0x157a, 0x00000001, 0x00000001,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\nstatic const u32 hainan_mgcg_cgcg_init[] =\n{\n\tmmRLC_CGTT_MGCG_OVERRIDE, 0xffffffff, 0xfffffffc,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\tmmCB_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_BCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_CP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_GDS_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_IA_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmCGTT_PA_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_PC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_RLC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SC_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SPI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQ_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SQG_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL1, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL2, 0xffffffff, 0x00000100,\n\tmmCGTT_SX_CLK_CTRL3, 0xffffffff, 0x00000100,\n\tmmCGTT_TCI_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_TCP_CLK_CTRL, 0xffffffff, 0x00000100,\n\tmmCGTT_VGT_CLK_CTRL, 0xffffffff, 0x06000100,\n\tmmDB_CGTT_CLK_CTRL_0, 0xffffffff, 0x00000100,\n\tmmTA_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmTCA_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTCC_CGTT_SCLK_CTRL, 0xffffffff, 0x00000100,\n\tmmTD_CGTT_CTRL, 0xffffffff, 0x00000100,\n\tmmGRBM_GFX_INDEX, 0xffffffff, 0xe0000000,\n\t0x2458, 0xffffffff, 0x00010000,\n\t0x2459, 0xffffffff, 0x00030002,\n\t0x245a, 0xffffffff, 0x00040007,\n\t0x245b, 0xffffffff, 0x00060005,\n\t0x245c, 0xffffffff, 0x00090008,\n\t0x245d, 0xffffffff, 0x00020001,\n\t0x245e, 0xffffffff, 0x00040003,\n\t0x245f, 0xffffffff, 0x00000007,\n\t0x2460, 0xffffffff, 0x00060005,\n\t0x2461, 0xffffffff, 0x00090008,\n\t0x2462, 0xffffffff, 0x00030002,\n\t0x2463, 0xffffffff, 0x00050004,\n\t0x2464, 0xffffffff, 0x00000008,\n\t0x2465, 0xffffffff, 0x00070006,\n\t0x2466, 0xffffffff, 0x000a0009,\n\t0x2467, 0xffffffff, 0x00040003,\n\t0x2468, 0xffffffff, 0x00060005,\n\t0x2469, 0xffffffff, 0x00000009,\n\t0x246a, 0xffffffff, 0x00080007,\n\t0x246b, 0xffffffff, 0x000b000a,\n\t0x246c, 0xffffffff, 0x00050004,\n\t0x246d, 0xffffffff, 0x00070006,\n\t0x246e, 0xffffffff, 0x0008000b,\n\t0x246f, 0xffffffff, 0x000a0009,\n\t0x2470, 0xffffffff, 0x000d000c,\n\t0x2471, 0xffffffff, 0x00060005,\n\t0x2472, 0xffffffff, 0x00080007,\n\t0x2473, 0xffffffff, 0x0000000b,\n\t0x2474, 0xffffffff, 0x000a0009,\n\t0x2475, 0xffffffff, 0x000d000c,\n\tmmCGTS_SM_CTRL_REG, 0xffffffff, 0x96940200,\n\tmmCP_RB_WPTR_POLL_CNTL, 0xffffffff, 0x00900100,\n\tmmRLC_GCPM_GENERAL_3, 0xffffffff, 0x00000080,\n\tmmRLC_CGCG_CGLS_CTRL, 0xffffffff, 0x0020003f,\n\t0x000c, 0xffffffff, 0x0000001c,\n\t0x000d, 0x000f0000, 0x000f0000,\n\t0x0583, 0xffffffff, 0x00000100,\n\t0x0409, 0xffffffff, 0x00000100,\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104,\n\tmmMC_CITF_MISC_WR_CG, 0x000c0000, 0x000c0000,\n\tmmMC_CITF_MISC_RD_CG, 0x000c0000, 0x000c0000,\n\tmmHDP_MEM_POWER_LS, 0x00000001, 0x00000001,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n\tmmCP_MEM_SLP_CNTL, 0x00000001, 0x00000001,\n\t0x3430, 0xfffffff0, 0x00000100,\n\t0x3630, 0xfffffff0, 0x00000100,\n};\n\n/* XXX: update when we support VCE */\n#if 0\n/* tahiti, pitcarin, verde */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_encode_array),\n\t.codec_array = tahiti_video_codecs_encode_array,\n};\n#else\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n#endif\n/* oland and hainan don't support encode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n/* tahiti, pitcarin, verde, oland */\nstatic const struct amdgpu_video_codec_info tahiti_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 41,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 2048,\n\t\t.max_height = 1152,\n\t\t.max_pixels_per_frame = 2048 * 1152,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tahiti_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tahiti_video_codecs_decode_array),\n\t.codec_array = tahiti_video_codecs_decode_array,\n};\n\n/* hainan doesn't support decode */\nstatic const struct amdgpu_video_codecs hainan_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\nstatic int si_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tif (encode)\n\t\t\t*codecs = &tahiti_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_OLAND:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tahiti_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_HAINAN:\n\t\tif (encode)\n\t\t\t*codecs = &hainan_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &hainan_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic u32 si_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tr = RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(AMDGPU_PCIE_INDEX, reg);\n\t(void)RREG32(AMDGPU_PCIE_INDEX);\n\tWREG32(AMDGPU_PCIE_DATA, v);\n\t(void)RREG32(AMDGPU_PCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_pciep_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tr = RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_pciep_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(PCIE_PORT_INDEX, ((reg) & 0xff));\n\t(void)RREG32(PCIE_PORT_INDEX);\n\tWREG32(PCIE_PORT_DATA, (v));\n\t(void)RREG32(PCIE_PORT_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 si_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tr = RREG32(SMC_IND_DATA_0);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(SMC_IND_INDEX_0, (reg));\n\tWREG32(SMC_IND_DATA_0, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 si_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void si_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic struct amdgpu_allowed_register_entry si_allowed_read_registers[] = {\n\t{GRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{DMA_STATUS_REG + DMA0_REGISTER_OFFSET},\n\t{DMA_STATUS_REG + DMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{GB_ADDR_CONFIG},\n\t{MC_ARB_RAMCFG},\n\t{GB_TILE_MODE0},\n\t{GB_TILE_MODE1},\n\t{GB_TILE_MODE2},\n\t{GB_TILE_MODE3},\n\t{GB_TILE_MODE4},\n\t{GB_TILE_MODE5},\n\t{GB_TILE_MODE6},\n\t{GB_TILE_MODE7},\n\t{GB_TILE_MODE8},\n\t{GB_TILE_MODE9},\n\t{GB_TILE_MODE10},\n\t{GB_TILE_MODE11},\n\t{GB_TILE_MODE12},\n\t{GB_TILE_MODE13},\n\t{GB_TILE_MODE14},\n\t{GB_TILE_MODE15},\n\t{GB_TILE_MODE16},\n\t{GB_TILE_MODE17},\n\t{GB_TILE_MODE18},\n\t{GB_TILE_MODE19},\n\t{GB_TILE_MODE20},\n\t{GB_TILE_MODE21},\n\t{GB_TILE_MODE22},\n\t{GB_TILE_MODE23},\n\t{GB_TILE_MODE24},\n\t{GB_TILE_MODE25},\n\t{GB_TILE_MODE26},\n\t{GB_TILE_MODE27},\n\t{GB_TILE_MODE28},\n\t{GB_TILE_MODE29},\n\t{GB_TILE_MODE30},\n\t{GB_TILE_MODE31},\n\t{CC_RB_BACKEND_DISABLE, true},\n\t{GC_USER_RB_BACKEND_DISABLE, true},\n\t{PA_SC_RASTER_CONFIG, true},\n};\n\nstatic uint32_t si_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\nstatic int si_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t     u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(si_allowed_read_registers); i++) {\n\t\tbool indexed = si_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != si_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = si_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic bool si_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(R600_BUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(AVIVO_D1VGA_CONTROL);\n\t\td2vga_control = RREG32(AVIVO_D2VGA_CONTROL);\n\t\tvga_render_control = RREG32(VGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32(R600_ROM_CNTL);\n\n\t/* enable the rom */\n\tWREG32(R600_BUS_CNTL, (bus_cntl & ~R600_BIOS_ROM_DIS));\n\tif (adev->mode_info.num_crtc) {\n\t\t/* Disable VGA mode */\n\t\tWREG32(AVIVO_D1VGA_CONTROL,\n\t\t       (d1vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(AVIVO_D2VGA_CONTROL,\n\t\t       (d2vga_control & ~(AVIVO_DVGA_CONTROL_MODE_ENABLE |\n\t\t\t\t\t  AVIVO_DVGA_CONTROL_TIMING_SELECT)));\n\t\tWREG32(VGA_RENDER_CONTROL,\n\t\t       (vga_render_control & C_000300_VGA_VSTATUS_CNTL));\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl | R600_SCK_OVERWRITE);\n\n\tr = amdgpu_read_bios(adev);\n\n\t/* restore regs */\n\tWREG32(R600_BUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(AVIVO_D1VGA_CONTROL, d1vga_control);\n\t\tWREG32(AVIVO_D2VGA_CONTROL, d2vga_control);\n\t\tWREG32(VGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32(R600_ROM_CNTL, rom_cntl);\n\treturn r;\n}\n\n#define mmROM_INDEX 0x2A\n#define mmROM_DATA  0x2B\n\nstatic bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\nstatic void si_set_clk_bypass_mode(struct amdgpu_device *adev)\n{\n\tu32 tmp, i;\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_BYPASS_EN;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp |= SPLL_CTLREQ_CHG;\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(SPLL_STATUS) & SPLL_CHG_STATUS)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL_2);\n\ttmp &= ~(SPLL_CTLREQ_CHG | SCLK_MUX_UPDATE);\n\tWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\n\n\ttmp = RREG32(MPLL_CNTL_MODE);\n\ttmp &= ~MPLL_MCLK_SEL;\n\tWREG32(MPLL_CNTL_MODE, tmp);\n}\n\nstatic void si_spll_powerdown(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp |= SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_RESET;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(CG_SPLL_FUNC_CNTL);\n\ttmp |= SPLL_SLEEP;\n\tWREG32(CG_SPLL_FUNC_CNTL, tmp);\n\n\ttmp = RREG32(SPLL_CNTL_MODE);\n\ttmp &= ~SPLL_SW_DIR_CONTROL;\n\tWREG32(SPLL_CNTL_MODE, tmp);\n}\n\nstatic int si_gpu_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t/* set mclk/sclk to bypass */\n\tsi_set_clk_bypass_mode(adev);\n\t/* powerdown spll */\n\tsi_spll_powerdown(adev);\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\t/* reset */\n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t/* wait for asic to come out of reset */\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t/* enable BM */\n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic int si_asic_supports_baco(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\nstatic enum amd_reset_method\nsi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_PCI)\n\t\treturn amdgpu_reset_method;\n\telse if (amdgpu_reset_method != AMD_RESET_METHOD_LEGACY &&\n\t\t amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t amdgpu_reset_method);\n\n\treturn AMD_RESET_METHOD_LEGACY;\n}\n\nstatic int si_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tswitch (si_asic_reset_method(adev)) {\n\tcase AMD_RESET_METHOD_PCI:\n\t\tdev_info(adev->dev, \"PCI reset\\n\");\n\t\tr = amdgpu_device_pci_reset(adev);\n\t\tbreak;\n\tdefault:\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = si_gpu_pci_config_reset(adev);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic u32 si_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic void si_vga_set_state(struct amdgpu_device *adev, bool state)\n{\n\tuint32_t temp;\n\n\ttemp = RREG32(CONFIG_CNTL);\n\tif (!state) {\n\t\ttemp &= ~(1<<0);\n\t\ttemp |= (1<<1);\n\t} else {\n\t\ttemp &= ~(1<<1);\n\t}\n\tWREG32(CONFIG_CNTL, temp);\n}\n\nstatic u32 si_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL_2);\n\tif (tmp & MUX_TCLK_TO_XCLK)\n\t\treturn TCLK;\n\n\ttmp = RREG32(CG_CLKPIN_CNTL);\n\tif (tmp & XTALIN_DIVIDE)\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\nstatic void si_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void si_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool si_need_full_reset(struct amdgpu_device *adev)\n{\n\t/* change this when we support soft reset */\n\treturn true;\n}\n\nstatic bool si_need_reset_on_init(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n\nstatic int si_get_pcie_lanes(struct amdgpu_device *adev)\n{\n\tu32 link_width_cntl;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\n\tswitch ((link_width_cntl & LC_LINK_WIDTH_RD_MASK) >> LC_LINK_WIDTH_RD_SHIFT) {\n\tcase LC_LINK_WIDTH_X1:\n\t\treturn 1;\n\tcase LC_LINK_WIDTH_X2:\n\t\treturn 2;\n\tcase LC_LINK_WIDTH_X4:\n\t\treturn 4;\n\tcase LC_LINK_WIDTH_X8:\n\t\treturn 8;\n\tcase LC_LINK_WIDTH_X0:\n\tcase LC_LINK_WIDTH_X16:\n\tdefault:\n\t\treturn 16;\n\t}\n}\n\nstatic void si_set_pcie_lanes(struct amdgpu_device *adev, int lanes)\n{\n\tu32 link_width_cntl, mask;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tswitch (lanes) {\n\tcase 0:\n\t\tmask = LC_LINK_WIDTH_X0;\n\t\tbreak;\n\tcase 1:\n\t\tmask = LC_LINK_WIDTH_X1;\n\t\tbreak;\n\tcase 2:\n\t\tmask = LC_LINK_WIDTH_X2;\n\t\tbreak;\n\tcase 4:\n\t\tmask = LC_LINK_WIDTH_X4;\n\t\tbreak;\n\tcase 8:\n\t\tmask = LC_LINK_WIDTH_X8;\n\t\tbreak;\n\tcase 16:\n\t\tmask = LC_LINK_WIDTH_X16;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"invalid pcie lane request: %d\\n\", lanes);\n\t\treturn;\n\t}\n\n\tlink_width_cntl = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\tlink_width_cntl &= ~LC_LINK_WIDTH_MASK;\n\tlink_width_cntl |= mask << LC_LINK_WIDTH_SHIFT;\n\tlink_width_cntl |= (LC_RECONFIG_NOW |\n\t\t\t    LC_RECONFIG_ARC_MISSING_ESCAPE);\n\n\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, link_width_cntl);\n}\n\nstatic void si_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t/* This reports 0 on APUs, so return to avoid writing/reading registers\n\t * that may or may not be different from their GPU counterparts\n\t */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t/* Set the 2 events that we wish to watch, defined above */\n\t/* Reg 40 is # received msgs, Reg 104 is # of posted requests sent */\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t/* Write to enable desired perf counters */\n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t/* Zero out and enable the perf counters\n\t * Write 0x5:\n\t * Bit 0 = Start all counters(1)\n\t * Bit 2 = Global counter reset enable(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t/* Load the shadow and disable the perf counters\n\t * Write 0x2:\n\t * Bit 0 = Stop counters(0)\n\t * Bit 1 = Load the shadow counters(1)\n\t */\n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t/* Read register values to get any >32bit overflow */\n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t/* Get the values and add the overflow */\n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t si_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t/* Get the number of NAKs received and generated */\n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t/* Add the total number of NAKs, i.e the number of replays */\n\treturn (nak_r + nak_g);\n}\n\nstatic int si_uvd_send_upll_ctlreq(struct amdgpu_device *adev,\n\t\t\t\t   unsigned cg_upll_func_cntl)\n{\n\tunsigned i;\n\n\t/* Make sure UPLL_CTLREQ is deasserted */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32(cg_upll_func_cntl) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned si_uvd_calc_upll_post_div(unsigned vco_freq,\n\t\t\t\t\t  unsigned target_freq,\n\t\t\t\t\t  unsigned pd_min,\n\t\t\t\t\t  unsigned pd_even)\n{\n\tunsigned post_div = vco_freq / target_freq;\n\n\t/* Adjust to post divider minimum value */\n\tif (post_div < pd_min)\n\t\tpost_div = pd_min;\n\n\t/* We alway need a frequency less than or equal the target */\n\tif ((vco_freq / post_div) > target_freq)\n\t\tpost_div += 1;\n\n\t/* Post dividers above a certain value must be even */\n\tif (post_div > pd_even && post_div % 2)\n\t\tpost_div += 1;\n\n\treturn post_div;\n}\n\n/**\n * si_calc_upll_dividers - calc UPLL clock dividers\n *\n * @adev: amdgpu_device pointer\n * @vclk: wanted VCLK\n * @dclk: wanted DCLK\n * @vco_min: minimum VCO frequency\n * @vco_max: maximum VCO frequency\n * @fb_factor: factor to multiply vco freq with\n * @fb_mask: limit and bitmask for feedback divider\n * @pd_min: post divider minimum\n * @pd_max: post divider maximum\n * @pd_even: post divider must be even above this value\n * @optimal_fb_div: resulting feedback divider\n * @optimal_vclk_div: resulting vclk post divider\n * @optimal_dclk_div: resulting dclk post divider\n *\n * Calculate dividers for UVDs UPLL (except APUs).\n * Returns zero on success; -EINVAL on error.\n */\nstatic int si_calc_upll_dividers(struct amdgpu_device *adev,\n\t\t\t\t unsigned vclk, unsigned dclk,\n\t\t\t\t unsigned vco_min, unsigned vco_max,\n\t\t\t\t unsigned fb_factor, unsigned fb_mask,\n\t\t\t\t unsigned pd_min, unsigned pd_max,\n\t\t\t\t unsigned pd_even,\n\t\t\t\t unsigned *optimal_fb_div,\n\t\t\t\t unsigned *optimal_vclk_div,\n\t\t\t\t unsigned *optimal_dclk_div)\n{\n\tunsigned vco_freq, ref_freq = adev->clock.spll.reference_freq;\n\n\t/* Start off with something large */\n\tunsigned optimal_score = ~0;\n\n\t/* Loop through vco from low to high */\n\tvco_min = max(max(vco_min, vclk), dclk);\n\tfor (vco_freq = vco_min; vco_freq <= vco_max; vco_freq += 100) {\n\t\tuint64_t fb_div = (uint64_t)vco_freq * fb_factor;\n\t\tunsigned vclk_div, dclk_div, score;\n\n\t\tdo_div(fb_div, ref_freq);\n\n\t\t/* fb div out of range ? */\n\t\tif (fb_div > fb_mask)\n\t\t\tbreak; /* It can oly get worse */\n\n\t\tfb_div &= fb_mask;\n\n\t\t/* Calc vclk divider with current vco freq */\n\t\tvclk_div = si_uvd_calc_upll_post_div(vco_freq, vclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (vclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc dclk divider with current vco freq */\n\t\tdclk_div = si_uvd_calc_upll_post_div(vco_freq, dclk,\n\t\t\t\t\t\t     pd_min, pd_even);\n\t\tif (dclk_div > pd_max)\n\t\t\tbreak; /* vco is too big, it has to stop */\n\n\t\t/* Calc score with current vco freq */\n\t\tscore = vclk - (vco_freq / vclk_div) + dclk - (vco_freq / dclk_div);\n\n\t\t/* Determine if this vco setting is better than current optimal settings */\n\t\tif (score < optimal_score) {\n\t\t\t*optimal_fb_div = fb_div;\n\t\t\t*optimal_vclk_div = vclk_div;\n\t\t\t*optimal_dclk_div = dclk_div;\n\t\t\toptimal_score = score;\n\t\t\tif (optimal_score == 0)\n\t\t\t\tbreak; /* It can't get better than this */\n\t\t}\n\t}\n\n\t/* Did we found a valid setup ? */\n\tif (optimal_score == ~0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int si_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tunsigned fb_div = 0, vclk_div = 0, dclk_div = 0;\n\tint r;\n\n\t/* Bypass vclk and dclk with bclk */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(1) | DCLK_SRC_SEL(1),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_BYPASS_EN_MASK, ~UPLL_BYPASS_EN_MASK);\n\n\tif (!vclk || !dclk) {\n\t\t/* Keep the Bypass mode */\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, vclk, dclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &vclk_div, &dclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_VCO_MODE_MASK, ~UPLL_VCO_MODE_MASK);\n\n\t/* Disable sleep mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_SLEEP_MASK);\n\n\t/* Deassert UPLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert UPLL_RESET again */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_RESET_MASK, ~UPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_P(CG_UPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_3, UPLL_FB_DIV(fb_div), ~UPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_REF_DIV_MASK);\n\n\tif (fb_div < 307200)\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4, 0, ~UPLL_SPARE_ISPARE9);\n\telse\n\t\tWREG32_P(CG_UPLL_FUNC_CNTL_4,\n\t\t\t UPLL_SPARE_ISPARE9,\n\t\t\t ~UPLL_SPARE_ISPARE9);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t UPLL_PDIV_A(vclk_div) | UPLL_PDIV_B(dclk_div),\n\t\t ~(UPLL_PDIV_A_MASK | UPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_BYPASS_EN_MASK);\n\n\tr = si_uvd_send_upll_ctlreq(adev, CG_UPLL_FUNC_CNTL);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_P(CG_UPLL_FUNC_CNTL_2,\n\t\t VCLK_SRC_SEL(2) | DCLK_SRC_SEL(2),\n\t\t ~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic int si_vce_send_vcepll_ctlreq(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t/* Make sure VCEPLL_CTLREQ is deasserted */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t/* Assert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t/* Wait for CTLACK and CTLACK2 to get asserted */\n\tfor (i = 0; i < SI_MAX_CTLACKS_ASSERTION_WAIT; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\n\t\tif ((RREG32_SMC(CG_VCEPLL_FUNC_CNTL) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t/* Deassert UPLL_CTLREQ */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == SI_MAX_CTLACKS_ASSERTION_WAIT) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tunsigned fb_div = 0, evclk_div = 0, ecclk_div = 0;\n\tint r;\n\n\t/* Bypass evclk and ecclk with bclk */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(1) | ECCLK_SRC_SEL(1),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\t/* Put PLL in bypass mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_BYPASS_EN_MASK,\n\t\t     ~VCEPLL_BYPASS_EN_MASK);\n\n\tif (!evclk || !ecclk) {\n\t\t/* Keep the Bypass mode, put PLL to sleep */\n\t\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t\t     ~VCEPLL_SLEEP_MASK);\n\t\treturn 0;\n\t}\n\n\tr = si_calc_upll_dividers(adev, evclk, ecclk, 125000, 250000,\n\t\t\t\t  16384, 0x03FFFFFF, 0, 128, 5,\n\t\t\t\t  &fb_div, &evclk_div, &ecclk_div);\n\tif (r)\n\t\treturn r;\n\n\t/* Set RESET_ANTI_MUX to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\n\n\t/* Set VCO_MODE to 1 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_VCO_MODE_MASK,\n\t\t     ~VCEPLL_VCO_MODE_MASK);\n\n\t/* Toggle VCEPLL_SLEEP to 1 then back to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\n\t\t     ~VCEPLL_SLEEP_MASK);\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_SLEEP_MASK);\n\n\t/* Deassert VCEPLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(1);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Assert VCEPLL_RESET again */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_RESET_MASK, ~VCEPLL_RESET_MASK);\n\n\t/* Disable spread spectrum. */\n\tWREG32_SMC_P(CG_VCEPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\n\n\t/* Set feedback divider */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_3,\n\t\t     VCEPLL_FB_DIV(fb_div),\n\t\t     ~VCEPLL_FB_DIV_MASK);\n\n\t/* Set ref divider to 0 */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_REF_DIV_MASK);\n\n\t/* Set PDIV_A and PDIV_B */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     VCEPLL_PDIV_A(evclk_div) | VCEPLL_PDIV_B(ecclk_div),\n\t\t     ~(VCEPLL_PDIV_A_MASK | VCEPLL_PDIV_B_MASK));\n\n\t/* Give the PLL some time to settle */\n\tmdelay(15);\n\n\t/* Deassert PLL_RESET */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\n\n\tmdelay(15);\n\n\t/* Switch from bypass mode to normal mode */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_BYPASS_EN_MASK);\n\n\tr = si_vce_send_vcepll_ctlreq(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Switch VCLK and DCLK selection */\n\tWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\n\t\t     EVCLK_SRC_SEL(16) | ECCLK_SRC_SEL(16),\n\t\t     ~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\n\n\tmdelay(100);\n\n\treturn 0;\n}\n\nstatic void si_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs si_asic_funcs =\n{\n\t.read_disabled_bios = &si_read_disabled_bios,\n\t.read_bios_from_rom = &si_read_bios_from_rom,\n\t.read_register = &si_read_register,\n\t.reset = &si_asic_reset,\n\t.reset_method = &si_asic_reset_method,\n\t.set_vga_state = &si_vga_set_state,\n\t.get_xclk = &si_get_xclk,\n\t.set_uvd_clocks = &si_set_uvd_clocks,\n\t.set_vce_clocks = &si_set_vce_clocks,\n\t.get_pcie_lanes = &si_get_pcie_lanes,\n\t.set_pcie_lanes = &si_set_pcie_lanes,\n\t.get_config_memsize = &si_get_config_memsize,\n\t.flush_hdp = &si_flush_hdp,\n\t.invalidate_hdp = &si_invalidate_hdp,\n\t.need_full_reset = &si_need_full_reset,\n\t.get_pcie_usage = &si_get_pcie_usage,\n\t.need_reset_on_init = &si_need_reset_on_init,\n\t.get_pcie_replay_count = &si_get_pcie_replay_count,\n\t.supports_baco = &si_asic_supports_baco,\n\t.pre_asic_init = &si_pre_asic_init,\n\t.query_video_codecs = &si_query_video_codecs,\n};\n\nstatic uint32_t si_get_rev_id(struct amdgpu_device *adev)\n{\n\treturn (RREG32(CC_DRM_ID_STRAPS) & CC_DRM_ID_STRAPS__ATI_REV_ID_MASK)\n\t\t>> CC_DRM_ID_STRAPS__ATI_REV_ID__SHIFT;\n}\n\nstatic int si_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->smc_rreg = &si_smc_rreg;\n\tadev->smc_wreg = &si_smc_wreg;\n\tadev->pcie_rreg = &si_pcie_rreg;\n\tadev->pcie_wreg = &si_pcie_wreg;\n\tadev->pciep_rreg = &si_pciep_rreg;\n\tadev->pciep_wreg = &si_pciep_wreg;\n\tadev->uvd_ctx_rreg = si_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = si_uvd_ctx_wreg;\n\tadev->didt_rreg = NULL;\n\tadev->didt_wreg = NULL;\n\n\tadev->asic_funcs = &si_asic_funcs;\n\n\tadev->rev_id = si_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = (adev->rev_id == 0) ? 1 :\n\t\t\t\t\t(adev->rev_id == 1) ? 5 : 6;\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 20;\n\t\tbreak;\n\n\tcase CHIP_VERDE:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\t//???\n\t\tadev->external_rev_id = adev->rev_id + 40;\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 60;\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tadev->cg_flags =\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\t/*AMD_CG_SUPPORT_GFX_CGCG |*/\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 70;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int si_common_sw_init(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\n\nstatic void si_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttahiti_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(tahiti_golden_registers2));\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tpitcairn_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(pitcairn_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_VERDE:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tverde_pg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(verde_pg_init));\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_golden_rlc_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_golden_rlc_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\toland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(oland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_golden_registers2,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_golden_registers2));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\thainan_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(hainan_mgcg_cgcg_init));\n\t\tbreak;\n\n\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void si_pcie_gen3_enable(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *root = adev->pdev->bus->self;\n\tu32 speed_cntl, current_data_rate;\n\tint i;\n\tu16 tmp16;\n\n\tif (pci_is_root_bus(adev->pdev->bus))\n\t\treturn;\n\n\tif (amdgpu_pcie_gen2 == 0)\n\t\treturn;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\tif (!(adev->pm.pcie_gen_mask & (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\tCAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)))\n\t\treturn;\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tcurrent_data_rate = (speed_cntl & LC_CURRENT_DATA_RATE_MASK) >>\n\t\tLC_CURRENT_DATA_RATE_SHIFT;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate == 2) {\n\t\t\tDRM_INFO(\"PCIE gen 3 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 3 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t} else if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2) {\n\t\tif (current_data_rate == 1) {\n\t\t\tDRM_INFO(\"PCIE gen 2 link speeds already enabled\\n\");\n\t\t\treturn;\n\t\t}\n\t\tDRM_INFO(\"enabling PCIE gen 2 link speeds, disable with amdgpu.pcie_gen2=0\\n\");\n\t}\n\n\tif (!pci_is_pcie(root) || !pci_is_pcie(adev->pdev))\n\t\treturn;\n\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3) {\n\t\tif (current_data_rate != 2) {\n\t\t\tu16 bridge_cfg, gpu_cfg;\n\t\t\tu16 bridge_cfg2, gpu_cfg2;\n\t\t\tu32 max_lw, current_lw, tmp;\n\n\t\t\tpcie_capability_set_word(root, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\t\t\tpcie_capability_set_word(adev->pdev, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_HAWD);\n\n\t\t\ttmp = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tmax_lw = (tmp & LC_DETECTED_LINK_WIDTH_MASK) >> LC_DETECTED_LINK_WIDTH_SHIFT;\n\t\t\tcurrent_lw = (tmp & LC_OPERATING_LINK_WIDTH_MASK) >> LC_OPERATING_LINK_WIDTH_SHIFT;\n\n\t\t\tif (current_lw < max_lw) {\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\t\tif (tmp & LC_RENEGOTIATION_SUPPORT) {\n\t\t\t\t\ttmp &= ~(LC_LINK_WIDTH_MASK | LC_UPCONFIGURE_DIS);\n\t\t\t\t\ttmp |= (max_lw << LC_LINK_WIDTH_SHIFT);\n\t\t\t\t\ttmp |= LC_UPCONFIGURE_SUPPORT | LC_RENEGOTIATE_EN | LC_RECONFIG_NOW;\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, tmp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (i = 0; i < 10; i++) {\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_DEVSTA,\n\t\t\t\t\t\t\t  &tmp16);\n\t\t\t\tif (tmp16 & PCI_EXP_DEVSTA_TRPND)\n\t\t\t\t\tbreak;\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &bridge_cfg);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t  &gpu_cfg);\n\n\t\t\t\tpcie_capability_read_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &bridge_cfg2);\n\t\t\t\tpcie_capability_read_word(adev->pdev,\n\t\t\t\t\t\t\t  PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t  &gpu_cfg2);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp |= LC_REDO_EQ;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\n\t\t\t\tmdelay(100);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   bridge_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD,\n\t\t\t\t\t\t\t\t   gpu_cfg &\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL_HAWD);\n\n\t\t\t\tpcie_capability_clear_and_set_word(root, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   bridge_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\t\t\t\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t   PCI_EXP_LNKCTL2_TX_MARGIN,\n\t\t\t\t\t\t\t\t   gpu_cfg2 &\n\t\t\t\t\t\t\t\t   (PCI_EXP_LNKCTL2_ENTER_COMP |\n\t\t\t\t\t\t\t\t    PCI_EXP_LNKCTL2_TX_MARGIN));\n\n\t\t\t\ttmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\n\t\t\t\ttmp &= ~LC_SET_QUIESCE;\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\n\t\t\t}\n\t\t}\n\t}\n\n\tspeed_cntl |= LC_FORCE_EN_SW_SPEED_CHANGE | LC_FORCE_DIS_HW_SPEED_CHANGE;\n\tspeed_cntl &= ~LC_FORCE_DIS_SW_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\ttmp16 = 0;\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_8_0GT; /* gen3 */\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2)\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_5_0GT; /* gen2 */\n\telse\n\t\ttmp16 |= PCI_EXP_LNKCTL2_TLS_2_5GT; /* gen1 */\n\tpcie_capability_clear_and_set_word(adev->pdev, PCI_EXP_LNKCTL2,\n\t\t\t\t\t   PCI_EXP_LNKCTL2_TLS, tmp16);\n\n\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\tspeed_cntl |= LC_INITIATE_LINK_SPEED_CHANGE;\n\tWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\n\t\tif ((speed_cntl & LC_INITIATE_LINK_SPEED_CHANGE) == 0)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n}\n\nstatic inline u32 si_pif_phy0_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY0_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy0_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY0_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY0_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic inline u32 si_pif_phy1_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tr = RREG32(EVERGREEN_PIF_PHY1_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic inline void si_pif_phy1_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(EVERGREEN_PIF_PHY1_INDEX, ((reg) & 0xffff));\n\tWREG32(EVERGREEN_PIF_PHY1_DATA, (v));\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\nstatic void si_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\tbool disable_l0s = false, disable_l1 = false, disable_plloff_in_l1 = false;\n\tbool disable_clkreq = false;\n\n\tif (!amdgpu_device_should_use_aspm(adev))\n\t\treturn;\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\tdata &= ~LC_XMIT_N_FTS_MASK;\n\tdata |= LC_XMIT_N_FTS(0x24) | LC_XMIT_N_FTS_OVERRIDE_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL3);\n\tdata |= LC_GO_TO_RECOVERY;\n\tif (orig != data)\n\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(PCIE_P_CNTL);\n\tdata |= P_IGNORE_EDB_ERR;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_P_CNTL, data);\n\n\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\tdata &= ~(LC_L0S_INACTIVITY_MASK | LC_L1_INACTIVITY_MASK);\n\tdata |= LC_PMI_TO_L1_DIS;\n\tif (!disable_l0s)\n\t\tdata |= LC_L0S_INACTIVITY(7);\n\n\tif (!disable_l1) {\n\t\tdata |= LC_L1_INACTIVITY(7);\n\t\tdata &= ~LC_PMI_TO_L1_DIS;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\n\t\tif (!disable_plloff_in_l1) {\n\t\t\tbool clk_req_support;\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\tdata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\n\t\t\tdata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\tif ((adev->asic_type != CHIP_OLAND) && (adev->asic_type != CHIP_HAINAN)) {\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_PWRDOWN_3, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_0);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_0_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_0, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_1);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_1_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_1, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_2);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_2_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_2, data);\n\n\t\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_PWRDOWN_3);\n\t\t\t\tdata &= ~PLL_RAMP_UP_TIME_3_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_PWRDOWN_3, data);\n\t\t\t}\n\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\n\t\t\tdata &= ~LC_DYN_LANES_PWR_STATE_MASK;\n\t\t\tdata |= LC_DYN_LANES_PWR_STATE(3);\n\t\t\tif (orig != data)\n\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, data);\n\n\t\t\torig = data = si_pif_phy0_rreg(adev,PB0_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy0_wreg(adev,PB0_PIF_CNTL, data);\n\n\t\t\torig = data = si_pif_phy1_rreg(adev,PB1_PIF_CNTL);\n\t\t\tdata &= ~LS2_EXIT_TIME_MASK;\n\t\t\tif ((adev->asic_type == CHIP_OLAND) || (adev->asic_type == CHIP_HAINAN))\n\t\t\t\tdata |= LS2_EXIT_TIME(5);\n\t\t\tif (orig != data)\n\t\t\t\tsi_pif_phy1_wreg(adev,PB1_PIF_CNTL, data);\n\n\t\t\tif (!disable_clkreq &&\n\t\t\t    !pci_is_root_bus(adev->pdev->bus)) {\n\t\t\t\tstruct pci_dev *root = adev->pdev->bus->self;\n\t\t\t\tu32 lnkcap;\n\n\t\t\t\tclk_req_support = false;\n\t\t\t\tpcie_capability_read_dword(root, PCI_EXP_LNKCAP, &lnkcap);\n\t\t\t\tif (lnkcap & PCI_EXP_LNKCAP_CLKPM)\n\t\t\t\t\tclk_req_support = true;\n\t\t\t} else {\n\t\t\t\tclk_req_support = false;\n\t\t\t}\n\n\t\t\tif (clk_req_support) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL2);\n\t\t\t\tdata |= LC_ALLOW_PDWN_IN_L1 | LC_ALLOW_PDWN_IN_L23;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL2, data);\n\n\t\t\t\torig = data = RREG32(THM_CLK_CNTL);\n\t\t\t\tdata &= ~(CMON_CLK_SEL_MASK | TMON_CLK_SEL_MASK);\n\t\t\t\tdata |= CMON_CLK_SEL(1) | TMON_CLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(THM_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(MISC_CLK_CNTL);\n\t\t\t\tdata &= ~(DEEP_SLEEP_CLK_SEL_MASK | ZCLK_SEL_MASK);\n\t\t\t\tdata |= DEEP_SLEEP_CLK_SEL(1) | ZCLK_SEL(1);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MISC_CLK_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL);\n\t\t\t\tdata &= ~BCLK_AS_XCLK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL, data);\n\n\t\t\t\torig = data = RREG32(CG_CLKPIN_CNTL_2);\n\t\t\t\tdata &= ~FORCE_BIF_REFCLK_EN;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(CG_CLKPIN_CNTL_2, data);\n\n\t\t\t\torig = data = RREG32(MPLL_BYPASSCLK_SEL);\n\t\t\t\tdata &= ~MPLL_CLKOUT_SEL_MASK;\n\t\t\t\tdata |= MPLL_CLKOUT_SEL(4);\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(MPLL_BYPASSCLK_SEL, data);\n\n\t\t\t\torig = data = RREG32(SPLL_CNTL_MODE);\n\t\t\t\tdata &= ~SPLL_REFCLK_SEL_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32(SPLL_CNTL_MODE, data);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t}\n\n\torig = data = RREG32_PCIE(PCIE_CNTL2);\n\tdata |= SLV_MEM_LS_EN | MST_MEM_LS_EN | REPLAY_MEM_LS_EN;\n\tif (orig != data)\n\t\tWREG32_PCIE(PCIE_CNTL2, data);\n\n\tif (!disable_l0s) {\n\t\tdata = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\n\t\tif((data & LC_N_FTS_MASK) == LC_N_FTS_MASK) {\n\t\t\tdata = RREG32_PCIE(PCIE_LC_STATUS1);\n\t\t\tif ((data & LC_REVERSE_XMIT) && (data & LC_REVERSE_RCVR)) {\n\t\t\t\torig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\n\t\t\t\tdata &= ~LC_L0S_INACTIVITY_MASK;\n\t\t\t\tif (orig != data)\n\t\t\t\t\tWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void si_fix_pci_max_read_req_size(struct amdgpu_device *adev)\n{\n\tint readrq;\n\tu16 v;\n\n\treadrq = pcie_get_readrq(adev->pdev);\n\tv = ffs(readrq) - 8;\n\tif ((v == 0) || (v == 6) || (v == 7))\n\t\tpcie_set_readrq(adev->pdev, 512);\n}\n\nstatic int si_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsi_fix_pci_max_read_req_size(adev);\n\tsi_init_golden_registers(adev);\n\tsi_pcie_gen3_enable(adev);\n\tsi_program_aspm(adev);\n\n\treturn 0;\n}\n\nstatic int si_common_hw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_fini(adev);\n}\n\nstatic int si_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_common_hw_init(adev);\n}\n\nstatic bool si_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int si_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_clockgating_state(void *handle,\n\t\t\t\t\t    enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int si_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs si_common_ip_funcs = {\n\t.name = \"si_common\",\n\t.early_init = si_common_early_init,\n\t.late_init = NULL,\n\t.sw_init = si_common_sw_init,\n\t.sw_fini = si_common_sw_fini,\n\t.hw_init = si_common_hw_init,\n\t.hw_fini = si_common_hw_fini,\n\t.suspend = si_common_suspend,\n\t.resume = si_common_resume,\n\t.is_idle = si_common_is_idle,\n\t.wait_for_idle = si_common_wait_for_idle,\n\t.soft_reset = si_common_soft_reset,\n\t.set_clockgating_state = si_common_set_clockgating_state,\n\t.set_powergating_state = si_common_set_powergating_state,\n\t.dump_ip_state = NULL,\n\t.print_ip_state = NULL,\n};\n\nstatic const struct amdgpu_ip_block_version si_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &si_common_ip_funcs,\n};\n\nint si_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC) && defined(CONFIG_DRM_AMD_DC_SI)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v6_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v3_1_ip_block);\n\t\t/* amdgpu_device_ip_block_add(adev, &vce_v1_0_ip_block); */\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tamdgpu_device_ip_block_add(adev, &si_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_dma_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &si_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn 0;\n}\n\n", "source_code_path": "si.c", "line_number": 1324, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/si.c b/drivers/gpu/drm/amd/amdgpu/si.c\nindex 8523547..c64b7ac 100644\n--- a/drivers/gpu/drm/amd/amdgpu/si.c\n+++ b/drivers/gpu/drm/amd/amdgpu/si.c\n@@ -1319,6 +1319,8 @@ static bool si_read_bios_from_rom(struct amdgpu_device *adev,\n\n        dw_ptr = (u32 *)bios;\n        length_dw = ALIGN(length_bytes, 4) / 4;\n+       if (length_dw * 4 > adev->bios_size)\n+               return false;\n        /* set rom index to 0 */\n        WREG32(mmROM_INDEX, 0);\n        for (i = 0; i < length_dw; i++)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: si.c\nLine: 1324\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t/* set rom index to 0 */\n\tWREG32(mmROM_INDEX, 0);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmROM_DATA);\n\n\treturn true;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\n**Issue:** Lack of Boundary Checks **Type:** BUFFER_OVERFLOW **File:** drivers/gpu/drm/amd/amdgpu/si.c **Line:** 1324\n\n**Description:**\nThe `si_read_bios_from_rom` function fails to enforce a safeguard against excessive data transfer, allowing the possibility of a buffer overflow that can compromise system security and lead to memory corruption.====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nThe function amdgpu_atombios_copy_swap uses fixed-size buffers src_tmp and dst_tmp of 5 u32 elements (20 bytes) each. However, it processes num_bytes of data without checking if this exceeds the buffer size. This can lead to a buffer overflow if num_bytes is greater than 20.\n\n", "diff_path": "amdgpu_atombios-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1625, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1625\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nThe function amdgpu_atombios_copy_swap uses fixed-size buffers src_tmp and dst_tmp of 5 u32 elements (20 bytes) each. However, it processes num_bytes of data without checking if this exceeds the buffer size. This can lead to a buffer overflow if num_bytes is greater than 20.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nA potential integer overflow has been detected in the amdgpu_atombios_copy_swap function. The ALIGN macro operation can produce a value that exceeds the maximum capacity of the u8 variable align_num_bytes.\n\n", "diff_path": "amdgpu_atombios-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1625, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1625\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nA potential integer overflow has been detected in the amdgpu_atombios_copy_swap function. The ALIGN macro operation can produce a value that exceeds the maximum capacity of the u8 variable align_num_bytes.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nThe amdgpu_atombios_copy_swap function is vulnerable to an integer overflow due to the assignment of the ALIGN macro result to the u8 variable align_num_bytes, which can lead to a value greater than 255 being stored in the variable.\n\n", "diff_path": "amdgpu_atombios-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1625, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1625\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nThe amdgpu_atombios_copy_swap function is vulnerable to an integer overflow due to the assignment of the ALIGN macro result to the u8 variable align_num_bytes, which can lead to a value greater than 255 being stored in the variable.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nA critical issue has been identified in the amdgpu_atombios_copy_swap function, where the ALIGN macro operation can produce an output that exceeds the maximum value that can be stored in the u8 variable align_num_bytes, potentially causing an integer overflow and resulting in unexpected behavior or data corruption.\n\n", "diff_path": "amdgpu_atombios-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1625, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..300e5d19a9e6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1625,6 +1625,10 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n        u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       if (align_num_bytes > sizeof(src_tmp)) {\n+        // Handle error: num_bytes too large\n+        return;\n+    }\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1625\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow\n\nCID: 1002\nSeverity: High\nType: Security\nCategory: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nFunction: amdgpu_atombios_copy_swap\nLine: 1627\n\nDescription:\nA critical issue has been identified in the amdgpu_atombios_copy_swap function, where the ALIGN macro operation can produce an output that exceeds the maximum value that can be stored in the u8 variable align_num_bytes, potentially causing an integer overflow and resulting in unexpected behavior or data corruption.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-3-bug.txt", "bug_report_text": "Integer Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nA potential integer overflow has been detected in the amdgpu_atombios_copy_swap function. The variable align_num_bytes, which is of type u8, is assigned the result of an ALIGN macro operation that can produce values greater than 255 for certain inputs of num_bytes.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n", "diff_path": "amdgpu_atombios-3-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1620, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1620\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nInteger Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nA potential integer overflow has been detected in the amdgpu_atombios_copy_swap function. The variable align_num_bytes, which is of type u8, is assigned the result of an ALIGN macro operation that can produce values greater than 255 for certain inputs of num_bytes.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-3-bug.txt", "bug_report_text": "Integer Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nThe amdgpu_atombios_copy_swap function is vulnerable to an integer overflow due to the assignment of the ALIGN macro result to the u8 variable align_num_bytes, which can exceed its maximum capacity.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n", "diff_path": "amdgpu_atombios-3-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1620, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1620\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nInteger Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nThe amdgpu_atombios_copy_swap function is vulnerable to an integer overflow due to the assignment of the ALIGN macro result to the u8 variable align_num_bytes, which can exceed its maximum capacity.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-3-bug.txt", "bug_report_text": "Integer Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nWe've found a potential integer overflow issue in the amdgpu_atombios_copy_swap function. When the ALIGN macro is applied to num_bytes, the result can be too large to fit in the u8 variable align_num_bytes, which can cause problems.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n", "diff_path": "amdgpu_atombios-3-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1620, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1620\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nInteger Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nWe've found a potential integer overflow issue in the amdgpu_atombios_copy_swap function. When the ALIGN macro is applied to num_bytes, the result can be too large to fit in the u8 variable align_num_bytes, which can cause problems.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-3-bug.txt", "bug_report_text": "Integer Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nA critical integer overflow vulnerability has been identified in the amdgpu_atombios_copy_swap function, where the ALIGN macro operation can produce an output that exceeds the maximum value that can be stored in the u8 variable align_num_bytes, potentially causing an integer overflow and resulting in unexpected behavior or data corruption.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n", "diff_path": "amdgpu_atombios-3-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 1620, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..2bca21d8d043 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -1624,7 +1624,7 @@ void amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n #ifdef __BIG_ENDIAN\n        u32 src_tmp[5], dst_tmp[5];\n        int i;\n-       u8 align_num_bytes = ALIGN(num_bytes, 4);\n+       u32 align_num_bytes = ALIGN(num_bytes, 4);\n\n        if (to_le) {\n                memcpy(src_tmp, src, num_bytes);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 1620\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n====================================\nRead the following bug report.\n============ Bug Report ============\nInteger Overflow in amdgpu_atombios_copy_swap\nCID: 2002\n\nSeverity: High\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 1627\nLocation: amdgpu_atombios_copy_swap function\nDescription\nA critical integer overflow vulnerability has been identified in the amdgpu_atombios_copy_swap function, where the ALIGN macro operation can produce an output that exceeds the maximum value that can be stored in the u8 variable align_num_bytes, potentially causing an integer overflow and resulting in unexpected behavior or data corruption.\n\nu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_bios-bug.txt", "bug_report_text": "Type: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nAn integer overflow vulnerability has been detected in the function amdgpu_read_bios_from_rom. The issue occurs when calculating the length of the BIOS to be allocated. The function uses the AMD_VBIOS_LENGTH macro to determine the initial length, then aligns it to 4 bytes using the ALIGN macro. This sequence of operations could potentially lead to an integer overflow, resulting in a smaller allocation than intended and possible buffer overflow when reading the BIOS.\n", "diff_path": "amdgpu_bios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n\n#include \"amdgpu.h\"\n#include \"atom.h\"\n\n#include <linux/device.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n/*\n * BIOS.\n */\n\n#define AMD_VBIOS_SIGNATURE \" 761295520\"\n#define AMD_VBIOS_SIGNATURE_OFFSET 0x30\n#define AMD_VBIOS_SIGNATURE_SIZE sizeof(AMD_VBIOS_SIGNATURE)\n#define AMD_VBIOS_SIGNATURE_END (AMD_VBIOS_SIGNATURE_OFFSET + AMD_VBIOS_SIGNATURE_SIZE)\n#define AMD_IS_VALID_VBIOS(p) ((p)[0] == 0x55 && (p)[1] == 0xAA)\n#define AMD_VBIOS_LENGTH(p) ((p)[2] << 9)\n\n/* Check if current bios is an ATOM BIOS.\n * Return true if it is ATOM BIOS. Otherwise, return false.\n */\nstatic bool check_atom_bios(uint8_t *bios, size_t size)\n{\n\tuint16_t tmp, bios_header_start;\n\n\tif (!bios || size < 0x49) {\n\t\tDRM_INFO(\"vbios mem is null or mem size is wrong\\n\");\n\t\treturn false;\n\t}\n\n\tif (!AMD_IS_VALID_VBIOS(bios)) {\n\t\tDRM_INFO(\"BIOS signature incorrect %x %x\\n\", bios[0], bios[1]);\n\t\treturn false;\n\t}\n\n\tbios_header_start = bios[0x48] | (bios[0x49] << 8);\n\tif (!bios_header_start) {\n\t\tDRM_INFO(\"Can't locate bios header\\n\");\n\t\treturn false;\n\t}\n\n\ttmp = bios_header_start + 4;\n\tif (size < tmp) {\n\t\tDRM_INFO(\"BIOS header is broken\\n\");\n\t\treturn false;\n\t}\n\n\tif (!memcmp(bios + tmp, \"ATOM\", 4) ||\n\t    !memcmp(bios + tmp, \"MOTA\", 4)) {\n\t\tDRM_DEBUG(\"ATOMBIOS detected\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* If you boot an IGP board with a discrete card as the primary,\n * the IGP rom is not accessible via the rom bar as the IGP rom is\n * part of the system bios.  On boot, the system bios puts a\n * copy of the igp rom at the start of vram if a discrete card is\n * present.\n */\nstatic bool igp_read_bios_from_vram(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tresource_size_t vram_base;\n\tresource_size_t size = 256 * 1024; /* ??? */\n\n\tif (!(adev->flags & AMD_IS_APU))\n\t\tif (amdgpu_device_need_post(adev))\n\t\t\treturn false;\n\n\t/* FB BAR not enabled */\n\tif (pci_resource_len(adev->pdev, 0) == 0)\n\t\treturn false;\n\n\tadev->bios = NULL;\n\tvram_base = pci_resource_start(adev->pdev, 0);\n\tbios = ioremap_wc(vram_base, size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tiounmap(bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool amdgpu_read_bios(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tsize_t size;\n\n\tadev->bios = NULL;\n\t/* XXX: some cards may return 0 for rom size? ddx has a workaround */\n\tbios = pci_map_rom(adev->pdev, &size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kzalloc(size, GFP_KERNEL);\n\tif (adev->bios == NULL) {\n\t\tpci_unmap_rom(adev->pdev, bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tpci_unmap_rom(adev->pdev, bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n{\n\tu8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n\tint len;\n\n\tif (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n\t\treturn false;\n\n\t/* validate VBIOS signature */\n\tif (amdgpu_asic_read_bios_from_rom(adev, &header[0], sizeof(header)) == false)\n\t\treturn false;\n\theader[AMD_VBIOS_SIGNATURE_END] = 0;\n\n\tif ((!AMD_IS_VALID_VBIOS(header)) ||\n\t\tmemcmp((char *)&header[AMD_VBIOS_SIGNATURE_OFFSET],\n\t\t       AMD_VBIOS_SIGNATURE,\n\t\t       strlen(AMD_VBIOS_SIGNATURE)) != 0)\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n\tadev->bios_size = len;\n\n\t/* read complete BIOS */\n\tamdgpu_asic_read_bios_from_rom(adev, adev->bios, len);\n\n\tif (!check_atom_bios(adev->bios, len)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_platform_bios(struct amdgpu_device *adev)\n{\n\tphys_addr_t rom = adev->pdev->rom;\n\tsize_t romlen = adev->pdev->romlen;\n\tvoid __iomem *bios;\n\n\tadev->bios = NULL;\n\n\tif (!rom || romlen == 0)\n\t\treturn false;\n\n\tadev->bios = kzalloc(romlen, GFP_KERNEL);\n\tif (!adev->bios)\n\t\treturn false;\n\n\tbios = ioremap(rom, romlen);\n\tif (!bios)\n\t\tgoto free_bios;\n\n\tmemcpy_fromio(adev->bios, bios, romlen);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, romlen))\n\t\tgoto free_bios;\n\n\tadev->bios_size = romlen;\n\n\treturn true;\nfree_bios:\n\tkfree(adev->bios);\n\treturn false;\n}\n\n#ifdef CONFIG_ACPI\n/* ATRM is used to get the BIOS on the discrete cards in\n * dual-gpu systems.\n */\n/* retrieve the ROM in 4k blocks */\n#define ATRM_BIOS_PAGE 4096\n/**\n * amdgpu_atrm_call - fetch a chunk of the vbios\n *\n * @atrm_handle: acpi ATRM handle\n * @bios: vbios image pointer\n * @offset: offset of vbios image data to fetch\n * @len: length of vbios image data to fetch\n *\n * Executes ATRM to fetch a chunk of the discrete\n * vbios image on PX systems (all asics).\n * Returns the length of the buffer fetched.\n */\nstatic int amdgpu_atrm_call(acpi_handle atrm_handle, uint8_t *bios,\n\t\t\t    int offset, int len)\n{\n\tacpi_status status;\n\tunion acpi_object atrm_arg_elements[2], *obj;\n\tstruct acpi_object_list atrm_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL};\n\n\tatrm_arg.count = 2;\n\tatrm_arg.pointer = &atrm_arg_elements[0];\n\n\tatrm_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[0].integer.value = offset;\n\n\tatrm_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[1].integer.value = len;\n\n\tstatus = acpi_evaluate_object(atrm_handle, NULL, &atrm_arg, &buffer);\n\tif (ACPI_FAILURE(status)) {\n\t\tDRM_ERROR(\"failed to evaluate ATRM got %s\\n\", acpi_format_exception(status));\n\t\treturn -ENODEV;\n\t}\n\n\tobj = (union acpi_object *)buffer.pointer;\n\tmemcpy(bios+offset, obj->buffer.pointer, obj->buffer.length);\n\tlen = obj->buffer.length;\n\tkfree(buffer.pointer);\n\treturn len;\n}\n\nstatic bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\tint ret;\n\tint size = 256 * 1024;\n\tint i;\n\tstruct pci_dev *pdev = NULL;\n\tacpi_handle dhandle, atrm_handle;\n\tacpi_status status;\n\tbool found = false;\n\n\t/* ATRM is for the discrete card only */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t/* ATRM is for on-platform devices only */\n\tif (dev_is_removable(&adev->pdev->dev))\n\t\treturn false;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tdhandle = ACPI_HANDLE(&pdev->dev);\n\t\tif (!dhandle)\n\t\t\tcontinue;\n\n\t\tstatus = acpi_get_handle(dhandle, \"ATRM\", &atrm_handle);\n\t\tif (ACPI_SUCCESS(status)) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found)\n\t\treturn false;\n\tpci_dev_put(pdev);\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tdev_err(adev->dev, \"Unable to allocate bios\\n\");\n\t\treturn false;\n\t}\n\n\tfor (i = 0; i < size / ATRM_BIOS_PAGE; i++) {\n\t\tret = amdgpu_atrm_call(atrm_handle,\n\t\t\t\t       adev->bios,\n\t\t\t\t       (i * ATRM_BIOS_PAGE),\n\t\t\t\t       ATRM_BIOS_PAGE);\n\t\tif (ret < ATRM_BIOS_PAGE)\n\t\t\tbreak;\n\t}\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\treturn true;\n}\n#else\nstatic inline bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nstatic bool amdgpu_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn igp_read_bios_from_vram(adev);\n\telse\n\t\treturn (!adev->asic_funcs || !adev->asic_funcs->read_disabled_bios) ?\n\t\t\tfalse : amdgpu_asic_read_disabled_bios(adev);\n}\n\n#ifdef CONFIG_ACPI\nstatic bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\tstruct acpi_table_header *hdr;\n\tacpi_size tbl_size;\n\tUEFI_ACPI_VFCT *vfct;\n\tunsigned int offset;\n\n\tif (!ACPI_SUCCESS(acpi_get_table(\"VFCT\", 1, &hdr)))\n\t\treturn false;\n\ttbl_size = hdr->length;\n\tif (tbl_size < sizeof(UEFI_ACPI_VFCT)) {\n\t\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #1),skipping\\n\");\n\t\treturn false;\n\t}\n\n\tvfct = (UEFI_ACPI_VFCT *)hdr;\n\toffset = vfct->VBIOSImageOffset;\n\n\twhile (offset < tbl_size) {\n\t\tGOP_VBIOS_CONTENT *vbios = (GOP_VBIOS_CONTENT *)((char *)hdr + offset);\n\t\tVFCT_IMAGE_HEADER *vhdr = &vbios->VbiosHeader;\n\n\t\toffset += sizeof(VFCT_IMAGE_HEADER);\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image header truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\toffset += vhdr->ImageLength;\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (vhdr->ImageLength &&\n\t\t    vhdr->PCIBus == adev->pdev->bus->number &&\n\t\t    vhdr->PCIDevice == PCI_SLOT(adev->pdev->devfn) &&\n\t\t    vhdr->PCIFunction == PCI_FUNC(adev->pdev->devfn) &&\n\t\t    vhdr->VendorID == adev->pdev->vendor &&\n\t\t    vhdr->DeviceID == adev->pdev->device) {\n\t\t\tadev->bios = kmemdup(&vbios->VbiosContent,\n\t\t\t\t\t     vhdr->ImageLength,\n\t\t\t\t\t     GFP_KERNEL);\n\n\t\t\tif (!check_atom_bios(adev->bios, vhdr->ImageLength)) {\n\t\t\t\tkfree(adev->bios);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tadev->bios_size = vhdr->ImageLength;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #2),skipping\\n\");\n\treturn false;\n}\n#else\nstatic inline bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nbool amdgpu_get_bios(struct amdgpu_device *adev)\n{\n\tif (amdgpu_atrm_get_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ATRM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_acpi_vfct_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VFCT\\n\");\n\t\tgoto success;\n\t}\n\n\tif (igp_read_bios_from_vram(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VRAM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios_from_rom(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_disabled_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from disabled ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_platform_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from platform\\n\");\n\t\tgoto success;\n\t}\n\n\tdev_err(adev->dev, \"Unable to locate a BIOS ROM\\n\");\n\treturn false;\n\nsuccess:\n\tadev->is_atom_fw = adev->asic_type >= CHIP_VEGA10;\n\treturn true;\n}\n\n/* helper function for soc15 and onwards to read bios from rom */\nbool amdgpu_soc15_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t     u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\tu32 rom_offset;\n\tu32 rom_index_offset;\n\tu32 rom_data_offset;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\tif (!adev->smuio.funcs ||\n\t    !adev->smuio.funcs->get_rom_index_offset ||\n\t    !adev->smuio.funcs->get_rom_data_offset)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\n\trom_index_offset =\n\t\tadev->smuio.funcs->get_rom_index_offset(adev);\n\trom_data_offset =\n\t\tadev->smuio.funcs->get_rom_data_offset(adev);\n\n\tif (adev->nbio.funcs &&\n\t    adev->nbio.funcs->get_rom_offset) {\n\t\trom_offset = adev->nbio.funcs->get_rom_offset(adev);\n\t\trom_offset = rom_offset << 17;\n\t} else {\n\t\trom_offset = 0;\n\t}\n\n\t/* set rom index to rom_offset */\n\tWREG32(rom_index_offset, rom_offset);\n\t/* read out the rom data */\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(rom_data_offset);\n\n\treturn true;\n}\n", "source_code_path": "amdgpu_bios.c", "line_number": 177, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs ||!adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_bios.c\nLine: 177\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nAn integer overflow vulnerability has been detected in the function amdgpu_read_bios_from_rom. The issue occurs when calculating the length of the BIOS to be allocated. The function uses the AMD_VBIOS_LENGTH macro to determine the initial length, then aligns it to 4 bytes using the ALIGN macro. This sequence of operations could potentially lead to an integer overflow, resulting in a smaller allocation than intended and possible buffer overflow when reading the BIOS.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_bios-bug.txt", "bug_report_text": "Type: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nAn integer overflow vulnerability has been identified in the amdgpu_read_bios_from_rom function. The calculation of the BIOS length using the AMD_VBIOS_LENGTH macro and ALIGN macro can produce an overflow, resulting in an incorrect allocation size.\n", "diff_path": "amdgpu_bios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n\n#include \"amdgpu.h\"\n#include \"atom.h\"\n\n#include <linux/device.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n/*\n * BIOS.\n */\n\n#define AMD_VBIOS_SIGNATURE \" 761295520\"\n#define AMD_VBIOS_SIGNATURE_OFFSET 0x30\n#define AMD_VBIOS_SIGNATURE_SIZE sizeof(AMD_VBIOS_SIGNATURE)\n#define AMD_VBIOS_SIGNATURE_END (AMD_VBIOS_SIGNATURE_OFFSET + AMD_VBIOS_SIGNATURE_SIZE)\n#define AMD_IS_VALID_VBIOS(p) ((p)[0] == 0x55 && (p)[1] == 0xAA)\n#define AMD_VBIOS_LENGTH(p) ((p)[2] << 9)\n\n/* Check if current bios is an ATOM BIOS.\n * Return true if it is ATOM BIOS. Otherwise, return false.\n */\nstatic bool check_atom_bios(uint8_t *bios, size_t size)\n{\n\tuint16_t tmp, bios_header_start;\n\n\tif (!bios || size < 0x49) {\n\t\tDRM_INFO(\"vbios mem is null or mem size is wrong\\n\");\n\t\treturn false;\n\t}\n\n\tif (!AMD_IS_VALID_VBIOS(bios)) {\n\t\tDRM_INFO(\"BIOS signature incorrect %x %x\\n\", bios[0], bios[1]);\n\t\treturn false;\n\t}\n\n\tbios_header_start = bios[0x48] | (bios[0x49] << 8);\n\tif (!bios_header_start) {\n\t\tDRM_INFO(\"Can't locate bios header\\n\");\n\t\treturn false;\n\t}\n\n\ttmp = bios_header_start + 4;\n\tif (size < tmp) {\n\t\tDRM_INFO(\"BIOS header is broken\\n\");\n\t\treturn false;\n\t}\n\n\tif (!memcmp(bios + tmp, \"ATOM\", 4) ||\n\t    !memcmp(bios + tmp, \"MOTA\", 4)) {\n\t\tDRM_DEBUG(\"ATOMBIOS detected\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* If you boot an IGP board with a discrete card as the primary,\n * the IGP rom is not accessible via the rom bar as the IGP rom is\n * part of the system bios.  On boot, the system bios puts a\n * copy of the igp rom at the start of vram if a discrete card is\n * present.\n */\nstatic bool igp_read_bios_from_vram(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tresource_size_t vram_base;\n\tresource_size_t size = 256 * 1024; /* ??? */\n\n\tif (!(adev->flags & AMD_IS_APU))\n\t\tif (amdgpu_device_need_post(adev))\n\t\t\treturn false;\n\n\t/* FB BAR not enabled */\n\tif (pci_resource_len(adev->pdev, 0) == 0)\n\t\treturn false;\n\n\tadev->bios = NULL;\n\tvram_base = pci_resource_start(adev->pdev, 0);\n\tbios = ioremap_wc(vram_base, size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tiounmap(bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool amdgpu_read_bios(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tsize_t size;\n\n\tadev->bios = NULL;\n\t/* XXX: some cards may return 0 for rom size? ddx has a workaround */\n\tbios = pci_map_rom(adev->pdev, &size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kzalloc(size, GFP_KERNEL);\n\tif (adev->bios == NULL) {\n\t\tpci_unmap_rom(adev->pdev, bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tpci_unmap_rom(adev->pdev, bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n{\n\tu8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n\tint len;\n\n\tif (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n\t\treturn false;\n\n\t/* validate VBIOS signature */\n\tif (amdgpu_asic_read_bios_from_rom(adev, &header[0], sizeof(header)) == false)\n\t\treturn false;\n\theader[AMD_VBIOS_SIGNATURE_END] = 0;\n\n\tif ((!AMD_IS_VALID_VBIOS(header)) ||\n\t\tmemcmp((char *)&header[AMD_VBIOS_SIGNATURE_OFFSET],\n\t\t       AMD_VBIOS_SIGNATURE,\n\t\t       strlen(AMD_VBIOS_SIGNATURE)) != 0)\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n\tadev->bios_size = len;\n\n\t/* read complete BIOS */\n\tamdgpu_asic_read_bios_from_rom(adev, adev->bios, len);\n\n\tif (!check_atom_bios(adev->bios, len)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_platform_bios(struct amdgpu_device *adev)\n{\n\tphys_addr_t rom = adev->pdev->rom;\n\tsize_t romlen = adev->pdev->romlen;\n\tvoid __iomem *bios;\n\n\tadev->bios = NULL;\n\n\tif (!rom || romlen == 0)\n\t\treturn false;\n\n\tadev->bios = kzalloc(romlen, GFP_KERNEL);\n\tif (!adev->bios)\n\t\treturn false;\n\n\tbios = ioremap(rom, romlen);\n\tif (!bios)\n\t\tgoto free_bios;\n\n\tmemcpy_fromio(adev->bios, bios, romlen);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, romlen))\n\t\tgoto free_bios;\n\n\tadev->bios_size = romlen;\n\n\treturn true;\nfree_bios:\n\tkfree(adev->bios);\n\treturn false;\n}\n\n#ifdef CONFIG_ACPI\n/* ATRM is used to get the BIOS on the discrete cards in\n * dual-gpu systems.\n */\n/* retrieve the ROM in 4k blocks */\n#define ATRM_BIOS_PAGE 4096\n/**\n * amdgpu_atrm_call - fetch a chunk of the vbios\n *\n * @atrm_handle: acpi ATRM handle\n * @bios: vbios image pointer\n * @offset: offset of vbios image data to fetch\n * @len: length of vbios image data to fetch\n *\n * Executes ATRM to fetch a chunk of the discrete\n * vbios image on PX systems (all asics).\n * Returns the length of the buffer fetched.\n */\nstatic int amdgpu_atrm_call(acpi_handle atrm_handle, uint8_t *bios,\n\t\t\t    int offset, int len)\n{\n\tacpi_status status;\n\tunion acpi_object atrm_arg_elements[2], *obj;\n\tstruct acpi_object_list atrm_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL};\n\n\tatrm_arg.count = 2;\n\tatrm_arg.pointer = &atrm_arg_elements[0];\n\n\tatrm_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[0].integer.value = offset;\n\n\tatrm_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[1].integer.value = len;\n\n\tstatus = acpi_evaluate_object(atrm_handle, NULL, &atrm_arg, &buffer);\n\tif (ACPI_FAILURE(status)) {\n\t\tDRM_ERROR(\"failed to evaluate ATRM got %s\\n\", acpi_format_exception(status));\n\t\treturn -ENODEV;\n\t}\n\n\tobj = (union acpi_object *)buffer.pointer;\n\tmemcpy(bios+offset, obj->buffer.pointer, obj->buffer.length);\n\tlen = obj->buffer.length;\n\tkfree(buffer.pointer);\n\treturn len;\n}\n\nstatic bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\tint ret;\n\tint size = 256 * 1024;\n\tint i;\n\tstruct pci_dev *pdev = NULL;\n\tacpi_handle dhandle, atrm_handle;\n\tacpi_status status;\n\tbool found = false;\n\n\t/* ATRM is for the discrete card only */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t/* ATRM is for on-platform devices only */\n\tif (dev_is_removable(&adev->pdev->dev))\n\t\treturn false;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tdhandle = ACPI_HANDLE(&pdev->dev);\n\t\tif (!dhandle)\n\t\t\tcontinue;\n\n\t\tstatus = acpi_get_handle(dhandle, \"ATRM\", &atrm_handle);\n\t\tif (ACPI_SUCCESS(status)) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found)\n\t\treturn false;\n\tpci_dev_put(pdev);\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tdev_err(adev->dev, \"Unable to allocate bios\\n\");\n\t\treturn false;\n\t}\n\n\tfor (i = 0; i < size / ATRM_BIOS_PAGE; i++) {\n\t\tret = amdgpu_atrm_call(atrm_handle,\n\t\t\t\t       adev->bios,\n\t\t\t\t       (i * ATRM_BIOS_PAGE),\n\t\t\t\t       ATRM_BIOS_PAGE);\n\t\tif (ret < ATRM_BIOS_PAGE)\n\t\t\tbreak;\n\t}\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\treturn true;\n}\n#else\nstatic inline bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nstatic bool amdgpu_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn igp_read_bios_from_vram(adev);\n\telse\n\t\treturn (!adev->asic_funcs || !adev->asic_funcs->read_disabled_bios) ?\n\t\t\tfalse : amdgpu_asic_read_disabled_bios(adev);\n}\n\n#ifdef CONFIG_ACPI\nstatic bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\tstruct acpi_table_header *hdr;\n\tacpi_size tbl_size;\n\tUEFI_ACPI_VFCT *vfct;\n\tunsigned int offset;\n\n\tif (!ACPI_SUCCESS(acpi_get_table(\"VFCT\", 1, &hdr)))\n\t\treturn false;\n\ttbl_size = hdr->length;\n\tif (tbl_size < sizeof(UEFI_ACPI_VFCT)) {\n\t\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #1),skipping\\n\");\n\t\treturn false;\n\t}\n\n\tvfct = (UEFI_ACPI_VFCT *)hdr;\n\toffset = vfct->VBIOSImageOffset;\n\n\twhile (offset < tbl_size) {\n\t\tGOP_VBIOS_CONTENT *vbios = (GOP_VBIOS_CONTENT *)((char *)hdr + offset);\n\t\tVFCT_IMAGE_HEADER *vhdr = &vbios->VbiosHeader;\n\n\t\toffset += sizeof(VFCT_IMAGE_HEADER);\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image header truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\toffset += vhdr->ImageLength;\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (vhdr->ImageLength &&\n\t\t    vhdr->PCIBus == adev->pdev->bus->number &&\n\t\t    vhdr->PCIDevice == PCI_SLOT(adev->pdev->devfn) &&\n\t\t    vhdr->PCIFunction == PCI_FUNC(adev->pdev->devfn) &&\n\t\t    vhdr->VendorID == adev->pdev->vendor &&\n\t\t    vhdr->DeviceID == adev->pdev->device) {\n\t\t\tadev->bios = kmemdup(&vbios->VbiosContent,\n\t\t\t\t\t     vhdr->ImageLength,\n\t\t\t\t\t     GFP_KERNEL);\n\n\t\t\tif (!check_atom_bios(adev->bios, vhdr->ImageLength)) {\n\t\t\t\tkfree(adev->bios);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tadev->bios_size = vhdr->ImageLength;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #2),skipping\\n\");\n\treturn false;\n}\n#else\nstatic inline bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nbool amdgpu_get_bios(struct amdgpu_device *adev)\n{\n\tif (amdgpu_atrm_get_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ATRM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_acpi_vfct_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VFCT\\n\");\n\t\tgoto success;\n\t}\n\n\tif (igp_read_bios_from_vram(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VRAM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios_from_rom(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_disabled_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from disabled ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_platform_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from platform\\n\");\n\t\tgoto success;\n\t}\n\n\tdev_err(adev->dev, \"Unable to locate a BIOS ROM\\n\");\n\treturn false;\n\nsuccess:\n\tadev->is_atom_fw = adev->asic_type >= CHIP_VEGA10;\n\treturn true;\n}\n\n/* helper function for soc15 and onwards to read bios from rom */\nbool amdgpu_soc15_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t     u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\tu32 rom_offset;\n\tu32 rom_index_offset;\n\tu32 rom_data_offset;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\tif (!adev->smuio.funcs ||\n\t    !adev->smuio.funcs->get_rom_index_offset ||\n\t    !adev->smuio.funcs->get_rom_data_offset)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\n\trom_index_offset =\n\t\tadev->smuio.funcs->get_rom_index_offset(adev);\n\trom_data_offset =\n\t\tadev->smuio.funcs->get_rom_data_offset(adev);\n\n\tif (adev->nbio.funcs &&\n\t    adev->nbio.funcs->get_rom_offset) {\n\t\trom_offset = adev->nbio.funcs->get_rom_offset(adev);\n\t\trom_offset = rom_offset << 17;\n\t} else {\n\t\trom_offset = 0;\n\t}\n\n\t/* set rom index to rom_offset */\n\tWREG32(rom_index_offset, rom_offset);\n\t/* read out the rom data */\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(rom_data_offset);\n\n\treturn true;\n}\n", "source_code_path": "amdgpu_bios.c", "line_number": 177, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs ||!adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_bios.c\nLine: 177\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nAn integer overflow vulnerability has been identified in the amdgpu_read_bios_from_rom function. The calculation of the BIOS length using the AMD_VBIOS_LENGTH macro and ALIGN macro can produce an overflow, resulting in an incorrect allocation size.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_bios-bug.txt", "bug_report_text": "Type: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nWe've found a potential integer overflow issue in the amdgpu_read_bios_from_rom function. When calculating the BIOS length, the combination of the AMD_VBIOS_LENGTH macro and ALIGN macro can lead to an overflow, causing the function to allocate a smaller buffer than intended, which may result in a buffer overflow when reading the BIOS.\n", "diff_path": "amdgpu_bios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n\n#include \"amdgpu.h\"\n#include \"atom.h\"\n\n#include <linux/device.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n/*\n * BIOS.\n */\n\n#define AMD_VBIOS_SIGNATURE \" 761295520\"\n#define AMD_VBIOS_SIGNATURE_OFFSET 0x30\n#define AMD_VBIOS_SIGNATURE_SIZE sizeof(AMD_VBIOS_SIGNATURE)\n#define AMD_VBIOS_SIGNATURE_END (AMD_VBIOS_SIGNATURE_OFFSET + AMD_VBIOS_SIGNATURE_SIZE)\n#define AMD_IS_VALID_VBIOS(p) ((p)[0] == 0x55 && (p)[1] == 0xAA)\n#define AMD_VBIOS_LENGTH(p) ((p)[2] << 9)\n\n/* Check if current bios is an ATOM BIOS.\n * Return true if it is ATOM BIOS. Otherwise, return false.\n */\nstatic bool check_atom_bios(uint8_t *bios, size_t size)\n{\n\tuint16_t tmp, bios_header_start;\n\n\tif (!bios || size < 0x49) {\n\t\tDRM_INFO(\"vbios mem is null or mem size is wrong\\n\");\n\t\treturn false;\n\t}\n\n\tif (!AMD_IS_VALID_VBIOS(bios)) {\n\t\tDRM_INFO(\"BIOS signature incorrect %x %x\\n\", bios[0], bios[1]);\n\t\treturn false;\n\t}\n\n\tbios_header_start = bios[0x48] | (bios[0x49] << 8);\n\tif (!bios_header_start) {\n\t\tDRM_INFO(\"Can't locate bios header\\n\");\n\t\treturn false;\n\t}\n\n\ttmp = bios_header_start + 4;\n\tif (size < tmp) {\n\t\tDRM_INFO(\"BIOS header is broken\\n\");\n\t\treturn false;\n\t}\n\n\tif (!memcmp(bios + tmp, \"ATOM\", 4) ||\n\t    !memcmp(bios + tmp, \"MOTA\", 4)) {\n\t\tDRM_DEBUG(\"ATOMBIOS detected\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* If you boot an IGP board with a discrete card as the primary,\n * the IGP rom is not accessible via the rom bar as the IGP rom is\n * part of the system bios.  On boot, the system bios puts a\n * copy of the igp rom at the start of vram if a discrete card is\n * present.\n */\nstatic bool igp_read_bios_from_vram(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tresource_size_t vram_base;\n\tresource_size_t size = 256 * 1024; /* ??? */\n\n\tif (!(adev->flags & AMD_IS_APU))\n\t\tif (amdgpu_device_need_post(adev))\n\t\t\treturn false;\n\n\t/* FB BAR not enabled */\n\tif (pci_resource_len(adev->pdev, 0) == 0)\n\t\treturn false;\n\n\tadev->bios = NULL;\n\tvram_base = pci_resource_start(adev->pdev, 0);\n\tbios = ioremap_wc(vram_base, size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tiounmap(bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool amdgpu_read_bios(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tsize_t size;\n\n\tadev->bios = NULL;\n\t/* XXX: some cards may return 0 for rom size? ddx has a workaround */\n\tbios = pci_map_rom(adev->pdev, &size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kzalloc(size, GFP_KERNEL);\n\tif (adev->bios == NULL) {\n\t\tpci_unmap_rom(adev->pdev, bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tpci_unmap_rom(adev->pdev, bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n{\n\tu8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n\tint len;\n\n\tif (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n\t\treturn false;\n\n\t/* validate VBIOS signature */\n\tif (amdgpu_asic_read_bios_from_rom(adev, &header[0], sizeof(header)) == false)\n\t\treturn false;\n\theader[AMD_VBIOS_SIGNATURE_END] = 0;\n\n\tif ((!AMD_IS_VALID_VBIOS(header)) ||\n\t\tmemcmp((char *)&header[AMD_VBIOS_SIGNATURE_OFFSET],\n\t\t       AMD_VBIOS_SIGNATURE,\n\t\t       strlen(AMD_VBIOS_SIGNATURE)) != 0)\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n\tadev->bios_size = len;\n\n\t/* read complete BIOS */\n\tamdgpu_asic_read_bios_from_rom(adev, adev->bios, len);\n\n\tif (!check_atom_bios(adev->bios, len)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_platform_bios(struct amdgpu_device *adev)\n{\n\tphys_addr_t rom = adev->pdev->rom;\n\tsize_t romlen = adev->pdev->romlen;\n\tvoid __iomem *bios;\n\n\tadev->bios = NULL;\n\n\tif (!rom || romlen == 0)\n\t\treturn false;\n\n\tadev->bios = kzalloc(romlen, GFP_KERNEL);\n\tif (!adev->bios)\n\t\treturn false;\n\n\tbios = ioremap(rom, romlen);\n\tif (!bios)\n\t\tgoto free_bios;\n\n\tmemcpy_fromio(adev->bios, bios, romlen);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, romlen))\n\t\tgoto free_bios;\n\n\tadev->bios_size = romlen;\n\n\treturn true;\nfree_bios:\n\tkfree(adev->bios);\n\treturn false;\n}\n\n#ifdef CONFIG_ACPI\n/* ATRM is used to get the BIOS on the discrete cards in\n * dual-gpu systems.\n */\n/* retrieve the ROM in 4k blocks */\n#define ATRM_BIOS_PAGE 4096\n/**\n * amdgpu_atrm_call - fetch a chunk of the vbios\n *\n * @atrm_handle: acpi ATRM handle\n * @bios: vbios image pointer\n * @offset: offset of vbios image data to fetch\n * @len: length of vbios image data to fetch\n *\n * Executes ATRM to fetch a chunk of the discrete\n * vbios image on PX systems (all asics).\n * Returns the length of the buffer fetched.\n */\nstatic int amdgpu_atrm_call(acpi_handle atrm_handle, uint8_t *bios,\n\t\t\t    int offset, int len)\n{\n\tacpi_status status;\n\tunion acpi_object atrm_arg_elements[2], *obj;\n\tstruct acpi_object_list atrm_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL};\n\n\tatrm_arg.count = 2;\n\tatrm_arg.pointer = &atrm_arg_elements[0];\n\n\tatrm_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[0].integer.value = offset;\n\n\tatrm_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[1].integer.value = len;\n\n\tstatus = acpi_evaluate_object(atrm_handle, NULL, &atrm_arg, &buffer);\n\tif (ACPI_FAILURE(status)) {\n\t\tDRM_ERROR(\"failed to evaluate ATRM got %s\\n\", acpi_format_exception(status));\n\t\treturn -ENODEV;\n\t}\n\n\tobj = (union acpi_object *)buffer.pointer;\n\tmemcpy(bios+offset, obj->buffer.pointer, obj->buffer.length);\n\tlen = obj->buffer.length;\n\tkfree(buffer.pointer);\n\treturn len;\n}\n\nstatic bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\tint ret;\n\tint size = 256 * 1024;\n\tint i;\n\tstruct pci_dev *pdev = NULL;\n\tacpi_handle dhandle, atrm_handle;\n\tacpi_status status;\n\tbool found = false;\n\n\t/* ATRM is for the discrete card only */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t/* ATRM is for on-platform devices only */\n\tif (dev_is_removable(&adev->pdev->dev))\n\t\treturn false;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tdhandle = ACPI_HANDLE(&pdev->dev);\n\t\tif (!dhandle)\n\t\t\tcontinue;\n\n\t\tstatus = acpi_get_handle(dhandle, \"ATRM\", &atrm_handle);\n\t\tif (ACPI_SUCCESS(status)) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found)\n\t\treturn false;\n\tpci_dev_put(pdev);\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tdev_err(adev->dev, \"Unable to allocate bios\\n\");\n\t\treturn false;\n\t}\n\n\tfor (i = 0; i < size / ATRM_BIOS_PAGE; i++) {\n\t\tret = amdgpu_atrm_call(atrm_handle,\n\t\t\t\t       adev->bios,\n\t\t\t\t       (i * ATRM_BIOS_PAGE),\n\t\t\t\t       ATRM_BIOS_PAGE);\n\t\tif (ret < ATRM_BIOS_PAGE)\n\t\t\tbreak;\n\t}\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\treturn true;\n}\n#else\nstatic inline bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nstatic bool amdgpu_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn igp_read_bios_from_vram(adev);\n\telse\n\t\treturn (!adev->asic_funcs || !adev->asic_funcs->read_disabled_bios) ?\n\t\t\tfalse : amdgpu_asic_read_disabled_bios(adev);\n}\n\n#ifdef CONFIG_ACPI\nstatic bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\tstruct acpi_table_header *hdr;\n\tacpi_size tbl_size;\n\tUEFI_ACPI_VFCT *vfct;\n\tunsigned int offset;\n\n\tif (!ACPI_SUCCESS(acpi_get_table(\"VFCT\", 1, &hdr)))\n\t\treturn false;\n\ttbl_size = hdr->length;\n\tif (tbl_size < sizeof(UEFI_ACPI_VFCT)) {\n\t\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #1),skipping\\n\");\n\t\treturn false;\n\t}\n\n\tvfct = (UEFI_ACPI_VFCT *)hdr;\n\toffset = vfct->VBIOSImageOffset;\n\n\twhile (offset < tbl_size) {\n\t\tGOP_VBIOS_CONTENT *vbios = (GOP_VBIOS_CONTENT *)((char *)hdr + offset);\n\t\tVFCT_IMAGE_HEADER *vhdr = &vbios->VbiosHeader;\n\n\t\toffset += sizeof(VFCT_IMAGE_HEADER);\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image header truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\toffset += vhdr->ImageLength;\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (vhdr->ImageLength &&\n\t\t    vhdr->PCIBus == adev->pdev->bus->number &&\n\t\t    vhdr->PCIDevice == PCI_SLOT(adev->pdev->devfn) &&\n\t\t    vhdr->PCIFunction == PCI_FUNC(adev->pdev->devfn) &&\n\t\t    vhdr->VendorID == adev->pdev->vendor &&\n\t\t    vhdr->DeviceID == adev->pdev->device) {\n\t\t\tadev->bios = kmemdup(&vbios->VbiosContent,\n\t\t\t\t\t     vhdr->ImageLength,\n\t\t\t\t\t     GFP_KERNEL);\n\n\t\t\tif (!check_atom_bios(adev->bios, vhdr->ImageLength)) {\n\t\t\t\tkfree(adev->bios);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tadev->bios_size = vhdr->ImageLength;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #2),skipping\\n\");\n\treturn false;\n}\n#else\nstatic inline bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nbool amdgpu_get_bios(struct amdgpu_device *adev)\n{\n\tif (amdgpu_atrm_get_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ATRM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_acpi_vfct_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VFCT\\n\");\n\t\tgoto success;\n\t}\n\n\tif (igp_read_bios_from_vram(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VRAM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios_from_rom(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_disabled_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from disabled ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_platform_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from platform\\n\");\n\t\tgoto success;\n\t}\n\n\tdev_err(adev->dev, \"Unable to locate a BIOS ROM\\n\");\n\treturn false;\n\nsuccess:\n\tadev->is_atom_fw = adev->asic_type >= CHIP_VEGA10;\n\treturn true;\n}\n\n/* helper function for soc15 and onwards to read bios from rom */\nbool amdgpu_soc15_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t     u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\tu32 rom_offset;\n\tu32 rom_index_offset;\n\tu32 rom_data_offset;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\tif (!adev->smuio.funcs ||\n\t    !adev->smuio.funcs->get_rom_index_offset ||\n\t    !adev->smuio.funcs->get_rom_data_offset)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\n\trom_index_offset =\n\t\tadev->smuio.funcs->get_rom_index_offset(adev);\n\trom_data_offset =\n\t\tadev->smuio.funcs->get_rom_data_offset(adev);\n\n\tif (adev->nbio.funcs &&\n\t    adev->nbio.funcs->get_rom_offset) {\n\t\trom_offset = adev->nbio.funcs->get_rom_offset(adev);\n\t\trom_offset = rom_offset << 17;\n\t} else {\n\t\trom_offset = 0;\n\t}\n\n\t/* set rom index to rom_offset */\n\tWREG32(rom_index_offset, rom_offset);\n\t/* read out the rom data */\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(rom_data_offset);\n\n\treturn true;\n}\n", "source_code_path": "amdgpu_bios.c", "line_number": 177, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs ||!adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_bios.c\nLine: 177\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nWe've found a potential integer overflow issue in the amdgpu_read_bios_from_rom function. When calculating the BIOS length, the combination of the AMD_VBIOS_LENGTH macro and ALIGN macro can lead to an overflow, causing the function to allocate a smaller buffer than intended, which may result in a buffer overflow when reading the BIOS.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_bios-bug.txt", "bug_report_text": "Type: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nA critical integer overflow vulnerability has been identified in the amdgpu_read_bios_from_rom function, where the calculation of the BIOS length using the AMD_VBIOS_LENGTH macro and ALIGN macro can produce an output that exceeds the maximum value that can be stored in the allocated buffer, potentially resulting in a buffer overflow and data corruption when reading the BIOS.\n", "diff_path": "amdgpu_bios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n\n#include \"amdgpu.h\"\n#include \"atom.h\"\n\n#include <linux/device.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n/*\n * BIOS.\n */\n\n#define AMD_VBIOS_SIGNATURE \" 761295520\"\n#define AMD_VBIOS_SIGNATURE_OFFSET 0x30\n#define AMD_VBIOS_SIGNATURE_SIZE sizeof(AMD_VBIOS_SIGNATURE)\n#define AMD_VBIOS_SIGNATURE_END (AMD_VBIOS_SIGNATURE_OFFSET + AMD_VBIOS_SIGNATURE_SIZE)\n#define AMD_IS_VALID_VBIOS(p) ((p)[0] == 0x55 && (p)[1] == 0xAA)\n#define AMD_VBIOS_LENGTH(p) ((p)[2] << 9)\n\n/* Check if current bios is an ATOM BIOS.\n * Return true if it is ATOM BIOS. Otherwise, return false.\n */\nstatic bool check_atom_bios(uint8_t *bios, size_t size)\n{\n\tuint16_t tmp, bios_header_start;\n\n\tif (!bios || size < 0x49) {\n\t\tDRM_INFO(\"vbios mem is null or mem size is wrong\\n\");\n\t\treturn false;\n\t}\n\n\tif (!AMD_IS_VALID_VBIOS(bios)) {\n\t\tDRM_INFO(\"BIOS signature incorrect %x %x\\n\", bios[0], bios[1]);\n\t\treturn false;\n\t}\n\n\tbios_header_start = bios[0x48] | (bios[0x49] << 8);\n\tif (!bios_header_start) {\n\t\tDRM_INFO(\"Can't locate bios header\\n\");\n\t\treturn false;\n\t}\n\n\ttmp = bios_header_start + 4;\n\tif (size < tmp) {\n\t\tDRM_INFO(\"BIOS header is broken\\n\");\n\t\treturn false;\n\t}\n\n\tif (!memcmp(bios + tmp, \"ATOM\", 4) ||\n\t    !memcmp(bios + tmp, \"MOTA\", 4)) {\n\t\tDRM_DEBUG(\"ATOMBIOS detected\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* If you boot an IGP board with a discrete card as the primary,\n * the IGP rom is not accessible via the rom bar as the IGP rom is\n * part of the system bios.  On boot, the system bios puts a\n * copy of the igp rom at the start of vram if a discrete card is\n * present.\n */\nstatic bool igp_read_bios_from_vram(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tresource_size_t vram_base;\n\tresource_size_t size = 256 * 1024; /* ??? */\n\n\tif (!(adev->flags & AMD_IS_APU))\n\t\tif (amdgpu_device_need_post(adev))\n\t\t\treturn false;\n\n\t/* FB BAR not enabled */\n\tif (pci_resource_len(adev->pdev, 0) == 0)\n\t\treturn false;\n\n\tadev->bios = NULL;\n\tvram_base = pci_resource_start(adev->pdev, 0);\n\tbios = ioremap_wc(vram_base, size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tiounmap(bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool amdgpu_read_bios(struct amdgpu_device *adev)\n{\n\tuint8_t __iomem *bios;\n\tsize_t size;\n\n\tadev->bios = NULL;\n\t/* XXX: some cards may return 0 for rom size? ddx has a workaround */\n\tbios = pci_map_rom(adev->pdev, &size);\n\tif (!bios)\n\t\treturn false;\n\n\tadev->bios = kzalloc(size, GFP_KERNEL);\n\tif (adev->bios == NULL) {\n\t\tpci_unmap_rom(adev->pdev, bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\tmemcpy_fromio(adev->bios, bios, size);\n\tpci_unmap_rom(adev->pdev, bios);\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n{\n\tu8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n\tint len;\n\n\tif (!adev->asic_funcs || !adev->asic_funcs->read_bios_from_rom)\n\t\treturn false;\n\n\t/* validate VBIOS signature */\n\tif (amdgpu_asic_read_bios_from_rom(adev, &header[0], sizeof(header)) == false)\n\t\treturn false;\n\theader[AMD_VBIOS_SIGNATURE_END] = 0;\n\n\tif ((!AMD_IS_VALID_VBIOS(header)) ||\n\t\tmemcmp((char *)&header[AMD_VBIOS_SIGNATURE_OFFSET],\n\t\t       AMD_VBIOS_SIGNATURE,\n\t\t       strlen(AMD_VBIOS_SIGNATURE)) != 0)\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n\tadev->bios_size = len;\n\n\t/* read complete BIOS */\n\tamdgpu_asic_read_bios_from_rom(adev, adev->bios, len);\n\n\tif (!check_atom_bios(adev->bios, len)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool amdgpu_read_platform_bios(struct amdgpu_device *adev)\n{\n\tphys_addr_t rom = adev->pdev->rom;\n\tsize_t romlen = adev->pdev->romlen;\n\tvoid __iomem *bios;\n\n\tadev->bios = NULL;\n\n\tif (!rom || romlen == 0)\n\t\treturn false;\n\n\tadev->bios = kzalloc(romlen, GFP_KERNEL);\n\tif (!adev->bios)\n\t\treturn false;\n\n\tbios = ioremap(rom, romlen);\n\tif (!bios)\n\t\tgoto free_bios;\n\n\tmemcpy_fromio(adev->bios, bios, romlen);\n\tiounmap(bios);\n\n\tif (!check_atom_bios(adev->bios, romlen))\n\t\tgoto free_bios;\n\n\tadev->bios_size = romlen;\n\n\treturn true;\nfree_bios:\n\tkfree(adev->bios);\n\treturn false;\n}\n\n#ifdef CONFIG_ACPI\n/* ATRM is used to get the BIOS on the discrete cards in\n * dual-gpu systems.\n */\n/* retrieve the ROM in 4k blocks */\n#define ATRM_BIOS_PAGE 4096\n/**\n * amdgpu_atrm_call - fetch a chunk of the vbios\n *\n * @atrm_handle: acpi ATRM handle\n * @bios: vbios image pointer\n * @offset: offset of vbios image data to fetch\n * @len: length of vbios image data to fetch\n *\n * Executes ATRM to fetch a chunk of the discrete\n * vbios image on PX systems (all asics).\n * Returns the length of the buffer fetched.\n */\nstatic int amdgpu_atrm_call(acpi_handle atrm_handle, uint8_t *bios,\n\t\t\t    int offset, int len)\n{\n\tacpi_status status;\n\tunion acpi_object atrm_arg_elements[2], *obj;\n\tstruct acpi_object_list atrm_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL};\n\n\tatrm_arg.count = 2;\n\tatrm_arg.pointer = &atrm_arg_elements[0];\n\n\tatrm_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[0].integer.value = offset;\n\n\tatrm_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\tatrm_arg_elements[1].integer.value = len;\n\n\tstatus = acpi_evaluate_object(atrm_handle, NULL, &atrm_arg, &buffer);\n\tif (ACPI_FAILURE(status)) {\n\t\tDRM_ERROR(\"failed to evaluate ATRM got %s\\n\", acpi_format_exception(status));\n\t\treturn -ENODEV;\n\t}\n\n\tobj = (union acpi_object *)buffer.pointer;\n\tmemcpy(bios+offset, obj->buffer.pointer, obj->buffer.length);\n\tlen = obj->buffer.length;\n\tkfree(buffer.pointer);\n\treturn len;\n}\n\nstatic bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\tint ret;\n\tint size = 256 * 1024;\n\tint i;\n\tstruct pci_dev *pdev = NULL;\n\tacpi_handle dhandle, atrm_handle;\n\tacpi_status status;\n\tbool found = false;\n\n\t/* ATRM is for the discrete card only */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t/* ATRM is for on-platform devices only */\n\tif (dev_is_removable(&adev->pdev->dev))\n\t\treturn false;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tdhandle = ACPI_HANDLE(&pdev->dev);\n\t\tif (!dhandle)\n\t\t\tcontinue;\n\n\t\tstatus = acpi_get_handle(dhandle, \"ATRM\", &atrm_handle);\n\t\tif (ACPI_SUCCESS(status)) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found)\n\t\treturn false;\n\tpci_dev_put(pdev);\n\n\tadev->bios = kmalloc(size, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tdev_err(adev->dev, \"Unable to allocate bios\\n\");\n\t\treturn false;\n\t}\n\n\tfor (i = 0; i < size / ATRM_BIOS_PAGE; i++) {\n\t\tret = amdgpu_atrm_call(atrm_handle,\n\t\t\t\t       adev->bios,\n\t\t\t\t       (i * ATRM_BIOS_PAGE),\n\t\t\t\t       ATRM_BIOS_PAGE);\n\t\tif (ret < ATRM_BIOS_PAGE)\n\t\t\tbreak;\n\t}\n\n\tif (!check_atom_bios(adev->bios, size)) {\n\t\tkfree(adev->bios);\n\t\treturn false;\n\t}\n\tadev->bios_size = size;\n\treturn true;\n}\n#else\nstatic inline bool amdgpu_atrm_get_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nstatic bool amdgpu_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn igp_read_bios_from_vram(adev);\n\telse\n\t\treturn (!adev->asic_funcs || !adev->asic_funcs->read_disabled_bios) ?\n\t\t\tfalse : amdgpu_asic_read_disabled_bios(adev);\n}\n\n#ifdef CONFIG_ACPI\nstatic bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\tstruct acpi_table_header *hdr;\n\tacpi_size tbl_size;\n\tUEFI_ACPI_VFCT *vfct;\n\tunsigned int offset;\n\n\tif (!ACPI_SUCCESS(acpi_get_table(\"VFCT\", 1, &hdr)))\n\t\treturn false;\n\ttbl_size = hdr->length;\n\tif (tbl_size < sizeof(UEFI_ACPI_VFCT)) {\n\t\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #1),skipping\\n\");\n\t\treturn false;\n\t}\n\n\tvfct = (UEFI_ACPI_VFCT *)hdr;\n\toffset = vfct->VBIOSImageOffset;\n\n\twhile (offset < tbl_size) {\n\t\tGOP_VBIOS_CONTENT *vbios = (GOP_VBIOS_CONTENT *)((char *)hdr + offset);\n\t\tVFCT_IMAGE_HEADER *vhdr = &vbios->VbiosHeader;\n\n\t\toffset += sizeof(VFCT_IMAGE_HEADER);\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image header truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\toffset += vhdr->ImageLength;\n\t\tif (offset > tbl_size) {\n\t\t\tdev_info(adev->dev, \"ACPI VFCT image truncated,skipping\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (vhdr->ImageLength &&\n\t\t    vhdr->PCIBus == adev->pdev->bus->number &&\n\t\t    vhdr->PCIDevice == PCI_SLOT(adev->pdev->devfn) &&\n\t\t    vhdr->PCIFunction == PCI_FUNC(adev->pdev->devfn) &&\n\t\t    vhdr->VendorID == adev->pdev->vendor &&\n\t\t    vhdr->DeviceID == adev->pdev->device) {\n\t\t\tadev->bios = kmemdup(&vbios->VbiosContent,\n\t\t\t\t\t     vhdr->ImageLength,\n\t\t\t\t\t     GFP_KERNEL);\n\n\t\t\tif (!check_atom_bios(adev->bios, vhdr->ImageLength)) {\n\t\t\t\tkfree(adev->bios);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tadev->bios_size = vhdr->ImageLength;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tdev_info(adev->dev, \"ACPI VFCT table present but broken (too short #2),skipping\\n\");\n\treturn false;\n}\n#else\nstatic inline bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev)\n{\n\treturn false;\n}\n#endif\n\nbool amdgpu_get_bios(struct amdgpu_device *adev)\n{\n\tif (amdgpu_atrm_get_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ATRM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_acpi_vfct_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VFCT\\n\");\n\t\tgoto success;\n\t}\n\n\tif (igp_read_bios_from_vram(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from VRAM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_bios_from_rom(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from ROM\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_disabled_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from disabled ROM BAR\\n\");\n\t\tgoto success;\n\t}\n\n\tif (amdgpu_read_platform_bios(adev)) {\n\t\tdev_info(adev->dev, \"Fetched VBIOS from platform\\n\");\n\t\tgoto success;\n\t}\n\n\tdev_err(adev->dev, \"Unable to locate a BIOS ROM\\n\");\n\treturn false;\n\nsuccess:\n\tadev->is_atom_fw = adev->asic_type >= CHIP_VEGA10;\n\treturn true;\n}\n\n/* helper function for soc15 and onwards to read bios from rom */\nbool amdgpu_soc15_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t     u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tu32 i, length_dw;\n\tu32 rom_offset;\n\tu32 rom_index_offset;\n\tu32 rom_data_offset;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t/* APU vbios image is part of sbios image */\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\tif (!adev->smuio.funcs ||\n\t    !adev->smuio.funcs->get_rom_index_offset ||\n\t    !adev->smuio.funcs->get_rom_data_offset)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\n\trom_index_offset =\n\t\tadev->smuio.funcs->get_rom_index_offset(adev);\n\trom_data_offset =\n\t\tadev->smuio.funcs->get_rom_data_offset(adev);\n\n\tif (adev->nbio.funcs &&\n\t    adev->nbio.funcs->get_rom_offset) {\n\t\trom_offset = adev->nbio.funcs->get_rom_offset(adev);\n\t\trom_offset = rom_offset << 17;\n\t} else {\n\t\trom_offset = 0;\n\t}\n\n\t/* set rom index to rom_offset */\n\tWREG32(rom_index_offset, rom_offset);\n\t/* read out the rom data */\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(rom_data_offset);\n\n\treturn true;\n}\n", "source_code_path": "amdgpu_bios.c", "line_number": 177, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nindex 42e64bce661e..428f93943093 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\n@@ -156,7 +156,8 @@ bool amdgpu_read_bios(struct amdgpu_device *adev)\n static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n {\n        u8 header[AMD_VBIOS_SIGNATURE_END+1] = {0};\n-       int len;\n+       size_t len;\n+       size_t aligned_len;\n\n        if (!adev->asic_funcs ||!adev->asic_funcs->read_bios_from_rom)\n                return false;\n@@ -174,7 +175,13 @@ static bool amdgpu_read_bios_from_rom(struct amdgpu_device *adev)\n\n        /* valid vbios, go on */\n        len = AMD_VBIOS_LENGTH(header);\n-       len = ALIGN(len, 4);\n+       if (len == 0 || len > MAX_BIOS_SIZE) {\n+                       DRM_ERROR(\"Invalid BIOS length\\n\");\n+                       return false;\n+       }\n+       aligned_len = ALIGN(len, 4);\n+       if (aligned_len < len || aligned_len > MAX_BIOS_SIZE)\n+                       return false;\n        adev->bios = kmalloc(len, GFP_KERNEL);\n        if (!adev->bios) {\n                DRM_ERROR(\"no memory to allocate for BIOS\\n\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_bios.c\nLine: 177\n\t\treturn false;\n\n\t/* valid vbios, go on */\n\tlen = AMD_VBIOS_LENGTH(header);\n\tlen = ALIGN(len, 4);\n\tadev->bios = kmalloc(len, GFP_KERNEL);\n\tif (!adev->bios) {\n\t\tDRM_ERROR(\"no memory to allocate for BIOS\\n\");\n\t\treturn false;\n\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Integer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_bios.c\nFunction: amdgpu_read_bios_from_rom\nLine: 177\n\nDescription:\nA critical integer overflow vulnerability has been identified in the amdgpu_read_bios_from_rom function, where the calculation of the BIOS length using the AMD_VBIOS_LENGTH macro and ALIGN macro can produce an output that exceeds the maximum value that can be stored in the allocated buffer, potentially resulting in a buffer overflow and data corruption when reading the BIOS.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_acpi-bug.txt", "bug_report_text": "Type: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: Potential buffer overflow in memcpy() operation\n\nThere is a potential buffer overflow in the call to memcpy(). The size argument\nto memcpy() is derived from untrusted input (info->buffer.pointer) and may exceed\nthe size of the destination buffer.\n\n", "diff_path": "amdgpu_acpi-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger “stop”, “enable” and “start” at D0, Driver Operational.\n\t * SBIOS trigger “stop” at D3, Driver Not Operational.\n\t * SBIOS trigger “stop” and “disable” at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n", "source_code_path": "amdgpu_acpi.c", "line_number": 252, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_acpi.c\nLine: 252\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: Potential buffer overflow in memcpy() operation\n\nThere is a potential buffer overflow in the call to memcpy(). The size argument\nto memcpy() is derived from untrusted input (info->buffer.pointer) and may exceed\nthe size of the destination buffer.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_acpi-bug.txt", "bug_report_text": "Type: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: A potential buffer overflow vulnerability has been identified in the memcpy() operation. The size argument is derived from untrusted input, which may exceed the destination buffer size.\n\n", "diff_path": "amdgpu_acpi-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger “stop”, “enable” and “start” at D0, Driver Operational.\n\t * SBIOS trigger “stop” at D3, Driver Not Operational.\n\t * SBIOS trigger “stop” and “disable” at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n", "source_code_path": "amdgpu_acpi.c", "line_number": 252, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_acpi.c\nLine: 252\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: A potential buffer overflow vulnerability has been identified in the memcpy() operation. The size argument is derived from untrusted input, which may exceed the destination buffer size.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_acpi-bug.txt", "bug_report_text": "Type: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: We've found a potential buffer overflow issue in the memcpy() call. The size of the copy is determined by untrusted input, which could be larger than the destination buffer, leading to a buffer overflow.\n\n", "diff_path": "amdgpu_acpi-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger “stop”, “enable” and “start” at D0, Driver Operational.\n\t * SBIOS trigger “stop” at D3, Driver Not Operational.\n\t * SBIOS trigger “stop” and “disable” at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n", "source_code_path": "amdgpu_acpi.c", "line_number": 252, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_acpi.c\nLine: 252\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: We've found a potential buffer overflow issue in the memcpy() call. The size of the copy is determined by untrusted input, which could be larger than the destination buffer, leading to a buffer overflow.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_acpi-bug.txt", "bug_report_text": "Type: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: A critical buffer overflow vulnerability has been identified in the memcpy() operation, where the size argument is derived from untrusted input (info->buffer.pointer) and may exceed the size of the destination buffer, potentially leading to a buffer overflow and data corruption.\n\n", "diff_path": "amdgpu_acpi-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger “stop”, “enable” and “start” at D0, Driver Operational.\n\t * SBIOS trigger “stop” at D3, Driver Not Operational.\n\t * SBIOS trigger “stop” and “disable” at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n", "source_code_path": "amdgpu_acpi.c", "line_number": 252, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..e653e060510e 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -252,8 +252,11 @@ static int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n                goto out;\n        }\n        size = min(sizeof(output), size);\n-\n-       memcpy(&output, info->buffer.pointer, size);\n+    size_t safe_size = min(sizeof(output), size);\n+    if (safe_size > info->buffer.length) {\n+        return -EINVAL;\n+    }\n+       memcpy(&output, info->buffer.pointer, safe_size);\n\n        /* TODO: check version? */\n        DRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_acpi.c\nLine: 252\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: BUFFER_OVERFLOW\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_atif_verify_interface\nLine: 256\n\nDescription: A critical buffer overflow vulnerability has been identified in the memcpy() operation, where the size argument is derived from untrusted input (info->buffer.pointer) and may exceed the size of the destination buffer, potentially leading to a buffer overflow and data corruption.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "alderaban-bug.txt", "bug_report_text": "CID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nA null pointer dereference vulnerability exists in the function aldebaran_get_reset_handler, which fails to check if'reset_ctl' or'reset_ctl->handle' is null before accessing their contents. This can lead to a system crash or unexpected behavior if either pointer is null when the function is called.\n\n", "diff_path": "alderaban-diff.txt", "diff_text": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl || !reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n", "code": "/*\n * Copyright 2021 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include \"aldebaran.h\"\n#include \"amdgpu_reset.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dpm.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_psp.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic bool aldebaran_is_mode2_default(struct amdgpu_reset_control *reset_ctl)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tif ((amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t     adev->gmc.xgmi.connected_to_cpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct amdgpu_reset_handler *\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n\t\telse\n\t\t\treset_context->method = amdgpu_asic_reset_method(adev);\n\t}\n\n\tif (reset_context->method != AMD_RESET_METHOD_NONE) {\n\t\tdev_dbg(adev->dev, \"Getting reset handler for method %d\\n\",\n\t\t\treset_context->method);\n\t\tfor_each_handler(i, handler, reset_ctl) {\n\t\t\tif (handler->reset_method == reset_context->method)\n\t\t\t\treturn handler;\n\t\t}\n\t}\n\n\tdev_dbg(adev->dev, \"Reset handler not found!\\n\");\n\n\treturn NULL;\n}\n\nstatic int aldebaran_mode2_suspend_ip(struct amdgpu_device *adev)\n{\n\tint r, i;\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int\naldebaran_mode2_prepare_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tint r = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tdev_dbg(adev->dev, \"Aldebaran prepare hw context\\n\");\n\t/* Don't suspend on bare metal if we are not going to HW reset the ASIC */\n\tif (!amdgpu_sriov_vf(adev))\n\t\tr = aldebaran_mode2_suspend_ip(adev);\n\n\treturn r;\n}\n\nstatic void aldebaran_async_reset(struct work_struct *work)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_reset_control *reset_ctl =\n\t\tcontainer_of(work, struct amdgpu_reset_control, reset_work);\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tfor_each_handler(i, handler, reset_ctl)\t{\n\t\tif (handler->reset_method == reset_ctl->active_reset) {\n\t\t\tdev_dbg(adev->dev, \"Resetting device\\n\");\n\t\t\thandler->do_reset(adev);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int aldebaran_mode2_reset(struct amdgpu_device *adev)\n{\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\tadev->asic_reset_res = amdgpu_dpm_mode2_reset(adev);\n\treturn adev->asic_reset_res;\n}\n\nstatic int\naldebaran_mode2_perform_reset(struct amdgpu_reset_control *reset_ctl,\n\t\t\t      struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tint r = 0;\n\n\tdev_dbg(adev->dev, \"aldebaran perform hw reset\\n\");\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_lock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_MODE2;\n\t}\n\t/*\n\t * Mode2 reset doesn't need any sync between nodes in XGMI hive, instead launch\n\t * them together so that they can be completed asynchronously on multiple nodes\n\t */\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t/* For XGMI run all resets in parallel to speed up the process */\n\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\tif (!queue_work(system_unbound_wq,\n\t\t\t\t\t&tmp_adev->reset_cntl->reset_work))\n\t\t\t\tr = -EALREADY;\n\t\t} else\n\t\t\tr = aldebaran_mode2_reset(tmp_adev);\n\t\tif (r) {\n\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t\tr, adev_to_drm(tmp_adev)->unique);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* For XGMI wait for all resets to complete before proceed */\n\tif (!r) {\n\t\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\tflush_work(&tmp_adev->reset_cntl->reset_work);\n\t\t\t\tr = tmp_adev->asic_reset_res;\n\t\t\t\tif (r)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_unlock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_NONE;\n\t}\n\n\treturn r;\n}\n\nstatic int aldebaran_mode2_restore_ip(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_firmware_info *ucode_list[AMDGPU_UCODE_ID_MAXIMUM];\n\tstruct amdgpu_firmware_info *ucode;\n\tstruct amdgpu_ip_block *cmn_block;\n\tint ucode_count = 0;\n\tint i, r;\n\n\tdev_dbg(adev->dev, \"Reloading ucodes after reset\\n\");\n\tfor (i = 0; i < adev->firmware.max_ucodes; i++) {\n\t\tucode = &adev->firmware.ucode[i];\n\t\tif (!ucode->fw)\n\t\t\tcontinue;\n\t\tswitch (ucode->ucode_id) {\n\t\tcase AMDGPU_UCODE_ID_SDMA0:\n\t\tcase AMDGPU_UCODE_ID_SDMA1:\n\t\tcase AMDGPU_UCODE_ID_SDMA2:\n\t\tcase AMDGPU_UCODE_ID_SDMA3:\n\t\tcase AMDGPU_UCODE_ID_SDMA4:\n\t\tcase AMDGPU_UCODE_ID_SDMA5:\n\t\tcase AMDGPU_UCODE_ID_SDMA6:\n\t\tcase AMDGPU_UCODE_ID_SDMA7:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1_JT:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_G:\n\t\t\tucode_list[ucode_count++] = ucode;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Reinit NBIF block */\n\tcmn_block =\n\t\tamdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_COMMON);\n\tif (unlikely(!cmn_block)) {\n\t\tdev_err(adev->dev, \"Failed to get BIF handle\\n\");\n\t\treturn -EINVAL;\n\t}\n\tr = cmn_block->version->funcs->resume(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Reinit GFXHUB */\n\tadev->gfxhub.funcs->init(adev);\n\tr = adev->gfxhub.funcs->gart_enable(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFXHUB gart reenable failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Reload GFX firmware */\n\tr = psp_load_fw_list(&adev->psp, ucode_list, ucode_count);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFX ucode load failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Resume RLC, FW needs RLC alive to complete reset process */\n\tadev->gfx.rlc.funcs->resume(adev);\n\n\t/* Wait for FW reset event complete */\n\tr = amdgpu_dpm_wait_for_event(adev, SMU_EVENT_RESET_COMPLETE, 0);\n\tif (r) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Failed to get response from firmware after reset\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_COMMON))\n\t\t\tcontinue;\n\n\t\tif (adev->ip_blocks[i].version->funcs->late_init) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->late_init(\n\t\t\t\t(void *)adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\"late_init of IP block <%s> failed %d after reset\\n\",\n\t\t\t\t\tadev->ip_blocks[i].version->funcs->name,\n\t\t\t\t\tr);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tadev->ip_blocks[i].status.late_initialized = true;\n\t}\n\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_GATE);\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_GATE);\n\n\treturn r;\n}\n\nstatic int\naldebaran_mode2_restore_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tstruct amdgpu_ras *con;\n\tint r;\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(reset_context->reset_req_dev, MP1_HWIP, 0) ==\n\t\t    IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tdev_info(tmp_adev->dev,\n\t\t\t \"GPU reset succeeded, trying to resume\\n\");\n\t\tr = aldebaran_mode2_restore_ip(tmp_adev);\n\t\tif (r)\n\t\t\tgoto end;\n\n\t\t/*\n\t\t * Add this ASIC as tracked as reset was already\n\t\t * complete successfully.\n\t\t */\n\t\tamdgpu_register_gpu_instance(tmp_adev);\n\n\t\t/* Resume RAS, ecc_irq */\n\t\tcon = amdgpu_ras_get_context(tmp_adev);\n\t\tif (!amdgpu_sriov_vf(tmp_adev) && con) {\n\t\t\tif (tmp_adev->sdma.ras &&\n\t\t\t\ttmp_adev->sdma.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->sdma.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->sdma.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"SDMA failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (tmp_adev->gfx.ras &&\n\t\t\t\ttmp_adev->gfx.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->gfx.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->gfx.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"GFX failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_ras_resume(tmp_adev);\n\n\t\t/* Update PSP FW topology after reset */\n\t\tif (reset_context->hive &&\n\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\tr = amdgpu_xgmi_update_topology(reset_context->hive,\n\t\t\t\t\t\t\ttmp_adev);\n\n\t\tif (!r) {\n\t\t\tamdgpu_irq_gpu_reset_resume_helper(tmp_adev);\n\n\t\t\tr = amdgpu_ib_ring_tests(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\"ib ring test failed (%d).\\n\", r);\n\t\t\t\tr = -EAGAIN;\n\t\t\t\ttmp_adev->asic_reset_res = r;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\t}\n\nend:\n\treturn r;\n}\n\nstatic struct amdgpu_reset_handler aldebaran_mode2_handler = {\n\t.reset_method\t\t= AMD_RESET_METHOD_MODE2,\n\t.prepare_env\t\t= NULL,\n\t.prepare_hwcontext\t= aldebaran_mode2_prepare_hwcontext,\n\t.perform_reset\t\t= aldebaran_mode2_perform_reset,\n\t.restore_hwcontext\t= aldebaran_mode2_restore_hwcontext,\n\t.restore_env\t\t= NULL,\n\t.do_reset\t\t= aldebaran_mode2_reset,\n};\n\nstatic struct amdgpu_reset_handler\n\t*aldebaran_rst_handlers[AMDGPU_RESET_MAX_HANDLERS] = {\n\t\t&aldebaran_mode2_handler,\n\t};\n\nint aldebaran_reset_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_reset_control *reset_ctl;\n\n\treset_ctl = kzalloc(sizeof(*reset_ctl), GFP_KERNEL);\n\tif (!reset_ctl)\n\t\treturn -ENOMEM;\n\n\treset_ctl->handle = adev;\n\treset_ctl->async_reset = aldebaran_async_reset;\n\treset_ctl->active_reset = AMD_RESET_METHOD_NONE;\n\treset_ctl->get_reset_handler = aldebaran_get_reset_handler;\n\n\tINIT_WORK(&reset_ctl->reset_work, reset_ctl->async_reset);\n\t/* Only mode2 is handled through reset control now */\n\treset_ctl->reset_handlers = &aldebaran_rst_handlers;\n\n\tadev->reset_cntl = reset_ctl;\n\n\treturn 0;\n}\n\nint aldebaran_reset_fini(struct amdgpu_device *adev)\n{\n\tkfree(adev->reset_cntl);\n\tadev->reset_cntl = NULL;\n\treturn 0;\n}\n", "source_code_path": "aldebaran.c", "line_number": 50, "generated_diff": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl ||!reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: aldebaran.c\nLine: 50\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n====================================\nRead the following bug report.\n============ Bug Report ============\nCID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nA null pointer dereference vulnerability exists in the function aldebaran_get_reset_handler, which fails to check if'reset_ctl' or'reset_ctl->handle' is null before accessing their contents. This can lead to a system crash or unexpected behavior if either pointer is null when the function is called.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "alderaban-bug.txt", "bug_report_text": "CID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nThe function aldebaran_get_reset_handler neglects to perform a null check on the'reset_ctl' and'reset_ctl->handle' pointers before accessing their contents. This can lead to a null pointer dereference if either pointer is null when the function is called, resulting in potential system crashes or data corruption.\n\n", "diff_path": "alderaban-diff.txt", "diff_text": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl || !reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n", "code": "/*\n * Copyright 2021 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include \"aldebaran.h\"\n#include \"amdgpu_reset.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dpm.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_psp.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic bool aldebaran_is_mode2_default(struct amdgpu_reset_control *reset_ctl)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tif ((amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t     adev->gmc.xgmi.connected_to_cpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct amdgpu_reset_handler *\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n\t\telse\n\t\t\treset_context->method = amdgpu_asic_reset_method(adev);\n\t}\n\n\tif (reset_context->method != AMD_RESET_METHOD_NONE) {\n\t\tdev_dbg(adev->dev, \"Getting reset handler for method %d\\n\",\n\t\t\treset_context->method);\n\t\tfor_each_handler(i, handler, reset_ctl) {\n\t\t\tif (handler->reset_method == reset_context->method)\n\t\t\t\treturn handler;\n\t\t}\n\t}\n\n\tdev_dbg(adev->dev, \"Reset handler not found!\\n\");\n\n\treturn NULL;\n}\n\nstatic int aldebaran_mode2_suspend_ip(struct amdgpu_device *adev)\n{\n\tint r, i;\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int\naldebaran_mode2_prepare_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tint r = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tdev_dbg(adev->dev, \"Aldebaran prepare hw context\\n\");\n\t/* Don't suspend on bare metal if we are not going to HW reset the ASIC */\n\tif (!amdgpu_sriov_vf(adev))\n\t\tr = aldebaran_mode2_suspend_ip(adev);\n\n\treturn r;\n}\n\nstatic void aldebaran_async_reset(struct work_struct *work)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_reset_control *reset_ctl =\n\t\tcontainer_of(work, struct amdgpu_reset_control, reset_work);\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tfor_each_handler(i, handler, reset_ctl)\t{\n\t\tif (handler->reset_method == reset_ctl->active_reset) {\n\t\t\tdev_dbg(adev->dev, \"Resetting device\\n\");\n\t\t\thandler->do_reset(adev);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int aldebaran_mode2_reset(struct amdgpu_device *adev)\n{\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\tadev->asic_reset_res = amdgpu_dpm_mode2_reset(adev);\n\treturn adev->asic_reset_res;\n}\n\nstatic int\naldebaran_mode2_perform_reset(struct amdgpu_reset_control *reset_ctl,\n\t\t\t      struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tint r = 0;\n\n\tdev_dbg(adev->dev, \"aldebaran perform hw reset\\n\");\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_lock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_MODE2;\n\t}\n\t/*\n\t * Mode2 reset doesn't need any sync between nodes in XGMI hive, instead launch\n\t * them together so that they can be completed asynchronously on multiple nodes\n\t */\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t/* For XGMI run all resets in parallel to speed up the process */\n\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\tif (!queue_work(system_unbound_wq,\n\t\t\t\t\t&tmp_adev->reset_cntl->reset_work))\n\t\t\t\tr = -EALREADY;\n\t\t} else\n\t\t\tr = aldebaran_mode2_reset(tmp_adev);\n\t\tif (r) {\n\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t\tr, adev_to_drm(tmp_adev)->unique);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* For XGMI wait for all resets to complete before proceed */\n\tif (!r) {\n\t\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\tflush_work(&tmp_adev->reset_cntl->reset_work);\n\t\t\t\tr = tmp_adev->asic_reset_res;\n\t\t\t\tif (r)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_unlock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_NONE;\n\t}\n\n\treturn r;\n}\n\nstatic int aldebaran_mode2_restore_ip(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_firmware_info *ucode_list[AMDGPU_UCODE_ID_MAXIMUM];\n\tstruct amdgpu_firmware_info *ucode;\n\tstruct amdgpu_ip_block *cmn_block;\n\tint ucode_count = 0;\n\tint i, r;\n\n\tdev_dbg(adev->dev, \"Reloading ucodes after reset\\n\");\n\tfor (i = 0; i < adev->firmware.max_ucodes; i++) {\n\t\tucode = &adev->firmware.ucode[i];\n\t\tif (!ucode->fw)\n\t\t\tcontinue;\n\t\tswitch (ucode->ucode_id) {\n\t\tcase AMDGPU_UCODE_ID_SDMA0:\n\t\tcase AMDGPU_UCODE_ID_SDMA1:\n\t\tcase AMDGPU_UCODE_ID_SDMA2:\n\t\tcase AMDGPU_UCODE_ID_SDMA3:\n\t\tcase AMDGPU_UCODE_ID_SDMA4:\n\t\tcase AMDGPU_UCODE_ID_SDMA5:\n\t\tcase AMDGPU_UCODE_ID_SDMA6:\n\t\tcase AMDGPU_UCODE_ID_SDMA7:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1_JT:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_G:\n\t\t\tucode_list[ucode_count++] = ucode;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Reinit NBIF block */\n\tcmn_block =\n\t\tamdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_COMMON);\n\tif (unlikely(!cmn_block)) {\n\t\tdev_err(adev->dev, \"Failed to get BIF handle\\n\");\n\t\treturn -EINVAL;\n\t}\n\tr = cmn_block->version->funcs->resume(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Reinit GFXHUB */\n\tadev->gfxhub.funcs->init(adev);\n\tr = adev->gfxhub.funcs->gart_enable(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFXHUB gart reenable failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Reload GFX firmware */\n\tr = psp_load_fw_list(&adev->psp, ucode_list, ucode_count);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFX ucode load failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Resume RLC, FW needs RLC alive to complete reset process */\n\tadev->gfx.rlc.funcs->resume(adev);\n\n\t/* Wait for FW reset event complete */\n\tr = amdgpu_dpm_wait_for_event(adev, SMU_EVENT_RESET_COMPLETE, 0);\n\tif (r) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Failed to get response from firmware after reset\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_COMMON))\n\t\t\tcontinue;\n\n\t\tif (adev->ip_blocks[i].version->funcs->late_init) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->late_init(\n\t\t\t\t(void *)adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\"late_init of IP block <%s> failed %d after reset\\n\",\n\t\t\t\t\tadev->ip_blocks[i].version->funcs->name,\n\t\t\t\t\tr);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tadev->ip_blocks[i].status.late_initialized = true;\n\t}\n\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_GATE);\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_GATE);\n\n\treturn r;\n}\n\nstatic int\naldebaran_mode2_restore_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tstruct amdgpu_ras *con;\n\tint r;\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(reset_context->reset_req_dev, MP1_HWIP, 0) ==\n\t\t    IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tdev_info(tmp_adev->dev,\n\t\t\t \"GPU reset succeeded, trying to resume\\n\");\n\t\tr = aldebaran_mode2_restore_ip(tmp_adev);\n\t\tif (r)\n\t\t\tgoto end;\n\n\t\t/*\n\t\t * Add this ASIC as tracked as reset was already\n\t\t * complete successfully.\n\t\t */\n\t\tamdgpu_register_gpu_instance(tmp_adev);\n\n\t\t/* Resume RAS, ecc_irq */\n\t\tcon = amdgpu_ras_get_context(tmp_adev);\n\t\tif (!amdgpu_sriov_vf(tmp_adev) && con) {\n\t\t\tif (tmp_adev->sdma.ras &&\n\t\t\t\ttmp_adev->sdma.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->sdma.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->sdma.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"SDMA failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (tmp_adev->gfx.ras &&\n\t\t\t\ttmp_adev->gfx.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->gfx.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->gfx.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"GFX failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_ras_resume(tmp_adev);\n\n\t\t/* Update PSP FW topology after reset */\n\t\tif (reset_context->hive &&\n\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\tr = amdgpu_xgmi_update_topology(reset_context->hive,\n\t\t\t\t\t\t\ttmp_adev);\n\n\t\tif (!r) {\n\t\t\tamdgpu_irq_gpu_reset_resume_helper(tmp_adev);\n\n\t\t\tr = amdgpu_ib_ring_tests(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\"ib ring test failed (%d).\\n\", r);\n\t\t\t\tr = -EAGAIN;\n\t\t\t\ttmp_adev->asic_reset_res = r;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\t}\n\nend:\n\treturn r;\n}\n\nstatic struct amdgpu_reset_handler aldebaran_mode2_handler = {\n\t.reset_method\t\t= AMD_RESET_METHOD_MODE2,\n\t.prepare_env\t\t= NULL,\n\t.prepare_hwcontext\t= aldebaran_mode2_prepare_hwcontext,\n\t.perform_reset\t\t= aldebaran_mode2_perform_reset,\n\t.restore_hwcontext\t= aldebaran_mode2_restore_hwcontext,\n\t.restore_env\t\t= NULL,\n\t.do_reset\t\t= aldebaran_mode2_reset,\n};\n\nstatic struct amdgpu_reset_handler\n\t*aldebaran_rst_handlers[AMDGPU_RESET_MAX_HANDLERS] = {\n\t\t&aldebaran_mode2_handler,\n\t};\n\nint aldebaran_reset_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_reset_control *reset_ctl;\n\n\treset_ctl = kzalloc(sizeof(*reset_ctl), GFP_KERNEL);\n\tif (!reset_ctl)\n\t\treturn -ENOMEM;\n\n\treset_ctl->handle = adev;\n\treset_ctl->async_reset = aldebaran_async_reset;\n\treset_ctl->active_reset = AMD_RESET_METHOD_NONE;\n\treset_ctl->get_reset_handler = aldebaran_get_reset_handler;\n\n\tINIT_WORK(&reset_ctl->reset_work, reset_ctl->async_reset);\n\t/* Only mode2 is handled through reset control now */\n\treset_ctl->reset_handlers = &aldebaran_rst_handlers;\n\n\tadev->reset_cntl = reset_ctl;\n\n\treturn 0;\n}\n\nint aldebaran_reset_fini(struct amdgpu_device *adev)\n{\n\tkfree(adev->reset_cntl);\n\tadev->reset_cntl = NULL;\n\treturn 0;\n}\n", "source_code_path": "aldebaran.c", "line_number": 50, "generated_diff": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl ||!reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: aldebaran.c\nLine: 50\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n====================================\nRead the following bug report.\n============ Bug Report ============\nCID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nThe function aldebaran_get_reset_handler neglects to perform a null check on the'reset_ctl' and'reset_ctl->handle' pointers before accessing their contents. This can lead to a null pointer dereference if either pointer is null when the function is called, resulting in potential system crashes or data corruption.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_amdkfd_gpuvm-bug.txt", "bug_report_text": "File: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription: Use of freed memory:\n *mem may have been freed inside of import_obj_create.\n\nConsider adding a check for NULL.\n", "diff_path": "amdgpu_amdkfd_gpuvm-diff.txt", "diff_text": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2014-2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include <linux/dma-buf.h>\n#include <linux/list.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/fdtable.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_object.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dma_buf.h\"\n#include <uapi/linux/kfd_ioctl.h>\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_smi_events.h\"\n\n/* Userptr restore delay, just long enough to allow consecutive VM\n * changes to accumulate\n */\n#define AMDGPU_USERPTR_RESTORE_DELAY_MS 1\n#define AMDGPU_RESERVE_MEM_LIMIT\t\t\t(3UL << 29)\n\n/*\n * Align VRAM availability to 2MB to avoid fragmentation caused by 4K allocations in the tail 2MB\n * BO chunk\n */\n#define VRAM_AVAILABLITY_ALIGN (1 << 21)\n\n/* Impose limit on how much memory KFD can use */\nstatic struct {\n\tuint64_t max_system_mem_limit;\n\tuint64_t max_ttm_mem_limit;\n\tint64_t system_mem_used;\n\tint64_t ttm_mem_used;\n\tspinlock_t mem_limit_lock;\n} kfd_mem_limit;\n\nstatic const char * const domain_bit_to_string[] = {\n\t\t\"CPU\",\n\t\t\"GTT\",\n\t\t\"VRAM\",\n\t\t\"GDS\",\n\t\t\"GWS\",\n\t\t\"OA\"\n};\n\n#define domain_string(domain) domain_bit_to_string[ffs(domain)-1]\n\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work);\n\nstatic bool kfd_mem_is_attached(struct amdgpu_vm *avm,\n\t\tstruct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list)\n\t\tif (entry->bo_va->base.vm == avm)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * reuse_dmamap() - Check whether adev can share the original\n * userptr BO\n *\n * If both adev and bo_adev are in direct mapping or\n * in the same iommu group, they can share the original BO.\n *\n * @adev: Device to which can or cannot share the original BO\n * @bo_adev: Device to which allocated BO belongs to\n *\n * Return: returns true if adev can share original userptr BO,\n * false otherwise.\n */\nstatic bool reuse_dmamap(struct amdgpu_device *adev, struct amdgpu_device *bo_adev)\n{\n\treturn (adev->ram_is_direct_mapped && bo_adev->ram_is_direct_mapped) ||\n\t\t\t(adev->dev->iommu_group == bo_adev->dev->iommu_group);\n}\n\n/* Set memory usage limits. Current, limits are\n *  System (TTM + userptr) memory - 15/16th System RAM\n *  TTM memory - 3/8th System RAM\n */\nvoid amdgpu_amdkfd_gpuvm_init_mem_limits(void)\n{\n\tstruct sysinfo si;\n\tuint64_t mem;\n\n\tif (kfd_mem_limit.max_system_mem_limit)\n\t\treturn;\n\n\tsi_meminfo(&si);\n\tmem = si.totalram - si.totalhigh;\n\tmem *= si.mem_unit;\n\n\tspin_lock_init(&kfd_mem_limit.mem_limit_lock);\n\tkfd_mem_limit.max_system_mem_limit = mem - (mem >> 6);\n\tif (kfd_mem_limit.max_system_mem_limit < 2 * AMDGPU_RESERVE_MEM_LIMIT)\n\t\tkfd_mem_limit.max_system_mem_limit >>= 1;\n\telse\n\t\tkfd_mem_limit.max_system_mem_limit -= AMDGPU_RESERVE_MEM_LIMIT;\n\n\tkfd_mem_limit.max_ttm_mem_limit = ttm_tt_pages_limit() << PAGE_SHIFT;\n\tpr_debug(\"Kernel memory limit %lluM, TTM limit %lluM\\n\",\n\t\t(kfd_mem_limit.max_system_mem_limit >> 20),\n\t\t(kfd_mem_limit.max_ttm_mem_limit >> 20));\n}\n\nvoid amdgpu_amdkfd_reserve_system_mem(uint64_t size)\n{\n\tkfd_mem_limit.system_mem_used += size;\n}\n\n/* Estimate page table size needed to represent a given memory size\n *\n * With 4KB pages, we need one 8 byte PTE for each 4KB of memory\n * (factor 512, >> 9). With 2MB pages, we need one 8 byte PTE for 2MB\n * of memory (factor 256K, >> 18). ROCm user mode tries to optimize\n * for 2MB pages for TLB efficiency. However, small allocations and\n * fragmented system memory still need some 4KB pages. We choose a\n * compromise that should work in most cases without reserving too\n * much memory for page tables unnecessarily (factor 16K, >> 14).\n */\n\n#define ESTIMATE_PT_SIZE(mem_size) max(((mem_size) >> 14), AMDGPU_VM_RESERVED_VRAM)\n\n/**\n * amdgpu_amdkfd_reserve_mem_limit() - Decrease available memory by size\n * of buffer.\n *\n * @adev: Device to which allocated BO belongs to\n * @size: Size of buffer, in bytes, encapsulated by B0. This should be\n * equivalent to amdgpu_bo_size(BO)\n * @alloc_flag: Flag used in allocating a BO as noted above\n * @xcp_id: xcp_id is used to get xcp from xcp manager, one xcp is\n * managed as one compute node in driver for app\n *\n * Return:\n *\treturns -ENOMEM in case of error, ZERO otherwise\n */\nint amdgpu_amdkfd_reserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tsize_t system_mem_needed, ttm_mem_needed, vram_needed;\n\tint ret = 0;\n\tuint64_t vram_size = 0;\n\n\tsystem_mem_needed = 0;\n\tttm_mem_needed = 0;\n\tvram_needed = 0;\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tsystem_mem_needed = size;\n\t\tttm_mem_needed = size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\t/*\n\t\t * Conservatively round up the allocation requirement to 2 MB\n\t\t * to avoid fragmentation caused by 4K allocations in the tail\n\t\t * 2M BO chunk.\n\t\t */\n\t\tvram_needed = size;\n\t\t/*\n\t\t * For GFX 9.4.3, get the VRAM size from XCP structs\n\t\t */\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\treturn -EINVAL;\n\n\t\tvram_size = KFD_XCP_MEMORY_SIZE(adev, xcp_id);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tsystem_mem_needed = size;\n\t\t\tttm_mem_needed = size;\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tsystem_mem_needed = size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (kfd_mem_limit.system_mem_used + system_mem_needed >\n\t    kfd_mem_limit.max_system_mem_limit)\n\t\tpr_debug(\"Set no_system_mem_limit=1 if using shared memory\\n\");\n\n\tif ((kfd_mem_limit.system_mem_used + system_mem_needed >\n\t     kfd_mem_limit.max_system_mem_limit && !no_system_mem_limit) ||\n\t    (kfd_mem_limit.ttm_mem_used + ttm_mem_needed >\n\t     kfd_mem_limit.max_ttm_mem_limit) ||\n\t    (adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] + vram_needed >\n\t     vram_size - reserved_for_pt - reserved_for_ras - atomic64_read(&adev->vram_pin_size))) {\n\t\tret = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\t/* Update memory accounting by decreasing available system\n\t * memory, TTM memory and GPU memory as computed above\n\t */\n\tWARN_ONCE(vram_needed && !adev,\n\t\t  \"adev reference can't be null when vram is used\");\n\tif (adev && xcp_id >= 0) {\n\t\tadev->kfd.vram_used[xcp_id] += vram_needed;\n\t\tadev->kfd.vram_used_aligned[xcp_id] +=\n\t\t\t\t(adev->flags & AMD_IS_APU) ?\n\t\t\t\tvram_needed :\n\t\t\t\tALIGN(vram_needed, VRAM_AVAILABLITY_ALIGN);\n\t}\n\tkfd_mem_limit.system_mem_used += system_mem_needed;\n\tkfd_mem_limit.ttm_mem_used += ttm_mem_needed;\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\treturn ret;\n}\n\nvoid amdgpu_amdkfd_unreserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tWARN_ONCE(!adev,\n\t\t\t  \"adev reference can't be null when alloc mem flags vram is set\");\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\tgoto release;\n\n\t\tif (adev) {\n\t\t\tadev->kfd.vram_used[xcp_id] -= size;\n\t\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -= size;\n\t\t\t\tkfd_mem_limit.system_mem_used -= size;\n\t\t\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t\t\t} else {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -=\n\t\t\t\t\tALIGN(size, VRAM_AVAILABLITY_ALIGN);\n\t\t\t}\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\tgoto release;\n\t}\n\tWARN_ONCE(adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] < 0,\n\t\t  \"KFD VRAM memory accounting unbalanced for xcp: %d\", xcp_id);\n\tWARN_ONCE(kfd_mem_limit.ttm_mem_used < 0,\n\t\t  \"KFD TTM memory accounting unbalanced\");\n\tWARN_ONCE(kfd_mem_limit.system_mem_used < 0,\n\t\t  \"KFD system memory accounting unbalanced\");\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n}\n\nvoid amdgpu_amdkfd_release_notify(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tu32 alloc_flags = bo->kfd_bo->alloc_flags;\n\tu64 size = amdgpu_bo_size(bo);\n\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, size, alloc_flags,\n\t\t\t\t\t  bo->xcp_id);\n\n\tkfree(bo->kfd_bo);\n}\n\n/**\n * create_dmamap_sg_bo() - Creates a amdgpu_bo object to reflect information\n * about USERPTR or DOOREBELL or MMIO BO.\n *\n * @adev: Device for which dmamap BO is being created\n * @mem: BO of peer device that is being DMA mapped. Provides parameters\n *\t in building the dmamap BO\n * @bo_out: Output parameter updated with handle of dmamap BO\n */\nstatic int\ncreate_dmamap_sg_bo(struct amdgpu_device *adev,\n\t\t struct kgd_mem *mem, struct amdgpu_bo **bo_out)\n{\n\tstruct drm_gem_object *gem_obj;\n\tint ret;\n\tuint64_t flags = 0;\n\n\tret = amdgpu_bo_reserve(mem->bo, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR)\n\t\tflags |= mem->bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t\t\tAMDGPU_GEM_CREATE_UNCACHED);\n\n\tret = amdgpu_gem_object_create(adev, mem->bo->tbo.base.size, 1,\n\t\t\tAMDGPU_GEM_DOMAIN_CPU, AMDGPU_GEM_CREATE_PREEMPTIBLE | flags,\n\t\t\tttm_bo_type_sg, mem->bo->tbo.base.resv, &gem_obj, 0);\n\n\tamdgpu_bo_unreserve(mem->bo);\n\n\tif (ret) {\n\t\tpr_err(\"Error in creating DMA mappable SG BO on domain: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\n\t*bo_out = gem_to_amdgpu_bo(gem_obj);\n\t(*bo_out)->parent = amdgpu_bo_ref(mem->bo);\n\treturn ret;\n}\n\n/* amdgpu_amdkfd_remove_eviction_fence - Removes eviction fence from BO's\n *  reservation object.\n *\n * @bo: [IN] Remove eviction fence(s) from this BO\n * @ef: [IN] This eviction fence is removed if it\n *  is present in the shared list.\n *\n * NOTE: Must be called with BO reserved i.e. bo->tbo.resv->lock held.\n */\nstatic int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tstruct amdgpu_amdkfd_fence *ef)\n{\n\tstruct dma_fence *replacement;\n\n\tif (!ef)\n\t\treturn -EINVAL;\n\n\t/* TODO: Instead of block before we should use the fence of the page\n\t * table update and TLB flush here directly.\n\t */\n\treplacement = dma_fence_get_stub();\n\tdma_resv_replace_fences(bo->tbo.base.resv, ef->base.context,\n\t\t\t\treplacement, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_fence_put(replacement);\n\treturn 0;\n}\n\nint amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo *root = bo;\n\tstruct amdgpu_vm_bo_base *vm_bo;\n\tstruct amdgpu_vm *vm;\n\tstruct amdkfd_process_info *info;\n\tstruct amdgpu_amdkfd_fence *ef;\n\tint ret;\n\n\t/* we can always get vm_bo from root PD bo.*/\n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tvm_bo = root->vm_bo;\n\tif (!vm_bo)\n\t\treturn 0;\n\n\tvm = vm_bo->vm;\n\tif (!vm)\n\t\treturn 0;\n\n\tinfo = vm->process_info;\n\tif (!info || !info->eviction_fence)\n\t\treturn 0;\n\n\tef = container_of(dma_fence_get(&info->eviction_fence->base),\n\t\t\tstruct amdgpu_amdkfd_fence, base);\n\n\tBUG_ON(!dma_resv_trylock(bo->tbo.base.resv));\n\tret = amdgpu_amdkfd_remove_eviction_fence(bo, ef);\n\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tdma_fence_put(&ef->base);\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,\n\t\t\t\t     bool wait)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint ret;\n\n\tif (WARN(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm),\n\t\t \"Called with userptr BO\"))\n\t\treturn -EINVAL;\n\n\t/* bo has been pinned, not need validate it */\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto validate_fail;\n\tif (wait)\n\t\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\nvalidate_fail:\n\treturn ret;\n}\n\nint amdgpu_amdkfd_bo_validate_and_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tuint32_t domain,\n\t\t\t\t\tstruct dma_fence *fence)\n{\n\tint ret = amdgpu_bo_reserve(bo, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = amdgpu_amdkfd_bo_validate(bo, domain, true);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tret = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tdma_resv_add_fence(bo->tbo.base.resv, fence,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\nunreserve_out:\n\tamdgpu_bo_unreserve(bo);\n\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_validate_vm_bo(void *_unused, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_amdkfd_bo_validate(bo, bo->allowed_domains, false);\n}\n\n/* vm_validate_pt_pd_bos - Validate page table and directory BOs\n *\n * Page directories are not updated here because huge page handling\n * during page table updates can invalidate page directory entries\n * again. Page directories are only updated after updating page\n * tables.\n */\nstatic int vm_validate_pt_pd_bos(struct amdgpu_vm *vm,\n\t\t\t\t struct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_validate(adev, vm, ticket,\n\t\t\t\t amdgpu_amdkfd_validate_vm_bo, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to validate PT BOs\\n\");\n\t\treturn ret;\n\t}\n\n\tvm->pd_phys_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\n\treturn 0;\n}\n\nstatic int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_sync_fence(sync, vm->last_update);\n}\n\nstatic uint64_t get_pte_flags(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tuint32_t mapping_flags = AMDGPU_VM_PAGE_READABLE |\n\t\t\t\t AMDGPU_VM_MTYPE_DEFAULT;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\treturn amdgpu_gem_va_map_flags(adev, mapping_flags);\n}\n\n/**\n * create_sg_table() - Create an sg_table for a contiguous DMA addr range\n * @addr: The starting address to point to\n * @size: Size of memory area in bytes being pointed to\n *\n * Allocates an instance of sg_table and initializes it to point to memory\n * area specified by input parameters. The address used to build is assumed\n * to be DMA mapped, if needed.\n *\n * DOORBELL or MMIO BOs use only one scatterlist node in their sg_table\n * because they are physically contiguous.\n *\n * Return: Initialized instance of SG Table or NULL\n */\nstatic struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n{\n\tstruct sg_table *sg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\n\tif (!sg)\n\t\treturn NULL;\n\tif (sg_alloc_table(sg, 1, GFP_KERNEL)) {\n\t\tkfree(sg);\n\t\treturn NULL;\n\t}\n\tsg_dma_address(sg->sgl) = addr;\n\tsg->sgl->length = size;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\tsg->sgl->dma_length = size;\n#endif\n\treturn sg;\n}\n\nstatic int\nkfd_mem_dmamap_userptr(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *src_ttm = mem->bo->tbo.ttm;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tint ret;\n\n\tif (WARN_ON(ttm->num_pages != src_ttm->num_pages))\n\t\treturn -EINVAL;\n\n\tttm->sg = kmalloc(sizeof(*ttm->sg), GFP_KERNEL);\n\tif (unlikely(!ttm->sg))\n\t\treturn -ENOMEM;\n\n\t/* Same sequence as in amdgpu_ttm_tt_pin_userptr */\n\tret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,\n\t\t\t\t\tttm->num_pages, 0,\n\t\t\t\t\t(u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\tret = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (unlikely(ret))\n\t\tgoto release_sg;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto unmap_sg;\n\n\treturn 0;\n\nunmap_sg:\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\nrelease_sg:\n\tpr_err(\"DMA map userptr failed: %d\\n\", ret);\n\tsg_free_table(ttm->sg);\nfree_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tint ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n/**\n * kfd_mem_dmamap_sg_bo() - Create DMA mapped sg_table to access DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * An access request from the device that owns DOORBELL does not require DMA mapping.\n * This is because the request doesn't go through PCIe root complex i.e. it instead\n * loops back. The need to DMA map arises only when accessing peer device's DOORBELL\n *\n * In contrast, all access requests for MMIO need to be DMA mapped without regard to\n * device ownership. This is because access requests for MMIO go through PCIe root\n * complex.\n *\n * This is accomplished in two steps:\n *   - Obtain DMA mapped address of DOORBELL or MMIO memory that could be used\n *         in updating requesting device's page table\n *   - Signal TTM to mark memory pointed to by requesting device's BO as GPU\n *         accessible. This allows an update of requesting device's page table\n *         with entries associated with DOOREBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *   - Mapping of DOORBELL or MMIO BO of same or peer device\n *   - Validating an evicted DOOREBELL or MMIO BO on device seeking access\n *\n * Return: ZERO if successful, NON-ZERO otherwise\n */\nstatic int\nkfd_mem_dmamap_sg_bo(struct kgd_mem *mem,\n\t\t     struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\tdma_addr_t dma_addr;\n\tbool mmio;\n\tint ret;\n\n\t/* Expect SG Table of dmapmap BO to be NULL */\n\tmmio = (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);\n\tif (unlikely(ttm->sg)) {\n\t\tpr_err(\"SG Table of %d BO for peer device is UNEXPECTEDLY NON-NULL\", mmio);\n\t\treturn -EINVAL;\n\t}\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_addr = mem->bo->tbo.sg->sgl->dma_address;\n\tpr_debug(\"%d BO size: %d\\n\", mmio, mem->bo->tbo.sg->sgl->length);\n\tpr_debug(\"%d BO address before DMA mapping: %llx\\n\", mmio, dma_addr);\n\tdma_addr = dma_map_resource(adev->dev, dma_addr,\n\t\t\tmem->bo->tbo.sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tret = dma_mapping_error(adev->dev, dma_addr);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tpr_debug(\"%d BO address after DMA mapping: %llx\\n\", mmio, dma_addr);\n\n\tttm->sg = create_sg_table(dma_addr, mem->bo->tbo.sg->sgl->length);\n\tif (unlikely(!ttm->sg)) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\treturn ret;\n\nfree_sg:\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\nunmap_sg:\n\tdma_unmap_resource(adev->dev, dma_addr, mem->bo->tbo.sg->sgl->length,\n\t\t\t   dir, DMA_ATTR_SKIP_CPU_SYNC);\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_attachment(struct kgd_mem *mem,\n\t\t\t  struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\treturn 0;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\treturn kfd_mem_dmamap_userptr(mem, attachment);\n\tcase KFD_MEM_ATT_DMABUF:\n\t\treturn kfd_mem_dmamap_dmabuf(attachment);\n\tcase KFD_MEM_ATT_SG:\n\t\treturn kfd_mem_dmamap_sg_bo(mem, attachment);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nkfd_mem_dmaunmap_userptr(struct kgd_mem *mem,\n\t\t\t struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = false};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\n\tif (unlikely(!ttm->sg))\n\t\treturn;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\t/* This is a no-op. We don't want to trigger eviction fences when\n\t * unmapping DMABufs. Therefore the invalidation (moving to system\n\t * domain) is done in kfd_mem_dmamap_dmabuf.\n\t */\n}\n\n/**\n * kfd_mem_dmaunmap_sg_bo() - Free DMA mapped sg_table of DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * The method performs following steps:\n *   - Signal TTM to mark memory pointed to by BO as GPU inaccessible\n *   - Free SG Table that is used to encapsulate DMA mapped memory of\n *          peer device's DOORBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *     UNMapping of DOORBELL or MMIO BO on a device having access to its memory\n *     Eviction of DOOREBELL or MMIO BO on device having access to its memory\n *\n * Return: void\n */\nstatic void\nkfd_mem_dmaunmap_sg_bo(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\n\tif (unlikely(!ttm->sg)) {\n\t\tpr_debug(\"SG Table of BO is NULL\");\n\t\treturn;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_unmap_resource(adev->dev, ttm->sg->sgl->dma_address,\n\t\t\tttm->sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\tbo->tbo.sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_attachment(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\tbreak;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\tkfd_mem_dmaunmap_userptr(mem, attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_DMABUF:\n\t\tkfd_mem_dmaunmap_dmabuf(attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_SG:\n\t\tkfd_mem_dmaunmap_sg_bo(mem, attachment);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic int kfd_mem_export_dmabuf(struct kgd_mem *mem)\n{\n\tif (!mem->dmabuf) {\n\t\tstruct amdgpu_device *bo_adev;\n\t\tstruct dma_buf *dmabuf;\n\t\tint r, fd;\n\n\t\tbo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\t\tr = drm_gem_prime_handle_to_fd(&bo_adev->ddev, bo_adev->kfd.client.file,\n\t\t\t\t\t       mem->gem_handle,\n\t\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\t\t       DRM_RDWR : 0, &fd);\n\t\tif (r)\n\t\t\treturn r;\n\t\tdmabuf = dma_buf_get(fd);\n\t\tclose_fd(fd);\n\t\tif (WARN_ON_ONCE(IS_ERR(dmabuf)))\n\t\t\treturn PTR_ERR(dmabuf);\n\t\tmem->dmabuf = dmabuf;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nkfd_mem_attach_dmabuf(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\t      struct amdgpu_bo **bo)\n{\n\tstruct drm_gem_object *gobj;\n\tint ret;\n\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tgobj = amdgpu_gem_prime_import(adev_to_drm(adev), mem->dmabuf);\n\tif (IS_ERR(gobj))\n\t\treturn PTR_ERR(gobj);\n\n\t*bo = gem_to_amdgpu_bo(gobj);\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\treturn 0;\n}\n\n/* kfd_mem_attach - Add a BO to a VM\n *\n * Everything that needs to bo done only once when a BO is first added\n * to a VM. It can later be mapped and unmapped many times without\n * repeating these steps.\n *\n * 0. Create BO for DMA mapping, if needed\n * 1. Allocate and initialize BO VA entry data structure\n * 2. Add BO to the VM\n * 3. Determine ASIC-specific PTE flags\n * 4. Alloc page tables and directories if needed\n * 4a.  Validate new page tables and directories\n */\nstatic int kfd_mem_attach(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tstruct amdgpu_vm *vm, bool is_aql)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tuint64_t va = mem->va;\n\tstruct kfd_mem_attachment *attachment[2] = {NULL, NULL};\n\tstruct amdgpu_bo *bo[2] = {NULL, NULL};\n\tstruct amdgpu_bo_va *bo_va;\n\tbool same_hive = false;\n\tint i, ret;\n\n\tif (!va) {\n\t\tpr_err(\"Invalid VA when adding BO to VM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Determine access to VRAM, MMIO and DOORBELL BOs of peer devices\n\t *\n\t * The access path of MMIO and DOORBELL BOs of is always over PCIe.\n\t * In contrast the access path of VRAM BOs depens upon the type of\n\t * link that connects the peer device. Access over PCIe is allowed\n\t * if peer device has large BAR. In contrast, access over xGMI is\n\t * allowed for both small and large BAR configurations of peer device\n\t */\n\tif ((adev != bo_adev && !(adev->flags & AMD_IS_APU)) &&\n\t    ((mem->domain == AMDGPU_GEM_DOMAIN_VRAM) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tif (mem->domain == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\tsame_hive = amdgpu_xgmi_same_hive(adev, bo_adev);\n\t\tif (!same_hive && !amdgpu_device_is_peer_accessible(bo_adev, adev))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i <= is_aql; i++) {\n\t\tattachment[i] = kzalloc(sizeof(*attachment[i]), GFP_KERNEL);\n\t\tif (unlikely(!attachment[i])) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tpr_debug(\"\\t add VA 0x%llx - 0x%llx to vm %p\\n\", va,\n\t\t\t va + bo_size, vm);\n\n\t\tif ((adev == bo_adev && !(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) ||\n\t\t    (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && reuse_dmamap(adev, bo_adev)) ||\n\t\t    (mem->domain == AMDGPU_GEM_DOMAIN_GTT && reuse_dmamap(adev, bo_adev)) ||\n\t\t    same_hive) {\n\t\t\t/* Mappings on the local GPU, or VRAM mappings in the\n\t\t\t * local hive, or userptr, or GTT mapping can reuse dma map\n\t\t\t * address space share the original BO\n\t\t\t */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = mem->bo;\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (i > 0) {\n\t\t\t/* Multiple mappings on the same GPU share the BO */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = bo[0];\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\t\t/* Create an SG BO to DMA-map userptrs on other GPUs */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_USERPTR;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Handle DOORBELL BOs of peer devices and MMIO BOs of local and peer devices */\n\t\t} else if (mem->bo->tbo.type == ttm_bo_type_sg) {\n\t\t\tWARN_ONCE(!(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL ||\n\t\t\t\t    mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP),\n\t\t\t\t  \"Handing invalid SG BO in ATTACH request\");\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SG;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Enable acces to GTT and VRAM BOs of peer devices */\n\t\t} else if (mem->domain == AMDGPU_GEM_DOMAIN_GTT ||\n\t\t\t   mem->domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tattachment[i]->type = KFD_MEM_ATT_DMABUF;\n\t\t\tret = kfd_mem_attach_dmabuf(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t\tpr_debug(\"Employ DMABUF mechanism to enable peer GPU access\\n\");\n\t\t} else {\n\t\t\tWARN_ONCE(true, \"Handling invalid ATTACH request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\t/* Add BO to VM internal data structures */\n\t\tret = amdgpu_bo_reserve(bo[i], false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Unable to reserve BO during memory attach\");\n\t\t\tgoto unwind;\n\t\t}\n\t\tbo_va = amdgpu_vm_bo_find(vm, bo[i]);\n\t\tif (!bo_va)\n\t\t\tbo_va = amdgpu_vm_bo_add(adev, vm, bo[i]);\n\t\telse\n\t\t\t++bo_va->ref_count;\n\t\tattachment[i]->bo_va = bo_va;\n\t\tamdgpu_bo_unreserve(bo[i]);\n\t\tif (unlikely(!attachment[i]->bo_va)) {\n\t\t\tret = -ENOMEM;\n\t\t\tpr_err(\"Failed to add BO object to VM. ret == %d\\n\",\n\t\t\t       ret);\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->va = va;\n\t\tattachment[i]->pte_flags = get_pte_flags(adev, mem);\n\t\tattachment[i]->adev = adev;\n\t\tlist_add(&attachment[i]->list, &mem->attachments);\n\n\t\tva += bo_size;\n\t}\n\n\treturn 0;\n\nunwind:\n\tfor (; i >= 0; i--) {\n\t\tif (!attachment[i])\n\t\t\tcontinue;\n\t\tif (attachment[i]->bo_va) {\n\t\t\tamdgpu_bo_reserve(bo[i], true);\n\t\t\tif (--attachment[i]->bo_va->ref_count == 0)\n\t\t\t\tamdgpu_vm_bo_del(adev, attachment[i]->bo_va);\n\t\t\tamdgpu_bo_unreserve(bo[i]);\n\t\t\tlist_del(&attachment[i]->list);\n\t\t}\n\t\tif (bo[i])\n\t\t\tdrm_gem_object_put(&bo[i]->tbo.base);\n\t\tkfree(attachment[i]);\n\t}\n\treturn ret;\n}\n\nstatic void kfd_mem_detach(struct kfd_mem_attachment *attachment)\n{\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\n\tpr_debug(\"\\t remove VA 0x%llx in entry %p\\n\",\n\t\t\tattachment->va, attachment);\n\tif (--attachment->bo_va->ref_count == 0)\n\t\tamdgpu_vm_bo_del(attachment->adev, attachment->bo_va);\n\tdrm_gem_object_put(&bo->tbo.base);\n\tlist_del(&attachment->list);\n\tkfree(attachment);\n}\n\nstatic void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,\n\t\t\t\tstruct amdkfd_process_info *process_info,\n\t\t\t\tbool userptr)\n{\n\tmutex_lock(&process_info->lock);\n\tif (userptr)\n\t\tlist_add_tail(&mem->validate_list,\n\t\t\t      &process_info->userptr_valid_list);\n\telse\n\t\tlist_add_tail(&mem->validate_list, &process_info->kfd_bo_list);\n\tmutex_unlock(&process_info->lock);\n}\n\nstatic void remove_kgd_mem_from_kfd_bo_list(struct kgd_mem *mem,\n\t\tstruct amdkfd_process_info *process_info)\n{\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n}\n\n/* Initializes user pages. It registers the MMU notifier and validates\n * the userptr BO in the GTT domain.\n *\n * The BO must already be on the userptr_valid_list. Otherwise an\n * eviction and restore may happen that leaves the new BO unmapped\n * with the user mode queues running.\n *\n * Takes the process_info->lock to protect against concurrent restore\n * workers.\n *\n * Returns 0 for success, negative errno for errors.\n */\nstatic int init_user_pages(struct kgd_mem *mem, uint64_t user_addr,\n\t\t\t   bool criu_resume)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct hmm_range *range;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->lock);\n\n\tret = amdgpu_ttm_tt_set_userptr(&bo->tbo, user_addr, 0);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to set userptr: %d\\n\", __func__, ret);\n\t\tgoto out;\n\t}\n\n\tret = amdgpu_hmm_register(bo, user_addr);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register MMU notifier: %d\\n\",\n\t\t       __func__, ret);\n\t\tgoto out;\n\t}\n\n\tif (criu_resume) {\n\t\t/*\n\t\t * During a CRIU restore operation, the userptr buffer objects\n\t\t * will be validated in the restore_userptr_work worker at a\n\t\t * later stage when it is scheduled by another ioctl called by\n\t\t * CRIU master process for the target pid for restore.\n\t\t */\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tmem->invalid++;\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t\tmutex_unlock(&process_info->lock);\n\t\treturn 0;\n\t}\n\n\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);\n\tif (ret) {\n\t\tif (ret == -EAGAIN)\n\t\t\tpr_debug(\"Failed to get user pages, try again\\n\");\n\t\telse\n\t\t\tpr_err(\"%s: Failed to get user pages: %d\\n\", __func__, ret);\n\t\tgoto unregister_out;\n\t}\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to reserve BO\\n\", __func__);\n\t\tgoto release_out;\n\t}\n\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\tamdgpu_bo_unreserve(bo);\n\nrelease_out:\n\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\nunregister_out:\n\tif (ret)\n\t\tamdgpu_hmm_unregister(bo);\nout:\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\n/* Reserving a BO and its page table BOs must happen atomically to\n * avoid deadlocks. Some operations update multiple VMs at once. Track\n * all the reservation info in a context structure. Optionally a sync\n * object can track VM updates.\n */\nstruct bo_vm_reservation_context {\n\t/* DRM execution context for the reservation */\n\tstruct drm_exec exec;\n\t/* Number of VMs reserved */\n\tunsigned int n_vms;\n\t/* Pointer to sync object */\n\tstruct amdgpu_sync *sync;\n};\n\nenum bo_vm_match {\n\tBO_VM_NOT_MAPPED = 0,\t/* Match VMs where a BO is not mapped */\n\tBO_VM_MAPPED,\t\t/* Match VMs where a BO is mapped     */\n\tBO_VM_ALL,\t\t/* Match all VMs a BO was added to    */\n};\n\n/**\n * reserve_bo_and_vm - reserve a BO and a VM unconditionally.\n * @mem: KFD BO structure.\n * @vm: the VM to reserve.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n */\nstatic int reserve_bo_and_vm(struct kgd_mem *mem,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct bo_vm_reservation_context *ctx)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tWARN_ON(!vm);\n\n\tctx->n_vms = 1;\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tret = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * reserve_bo_and_cond_vms - reserve a BO and some VMs conditionally\n * @mem: KFD BO structure.\n * @vm: the VM to reserve. If NULL, then all VMs associated with the BO\n * is used. Otherwise, a single VM associated with the BO.\n * @map_type: the mapping status that will be used to filter the VMs.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n *\n * Returns 0 for success, negative for failure.\n */\nstatic int reserve_bo_and_cond_vms(struct kgd_mem *mem,\n\t\t\t\tstruct amdgpu_vm *vm, enum bo_vm_match map_type,\n\t\t\t\tstruct bo_vm_reservation_context *ctx)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tctx->n_vms = 0;\n\t\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\t\tif ((vm && vm != entry->bo_va->base.vm) ||\n\t\t\t\t(entry->is_mapped != map_type\n\t\t\t\t&& map_type != BO_VM_ALL))\n\t\t\t\tcontinue;\n\n\t\t\tret = amdgpu_vm_lock_pd(entry->bo_va->base.vm,\n\t\t\t\t\t\t&ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto error;\n\t\t\t++ctx->n_vms;\n\t\t}\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * unreserve_bo_and_vms - Unreserve BO and VMs from a reservation context\n * @ctx: Reservation context to unreserve\n * @wait: Optionally wait for a sync object representing pending VM updates\n * @intr: Whether the wait is interruptible\n *\n * Also frees any resources allocated in\n * reserve_bo_and_(cond_)vm(s). Returns the status from\n * amdgpu_sync_wait.\n */\nstatic int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,\n\t\t\t\t bool wait, bool intr)\n{\n\tint ret = 0;\n\n\tif (wait)\n\t\tret = amdgpu_sync_wait(ctx->sync, intr);\n\n\tdrm_exec_fini(&ctx->exec);\n\tctx->sync = NULL;\n\treturn ret;\n}\n\nstatic void unmap_bo_from_gpuvm(struct kgd_mem *mem,\n\t\t\t\tstruct kfd_mem_attachment *entry,\n\t\t\t\tstruct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\n\tamdgpu_vm_bo_unmap(adev, bo_va, entry->va);\n\n\tamdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);\n\n\tamdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int update_gpuvm_pte(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *entry,\n\t\t\t    struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tint ret;\n\n\tret = kfd_mem_dmamap_attachment(mem, entry);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Update the page tables  */\n\tret = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (ret) {\n\t\tpr_err(\"amdgpu_vm_bo_update failed\\n\");\n\t\treturn ret;\n\t}\n\n\treturn amdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int map_bo_to_gpuvm(struct kgd_mem *mem,\n\t\t\t   struct kfd_mem_attachment *entry,\n\t\t\t   struct amdgpu_sync *sync,\n\t\t\t   bool no_update_pte)\n{\n\tint ret;\n\n\t/* Set virtual address for the allocation */\n\tret = amdgpu_vm_bo_map(entry->adev, entry->bo_va, entry->va, 0,\n\t\t\t       amdgpu_bo_size(entry->bo_va->base.bo),\n\t\t\t       entry->pte_flags);\n\tif (ret) {\n\t\tpr_err(\"Failed to map VA 0x%llx in vm. ret %d\\n\",\n\t\t\t\tentry->va, ret);\n\t\treturn ret;\n\t}\n\n\tif (no_update_pte)\n\t\treturn 0;\n\n\tret = update_gpuvm_pte(mem, entry, sync);\n\tif (ret) {\n\t\tpr_err(\"update_gpuvm_pte() failed\\n\");\n\t\tgoto update_gpuvm_pte_failed;\n\t}\n\n\treturn 0;\n\nupdate_gpuvm_pte_failed:\n\tunmap_bo_from_gpuvm(mem, entry, sync);\n\tkfd_mem_dmaunmap_attachment(mem, entry);\n\treturn ret;\n}\n\nstatic int process_validate_vms(struct amdkfd_process_info *process_info,\n\t\t\t\tstruct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_validate_pt_pd_bos(peer_vm, ticket);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_sync_pds_resv(struct amdkfd_process_info *process_info,\n\t\t\t\t struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *pd = peer_vm->root.bo;\n\n\t\tret = amdgpu_sync_resv(NULL, sync, pd->tbo.base.resv,\n\t\t\t\t       AMDGPU_SYNC_NE_OWNER,\n\t\t\t\t       AMDGPU_FENCE_OWNER_KFD);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_update_pds(struct amdkfd_process_info *process_info,\n\t\t\t      struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_update_pds(peer_vm, sync);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,\n\t\t       struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *info = NULL;\n\tint ret;\n\n\tif (!*process_info) {\n\t\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tmutex_init(&info->lock);\n\t\tmutex_init(&info->notifier_lock);\n\t\tINIT_LIST_HEAD(&info->vm_list_head);\n\t\tINIT_LIST_HEAD(&info->kfd_bo_list);\n\t\tINIT_LIST_HEAD(&info->userptr_valid_list);\n\t\tINIT_LIST_HEAD(&info->userptr_inval_list);\n\n\t\tinfo->eviction_fence =\n\t\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t\t   current->mm,\n\t\t\t\t\t\t   NULL);\n\t\tif (!info->eviction_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto create_evict_fence_fail;\n\t\t}\n\n\t\tinfo->pid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\t\tINIT_DELAYED_WORK(&info->restore_userptr_work,\n\t\t\t\t  amdgpu_amdkfd_restore_userptr_worker);\n\n\t\t*process_info = info;\n\t}\n\n\tvm->process_info = *process_info;\n\n\t/* Validate page directory and attach eviction fence */\n\tret = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (ret)\n\t\tgoto reserve_pd_fail;\n\tret = vm_validate_pt_pd_bos(vm, NULL);\n\tif (ret) {\n\t\tpr_err(\"validate_pt_pd_bos() failed\\n\");\n\t\tgoto validate_pd_fail;\n\t}\n\tret = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t  AMDGPU_FENCE_OWNER_KFD, false);\n\tif (ret)\n\t\tgoto wait_pd_fail;\n\tret = dma_resv_reserve_fences(vm->root.bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(vm->root.bo->tbo.base.resv,\n\t\t\t   &vm->process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\n\t/* Update process info */\n\tmutex_lock(&vm->process_info->lock);\n\tlist_add_tail(&vm->vm_list_node,\n\t\t\t&(vm->process_info->vm_list_head));\n\tvm->process_info->n_vms++;\n\n\t*ef = dma_fence_get(&vm->process_info->eviction_fence->base);\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn 0;\n\nreserve_shared_fail:\nwait_pd_fail:\nvalidate_pd_fail:\n\tamdgpu_bo_unreserve(vm->root.bo);\nreserve_pd_fail:\n\tvm->process_info = NULL;\n\tif (info) {\n\t\tdma_fence_put(&info->eviction_fence->base);\n\t\t*process_info = NULL;\n\t\tput_pid(info->pid);\ncreate_evict_fence_fail:\n\t\tmutex_destroy(&info->lock);\n\t\tmutex_destroy(&info->notifier_lock);\n\t\tkfree(info);\n\t}\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_pin_bo() - Pins a BO using following criteria\n * @bo: Handle of buffer object being pinned\n * @domain: Domain into which BO should be pinned\n *\n *   - USERPTR BOs are UNPINNABLE and will return error\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count incremented. It is valid to PIN a BO multiple times\n *\n * Return: ZERO if successful in pinning, Non-Zero in case of error.\n */\nstatic int amdgpu_amdkfd_gpuvm_pin_bo(struct amdgpu_bo *bo, u32 domain)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {\n\t\t/*\n\t\t * If bo is not contiguous on VRAM, move to system memory first to ensure\n\t\t * we can get contiguous VRAM space after evicting other BOs.\n\t\t */\n\t\tif (!(bo->tbo.resource->placement & TTM_PL_FLAG_CONTIGUOUS)) {\n\t\t\tstruct ttm_operation_ctx ctx = { true, false };\n\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_debug(\"validate bo 0x%p to GTT failed %d\\n\", &bo->tbo, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n\tif (ret)\n\t\tpr_err(\"Error in Pinning BO to domain: %d\\n\", domain);\n\n\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\nout:\n\tamdgpu_bo_unreserve(bo);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_unpin_bo() - Unpins BO using following criteria\n * @bo: Handle of buffer object being unpinned\n *\n *   - Is a illegal request for USERPTR BOs and is ignored\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count decremented. Calls to UNPIN must balance calls to PIN\n */\nstatic void amdgpu_amdkfd_gpuvm_unpin_bo(struct amdgpu_bo *bo)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn;\n\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_set_vm_pasid(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_vm *avm, u32 pasid)\n\n{\n\tint ret;\n\n\t/* Free the original amdgpu allocated pasid,\n\t * will be replaced with kfd allocated pasid.\n\t */\n\tif (avm->pasid) {\n\t\tamdgpu_pasid_free(avm->pasid);\n\t\tamdgpu_vm_set_pasid(adev, avm, 0);\n\t}\n\n\tret = amdgpu_vm_set_pasid(adev, avm, pasid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_acquire_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *avm,\n\t\t\t\t\t   void **process_info,\n\t\t\t\t\t   struct dma_fence **ef)\n{\n\tint ret;\n\n\t/* Already a compute VM? */\n\tif (avm->process_info)\n\t\treturn -EINVAL;\n\n\t/* Convert VM into a compute VM */\n\tret = amdgpu_vm_make_compute(adev, avm);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Initialize KFD part of the VM and process info */\n\tret = init_kfd_vm(avm, process_info, ef);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_vm_set_task_info(avm);\n\n\treturn 0;\n}\n\nvoid amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm)\n{\n\tstruct amdkfd_process_info *process_info = vm->process_info;\n\n\tif (!process_info)\n\t\treturn;\n\n\t/* Update process info */\n\tmutex_lock(&process_info->lock);\n\tprocess_info->n_vms--;\n\tlist_del(&vm->vm_list_node);\n\tmutex_unlock(&process_info->lock);\n\n\tvm->process_info = NULL;\n\n\t/* Release per-process resources when last compute VM is destroyed */\n\tif (!process_info->n_vms) {\n\t\tWARN_ON(!list_empty(&process_info->kfd_bo_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_valid_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_inval_list));\n\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tcancel_delayed_work_sync(&process_info->restore_userptr_work);\n\t\tput_pid(process_info->pid);\n\t\tmutex_destroy(&process_info->lock);\n\t\tmutex_destroy(&process_info->notifier_lock);\n\t\tkfree(process_info);\n\t}\n}\n\nvoid amdgpu_amdkfd_gpuvm_release_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t    void *drm_priv)\n{\n\tstruct amdgpu_vm *avm;\n\n\tif (WARN_ON(!adev || !drm_priv))\n\t\treturn;\n\n\tavm = drm_priv_to_vm(drm_priv);\n\n\tpr_debug(\"Releasing process vm %p\\n\", avm);\n\n\t/* The original pasid of amdgpu vm has already been\n\t * released during making a amdgpu vm to a compute vm\n\t * The current pasid is managed by kfd and will be\n\t * released on kfd process destroy. Set amdgpu pasid\n\t * to 0 to avoid duplicate release.\n\t */\n\tamdgpu_vm_release_compute(adev, avm);\n}\n\nuint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *pd = avm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\n\tif (adev->asic_type < CHIP_VEGA10)\n\t\treturn avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;\n\treturn avm->pd_phys_addr;\n}\n\nvoid amdgpu_amdkfd_block_mmu_notifications(void *p)\n{\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tWRITE_ONCE(pinfo->block_mmu_notifications, true);\n\tmutex_unlock(&pinfo->lock);\n}\n\nint amdgpu_amdkfd_criu_resume(void *p)\n{\n\tint ret = 0;\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tpr_debug(\"scheduling work\\n\");\n\tmutex_lock(&pinfo->notifier_lock);\n\tpinfo->evicted_bos++;\n\tmutex_unlock(&pinfo->notifier_lock);\n\tif (!READ_ONCE(pinfo->block_mmu_notifications)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tWRITE_ONCE(pinfo->block_mmu_notifications, false);\n\tqueue_delayed_work(system_freezable_wq,\n\t\t\t   &pinfo->restore_userptr_work, 0);\n\nout_unlock:\n\tmutex_unlock(&pinfo->lock);\n\treturn ret;\n}\n\nsize_t amdgpu_amdkfd_get_available_memory(struct amdgpu_device *adev,\n\t\t\t\t\t  uint8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tssize_t available;\n\tuint64_t vram_available, system_mem_available, ttm_mem_available;\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tvram_available = KFD_XCP_MEMORY_SIZE(adev, xcp_id)\n\t\t- adev->kfd.vram_used_aligned[xcp_id]\n\t\t- atomic64_read(&adev->vram_pin_size)\n\t\t- reserved_for_pt\n\t\t- reserved_for_ras;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tsystem_mem_available = no_system_mem_limit ?\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit :\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit -\n\t\t\t\t\tkfd_mem_limit.system_mem_used;\n\n\t\tttm_mem_available = kfd_mem_limit.max_ttm_mem_limit -\n\t\t\t\tkfd_mem_limit.ttm_mem_used;\n\n\t\tavailable = min3(system_mem_available, ttm_mem_available,\n\t\t\t\t vram_available);\n\t\tavailable = ALIGN_DOWN(available, PAGE_SIZE);\n\t} else {\n\t\tavailable = ALIGN_DOWN(vram_available, VRAM_AVAILABLITY_ALIGN);\n\t}\n\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\n\treturn available;\n}\n\nint amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, uint64_t va, uint64_t size,\n\t\tvoid *drm_priv, struct kgd_mem **mem,\n\t\tuint64_t *offset, uint32_t flags, bool criu_resume)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_fpriv *fpriv = container_of(avm, struct amdgpu_fpriv, vm);\n\tenum ttm_bo_type bo_type = ttm_bo_type_device;\n\tstruct sg_table *sg = NULL;\n\tuint64_t user_addr = 0;\n\tstruct amdgpu_bo *bo;\n\tstruct drm_gem_object *gobj = NULL;\n\tu32 domain, alloc_domain;\n\tuint64_t aligned_size;\n\tint8_t xcp_id = -1;\n\tu64 alloc_flags;\n\tint ret;\n\n\t/*\n\t * Check on which domain to allocate BO\n\t */\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_flags = 0;\n\t\t} else {\n\t\t\talloc_flags = AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\t\t\talloc_flags |= (flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) ?\n\t\t\tAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED : 0;\n\n\t\t\t/* For contiguous VRAM allocation */\n\t\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_CONTIGUOUS)\n\t\t\t\talloc_flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\t\t}\n\t\txcp_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ?\n\t\t\t\t\t0 : fpriv->xcp_id;\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_flags = 0;\n\t} else {\n\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_domain = AMDGPU_GEM_DOMAIN_CPU;\n\t\talloc_flags = AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\tif (!offset || !*offset)\n\t\t\t\treturn -EINVAL;\n\t\t\tuser_addr = untagged_addr(*offset);\n\t\t} else if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\t\tbo_type = ttm_bo_type_sg;\n\t\t\tif (size > UINT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\tsg = create_sg_table(*offset, size);\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_EXT_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_EXT_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_UNCACHED)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_UNCACHED;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\t(*mem)->aql_queue = !!(flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);\n\n\t/* Workaround for AQL queue wraparound bug. Map the same\n\t * memory twice. That means we only actually allocate half\n\t * the memory.\n\t */\n\tif ((*mem)->aql_queue)\n\t\tsize >>= 1;\n\taligned_size = PAGE_ALIGN(size);\n\n\t(*mem)->alloc_flags = flags;\n\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\tret = amdgpu_amdkfd_reserve_mem_limit(adev, aligned_size, flags,\n\t\t\t\t\t      xcp_id);\n\tif (ret) {\n\t\tpr_debug(\"Insufficient memory\\n\");\n\t\tgoto err_reserve_limit;\n\t}\n\n\tpr_debug(\"\\tcreate BO VA 0x%llx size 0x%llx domain %s xcp_id %d\\n\",\n\t\t va, (*mem)->aql_queue ? size << 1 : size,\n\t\t domain_string(alloc_domain), xcp_id);\n\n\tret = amdgpu_gem_object_create(adev, aligned_size, 1, alloc_domain, alloc_flags,\n\t\t\t\t       bo_type, NULL, &gobj, xcp_id + 1);\n\tif (ret) {\n\t\tpr_debug(\"Failed to create BO on domain %s. ret %d\\n\",\n\t\t\t domain_string(alloc_domain), ret);\n\t\tgoto err_bo_create;\n\t}\n\tret = drm_vma_node_allow(&gobj->vma_node, drm_priv);\n\tif (ret) {\n\t\tpr_debug(\"Failed to allow vma node access. ret %d\\n\", ret);\n\t\tgoto err_node_allow;\n\t}\n\tret = drm_gem_handle_create(adev->kfd.client.file, gobj, &(*mem)->gem_handle);\n\tif (ret)\n\t\tgoto err_gem_handle_create;\n\tbo = gem_to_amdgpu_bo(gobj);\n\tif (bo_type == ttm_bo_type_sg) {\n\t\tbo->tbo.sg = sg;\n\t\tbo->tbo.ttm->sg = sg;\n\t}\n\tbo->kfd_bo = *mem;\n\t(*mem)->bo = bo;\n\tif (user_addr)\n\t\tbo->flags |= AMDGPU_AMDKFD_CREATE_USERPTR_BO;\n\n\t(*mem)->va = va;\n\t(*mem)->domain = domain;\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);\n\n\tif (user_addr) {\n\t\tpr_debug(\"creating userptr BO for user_addr = %llx\\n\", user_addr);\n\t\tret = init_user_pages(*mem, user_addr, criu_resume);\n\t\tif (ret)\n\t\t\tgoto allocate_init_user_pages_failed;\n\t} else  if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tret = amdgpu_amdkfd_gpuvm_pin_bo(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (ret) {\n\t\t\tpr_err(\"Pinning MMIO/DOORBELL BO during ALLOC FAILED\\n\");\n\t\t\tgoto err_pin_bo;\n\t\t}\n\t\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t} else {\n\t\tmutex_lock(&avm->process_info->lock);\n\t\tif (avm->process_info->eviction_fence &&\n\t\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\t\tmutex_unlock(&avm->process_info->lock);\n\t\tif (ret)\n\t\t\tgoto err_validate_bo;\n\t}\n\n\tif (offset)\n\t\t*offset = amdgpu_bo_mmap_offset(bo);\n\n\treturn 0;\n\nallocate_init_user_pages_failed:\nerr_pin_bo:\nerr_validate_bo:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_gem_handle_delete(adev->kfd.client.file, (*mem)->gem_handle);\nerr_gem_handle_create:\n\tdrm_vma_node_revoke(&gobj->vma_node, drm_priv);\nerr_node_allow:\n\t/* Don't unreserve system mem limit twice */\n\tgoto err_reserve_limit;\nerr_bo_create:\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, aligned_size, flags, xcp_id);\nerr_reserve_limit:\n\tamdgpu_sync_free(&(*mem)->sync);\n\tmutex_destroy(&(*mem)->lock);\n\tif (gobj)\n\t\tdrm_gem_object_put(gobj);\n\telse\n\t\tkfree(*mem);\nerr:\n\tif (sg) {\n\t\tsg_free_table(sg);\n\t\tkfree(sg);\n\t}\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_free_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv,\n\t\tuint64_t *size)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tbool use_release_notifier = (mem->bo->kfd_bo == mem);\n\tstruct kfd_mem_attachment *entry, *tmp;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned int mapped_to_gpu_memory;\n\tint ret;\n\tbool is_imported = false;\n\n\tmutex_lock(&mem->lock);\n\n\t/* Unpin MMIO/DOORBELL BO's that were pinned during allocation */\n\tif (mem->alloc_flags &\n\t    (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t     KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tamdgpu_amdkfd_gpuvm_unpin_bo(mem->bo);\n\t}\n\n\tmapped_to_gpu_memory = mem->mapped_to_gpu_memory;\n\tis_imported = mem->is_imported;\n\tmutex_unlock(&mem->lock);\n\t/* lock is not needed after this, since mem is unused and will\n\t * be freed anyway\n\t */\n\n\tif (mapped_to_gpu_memory > 0) {\n\t\tpr_debug(\"BO VA 0x%llx size 0x%lx is still mapped.\\n\",\n\t\t\t\tmem->va, bo_size);\n\t\treturn -EBUSY;\n\t}\n\n\t/* Make sure restore workers don't access the BO any more */\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n\n\t/* Cleanup user pages and MMU notifiers */\n\tif (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\tamdgpu_hmm_unregister(mem->bo);\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tamdgpu_ttm_tt_discard_user_pages(mem->bo->tbo.ttm, mem->range);\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t}\n\n\tret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\tprocess_info->eviction_fence);\n\tpr_debug(\"Release VA 0x%llx - 0x%llx\\n\", mem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue));\n\n\t/* Remove from VM internal data structures */\n\tlist_for_each_entry_safe(entry, tmp, &mem->attachments, list) {\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t\tkfd_mem_detach(entry);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\t/* Free the sync object */\n\tamdgpu_sync_free(&mem->sync);\n\n\t/* If the SG is not NULL, it's one we created for a doorbell or mmio\n\t * remap BO. We need to free it.\n\t */\n\tif (mem->bo->tbo.sg) {\n\t\tsg_free_table(mem->bo->tbo.sg);\n\t\tkfree(mem->bo->tbo.sg);\n\t}\n\n\t/* Update the size of the BO being freed if it was allocated from\n\t * VRAM and is not imported. For APP APU VRAM allocations are done\n\t * in GTT domain\n\t */\n\tif (size) {\n\t\tif (!is_imported &&\n\t\t   (mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM ||\n\t\t   ((adev->flags & AMD_IS_APU) &&\n\t\t    mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT)))\n\t\t\t*size = bo_size;\n\t\telse\n\t\t\t*size = 0;\n\t}\n\n\t/* Free the BO*/\n\tdrm_vma_node_revoke(&mem->bo->tbo.base.vma_node, drm_priv);\n\tdrm_gem_handle_delete(adev->kfd.client.file, mem->gem_handle);\n\tif (mem->dmabuf) {\n\t\tdma_buf_put(mem->dmabuf);\n\t\tmem->dmabuf = NULL;\n\t}\n\tmutex_destroy(&mem->lock);\n\n\t/* If this releases the last reference, it will end up calling\n\t * amdgpu_amdkfd_release_notify and kfree the mem struct. That's why\n\t * this needs to be the last call here.\n\t */\n\tdrm_gem_object_put(&mem->bo->tbo.base);\n\n\t/*\n\t * For kgd_mem allocated in amdgpu_amdkfd_gpuvm_import_dmabuf(),\n\t * explicitly free it here.\n\t */\n\tif (!use_release_notifier)\n\t\tkfree(mem);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tvoid *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tint ret;\n\tstruct amdgpu_bo *bo;\n\tuint32_t domain;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned long bo_size;\n\tbool is_invalid_userptr = false;\n\n\tbo = mem->bo;\n\tif (!bo) {\n\t\tpr_err(\"Invalid BO when mapping memory to GPU\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Make sure restore is not running concurrently. Since we\n\t * don't map invalid userptr BOs, we rely on the next restore\n\t * worker to do the mapping\n\t */\n\tmutex_lock(&mem->process_info->lock);\n\n\t/* Lock notifier lock. If we find an invalid userptr BO, we can be\n\t * sure that the MMU notifier is no longer running\n\t * concurrently and the queues are actually stopped\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tmutex_lock(&mem->process_info->notifier_lock);\n\t\tis_invalid_userptr = !!mem->invalid;\n\t\tmutex_unlock(&mem->process_info->notifier_lock);\n\t}\n\n\tmutex_lock(&mem->lock);\n\n\tdomain = mem->domain;\n\tbo_size = bo->tbo.base.size;\n\n\tpr_debug(\"Map VA 0x%llx - 0x%llx to vm %p domain %s\\n\",\n\t\t\tmem->va,\n\t\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\t\tavm, domain_string(domain));\n\n\tif (!kfd_mem_is_attached(avm, mem)) {\n\t\tret = kfd_mem_attach(adev, mem, avm, mem->aql_queue);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = reserve_bo_and_vm(mem, avm, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\t/* Userptr can be marked as \"not invalid\", but not actually be\n\t * validated yet (still in the system domain). In that case\n\t * the queues are still stopped and we can leave mapping for\n\t * the next restore worker\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_SYSTEM)\n\t\tis_invalid_userptr = true;\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto out_unreserve;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t map VA 0x%llx - 0x%llx in entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = map_bo_to_gpuvm(mem, entry, ctx.sync,\n\t\t\t\t      is_invalid_userptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map bo to gpuvm\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tret = vm_update_pds(avm, ctx.sync);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to update page directories\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tentry->is_mapped = true;\n\t\tmem->mapped_to_gpu_memory++;\n\t\tpr_debug(\"\\t INC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\tgoto out;\n\nout_unreserve:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->process_info->lock);\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_dmaunmap_mem(struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_vm *vm;\n\tint ret;\n\n\tvm = drm_priv_to_vm(drm_priv);\n\n\tmutex_lock(&mem->lock);\n\n\tret = amdgpu_bo_reserve(mem->bo, true);\n\tif (ret)\n\t\tgoto out;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != vm)\n\t\t\tcontinue;\n\t\tif (entry->bo_va->base.bo->tbo.ttm &&\n\t\t    !entry->bo_va->base.bo->tbo.ttm->sg)\n\t\t\tcontinue;\n\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t}\n\n\tamdgpu_bo_unreserve(mem->bo);\nout:\n\tmutex_unlock(&mem->lock);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\n\tret = reserve_bo_and_cond_vms(mem, avm, BO_VM_MAPPED, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\t/* If no VMs were reserved, it means the BO wasn't actually mapped */\n\tif (ctx.n_vms == 0) {\n\t\tret = -EINVAL;\n\t\tgoto unreserve_out;\n\t}\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto unreserve_out;\n\n\tpr_debug(\"Unmap VA 0x%llx - 0x%llx from vm %p\\n\",\n\t\tmem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\tavm);\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || !entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t unmap VA 0x%llx - 0x%llx from entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tunmap_bo_from_gpuvm(mem, entry, ctx.sync);\n\t\tentry->is_mapped = false;\n\n\t\tmem->mapped_to_gpu_memory--;\n\t\tpr_debug(\"\\t DEC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\nunreserve_out:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_sync_memory(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tmutex_lock(&mem->lock);\n\tamdgpu_sync_clone(&mem->sync, &sync);\n\tmutex_unlock(&mem->lock);\n\n\tret = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_map_gtt_bo_to_gart - Map BO to GART and increment reference count\n * @bo: Buffer object to be mapped\n *\n * Before return, bo reference count is incremented. To release the reference and unpin/\n * unmap the BO, call amdgpu_amdkfd_free_gtt_mem.\n */\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo)\n{\n\tint ret;\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto err_reserve_bo_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto err_pin_bo_failed;\n\t}\n\n\tret = amdgpu_ttm_alloc_gart(&bo->tbo);\n\tif (ret) {\n\t\tpr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\n\t\tgoto err_map_bo_gart_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, bo->vm_bo->vm->process_info->eviction_fence);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tbo = amdgpu_bo_ref(bo);\n\n\treturn 0;\n\nerr_map_bo_gart_failed:\n\tamdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\n\tamdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Map a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be mapped for CPU access\n * @kptr[out]: pointer in kernel CPU address space\n * @size[out]: size of the buffer\n *\n * Pins the BO and maps it for kernel CPU access. The eviction fence is removed\n * from the BO, since pinned BOs cannot be evicted. The bo must remain on the\n * validate_list, so the GPU mapping can be restored after a page table was\n * evicted.\n *\n * Return: 0 on success, error code on failure\n */\nint amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_mem *mem,\n\t\t\t\t\t     void **kptr, uint64_t *size)\n{\n\tint ret;\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tpr_err(\"userptr can't be mapped to kernel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&mem->process_info->lock);\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto bo_reserve_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto pin_failed;\n\t}\n\n\tret = amdgpu_bo_kmap(bo, kptr);\n\tif (ret) {\n\t\tpr_err(\"Failed to map bo to kernel. ret %d\\n\", ret);\n\t\tgoto kmap_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, mem->process_info->eviction_fence);\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tmutex_unlock(&mem->process_info->lock);\n\treturn 0;\n\nkmap_failed:\n\tamdgpu_bo_unpin(bo);\npin_failed:\n\tamdgpu_bo_unreserve(bo);\nbo_reserve_failed:\n\tmutex_unlock(&mem->process_info->lock);\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Unmap a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be unmapped for CPU access\n *\n * Removes the kernel CPU mapping and unpins the BO. It does not restore the\n * eviction fence, so this function should only be used for cleanup before the\n * BO is destroyed.\n */\nvoid amdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(struct kgd_mem *mem)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct kfd_vm_fault_info *mem)\n{\n\tif (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {\n\t\t*mem = *adev->gmc.vm_fault_info;\n\t\tmb(); /* make sure read happened */\n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\t}\n\treturn 0;\n}\n\nstatic int import_obj_create(struct amdgpu_device *adev,\n\t\t\t     struct dma_buf *dma_buf,\n\t\t\t     struct drm_gem_object *obj,\n\t\t\t     uint64_t va, void *drm_priv,\n\t\t\t     struct kgd_mem **mem, uint64_t *size,\n\t\t\t     uint64_t *mmap_offset)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tbo = gem_to_amdgpu_bo(obj);\n\tif (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT)))\n\t\t/* Only VRAM and GTT BOs are supported */\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tret = drm_vma_node_allow(&obj->vma_node, drm_priv);\n\tif (ret)\n\t\tgoto err_free_mem;\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tif (mmap_offset)\n\t\t*mmap_offset = amdgpu_bo_mmap_offset(bo);\n\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\n\t(*mem)->alloc_flags =\n\t\t((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?\n\t\tKFD_IOC_ALLOC_MEM_FLAGS_VRAM : KFD_IOC_ALLOC_MEM_FLAGS_GTT)\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\n\tget_dma_buf(dma_buf);\n\t(*mem)->dmabuf = dma_buf;\n\t(*mem)->bo = bo;\n\t(*mem)->va = va;\n\t(*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&\n\t\t\t !(adev->flags & AMD_IS_APU) ?\n\t\t\t AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;\n\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\t(*mem)->is_imported = true;\n\n\tmutex_lock(&avm->process_info->lock);\n\tif (avm->process_info->eviction_fence &&\n\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, (*mem)->domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\tmutex_unlock(&avm->process_info->lock);\n\tif (ret)\n\t\tgoto err_remove_mem;\n\n\treturn 0;\n\nerr_remove_mem:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_vma_node_revoke(&obj->vma_node, drm_priv);\nerr_free_mem:\n\tkfree(*mem);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n\t\t\t\t\t uint64_t va, void *drm_priv,\n\t\t\t\t\t struct kgd_mem **mem, uint64_t *size,\n\t\t\t\t\t uint64_t *mmap_offset)\n{\n\tstruct drm_gem_object *obj;\n\tuint32_t handle;\n\tint ret;\n\n\tret = drm_gem_prime_fd_to_handle(&adev->ddev, adev->kfd.client.file, fd,\n\t\t\t\t\t &handle);\n\tif (ret)\n\t\treturn ret;\n\tobj = drm_gem_object_lookup(adev->kfd.client.file, handle);\n\tif (!obj) {\n\t\tret = -EINVAL;\n\t\tgoto err_release_handle;\n\t}\n\n\tret = import_obj_create(adev, obj->dma_buf, obj, va, drm_priv, mem, size,\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\nerr_release_handle:\n\tdrm_gem_handle_delete(adev->kfd.client.file, handle);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_mem *mem,\n\t\t\t\t      struct dma_buf **dma_buf)\n{\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\tgoto out;\n\n\tget_dma_buf(mem->dmabuf);\n\t*dma_buf = mem->dmabuf;\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\n/* Evict a userptr BO by stopping the queues if necessary\n *\n * Runs in MMU notifier, may be in RECLAIM_FS context. This means it\n * cannot do any memory allocations, and cannot take any locks that\n * are held elsewhere while allocating memory.\n *\n * It doesn't do anything to the BO itself. The real work happens in\n * restore, where we get updated page addresses. This function only\n * ensures that GPU access to the BO is stopped.\n */\nint amdgpu_amdkfd_evict_userptr(struct mmu_interval_notifier *mni,\n\t\t\t\tunsigned long cur_seq, struct kgd_mem *mem)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tint r = 0;\n\n\t/* Do not process MMU notifications during CRIU restore until\n\t * KFD_CRIU_OP_RESUME IOCTL is received\n\t */\n\tif (READ_ONCE(process_info->block_mmu_notifications))\n\t\treturn 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmem->invalid++;\n\tif (++process_info->evicted_bos == 1) {\n\t\t/* First eviction, stop the queues */\n\t\tr = kgd2kfd_quiesce_mm(mni->mm,\n\t\t\t\t       KFD_QUEUE_EVICTION_TRIGGER_USERPTR);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to quiesce KFD\\n\");\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\t}\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn r;\n}\n\n/* Update invalid userptr BOs\n *\n * Moves invalidated (evicted) userptr BOs from userptr_valid_list to\n * userptr_inval_list and updates user pages for all BOs that have\n * been invalidated since their last update.\n */\nstatic int update_invalid_user_pages(struct amdkfd_process_info *process_info,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tuint32_t invalid;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\n\t/* Move all invalidated BOs to the userptr_inval_list */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_valid_list,\n\t\t\t\t validate_list)\n\t\tif (mem->invalid)\n\t\t\tlist_move_tail(&mem->validate_list,\n\t\t\t\t       &process_info->userptr_inval_list);\n\n\t/* Go through userptr_inval_list and update any invalid user_pages */\n\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t    validate_list) {\n\t\tinvalid = mem->invalid;\n\t\tif (!invalid)\n\t\t\t/* BO hasn't been invalidated since the last\n\t\t\t * revalidation attempt. Keep its page list.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\tbo = mem->bo;\n\n\t\tamdgpu_ttm_tt_discard_user_pages(bo->tbo.ttm, mem->range);\n\t\tmem->range = NULL;\n\n\t\t/* BO reservations and getting user pages (hmm_range_fault)\n\t\t * must happen outside the notifier lock\n\t\t */\n\t\tmutex_unlock(&process_info->notifier_lock);\n\n\t\t/* Move the BO to system (CPU) domain if necessary to unmap\n\t\t * and free the SG table\n\t\t */\n\t\tif (bo->tbo.resource->mem_type != TTM_PL_SYSTEM) {\n\t\t\tif (amdgpu_bo_reserve(bo, true))\n\t\t\t\treturn -EAGAIN;\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Failed to invalidate userptr BO\\n\",\n\t\t\t\t       __func__);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\t/* Get updated user pages */\n\t\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t   &mem->range);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed %d to get user pages\\n\", ret);\n\n\t\t\t/* Return -EFAULT bad address error as success. It will\n\t\t\t * fail later with a VM fault if the GPU tries to access\n\t\t\t * it. Better than hanging indefinitely with stalled\n\t\t\t * user mode queues.\n\t\t\t *\n\t\t\t * Return other error -EBUSY or -ENOMEM to retry restore\n\t\t\t */\n\t\t\tif (ret != -EFAULT)\n\t\t\t\treturn ret;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\tmutex_lock(&process_info->notifier_lock);\n\n\t\t/* Mark the BO as valid unless it was invalidated\n\t\t * again concurrently.\n\t\t */\n\t\tif (mem->invalid != invalid) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t /* set mem valid if mem has hmm range associated */\n\t\tif (mem->range)\n\t\t\tmem->invalid = 0;\n\t}\n\nunlock_out:\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn ret;\n}\n\n/* Validate invalid userptr BOs\n *\n * Validates BOs on the userptr_inval_list. Also updates GPUVM page tables\n * with new page addresses and waits for the page table updates to complete.\n */\nstatic int validate_invalid_user_pages(struct amdkfd_process_info *process_info)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0, 0);\n\t/* Reserve all BOs and page tables for validation */\n\tdrm_exec_until_all_locked(&exec) {\n\t\t/* Reserve all the page directories */\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\t/* Reserve the userptr_inval_list entries to resv_list */\n\t\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\tret = process_validate_vms(process_info, NULL);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\t/* Validate BOs and update GPUVM page tables */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tbo = mem->bo;\n\n\t\t/* Validate the BO if we got user pages */\n\t\tif (bo->tbo.ttm->pages[0]) {\n\t\t\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\n\t\t/* Update mapping. If the BO was not validated\n\t\t * (because we couldn't get user pages), this will\n\t\t * clear the page table entries, which will result in\n\t\t * VM faults if the GPU tries to access the invalid\n\t\t * memory.\n\t\t */\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: update PTE failed\\n\", __func__);\n\t\t\t\t/* make sure this gets validated again */\n\t\t\t\tmutex_lock(&process_info->notifier_lock);\n\t\t\t\tmem->invalid++;\n\t\t\t\tmutex_unlock(&process_info->notifier_lock);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync);\n\nunreserve_out:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_wait(&sync, false);\n\tamdgpu_sync_free(&sync);\n\n\treturn ret;\n}\n\n/* Confirm that all user pages are valid while holding the notifier lock\n *\n * Moves valid BOs from the userptr_inval_list back to userptr_val_list.\n */\nstatic int confirm_valid_user_pages_locked(struct amdkfd_process_info *process_info)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tbool valid;\n\n\t\t/* keep mem without hmm range at userptr_inval_list */\n\t\tif (!mem->range)\n\t\t\tcontinue;\n\n\t\t/* Only check mem with hmm range associated */\n\t\tvalid = amdgpu_ttm_tt_get_user_pages_done(\n\t\t\t\t\tmem->bo->tbo.ttm, mem->range);\n\n\t\tmem->range = NULL;\n\t\tif (!valid) {\n\t\t\tWARN(!mem->invalid, \"Invalid BO not marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem->invalid) {\n\t\t\tWARN(1, \"Valid BO is marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move_tail(&mem->validate_list,\n\t\t\t       &process_info->userptr_valid_list);\n\t}\n\n\treturn ret;\n}\n\n/* Worker callback to restore evicted userptr BOs\n *\n * Tries to update and validate all userptr BOs. If successful and no\n * concurrent evictions happened, the queues are restarted. Otherwise,\n * reschedule for another attempt later.\n */\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info =\n\t\tcontainer_of(dwork, struct amdkfd_process_info,\n\t\t\t     restore_userptr_work);\n\tstruct task_struct *usertask;\n\tstruct mm_struct *mm;\n\tuint32_t evicted_bos;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tevicted_bos = process_info->evicted_bos;\n\tmutex_unlock(&process_info->notifier_lock);\n\tif (!evicted_bos)\n\t\treturn;\n\n\t/* Reference task and mm in case of concurrent process termination */\n\tusertask = get_pid_task(process_info->pid, PIDTYPE_PID);\n\tif (!usertask)\n\t\treturn;\n\tmm = get_task_mm(usertask);\n\tif (!mm) {\n\t\tput_task_struct(usertask);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\n\tif (update_invalid_user_pages(process_info, mm))\n\t\tgoto unlock_out;\n\t/* userptr_inval_list can be empty if all evicted userptr BOs\n\t * have been freed. In that case there is nothing to validate\n\t * and we can just restart the queues.\n\t */\n\tif (!list_empty(&process_info->userptr_inval_list)) {\n\t\tif (validate_invalid_user_pages(process_info))\n\t\t\tgoto unlock_out;\n\t}\n\t/* Final check for concurrent evicton and atomic update. If\n\t * another eviction happens after successful update, it will\n\t * be a first eviction that calls quiesce_mm. The eviction\n\t * reference counting inside KFD will handle this case.\n\t */\n\tmutex_lock(&process_info->notifier_lock);\n\tif (process_info->evicted_bos != evicted_bos)\n\t\tgoto unlock_notifier_out;\n\n\tif (confirm_valid_user_pages_locked(process_info)) {\n\t\tWARN(1, \"User pages unexpectedly invalid\");\n\t\tgoto unlock_notifier_out;\n\t}\n\n\tprocess_info->evicted_bos = evicted_bos = 0;\n\n\tif (kgd2kfd_resume_mm(mm)) {\n\t\tpr_err(\"%s: Failed to resume KFD\\n\", __func__);\n\t\t/* No recovery from this failure. Probably the CP is\n\t\t * hanging. No point trying again.\n\t\t */\n\t}\n\nunlock_notifier_out:\n\tmutex_unlock(&process_info->notifier_lock);\nunlock_out:\n\tmutex_unlock(&process_info->lock);\n\n\t/* If validation failed, reschedule another attempt */\n\tif (evicted_bos) {\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n\tput_task_struct(usertask);\n}\n\nstatic void replace_eviction_fence(struct dma_fence __rcu **ef,\n\t\t\t\t   struct dma_fence *new_ef)\n{\n\tstruct dma_fence *old_ef = rcu_replace_pointer(*ef, new_ef, true\n\t\t/* protected by process_info->lock */);\n\n\t/* If we're replacing an unsignaled eviction fence, that fence will\n\t * never be signaled, and if anyone is still waiting on that fence,\n\t * they will hang forever. This should never happen. We should only\n\t * replace the fence in restore_work that only gets scheduled after\n\t * eviction work signaled the fence.\n\t */\n\tWARN_ONCE(!dma_fence_is_signaled(old_ef),\n\t\t  \"Replacing unsignaled eviction fence\");\n\tdma_fence_put(old_ef);\n}\n\n/** amdgpu_amdkfd_gpuvm_restore_process_bos - Restore all BOs for the given\n *   KFD process identified by process_info\n *\n * @process_info: amdkfd_process_info of the KFD process\n *\n * After memory eviction, restore thread calls this function. The function\n * should be called when the Process is still valid. BO restore involves -\n *\n * 1.  Release old eviction fence and create new one\n * 2.  Get two copies of PD BO list from all the VMs. Keep one copy as pd_list.\n * 3   Use the second PD list and kfd_bo_list to create a list (ctx.list) of\n *     BOs that need to be reserved.\n * 4.  Reserve all the BOs\n * 5.  Validate of PD and PT BOs.\n * 6.  Validate all KFD BOs using kfd_bo_list and Map them and add new fence\n * 7.  Add fence to all PD and PT BOs.\n * 8.  Unreserve all BOs\n */\nint amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct dma_fence __rcu **ef)\n{\n\tstruct amdkfd_process_info *process_info = info;\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem;\n\tstruct list_head duplicate_save;\n\tstruct amdgpu_sync sync_obj;\n\tunsigned long failed_size = 0;\n\tunsigned long total_size = 0;\n\tstruct drm_exec exec;\n\tint ret;\n\n\tINIT_LIST_HEAD(&duplicate_save);\n\n\tmutex_lock(&process_info->lock);\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"Locking VM PD failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Reserve all BOs and page tables/directory. Add all BOs from\n\t\t * kfd_bo_list to ctx.list\n\t\t */\n\t\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"drm_exec_prepare_obj failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tamdgpu_sync_create(&sync_obj);\n\n\t/* Validate BOs managed by KFD */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\n\t\tstruct amdgpu_bo *bo = mem->bo;\n\t\tuint32_t domain = mem->domain;\n\t\tstruct dma_resv_iter cursor;\n\t\tstruct dma_fence *fence;\n\n\t\ttotal_size += amdgpu_bo_size(bo);\n\n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: Validate BOs failed\\n\");\n\t\t\tfailed_size += amdgpu_bo_size(bo);\n\t\t\tret = amdgpu_amdkfd_bo_validate(bo,\n\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT, false);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tdma_resv_for_each_fence(&cursor, bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_KERNEL, fence) {\n\t\t\tret = amdgpu_sync_fence(&sync_obj, fence);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Sync BO fence failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (failed_size)\n\t\tpr_debug(\"0x%lx/0x%lx in system\\n\", failed_size, total_size);\n\n\t/* Validate PDs, PTs and evicted DMABuf imports last. Otherwise BO\n\t * validations above would invalidate DMABuf imports again.\n\t */\n\tret = process_validate_vms(process_info, &exec.ticket);\n\tif (ret) {\n\t\tpr_debug(\"Validating VMs failed, ret: %d\\n\", ret);\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Update mappings managed by KFD. */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync_obj);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: update PTE failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update mappings not managed by KFD */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\tvm_list_node) {\n\t\tstruct amdgpu_device *adev = amdgpu_ttm_adev(\n\t\t\tpeer_vm->root.bo->tbo.bdev);\n\n\t\tret = amdgpu_vm_handle_moved(adev, peer_vm, &exec.ticket);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: handle moved failed. Try again\\n\");\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: update PDs failed. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Sync with fences on all the page tables. They implicitly depend on any\n\t * move fences from amdgpu_vm_handle_moved above.\n\t */\n\tret = process_sync_pds_resv(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: Failed to sync to PD BO moving fence. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Wait for validate and PT updates to finish */\n\tamdgpu_sync_wait(&sync_obj, false);\n\n\t/* The old eviction fence may be unsignaled if restore happens\n\t * after a GPU reset or suspend/resume. Keep the old fence in that\n\t * case. Otherwise release the old eviction fence and create new\n\t * one, because fence only goes from unsignaled to signaled once\n\t * and cannot be reused. Use context and mm from the old fence.\n\t *\n\t * If an old eviction fence signals after this check, that's OK.\n\t * Anyone signaling an eviction fence must stop the queues first\n\t * and schedule another restore worker.\n\t */\n\tif (dma_fence_is_signaled(&process_info->eviction_fence->base)) {\n\t\tstruct amdgpu_amdkfd_fence *new_fence =\n\t\t\tamdgpu_amdkfd_fence_create(\n\t\t\t\tprocess_info->eviction_fence->base.context,\n\t\t\t\tprocess_info->eviction_fence->mm,\n\t\t\t\tNULL);\n\n\t\tif (!new_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tprocess_info->eviction_fence = new_fence;\n\t\treplace_eviction_fence(ef, dma_fence_get(&new_fence->base));\n\t} else {\n\t\tWARN_ONCE(*ef != &process_info->eviction_fence->base,\n\t\t\t  \"KFD eviction fence doesn't match KGD process_info\");\n\t}\n\n\t/* Attach new eviction fence to all BOs except pinned ones */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list, validate_list) {\n\t\tif (mem->bo->tbo.pin_count)\n\t\t\tcontinue;\n\n\t\tdma_resv_add_fence(mem->bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\t/* Attach eviction fence to PD / PT BOs and DMABuf imports */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *bo = peer_vm->root.bo;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\nvalidate_map_fail:\n\tamdgpu_sync_free(&sync_obj);\nttm_reserve_fail:\n\tdrm_exec_fini(&exec);\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_add_gws_to_process(void *info, void *gws, struct kgd_mem **mem)\n{\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct amdgpu_bo *gws_bo = (struct amdgpu_bo *)gws;\n\tint ret;\n\n\tif (!info || !gws)\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&(*mem)->lock);\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\t(*mem)->bo = amdgpu_bo_ref(gws_bo);\n\t(*mem)->domain = AMDGPU_GEM_DOMAIN_GWS;\n\t(*mem)->process_info = process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\n\t/* Validate gws bo the first time it is added to process */\n\tmutex_lock(&(*mem)->process_info->lock);\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\tgoto bo_reservation_failure;\n\t}\n\n\tret = amdgpu_amdkfd_bo_validate(gws_bo, AMDGPU_GEM_DOMAIN_GWS, true);\n\tif (ret) {\n\t\tpr_err(\"GWS BO validate failed %d\\n\", ret);\n\t\tgoto bo_validation_failure;\n\t}\n\t/* GWS resource is shared b/t amdgpu and amdkfd\n\t * Add process eviction fence to bo so they can\n\t * evict each other.\n\t */\n\tret = dma_resv_reserve_fences(gws_bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(gws_bo->tbo.base.resv,\n\t\t\t   &process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(gws_bo);\n\tmutex_unlock(&(*mem)->process_info->lock);\n\n\treturn ret;\n\nreserve_shared_fail:\nbo_validation_failure:\n\tamdgpu_bo_unreserve(gws_bo);\nbo_reservation_failure:\n\tmutex_unlock(&(*mem)->process_info->lock);\n\tamdgpu_sync_free(&(*mem)->sync);\n\tremove_kgd_mem_from_kfd_bo_list(*mem, process_info);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&(*mem)->lock);\n\tkfree(*mem);\n\t*mem = NULL;\n\treturn ret;\n}\n\nint amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)\n{\n\tint ret;\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\tstruct amdgpu_bo *gws_bo = kgd_mem->bo;\n\n\t/* Remove BO from process's validate list so restore worker won't touch\n\t * it anymore\n\t */\n\tremove_kgd_mem_from_kfd_bo_list(kgd_mem, process_info);\n\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\t//TODO add BO back to validate_list?\n\t\treturn ret;\n\t}\n\tamdgpu_amdkfd_remove_eviction_fence(gws_bo,\n\t\t\tprocess_info->eviction_fence);\n\tamdgpu_bo_unreserve(gws_bo);\n\tamdgpu_sync_free(&kgd_mem->sync);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&kgd_mem->lock);\n\tkfree(mem);\n\treturn 0;\n}\n\n/* Returns GPU-specific tiling mode information */\nint amdgpu_amdkfd_get_tile_config(struct amdgpu_device *adev,\n\t\t\t\tstruct tile_config *config)\n{\n\tconfig->gb_addr_config = adev->gfx.config.gb_addr_config;\n\tconfig->tile_config_ptr = adev->gfx.config.tile_mode_array;\n\tconfig->num_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.tile_mode_array);\n\tconfig->macro_tile_config_ptr =\n\t\t\tadev->gfx.config.macrotile_mode_array;\n\tconfig->num_macro_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.macrotile_mode_array);\n\n\t/* Those values are not set from GFX9 onwards */\n\tconfig->num_banks = adev->gfx.config.num_banks;\n\tconfig->num_ranks = adev->gfx.config.num_ranks;\n\n\treturn 0;\n}\n\nbool amdgpu_amdkfd_bo_mapped_to_dev(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->is_mapped && entry->adev == adev)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_kfd_mem_limits(struct seq_file *m, void *data)\n{\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tseq_printf(m, \"System mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.system_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_system_mem_limit >> 20));\n\tseq_printf(m, \"TTM mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.ttm_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_ttm_mem_limit >> 20));\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\treturn 0;\n}\n\n#endif\n", "source_code_path": "amdgpu_amdkfd_gpuvm.c", "line_number": 2464, "generated_diff": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_amdkfd_gpuvm.c\nLine: 2464\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\n====================================\nRead the following bug report.\n============ Bug Report ============\nFile: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription: Use of freed memory:\n *mem may have been freed inside of import_obj_create.\n\nConsider adding a check for NULL.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_amdkfd_gpuvm-bug.txt", "bug_report_text": "File: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription:The function amdgpu_amdkfd_gpuvm_import_dmabuf_fd may access memory that has already been freed. Specifically, the '*mem' pointer may have been released in the import_obj_create function, leaving it pointing to invalid memory. To prevent potential crashes or data corruption, a null check should be added to ensure '*mem' is valid before accessing it.\n", "diff_path": "amdgpu_amdkfd_gpuvm-diff.txt", "diff_text": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2014-2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include <linux/dma-buf.h>\n#include <linux/list.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/fdtable.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_object.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dma_buf.h\"\n#include <uapi/linux/kfd_ioctl.h>\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_smi_events.h\"\n\n/* Userptr restore delay, just long enough to allow consecutive VM\n * changes to accumulate\n */\n#define AMDGPU_USERPTR_RESTORE_DELAY_MS 1\n#define AMDGPU_RESERVE_MEM_LIMIT\t\t\t(3UL << 29)\n\n/*\n * Align VRAM availability to 2MB to avoid fragmentation caused by 4K allocations in the tail 2MB\n * BO chunk\n */\n#define VRAM_AVAILABLITY_ALIGN (1 << 21)\n\n/* Impose limit on how much memory KFD can use */\nstatic struct {\n\tuint64_t max_system_mem_limit;\n\tuint64_t max_ttm_mem_limit;\n\tint64_t system_mem_used;\n\tint64_t ttm_mem_used;\n\tspinlock_t mem_limit_lock;\n} kfd_mem_limit;\n\nstatic const char * const domain_bit_to_string[] = {\n\t\t\"CPU\",\n\t\t\"GTT\",\n\t\t\"VRAM\",\n\t\t\"GDS\",\n\t\t\"GWS\",\n\t\t\"OA\"\n};\n\n#define domain_string(domain) domain_bit_to_string[ffs(domain)-1]\n\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work);\n\nstatic bool kfd_mem_is_attached(struct amdgpu_vm *avm,\n\t\tstruct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list)\n\t\tif (entry->bo_va->base.vm == avm)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * reuse_dmamap() - Check whether adev can share the original\n * userptr BO\n *\n * If both adev and bo_adev are in direct mapping or\n * in the same iommu group, they can share the original BO.\n *\n * @adev: Device to which can or cannot share the original BO\n * @bo_adev: Device to which allocated BO belongs to\n *\n * Return: returns true if adev can share original userptr BO,\n * false otherwise.\n */\nstatic bool reuse_dmamap(struct amdgpu_device *adev, struct amdgpu_device *bo_adev)\n{\n\treturn (adev->ram_is_direct_mapped && bo_adev->ram_is_direct_mapped) ||\n\t\t\t(adev->dev->iommu_group == bo_adev->dev->iommu_group);\n}\n\n/* Set memory usage limits. Current, limits are\n *  System (TTM + userptr) memory - 15/16th System RAM\n *  TTM memory - 3/8th System RAM\n */\nvoid amdgpu_amdkfd_gpuvm_init_mem_limits(void)\n{\n\tstruct sysinfo si;\n\tuint64_t mem;\n\n\tif (kfd_mem_limit.max_system_mem_limit)\n\t\treturn;\n\n\tsi_meminfo(&si);\n\tmem = si.totalram - si.totalhigh;\n\tmem *= si.mem_unit;\n\n\tspin_lock_init(&kfd_mem_limit.mem_limit_lock);\n\tkfd_mem_limit.max_system_mem_limit = mem - (mem >> 6);\n\tif (kfd_mem_limit.max_system_mem_limit < 2 * AMDGPU_RESERVE_MEM_LIMIT)\n\t\tkfd_mem_limit.max_system_mem_limit >>= 1;\n\telse\n\t\tkfd_mem_limit.max_system_mem_limit -= AMDGPU_RESERVE_MEM_LIMIT;\n\n\tkfd_mem_limit.max_ttm_mem_limit = ttm_tt_pages_limit() << PAGE_SHIFT;\n\tpr_debug(\"Kernel memory limit %lluM, TTM limit %lluM\\n\",\n\t\t(kfd_mem_limit.max_system_mem_limit >> 20),\n\t\t(kfd_mem_limit.max_ttm_mem_limit >> 20));\n}\n\nvoid amdgpu_amdkfd_reserve_system_mem(uint64_t size)\n{\n\tkfd_mem_limit.system_mem_used += size;\n}\n\n/* Estimate page table size needed to represent a given memory size\n *\n * With 4KB pages, we need one 8 byte PTE for each 4KB of memory\n * (factor 512, >> 9). With 2MB pages, we need one 8 byte PTE for 2MB\n * of memory (factor 256K, >> 18). ROCm user mode tries to optimize\n * for 2MB pages for TLB efficiency. However, small allocations and\n * fragmented system memory still need some 4KB pages. We choose a\n * compromise that should work in most cases without reserving too\n * much memory for page tables unnecessarily (factor 16K, >> 14).\n */\n\n#define ESTIMATE_PT_SIZE(mem_size) max(((mem_size) >> 14), AMDGPU_VM_RESERVED_VRAM)\n\n/**\n * amdgpu_amdkfd_reserve_mem_limit() - Decrease available memory by size\n * of buffer.\n *\n * @adev: Device to which allocated BO belongs to\n * @size: Size of buffer, in bytes, encapsulated by B0. This should be\n * equivalent to amdgpu_bo_size(BO)\n * @alloc_flag: Flag used in allocating a BO as noted above\n * @xcp_id: xcp_id is used to get xcp from xcp manager, one xcp is\n * managed as one compute node in driver for app\n *\n * Return:\n *\treturns -ENOMEM in case of error, ZERO otherwise\n */\nint amdgpu_amdkfd_reserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tsize_t system_mem_needed, ttm_mem_needed, vram_needed;\n\tint ret = 0;\n\tuint64_t vram_size = 0;\n\n\tsystem_mem_needed = 0;\n\tttm_mem_needed = 0;\n\tvram_needed = 0;\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tsystem_mem_needed = size;\n\t\tttm_mem_needed = size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\t/*\n\t\t * Conservatively round up the allocation requirement to 2 MB\n\t\t * to avoid fragmentation caused by 4K allocations in the tail\n\t\t * 2M BO chunk.\n\t\t */\n\t\tvram_needed = size;\n\t\t/*\n\t\t * For GFX 9.4.3, get the VRAM size from XCP structs\n\t\t */\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\treturn -EINVAL;\n\n\t\tvram_size = KFD_XCP_MEMORY_SIZE(adev, xcp_id);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tsystem_mem_needed = size;\n\t\t\tttm_mem_needed = size;\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tsystem_mem_needed = size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (kfd_mem_limit.system_mem_used + system_mem_needed >\n\t    kfd_mem_limit.max_system_mem_limit)\n\t\tpr_debug(\"Set no_system_mem_limit=1 if using shared memory\\n\");\n\n\tif ((kfd_mem_limit.system_mem_used + system_mem_needed >\n\t     kfd_mem_limit.max_system_mem_limit && !no_system_mem_limit) ||\n\t    (kfd_mem_limit.ttm_mem_used + ttm_mem_needed >\n\t     kfd_mem_limit.max_ttm_mem_limit) ||\n\t    (adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] + vram_needed >\n\t     vram_size - reserved_for_pt - reserved_for_ras - atomic64_read(&adev->vram_pin_size))) {\n\t\tret = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\t/* Update memory accounting by decreasing available system\n\t * memory, TTM memory and GPU memory as computed above\n\t */\n\tWARN_ONCE(vram_needed && !adev,\n\t\t  \"adev reference can't be null when vram is used\");\n\tif (adev && xcp_id >= 0) {\n\t\tadev->kfd.vram_used[xcp_id] += vram_needed;\n\t\tadev->kfd.vram_used_aligned[xcp_id] +=\n\t\t\t\t(adev->flags & AMD_IS_APU) ?\n\t\t\t\tvram_needed :\n\t\t\t\tALIGN(vram_needed, VRAM_AVAILABLITY_ALIGN);\n\t}\n\tkfd_mem_limit.system_mem_used += system_mem_needed;\n\tkfd_mem_limit.ttm_mem_used += ttm_mem_needed;\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\treturn ret;\n}\n\nvoid amdgpu_amdkfd_unreserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tWARN_ONCE(!adev,\n\t\t\t  \"adev reference can't be null when alloc mem flags vram is set\");\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\tgoto release;\n\n\t\tif (adev) {\n\t\t\tadev->kfd.vram_used[xcp_id] -= size;\n\t\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -= size;\n\t\t\t\tkfd_mem_limit.system_mem_used -= size;\n\t\t\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t\t\t} else {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -=\n\t\t\t\t\tALIGN(size, VRAM_AVAILABLITY_ALIGN);\n\t\t\t}\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\tgoto release;\n\t}\n\tWARN_ONCE(adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] < 0,\n\t\t  \"KFD VRAM memory accounting unbalanced for xcp: %d\", xcp_id);\n\tWARN_ONCE(kfd_mem_limit.ttm_mem_used < 0,\n\t\t  \"KFD TTM memory accounting unbalanced\");\n\tWARN_ONCE(kfd_mem_limit.system_mem_used < 0,\n\t\t  \"KFD system memory accounting unbalanced\");\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n}\n\nvoid amdgpu_amdkfd_release_notify(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tu32 alloc_flags = bo->kfd_bo->alloc_flags;\n\tu64 size = amdgpu_bo_size(bo);\n\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, size, alloc_flags,\n\t\t\t\t\t  bo->xcp_id);\n\n\tkfree(bo->kfd_bo);\n}\n\n/**\n * create_dmamap_sg_bo() - Creates a amdgpu_bo object to reflect information\n * about USERPTR or DOOREBELL or MMIO BO.\n *\n * @adev: Device for which dmamap BO is being created\n * @mem: BO of peer device that is being DMA mapped. Provides parameters\n *\t in building the dmamap BO\n * @bo_out: Output parameter updated with handle of dmamap BO\n */\nstatic int\ncreate_dmamap_sg_bo(struct amdgpu_device *adev,\n\t\t struct kgd_mem *mem, struct amdgpu_bo **bo_out)\n{\n\tstruct drm_gem_object *gem_obj;\n\tint ret;\n\tuint64_t flags = 0;\n\n\tret = amdgpu_bo_reserve(mem->bo, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR)\n\t\tflags |= mem->bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t\t\tAMDGPU_GEM_CREATE_UNCACHED);\n\n\tret = amdgpu_gem_object_create(adev, mem->bo->tbo.base.size, 1,\n\t\t\tAMDGPU_GEM_DOMAIN_CPU, AMDGPU_GEM_CREATE_PREEMPTIBLE | flags,\n\t\t\tttm_bo_type_sg, mem->bo->tbo.base.resv, &gem_obj, 0);\n\n\tamdgpu_bo_unreserve(mem->bo);\n\n\tif (ret) {\n\t\tpr_err(\"Error in creating DMA mappable SG BO on domain: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\n\t*bo_out = gem_to_amdgpu_bo(gem_obj);\n\t(*bo_out)->parent = amdgpu_bo_ref(mem->bo);\n\treturn ret;\n}\n\n/* amdgpu_amdkfd_remove_eviction_fence - Removes eviction fence from BO's\n *  reservation object.\n *\n * @bo: [IN] Remove eviction fence(s) from this BO\n * @ef: [IN] This eviction fence is removed if it\n *  is present in the shared list.\n *\n * NOTE: Must be called with BO reserved i.e. bo->tbo.resv->lock held.\n */\nstatic int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tstruct amdgpu_amdkfd_fence *ef)\n{\n\tstruct dma_fence *replacement;\n\n\tif (!ef)\n\t\treturn -EINVAL;\n\n\t/* TODO: Instead of block before we should use the fence of the page\n\t * table update and TLB flush here directly.\n\t */\n\treplacement = dma_fence_get_stub();\n\tdma_resv_replace_fences(bo->tbo.base.resv, ef->base.context,\n\t\t\t\treplacement, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_fence_put(replacement);\n\treturn 0;\n}\n\nint amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo *root = bo;\n\tstruct amdgpu_vm_bo_base *vm_bo;\n\tstruct amdgpu_vm *vm;\n\tstruct amdkfd_process_info *info;\n\tstruct amdgpu_amdkfd_fence *ef;\n\tint ret;\n\n\t/* we can always get vm_bo from root PD bo.*/\n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tvm_bo = root->vm_bo;\n\tif (!vm_bo)\n\t\treturn 0;\n\n\tvm = vm_bo->vm;\n\tif (!vm)\n\t\treturn 0;\n\n\tinfo = vm->process_info;\n\tif (!info || !info->eviction_fence)\n\t\treturn 0;\n\n\tef = container_of(dma_fence_get(&info->eviction_fence->base),\n\t\t\tstruct amdgpu_amdkfd_fence, base);\n\n\tBUG_ON(!dma_resv_trylock(bo->tbo.base.resv));\n\tret = amdgpu_amdkfd_remove_eviction_fence(bo, ef);\n\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tdma_fence_put(&ef->base);\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,\n\t\t\t\t     bool wait)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint ret;\n\n\tif (WARN(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm),\n\t\t \"Called with userptr BO\"))\n\t\treturn -EINVAL;\n\n\t/* bo has been pinned, not need validate it */\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto validate_fail;\n\tif (wait)\n\t\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\nvalidate_fail:\n\treturn ret;\n}\n\nint amdgpu_amdkfd_bo_validate_and_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tuint32_t domain,\n\t\t\t\t\tstruct dma_fence *fence)\n{\n\tint ret = amdgpu_bo_reserve(bo, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = amdgpu_amdkfd_bo_validate(bo, domain, true);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tret = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tdma_resv_add_fence(bo->tbo.base.resv, fence,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\nunreserve_out:\n\tamdgpu_bo_unreserve(bo);\n\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_validate_vm_bo(void *_unused, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_amdkfd_bo_validate(bo, bo->allowed_domains, false);\n}\n\n/* vm_validate_pt_pd_bos - Validate page table and directory BOs\n *\n * Page directories are not updated here because huge page handling\n * during page table updates can invalidate page directory entries\n * again. Page directories are only updated after updating page\n * tables.\n */\nstatic int vm_validate_pt_pd_bos(struct amdgpu_vm *vm,\n\t\t\t\t struct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_validate(adev, vm, ticket,\n\t\t\t\t amdgpu_amdkfd_validate_vm_bo, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to validate PT BOs\\n\");\n\t\treturn ret;\n\t}\n\n\tvm->pd_phys_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\n\treturn 0;\n}\n\nstatic int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_sync_fence(sync, vm->last_update);\n}\n\nstatic uint64_t get_pte_flags(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tuint32_t mapping_flags = AMDGPU_VM_PAGE_READABLE |\n\t\t\t\t AMDGPU_VM_MTYPE_DEFAULT;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\treturn amdgpu_gem_va_map_flags(adev, mapping_flags);\n}\n\n/**\n * create_sg_table() - Create an sg_table for a contiguous DMA addr range\n * @addr: The starting address to point to\n * @size: Size of memory area in bytes being pointed to\n *\n * Allocates an instance of sg_table and initializes it to point to memory\n * area specified by input parameters. The address used to build is assumed\n * to be DMA mapped, if needed.\n *\n * DOORBELL or MMIO BOs use only one scatterlist node in their sg_table\n * because they are physically contiguous.\n *\n * Return: Initialized instance of SG Table or NULL\n */\nstatic struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n{\n\tstruct sg_table *sg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\n\tif (!sg)\n\t\treturn NULL;\n\tif (sg_alloc_table(sg, 1, GFP_KERNEL)) {\n\t\tkfree(sg);\n\t\treturn NULL;\n\t}\n\tsg_dma_address(sg->sgl) = addr;\n\tsg->sgl->length = size;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\tsg->sgl->dma_length = size;\n#endif\n\treturn sg;\n}\n\nstatic int\nkfd_mem_dmamap_userptr(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *src_ttm = mem->bo->tbo.ttm;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tint ret;\n\n\tif (WARN_ON(ttm->num_pages != src_ttm->num_pages))\n\t\treturn -EINVAL;\n\n\tttm->sg = kmalloc(sizeof(*ttm->sg), GFP_KERNEL);\n\tif (unlikely(!ttm->sg))\n\t\treturn -ENOMEM;\n\n\t/* Same sequence as in amdgpu_ttm_tt_pin_userptr */\n\tret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,\n\t\t\t\t\tttm->num_pages, 0,\n\t\t\t\t\t(u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\tret = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (unlikely(ret))\n\t\tgoto release_sg;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto unmap_sg;\n\n\treturn 0;\n\nunmap_sg:\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\nrelease_sg:\n\tpr_err(\"DMA map userptr failed: %d\\n\", ret);\n\tsg_free_table(ttm->sg);\nfree_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tint ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n/**\n * kfd_mem_dmamap_sg_bo() - Create DMA mapped sg_table to access DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * An access request from the device that owns DOORBELL does not require DMA mapping.\n * This is because the request doesn't go through PCIe root complex i.e. it instead\n * loops back. The need to DMA map arises only when accessing peer device's DOORBELL\n *\n * In contrast, all access requests for MMIO need to be DMA mapped without regard to\n * device ownership. This is because access requests for MMIO go through PCIe root\n * complex.\n *\n * This is accomplished in two steps:\n *   - Obtain DMA mapped address of DOORBELL or MMIO memory that could be used\n *         in updating requesting device's page table\n *   - Signal TTM to mark memory pointed to by requesting device's BO as GPU\n *         accessible. This allows an update of requesting device's page table\n *         with entries associated with DOOREBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *   - Mapping of DOORBELL or MMIO BO of same or peer device\n *   - Validating an evicted DOOREBELL or MMIO BO on device seeking access\n *\n * Return: ZERO if successful, NON-ZERO otherwise\n */\nstatic int\nkfd_mem_dmamap_sg_bo(struct kgd_mem *mem,\n\t\t     struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\tdma_addr_t dma_addr;\n\tbool mmio;\n\tint ret;\n\n\t/* Expect SG Table of dmapmap BO to be NULL */\n\tmmio = (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);\n\tif (unlikely(ttm->sg)) {\n\t\tpr_err(\"SG Table of %d BO for peer device is UNEXPECTEDLY NON-NULL\", mmio);\n\t\treturn -EINVAL;\n\t}\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_addr = mem->bo->tbo.sg->sgl->dma_address;\n\tpr_debug(\"%d BO size: %d\\n\", mmio, mem->bo->tbo.sg->sgl->length);\n\tpr_debug(\"%d BO address before DMA mapping: %llx\\n\", mmio, dma_addr);\n\tdma_addr = dma_map_resource(adev->dev, dma_addr,\n\t\t\tmem->bo->tbo.sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tret = dma_mapping_error(adev->dev, dma_addr);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tpr_debug(\"%d BO address after DMA mapping: %llx\\n\", mmio, dma_addr);\n\n\tttm->sg = create_sg_table(dma_addr, mem->bo->tbo.sg->sgl->length);\n\tif (unlikely(!ttm->sg)) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\treturn ret;\n\nfree_sg:\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\nunmap_sg:\n\tdma_unmap_resource(adev->dev, dma_addr, mem->bo->tbo.sg->sgl->length,\n\t\t\t   dir, DMA_ATTR_SKIP_CPU_SYNC);\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_attachment(struct kgd_mem *mem,\n\t\t\t  struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\treturn 0;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\treturn kfd_mem_dmamap_userptr(mem, attachment);\n\tcase KFD_MEM_ATT_DMABUF:\n\t\treturn kfd_mem_dmamap_dmabuf(attachment);\n\tcase KFD_MEM_ATT_SG:\n\t\treturn kfd_mem_dmamap_sg_bo(mem, attachment);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nkfd_mem_dmaunmap_userptr(struct kgd_mem *mem,\n\t\t\t struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = false};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\n\tif (unlikely(!ttm->sg))\n\t\treturn;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\t/* This is a no-op. We don't want to trigger eviction fences when\n\t * unmapping DMABufs. Therefore the invalidation (moving to system\n\t * domain) is done in kfd_mem_dmamap_dmabuf.\n\t */\n}\n\n/**\n * kfd_mem_dmaunmap_sg_bo() - Free DMA mapped sg_table of DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * The method performs following steps:\n *   - Signal TTM to mark memory pointed to by BO as GPU inaccessible\n *   - Free SG Table that is used to encapsulate DMA mapped memory of\n *          peer device's DOORBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *     UNMapping of DOORBELL or MMIO BO on a device having access to its memory\n *     Eviction of DOOREBELL or MMIO BO on device having access to its memory\n *\n * Return: void\n */\nstatic void\nkfd_mem_dmaunmap_sg_bo(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\n\tif (unlikely(!ttm->sg)) {\n\t\tpr_debug(\"SG Table of BO is NULL\");\n\t\treturn;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_unmap_resource(adev->dev, ttm->sg->sgl->dma_address,\n\t\t\tttm->sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\tbo->tbo.sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_attachment(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\tbreak;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\tkfd_mem_dmaunmap_userptr(mem, attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_DMABUF:\n\t\tkfd_mem_dmaunmap_dmabuf(attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_SG:\n\t\tkfd_mem_dmaunmap_sg_bo(mem, attachment);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic int kfd_mem_export_dmabuf(struct kgd_mem *mem)\n{\n\tif (!mem->dmabuf) {\n\t\tstruct amdgpu_device *bo_adev;\n\t\tstruct dma_buf *dmabuf;\n\t\tint r, fd;\n\n\t\tbo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\t\tr = drm_gem_prime_handle_to_fd(&bo_adev->ddev, bo_adev->kfd.client.file,\n\t\t\t\t\t       mem->gem_handle,\n\t\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\t\t       DRM_RDWR : 0, &fd);\n\t\tif (r)\n\t\t\treturn r;\n\t\tdmabuf = dma_buf_get(fd);\n\t\tclose_fd(fd);\n\t\tif (WARN_ON_ONCE(IS_ERR(dmabuf)))\n\t\t\treturn PTR_ERR(dmabuf);\n\t\tmem->dmabuf = dmabuf;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nkfd_mem_attach_dmabuf(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\t      struct amdgpu_bo **bo)\n{\n\tstruct drm_gem_object *gobj;\n\tint ret;\n\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tgobj = amdgpu_gem_prime_import(adev_to_drm(adev), mem->dmabuf);\n\tif (IS_ERR(gobj))\n\t\treturn PTR_ERR(gobj);\n\n\t*bo = gem_to_amdgpu_bo(gobj);\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\treturn 0;\n}\n\n/* kfd_mem_attach - Add a BO to a VM\n *\n * Everything that needs to bo done only once when a BO is first added\n * to a VM. It can later be mapped and unmapped many times without\n * repeating these steps.\n *\n * 0. Create BO for DMA mapping, if needed\n * 1. Allocate and initialize BO VA entry data structure\n * 2. Add BO to the VM\n * 3. Determine ASIC-specific PTE flags\n * 4. Alloc page tables and directories if needed\n * 4a.  Validate new page tables and directories\n */\nstatic int kfd_mem_attach(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tstruct amdgpu_vm *vm, bool is_aql)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tuint64_t va = mem->va;\n\tstruct kfd_mem_attachment *attachment[2] = {NULL, NULL};\n\tstruct amdgpu_bo *bo[2] = {NULL, NULL};\n\tstruct amdgpu_bo_va *bo_va;\n\tbool same_hive = false;\n\tint i, ret;\n\n\tif (!va) {\n\t\tpr_err(\"Invalid VA when adding BO to VM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Determine access to VRAM, MMIO and DOORBELL BOs of peer devices\n\t *\n\t * The access path of MMIO and DOORBELL BOs of is always over PCIe.\n\t * In contrast the access path of VRAM BOs depens upon the type of\n\t * link that connects the peer device. Access over PCIe is allowed\n\t * if peer device has large BAR. In contrast, access over xGMI is\n\t * allowed for both small and large BAR configurations of peer device\n\t */\n\tif ((adev != bo_adev && !(adev->flags & AMD_IS_APU)) &&\n\t    ((mem->domain == AMDGPU_GEM_DOMAIN_VRAM) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tif (mem->domain == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\tsame_hive = amdgpu_xgmi_same_hive(adev, bo_adev);\n\t\tif (!same_hive && !amdgpu_device_is_peer_accessible(bo_adev, adev))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i <= is_aql; i++) {\n\t\tattachment[i] = kzalloc(sizeof(*attachment[i]), GFP_KERNEL);\n\t\tif (unlikely(!attachment[i])) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tpr_debug(\"\\t add VA 0x%llx - 0x%llx to vm %p\\n\", va,\n\t\t\t va + bo_size, vm);\n\n\t\tif ((adev == bo_adev && !(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) ||\n\t\t    (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && reuse_dmamap(adev, bo_adev)) ||\n\t\t    (mem->domain == AMDGPU_GEM_DOMAIN_GTT && reuse_dmamap(adev, bo_adev)) ||\n\t\t    same_hive) {\n\t\t\t/* Mappings on the local GPU, or VRAM mappings in the\n\t\t\t * local hive, or userptr, or GTT mapping can reuse dma map\n\t\t\t * address space share the original BO\n\t\t\t */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = mem->bo;\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (i > 0) {\n\t\t\t/* Multiple mappings on the same GPU share the BO */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = bo[0];\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\t\t/* Create an SG BO to DMA-map userptrs on other GPUs */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_USERPTR;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Handle DOORBELL BOs of peer devices and MMIO BOs of local and peer devices */\n\t\t} else if (mem->bo->tbo.type == ttm_bo_type_sg) {\n\t\t\tWARN_ONCE(!(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL ||\n\t\t\t\t    mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP),\n\t\t\t\t  \"Handing invalid SG BO in ATTACH request\");\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SG;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Enable acces to GTT and VRAM BOs of peer devices */\n\t\t} else if (mem->domain == AMDGPU_GEM_DOMAIN_GTT ||\n\t\t\t   mem->domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tattachment[i]->type = KFD_MEM_ATT_DMABUF;\n\t\t\tret = kfd_mem_attach_dmabuf(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t\tpr_debug(\"Employ DMABUF mechanism to enable peer GPU access\\n\");\n\t\t} else {\n\t\t\tWARN_ONCE(true, \"Handling invalid ATTACH request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\t/* Add BO to VM internal data structures */\n\t\tret = amdgpu_bo_reserve(bo[i], false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Unable to reserve BO during memory attach\");\n\t\t\tgoto unwind;\n\t\t}\n\t\tbo_va = amdgpu_vm_bo_find(vm, bo[i]);\n\t\tif (!bo_va)\n\t\t\tbo_va = amdgpu_vm_bo_add(adev, vm, bo[i]);\n\t\telse\n\t\t\t++bo_va->ref_count;\n\t\tattachment[i]->bo_va = bo_va;\n\t\tamdgpu_bo_unreserve(bo[i]);\n\t\tif (unlikely(!attachment[i]->bo_va)) {\n\t\t\tret = -ENOMEM;\n\t\t\tpr_err(\"Failed to add BO object to VM. ret == %d\\n\",\n\t\t\t       ret);\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->va = va;\n\t\tattachment[i]->pte_flags = get_pte_flags(adev, mem);\n\t\tattachment[i]->adev = adev;\n\t\tlist_add(&attachment[i]->list, &mem->attachments);\n\n\t\tva += bo_size;\n\t}\n\n\treturn 0;\n\nunwind:\n\tfor (; i >= 0; i--) {\n\t\tif (!attachment[i])\n\t\t\tcontinue;\n\t\tif (attachment[i]->bo_va) {\n\t\t\tamdgpu_bo_reserve(bo[i], true);\n\t\t\tif (--attachment[i]->bo_va->ref_count == 0)\n\t\t\t\tamdgpu_vm_bo_del(adev, attachment[i]->bo_va);\n\t\t\tamdgpu_bo_unreserve(bo[i]);\n\t\t\tlist_del(&attachment[i]->list);\n\t\t}\n\t\tif (bo[i])\n\t\t\tdrm_gem_object_put(&bo[i]->tbo.base);\n\t\tkfree(attachment[i]);\n\t}\n\treturn ret;\n}\n\nstatic void kfd_mem_detach(struct kfd_mem_attachment *attachment)\n{\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\n\tpr_debug(\"\\t remove VA 0x%llx in entry %p\\n\",\n\t\t\tattachment->va, attachment);\n\tif (--attachment->bo_va->ref_count == 0)\n\t\tamdgpu_vm_bo_del(attachment->adev, attachment->bo_va);\n\tdrm_gem_object_put(&bo->tbo.base);\n\tlist_del(&attachment->list);\n\tkfree(attachment);\n}\n\nstatic void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,\n\t\t\t\tstruct amdkfd_process_info *process_info,\n\t\t\t\tbool userptr)\n{\n\tmutex_lock(&process_info->lock);\n\tif (userptr)\n\t\tlist_add_tail(&mem->validate_list,\n\t\t\t      &process_info->userptr_valid_list);\n\telse\n\t\tlist_add_tail(&mem->validate_list, &process_info->kfd_bo_list);\n\tmutex_unlock(&process_info->lock);\n}\n\nstatic void remove_kgd_mem_from_kfd_bo_list(struct kgd_mem *mem,\n\t\tstruct amdkfd_process_info *process_info)\n{\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n}\n\n/* Initializes user pages. It registers the MMU notifier and validates\n * the userptr BO in the GTT domain.\n *\n * The BO must already be on the userptr_valid_list. Otherwise an\n * eviction and restore may happen that leaves the new BO unmapped\n * with the user mode queues running.\n *\n * Takes the process_info->lock to protect against concurrent restore\n * workers.\n *\n * Returns 0 for success, negative errno for errors.\n */\nstatic int init_user_pages(struct kgd_mem *mem, uint64_t user_addr,\n\t\t\t   bool criu_resume)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct hmm_range *range;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->lock);\n\n\tret = amdgpu_ttm_tt_set_userptr(&bo->tbo, user_addr, 0);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to set userptr: %d\\n\", __func__, ret);\n\t\tgoto out;\n\t}\n\n\tret = amdgpu_hmm_register(bo, user_addr);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register MMU notifier: %d\\n\",\n\t\t       __func__, ret);\n\t\tgoto out;\n\t}\n\n\tif (criu_resume) {\n\t\t/*\n\t\t * During a CRIU restore operation, the userptr buffer objects\n\t\t * will be validated in the restore_userptr_work worker at a\n\t\t * later stage when it is scheduled by another ioctl called by\n\t\t * CRIU master process for the target pid for restore.\n\t\t */\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tmem->invalid++;\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t\tmutex_unlock(&process_info->lock);\n\t\treturn 0;\n\t}\n\n\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);\n\tif (ret) {\n\t\tif (ret == -EAGAIN)\n\t\t\tpr_debug(\"Failed to get user pages, try again\\n\");\n\t\telse\n\t\t\tpr_err(\"%s: Failed to get user pages: %d\\n\", __func__, ret);\n\t\tgoto unregister_out;\n\t}\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to reserve BO\\n\", __func__);\n\t\tgoto release_out;\n\t}\n\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\tamdgpu_bo_unreserve(bo);\n\nrelease_out:\n\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\nunregister_out:\n\tif (ret)\n\t\tamdgpu_hmm_unregister(bo);\nout:\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\n/* Reserving a BO and its page table BOs must happen atomically to\n * avoid deadlocks. Some operations update multiple VMs at once. Track\n * all the reservation info in a context structure. Optionally a sync\n * object can track VM updates.\n */\nstruct bo_vm_reservation_context {\n\t/* DRM execution context for the reservation */\n\tstruct drm_exec exec;\n\t/* Number of VMs reserved */\n\tunsigned int n_vms;\n\t/* Pointer to sync object */\n\tstruct amdgpu_sync *sync;\n};\n\nenum bo_vm_match {\n\tBO_VM_NOT_MAPPED = 0,\t/* Match VMs where a BO is not mapped */\n\tBO_VM_MAPPED,\t\t/* Match VMs where a BO is mapped     */\n\tBO_VM_ALL,\t\t/* Match all VMs a BO was added to    */\n};\n\n/**\n * reserve_bo_and_vm - reserve a BO and a VM unconditionally.\n * @mem: KFD BO structure.\n * @vm: the VM to reserve.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n */\nstatic int reserve_bo_and_vm(struct kgd_mem *mem,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct bo_vm_reservation_context *ctx)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tWARN_ON(!vm);\n\n\tctx->n_vms = 1;\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tret = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * reserve_bo_and_cond_vms - reserve a BO and some VMs conditionally\n * @mem: KFD BO structure.\n * @vm: the VM to reserve. If NULL, then all VMs associated with the BO\n * is used. Otherwise, a single VM associated with the BO.\n * @map_type: the mapping status that will be used to filter the VMs.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n *\n * Returns 0 for success, negative for failure.\n */\nstatic int reserve_bo_and_cond_vms(struct kgd_mem *mem,\n\t\t\t\tstruct amdgpu_vm *vm, enum bo_vm_match map_type,\n\t\t\t\tstruct bo_vm_reservation_context *ctx)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tctx->n_vms = 0;\n\t\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\t\tif ((vm && vm != entry->bo_va->base.vm) ||\n\t\t\t\t(entry->is_mapped != map_type\n\t\t\t\t&& map_type != BO_VM_ALL))\n\t\t\t\tcontinue;\n\n\t\t\tret = amdgpu_vm_lock_pd(entry->bo_va->base.vm,\n\t\t\t\t\t\t&ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto error;\n\t\t\t++ctx->n_vms;\n\t\t}\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * unreserve_bo_and_vms - Unreserve BO and VMs from a reservation context\n * @ctx: Reservation context to unreserve\n * @wait: Optionally wait for a sync object representing pending VM updates\n * @intr: Whether the wait is interruptible\n *\n * Also frees any resources allocated in\n * reserve_bo_and_(cond_)vm(s). Returns the status from\n * amdgpu_sync_wait.\n */\nstatic int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,\n\t\t\t\t bool wait, bool intr)\n{\n\tint ret = 0;\n\n\tif (wait)\n\t\tret = amdgpu_sync_wait(ctx->sync, intr);\n\n\tdrm_exec_fini(&ctx->exec);\n\tctx->sync = NULL;\n\treturn ret;\n}\n\nstatic void unmap_bo_from_gpuvm(struct kgd_mem *mem,\n\t\t\t\tstruct kfd_mem_attachment *entry,\n\t\t\t\tstruct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\n\tamdgpu_vm_bo_unmap(adev, bo_va, entry->va);\n\n\tamdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);\n\n\tamdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int update_gpuvm_pte(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *entry,\n\t\t\t    struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tint ret;\n\n\tret = kfd_mem_dmamap_attachment(mem, entry);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Update the page tables  */\n\tret = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (ret) {\n\t\tpr_err(\"amdgpu_vm_bo_update failed\\n\");\n\t\treturn ret;\n\t}\n\n\treturn amdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int map_bo_to_gpuvm(struct kgd_mem *mem,\n\t\t\t   struct kfd_mem_attachment *entry,\n\t\t\t   struct amdgpu_sync *sync,\n\t\t\t   bool no_update_pte)\n{\n\tint ret;\n\n\t/* Set virtual address for the allocation */\n\tret = amdgpu_vm_bo_map(entry->adev, entry->bo_va, entry->va, 0,\n\t\t\t       amdgpu_bo_size(entry->bo_va->base.bo),\n\t\t\t       entry->pte_flags);\n\tif (ret) {\n\t\tpr_err(\"Failed to map VA 0x%llx in vm. ret %d\\n\",\n\t\t\t\tentry->va, ret);\n\t\treturn ret;\n\t}\n\n\tif (no_update_pte)\n\t\treturn 0;\n\n\tret = update_gpuvm_pte(mem, entry, sync);\n\tif (ret) {\n\t\tpr_err(\"update_gpuvm_pte() failed\\n\");\n\t\tgoto update_gpuvm_pte_failed;\n\t}\n\n\treturn 0;\n\nupdate_gpuvm_pte_failed:\n\tunmap_bo_from_gpuvm(mem, entry, sync);\n\tkfd_mem_dmaunmap_attachment(mem, entry);\n\treturn ret;\n}\n\nstatic int process_validate_vms(struct amdkfd_process_info *process_info,\n\t\t\t\tstruct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_validate_pt_pd_bos(peer_vm, ticket);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_sync_pds_resv(struct amdkfd_process_info *process_info,\n\t\t\t\t struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *pd = peer_vm->root.bo;\n\n\t\tret = amdgpu_sync_resv(NULL, sync, pd->tbo.base.resv,\n\t\t\t\t       AMDGPU_SYNC_NE_OWNER,\n\t\t\t\t       AMDGPU_FENCE_OWNER_KFD);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_update_pds(struct amdkfd_process_info *process_info,\n\t\t\t      struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_update_pds(peer_vm, sync);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,\n\t\t       struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *info = NULL;\n\tint ret;\n\n\tif (!*process_info) {\n\t\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tmutex_init(&info->lock);\n\t\tmutex_init(&info->notifier_lock);\n\t\tINIT_LIST_HEAD(&info->vm_list_head);\n\t\tINIT_LIST_HEAD(&info->kfd_bo_list);\n\t\tINIT_LIST_HEAD(&info->userptr_valid_list);\n\t\tINIT_LIST_HEAD(&info->userptr_inval_list);\n\n\t\tinfo->eviction_fence =\n\t\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t\t   current->mm,\n\t\t\t\t\t\t   NULL);\n\t\tif (!info->eviction_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto create_evict_fence_fail;\n\t\t}\n\n\t\tinfo->pid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\t\tINIT_DELAYED_WORK(&info->restore_userptr_work,\n\t\t\t\t  amdgpu_amdkfd_restore_userptr_worker);\n\n\t\t*process_info = info;\n\t}\n\n\tvm->process_info = *process_info;\n\n\t/* Validate page directory and attach eviction fence */\n\tret = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (ret)\n\t\tgoto reserve_pd_fail;\n\tret = vm_validate_pt_pd_bos(vm, NULL);\n\tif (ret) {\n\t\tpr_err(\"validate_pt_pd_bos() failed\\n\");\n\t\tgoto validate_pd_fail;\n\t}\n\tret = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t  AMDGPU_FENCE_OWNER_KFD, false);\n\tif (ret)\n\t\tgoto wait_pd_fail;\n\tret = dma_resv_reserve_fences(vm->root.bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(vm->root.bo->tbo.base.resv,\n\t\t\t   &vm->process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\n\t/* Update process info */\n\tmutex_lock(&vm->process_info->lock);\n\tlist_add_tail(&vm->vm_list_node,\n\t\t\t&(vm->process_info->vm_list_head));\n\tvm->process_info->n_vms++;\n\n\t*ef = dma_fence_get(&vm->process_info->eviction_fence->base);\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn 0;\n\nreserve_shared_fail:\nwait_pd_fail:\nvalidate_pd_fail:\n\tamdgpu_bo_unreserve(vm->root.bo);\nreserve_pd_fail:\n\tvm->process_info = NULL;\n\tif (info) {\n\t\tdma_fence_put(&info->eviction_fence->base);\n\t\t*process_info = NULL;\n\t\tput_pid(info->pid);\ncreate_evict_fence_fail:\n\t\tmutex_destroy(&info->lock);\n\t\tmutex_destroy(&info->notifier_lock);\n\t\tkfree(info);\n\t}\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_pin_bo() - Pins a BO using following criteria\n * @bo: Handle of buffer object being pinned\n * @domain: Domain into which BO should be pinned\n *\n *   - USERPTR BOs are UNPINNABLE and will return error\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count incremented. It is valid to PIN a BO multiple times\n *\n * Return: ZERO if successful in pinning, Non-Zero in case of error.\n */\nstatic int amdgpu_amdkfd_gpuvm_pin_bo(struct amdgpu_bo *bo, u32 domain)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {\n\t\t/*\n\t\t * If bo is not contiguous on VRAM, move to system memory first to ensure\n\t\t * we can get contiguous VRAM space after evicting other BOs.\n\t\t */\n\t\tif (!(bo->tbo.resource->placement & TTM_PL_FLAG_CONTIGUOUS)) {\n\t\t\tstruct ttm_operation_ctx ctx = { true, false };\n\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_debug(\"validate bo 0x%p to GTT failed %d\\n\", &bo->tbo, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n\tif (ret)\n\t\tpr_err(\"Error in Pinning BO to domain: %d\\n\", domain);\n\n\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\nout:\n\tamdgpu_bo_unreserve(bo);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_unpin_bo() - Unpins BO using following criteria\n * @bo: Handle of buffer object being unpinned\n *\n *   - Is a illegal request for USERPTR BOs and is ignored\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count decremented. Calls to UNPIN must balance calls to PIN\n */\nstatic void amdgpu_amdkfd_gpuvm_unpin_bo(struct amdgpu_bo *bo)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn;\n\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_set_vm_pasid(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_vm *avm, u32 pasid)\n\n{\n\tint ret;\n\n\t/* Free the original amdgpu allocated pasid,\n\t * will be replaced with kfd allocated pasid.\n\t */\n\tif (avm->pasid) {\n\t\tamdgpu_pasid_free(avm->pasid);\n\t\tamdgpu_vm_set_pasid(adev, avm, 0);\n\t}\n\n\tret = amdgpu_vm_set_pasid(adev, avm, pasid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_acquire_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *avm,\n\t\t\t\t\t   void **process_info,\n\t\t\t\t\t   struct dma_fence **ef)\n{\n\tint ret;\n\n\t/* Already a compute VM? */\n\tif (avm->process_info)\n\t\treturn -EINVAL;\n\n\t/* Convert VM into a compute VM */\n\tret = amdgpu_vm_make_compute(adev, avm);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Initialize KFD part of the VM and process info */\n\tret = init_kfd_vm(avm, process_info, ef);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_vm_set_task_info(avm);\n\n\treturn 0;\n}\n\nvoid amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm)\n{\n\tstruct amdkfd_process_info *process_info = vm->process_info;\n\n\tif (!process_info)\n\t\treturn;\n\n\t/* Update process info */\n\tmutex_lock(&process_info->lock);\n\tprocess_info->n_vms--;\n\tlist_del(&vm->vm_list_node);\n\tmutex_unlock(&process_info->lock);\n\n\tvm->process_info = NULL;\n\n\t/* Release per-process resources when last compute VM is destroyed */\n\tif (!process_info->n_vms) {\n\t\tWARN_ON(!list_empty(&process_info->kfd_bo_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_valid_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_inval_list));\n\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tcancel_delayed_work_sync(&process_info->restore_userptr_work);\n\t\tput_pid(process_info->pid);\n\t\tmutex_destroy(&process_info->lock);\n\t\tmutex_destroy(&process_info->notifier_lock);\n\t\tkfree(process_info);\n\t}\n}\n\nvoid amdgpu_amdkfd_gpuvm_release_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t    void *drm_priv)\n{\n\tstruct amdgpu_vm *avm;\n\n\tif (WARN_ON(!adev || !drm_priv))\n\t\treturn;\n\n\tavm = drm_priv_to_vm(drm_priv);\n\n\tpr_debug(\"Releasing process vm %p\\n\", avm);\n\n\t/* The original pasid of amdgpu vm has already been\n\t * released during making a amdgpu vm to a compute vm\n\t * The current pasid is managed by kfd and will be\n\t * released on kfd process destroy. Set amdgpu pasid\n\t * to 0 to avoid duplicate release.\n\t */\n\tamdgpu_vm_release_compute(adev, avm);\n}\n\nuint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *pd = avm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\n\tif (adev->asic_type < CHIP_VEGA10)\n\t\treturn avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;\n\treturn avm->pd_phys_addr;\n}\n\nvoid amdgpu_amdkfd_block_mmu_notifications(void *p)\n{\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tWRITE_ONCE(pinfo->block_mmu_notifications, true);\n\tmutex_unlock(&pinfo->lock);\n}\n\nint amdgpu_amdkfd_criu_resume(void *p)\n{\n\tint ret = 0;\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tpr_debug(\"scheduling work\\n\");\n\tmutex_lock(&pinfo->notifier_lock);\n\tpinfo->evicted_bos++;\n\tmutex_unlock(&pinfo->notifier_lock);\n\tif (!READ_ONCE(pinfo->block_mmu_notifications)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tWRITE_ONCE(pinfo->block_mmu_notifications, false);\n\tqueue_delayed_work(system_freezable_wq,\n\t\t\t   &pinfo->restore_userptr_work, 0);\n\nout_unlock:\n\tmutex_unlock(&pinfo->lock);\n\treturn ret;\n}\n\nsize_t amdgpu_amdkfd_get_available_memory(struct amdgpu_device *adev,\n\t\t\t\t\t  uint8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tssize_t available;\n\tuint64_t vram_available, system_mem_available, ttm_mem_available;\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tvram_available = KFD_XCP_MEMORY_SIZE(adev, xcp_id)\n\t\t- adev->kfd.vram_used_aligned[xcp_id]\n\t\t- atomic64_read(&adev->vram_pin_size)\n\t\t- reserved_for_pt\n\t\t- reserved_for_ras;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tsystem_mem_available = no_system_mem_limit ?\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit :\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit -\n\t\t\t\t\tkfd_mem_limit.system_mem_used;\n\n\t\tttm_mem_available = kfd_mem_limit.max_ttm_mem_limit -\n\t\t\t\tkfd_mem_limit.ttm_mem_used;\n\n\t\tavailable = min3(system_mem_available, ttm_mem_available,\n\t\t\t\t vram_available);\n\t\tavailable = ALIGN_DOWN(available, PAGE_SIZE);\n\t} else {\n\t\tavailable = ALIGN_DOWN(vram_available, VRAM_AVAILABLITY_ALIGN);\n\t}\n\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\n\treturn available;\n}\n\nint amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, uint64_t va, uint64_t size,\n\t\tvoid *drm_priv, struct kgd_mem **mem,\n\t\tuint64_t *offset, uint32_t flags, bool criu_resume)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_fpriv *fpriv = container_of(avm, struct amdgpu_fpriv, vm);\n\tenum ttm_bo_type bo_type = ttm_bo_type_device;\n\tstruct sg_table *sg = NULL;\n\tuint64_t user_addr = 0;\n\tstruct amdgpu_bo *bo;\n\tstruct drm_gem_object *gobj = NULL;\n\tu32 domain, alloc_domain;\n\tuint64_t aligned_size;\n\tint8_t xcp_id = -1;\n\tu64 alloc_flags;\n\tint ret;\n\n\t/*\n\t * Check on which domain to allocate BO\n\t */\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_flags = 0;\n\t\t} else {\n\t\t\talloc_flags = AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\t\t\talloc_flags |= (flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) ?\n\t\t\tAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED : 0;\n\n\t\t\t/* For contiguous VRAM allocation */\n\t\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_CONTIGUOUS)\n\t\t\t\talloc_flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\t\t}\n\t\txcp_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ?\n\t\t\t\t\t0 : fpriv->xcp_id;\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_flags = 0;\n\t} else {\n\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_domain = AMDGPU_GEM_DOMAIN_CPU;\n\t\talloc_flags = AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\tif (!offset || !*offset)\n\t\t\t\treturn -EINVAL;\n\t\t\tuser_addr = untagged_addr(*offset);\n\t\t} else if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\t\tbo_type = ttm_bo_type_sg;\n\t\t\tif (size > UINT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\tsg = create_sg_table(*offset, size);\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_EXT_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_EXT_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_UNCACHED)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_UNCACHED;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\t(*mem)->aql_queue = !!(flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);\n\n\t/* Workaround for AQL queue wraparound bug. Map the same\n\t * memory twice. That means we only actually allocate half\n\t * the memory.\n\t */\n\tif ((*mem)->aql_queue)\n\t\tsize >>= 1;\n\taligned_size = PAGE_ALIGN(size);\n\n\t(*mem)->alloc_flags = flags;\n\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\tret = amdgpu_amdkfd_reserve_mem_limit(adev, aligned_size, flags,\n\t\t\t\t\t      xcp_id);\n\tif (ret) {\n\t\tpr_debug(\"Insufficient memory\\n\");\n\t\tgoto err_reserve_limit;\n\t}\n\n\tpr_debug(\"\\tcreate BO VA 0x%llx size 0x%llx domain %s xcp_id %d\\n\",\n\t\t va, (*mem)->aql_queue ? size << 1 : size,\n\t\t domain_string(alloc_domain), xcp_id);\n\n\tret = amdgpu_gem_object_create(adev, aligned_size, 1, alloc_domain, alloc_flags,\n\t\t\t\t       bo_type, NULL, &gobj, xcp_id + 1);\n\tif (ret) {\n\t\tpr_debug(\"Failed to create BO on domain %s. ret %d\\n\",\n\t\t\t domain_string(alloc_domain), ret);\n\t\tgoto err_bo_create;\n\t}\n\tret = drm_vma_node_allow(&gobj->vma_node, drm_priv);\n\tif (ret) {\n\t\tpr_debug(\"Failed to allow vma node access. ret %d\\n\", ret);\n\t\tgoto err_node_allow;\n\t}\n\tret = drm_gem_handle_create(adev->kfd.client.file, gobj, &(*mem)->gem_handle);\n\tif (ret)\n\t\tgoto err_gem_handle_create;\n\tbo = gem_to_amdgpu_bo(gobj);\n\tif (bo_type == ttm_bo_type_sg) {\n\t\tbo->tbo.sg = sg;\n\t\tbo->tbo.ttm->sg = sg;\n\t}\n\tbo->kfd_bo = *mem;\n\t(*mem)->bo = bo;\n\tif (user_addr)\n\t\tbo->flags |= AMDGPU_AMDKFD_CREATE_USERPTR_BO;\n\n\t(*mem)->va = va;\n\t(*mem)->domain = domain;\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);\n\n\tif (user_addr) {\n\t\tpr_debug(\"creating userptr BO for user_addr = %llx\\n\", user_addr);\n\t\tret = init_user_pages(*mem, user_addr, criu_resume);\n\t\tif (ret)\n\t\t\tgoto allocate_init_user_pages_failed;\n\t} else  if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tret = amdgpu_amdkfd_gpuvm_pin_bo(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (ret) {\n\t\t\tpr_err(\"Pinning MMIO/DOORBELL BO during ALLOC FAILED\\n\");\n\t\t\tgoto err_pin_bo;\n\t\t}\n\t\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t} else {\n\t\tmutex_lock(&avm->process_info->lock);\n\t\tif (avm->process_info->eviction_fence &&\n\t\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\t\tmutex_unlock(&avm->process_info->lock);\n\t\tif (ret)\n\t\t\tgoto err_validate_bo;\n\t}\n\n\tif (offset)\n\t\t*offset = amdgpu_bo_mmap_offset(bo);\n\n\treturn 0;\n\nallocate_init_user_pages_failed:\nerr_pin_bo:\nerr_validate_bo:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_gem_handle_delete(adev->kfd.client.file, (*mem)->gem_handle);\nerr_gem_handle_create:\n\tdrm_vma_node_revoke(&gobj->vma_node, drm_priv);\nerr_node_allow:\n\t/* Don't unreserve system mem limit twice */\n\tgoto err_reserve_limit;\nerr_bo_create:\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, aligned_size, flags, xcp_id);\nerr_reserve_limit:\n\tamdgpu_sync_free(&(*mem)->sync);\n\tmutex_destroy(&(*mem)->lock);\n\tif (gobj)\n\t\tdrm_gem_object_put(gobj);\n\telse\n\t\tkfree(*mem);\nerr:\n\tif (sg) {\n\t\tsg_free_table(sg);\n\t\tkfree(sg);\n\t}\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_free_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv,\n\t\tuint64_t *size)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tbool use_release_notifier = (mem->bo->kfd_bo == mem);\n\tstruct kfd_mem_attachment *entry, *tmp;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned int mapped_to_gpu_memory;\n\tint ret;\n\tbool is_imported = false;\n\n\tmutex_lock(&mem->lock);\n\n\t/* Unpin MMIO/DOORBELL BO's that were pinned during allocation */\n\tif (mem->alloc_flags &\n\t    (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t     KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tamdgpu_amdkfd_gpuvm_unpin_bo(mem->bo);\n\t}\n\n\tmapped_to_gpu_memory = mem->mapped_to_gpu_memory;\n\tis_imported = mem->is_imported;\n\tmutex_unlock(&mem->lock);\n\t/* lock is not needed after this, since mem is unused and will\n\t * be freed anyway\n\t */\n\n\tif (mapped_to_gpu_memory > 0) {\n\t\tpr_debug(\"BO VA 0x%llx size 0x%lx is still mapped.\\n\",\n\t\t\t\tmem->va, bo_size);\n\t\treturn -EBUSY;\n\t}\n\n\t/* Make sure restore workers don't access the BO any more */\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n\n\t/* Cleanup user pages and MMU notifiers */\n\tif (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\tamdgpu_hmm_unregister(mem->bo);\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tamdgpu_ttm_tt_discard_user_pages(mem->bo->tbo.ttm, mem->range);\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t}\n\n\tret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\tprocess_info->eviction_fence);\n\tpr_debug(\"Release VA 0x%llx - 0x%llx\\n\", mem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue));\n\n\t/* Remove from VM internal data structures */\n\tlist_for_each_entry_safe(entry, tmp, &mem->attachments, list) {\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t\tkfd_mem_detach(entry);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\t/* Free the sync object */\n\tamdgpu_sync_free(&mem->sync);\n\n\t/* If the SG is not NULL, it's one we created for a doorbell or mmio\n\t * remap BO. We need to free it.\n\t */\n\tif (mem->bo->tbo.sg) {\n\t\tsg_free_table(mem->bo->tbo.sg);\n\t\tkfree(mem->bo->tbo.sg);\n\t}\n\n\t/* Update the size of the BO being freed if it was allocated from\n\t * VRAM and is not imported. For APP APU VRAM allocations are done\n\t * in GTT domain\n\t */\n\tif (size) {\n\t\tif (!is_imported &&\n\t\t   (mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM ||\n\t\t   ((adev->flags & AMD_IS_APU) &&\n\t\t    mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT)))\n\t\t\t*size = bo_size;\n\t\telse\n\t\t\t*size = 0;\n\t}\n\n\t/* Free the BO*/\n\tdrm_vma_node_revoke(&mem->bo->tbo.base.vma_node, drm_priv);\n\tdrm_gem_handle_delete(adev->kfd.client.file, mem->gem_handle);\n\tif (mem->dmabuf) {\n\t\tdma_buf_put(mem->dmabuf);\n\t\tmem->dmabuf = NULL;\n\t}\n\tmutex_destroy(&mem->lock);\n\n\t/* If this releases the last reference, it will end up calling\n\t * amdgpu_amdkfd_release_notify and kfree the mem struct. That's why\n\t * this needs to be the last call here.\n\t */\n\tdrm_gem_object_put(&mem->bo->tbo.base);\n\n\t/*\n\t * For kgd_mem allocated in amdgpu_amdkfd_gpuvm_import_dmabuf(),\n\t * explicitly free it here.\n\t */\n\tif (!use_release_notifier)\n\t\tkfree(mem);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tvoid *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tint ret;\n\tstruct amdgpu_bo *bo;\n\tuint32_t domain;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned long bo_size;\n\tbool is_invalid_userptr = false;\n\n\tbo = mem->bo;\n\tif (!bo) {\n\t\tpr_err(\"Invalid BO when mapping memory to GPU\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Make sure restore is not running concurrently. Since we\n\t * don't map invalid userptr BOs, we rely on the next restore\n\t * worker to do the mapping\n\t */\n\tmutex_lock(&mem->process_info->lock);\n\n\t/* Lock notifier lock. If we find an invalid userptr BO, we can be\n\t * sure that the MMU notifier is no longer running\n\t * concurrently and the queues are actually stopped\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tmutex_lock(&mem->process_info->notifier_lock);\n\t\tis_invalid_userptr = !!mem->invalid;\n\t\tmutex_unlock(&mem->process_info->notifier_lock);\n\t}\n\n\tmutex_lock(&mem->lock);\n\n\tdomain = mem->domain;\n\tbo_size = bo->tbo.base.size;\n\n\tpr_debug(\"Map VA 0x%llx - 0x%llx to vm %p domain %s\\n\",\n\t\t\tmem->va,\n\t\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\t\tavm, domain_string(domain));\n\n\tif (!kfd_mem_is_attached(avm, mem)) {\n\t\tret = kfd_mem_attach(adev, mem, avm, mem->aql_queue);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = reserve_bo_and_vm(mem, avm, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\t/* Userptr can be marked as \"not invalid\", but not actually be\n\t * validated yet (still in the system domain). In that case\n\t * the queues are still stopped and we can leave mapping for\n\t * the next restore worker\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_SYSTEM)\n\t\tis_invalid_userptr = true;\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto out_unreserve;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t map VA 0x%llx - 0x%llx in entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = map_bo_to_gpuvm(mem, entry, ctx.sync,\n\t\t\t\t      is_invalid_userptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map bo to gpuvm\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tret = vm_update_pds(avm, ctx.sync);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to update page directories\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tentry->is_mapped = true;\n\t\tmem->mapped_to_gpu_memory++;\n\t\tpr_debug(\"\\t INC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\tgoto out;\n\nout_unreserve:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->process_info->lock);\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_dmaunmap_mem(struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_vm *vm;\n\tint ret;\n\n\tvm = drm_priv_to_vm(drm_priv);\n\n\tmutex_lock(&mem->lock);\n\n\tret = amdgpu_bo_reserve(mem->bo, true);\n\tif (ret)\n\t\tgoto out;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != vm)\n\t\t\tcontinue;\n\t\tif (entry->bo_va->base.bo->tbo.ttm &&\n\t\t    !entry->bo_va->base.bo->tbo.ttm->sg)\n\t\t\tcontinue;\n\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t}\n\n\tamdgpu_bo_unreserve(mem->bo);\nout:\n\tmutex_unlock(&mem->lock);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\n\tret = reserve_bo_and_cond_vms(mem, avm, BO_VM_MAPPED, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\t/* If no VMs were reserved, it means the BO wasn't actually mapped */\n\tif (ctx.n_vms == 0) {\n\t\tret = -EINVAL;\n\t\tgoto unreserve_out;\n\t}\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto unreserve_out;\n\n\tpr_debug(\"Unmap VA 0x%llx - 0x%llx from vm %p\\n\",\n\t\tmem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\tavm);\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || !entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t unmap VA 0x%llx - 0x%llx from entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tunmap_bo_from_gpuvm(mem, entry, ctx.sync);\n\t\tentry->is_mapped = false;\n\n\t\tmem->mapped_to_gpu_memory--;\n\t\tpr_debug(\"\\t DEC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\nunreserve_out:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_sync_memory(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tmutex_lock(&mem->lock);\n\tamdgpu_sync_clone(&mem->sync, &sync);\n\tmutex_unlock(&mem->lock);\n\n\tret = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_map_gtt_bo_to_gart - Map BO to GART and increment reference count\n * @bo: Buffer object to be mapped\n *\n * Before return, bo reference count is incremented. To release the reference and unpin/\n * unmap the BO, call amdgpu_amdkfd_free_gtt_mem.\n */\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo)\n{\n\tint ret;\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto err_reserve_bo_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto err_pin_bo_failed;\n\t}\n\n\tret = amdgpu_ttm_alloc_gart(&bo->tbo);\n\tif (ret) {\n\t\tpr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\n\t\tgoto err_map_bo_gart_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, bo->vm_bo->vm->process_info->eviction_fence);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tbo = amdgpu_bo_ref(bo);\n\n\treturn 0;\n\nerr_map_bo_gart_failed:\n\tamdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\n\tamdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Map a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be mapped for CPU access\n * @kptr[out]: pointer in kernel CPU address space\n * @size[out]: size of the buffer\n *\n * Pins the BO and maps it for kernel CPU access. The eviction fence is removed\n * from the BO, since pinned BOs cannot be evicted. The bo must remain on the\n * validate_list, so the GPU mapping can be restored after a page table was\n * evicted.\n *\n * Return: 0 on success, error code on failure\n */\nint amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_mem *mem,\n\t\t\t\t\t     void **kptr, uint64_t *size)\n{\n\tint ret;\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tpr_err(\"userptr can't be mapped to kernel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&mem->process_info->lock);\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto bo_reserve_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto pin_failed;\n\t}\n\n\tret = amdgpu_bo_kmap(bo, kptr);\n\tif (ret) {\n\t\tpr_err(\"Failed to map bo to kernel. ret %d\\n\", ret);\n\t\tgoto kmap_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, mem->process_info->eviction_fence);\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tmutex_unlock(&mem->process_info->lock);\n\treturn 0;\n\nkmap_failed:\n\tamdgpu_bo_unpin(bo);\npin_failed:\n\tamdgpu_bo_unreserve(bo);\nbo_reserve_failed:\n\tmutex_unlock(&mem->process_info->lock);\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Unmap a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be unmapped for CPU access\n *\n * Removes the kernel CPU mapping and unpins the BO. It does not restore the\n * eviction fence, so this function should only be used for cleanup before the\n * BO is destroyed.\n */\nvoid amdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(struct kgd_mem *mem)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct kfd_vm_fault_info *mem)\n{\n\tif (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {\n\t\t*mem = *adev->gmc.vm_fault_info;\n\t\tmb(); /* make sure read happened */\n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\t}\n\treturn 0;\n}\n\nstatic int import_obj_create(struct amdgpu_device *adev,\n\t\t\t     struct dma_buf *dma_buf,\n\t\t\t     struct drm_gem_object *obj,\n\t\t\t     uint64_t va, void *drm_priv,\n\t\t\t     struct kgd_mem **mem, uint64_t *size,\n\t\t\t     uint64_t *mmap_offset)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tbo = gem_to_amdgpu_bo(obj);\n\tif (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT)))\n\t\t/* Only VRAM and GTT BOs are supported */\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tret = drm_vma_node_allow(&obj->vma_node, drm_priv);\n\tif (ret)\n\t\tgoto err_free_mem;\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tif (mmap_offset)\n\t\t*mmap_offset = amdgpu_bo_mmap_offset(bo);\n\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\n\t(*mem)->alloc_flags =\n\t\t((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?\n\t\tKFD_IOC_ALLOC_MEM_FLAGS_VRAM : KFD_IOC_ALLOC_MEM_FLAGS_GTT)\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\n\tget_dma_buf(dma_buf);\n\t(*mem)->dmabuf = dma_buf;\n\t(*mem)->bo = bo;\n\t(*mem)->va = va;\n\t(*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&\n\t\t\t !(adev->flags & AMD_IS_APU) ?\n\t\t\t AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;\n\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\t(*mem)->is_imported = true;\n\n\tmutex_lock(&avm->process_info->lock);\n\tif (avm->process_info->eviction_fence &&\n\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, (*mem)->domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\tmutex_unlock(&avm->process_info->lock);\n\tif (ret)\n\t\tgoto err_remove_mem;\n\n\treturn 0;\n\nerr_remove_mem:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_vma_node_revoke(&obj->vma_node, drm_priv);\nerr_free_mem:\n\tkfree(*mem);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n\t\t\t\t\t uint64_t va, void *drm_priv,\n\t\t\t\t\t struct kgd_mem **mem, uint64_t *size,\n\t\t\t\t\t uint64_t *mmap_offset)\n{\n\tstruct drm_gem_object *obj;\n\tuint32_t handle;\n\tint ret;\n\n\tret = drm_gem_prime_fd_to_handle(&adev->ddev, adev->kfd.client.file, fd,\n\t\t\t\t\t &handle);\n\tif (ret)\n\t\treturn ret;\n\tobj = drm_gem_object_lookup(adev->kfd.client.file, handle);\n\tif (!obj) {\n\t\tret = -EINVAL;\n\t\tgoto err_release_handle;\n\t}\n\n\tret = import_obj_create(adev, obj->dma_buf, obj, va, drm_priv, mem, size,\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\nerr_release_handle:\n\tdrm_gem_handle_delete(adev->kfd.client.file, handle);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_mem *mem,\n\t\t\t\t      struct dma_buf **dma_buf)\n{\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\tgoto out;\n\n\tget_dma_buf(mem->dmabuf);\n\t*dma_buf = mem->dmabuf;\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\n/* Evict a userptr BO by stopping the queues if necessary\n *\n * Runs in MMU notifier, may be in RECLAIM_FS context. This means it\n * cannot do any memory allocations, and cannot take any locks that\n * are held elsewhere while allocating memory.\n *\n * It doesn't do anything to the BO itself. The real work happens in\n * restore, where we get updated page addresses. This function only\n * ensures that GPU access to the BO is stopped.\n */\nint amdgpu_amdkfd_evict_userptr(struct mmu_interval_notifier *mni,\n\t\t\t\tunsigned long cur_seq, struct kgd_mem *mem)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tint r = 0;\n\n\t/* Do not process MMU notifications during CRIU restore until\n\t * KFD_CRIU_OP_RESUME IOCTL is received\n\t */\n\tif (READ_ONCE(process_info->block_mmu_notifications))\n\t\treturn 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmem->invalid++;\n\tif (++process_info->evicted_bos == 1) {\n\t\t/* First eviction, stop the queues */\n\t\tr = kgd2kfd_quiesce_mm(mni->mm,\n\t\t\t\t       KFD_QUEUE_EVICTION_TRIGGER_USERPTR);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to quiesce KFD\\n\");\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\t}\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn r;\n}\n\n/* Update invalid userptr BOs\n *\n * Moves invalidated (evicted) userptr BOs from userptr_valid_list to\n * userptr_inval_list and updates user pages for all BOs that have\n * been invalidated since their last update.\n */\nstatic int update_invalid_user_pages(struct amdkfd_process_info *process_info,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tuint32_t invalid;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\n\t/* Move all invalidated BOs to the userptr_inval_list */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_valid_list,\n\t\t\t\t validate_list)\n\t\tif (mem->invalid)\n\t\t\tlist_move_tail(&mem->validate_list,\n\t\t\t\t       &process_info->userptr_inval_list);\n\n\t/* Go through userptr_inval_list and update any invalid user_pages */\n\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t    validate_list) {\n\t\tinvalid = mem->invalid;\n\t\tif (!invalid)\n\t\t\t/* BO hasn't been invalidated since the last\n\t\t\t * revalidation attempt. Keep its page list.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\tbo = mem->bo;\n\n\t\tamdgpu_ttm_tt_discard_user_pages(bo->tbo.ttm, mem->range);\n\t\tmem->range = NULL;\n\n\t\t/* BO reservations and getting user pages (hmm_range_fault)\n\t\t * must happen outside the notifier lock\n\t\t */\n\t\tmutex_unlock(&process_info->notifier_lock);\n\n\t\t/* Move the BO to system (CPU) domain if necessary to unmap\n\t\t * and free the SG table\n\t\t */\n\t\tif (bo->tbo.resource->mem_type != TTM_PL_SYSTEM) {\n\t\t\tif (amdgpu_bo_reserve(bo, true))\n\t\t\t\treturn -EAGAIN;\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Failed to invalidate userptr BO\\n\",\n\t\t\t\t       __func__);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\t/* Get updated user pages */\n\t\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t   &mem->range);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed %d to get user pages\\n\", ret);\n\n\t\t\t/* Return -EFAULT bad address error as success. It will\n\t\t\t * fail later with a VM fault if the GPU tries to access\n\t\t\t * it. Better than hanging indefinitely with stalled\n\t\t\t * user mode queues.\n\t\t\t *\n\t\t\t * Return other error -EBUSY or -ENOMEM to retry restore\n\t\t\t */\n\t\t\tif (ret != -EFAULT)\n\t\t\t\treturn ret;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\tmutex_lock(&process_info->notifier_lock);\n\n\t\t/* Mark the BO as valid unless it was invalidated\n\t\t * again concurrently.\n\t\t */\n\t\tif (mem->invalid != invalid) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t /* set mem valid if mem has hmm range associated */\n\t\tif (mem->range)\n\t\t\tmem->invalid = 0;\n\t}\n\nunlock_out:\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn ret;\n}\n\n/* Validate invalid userptr BOs\n *\n * Validates BOs on the userptr_inval_list. Also updates GPUVM page tables\n * with new page addresses and waits for the page table updates to complete.\n */\nstatic int validate_invalid_user_pages(struct amdkfd_process_info *process_info)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0, 0);\n\t/* Reserve all BOs and page tables for validation */\n\tdrm_exec_until_all_locked(&exec) {\n\t\t/* Reserve all the page directories */\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\t/* Reserve the userptr_inval_list entries to resv_list */\n\t\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\tret = process_validate_vms(process_info, NULL);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\t/* Validate BOs and update GPUVM page tables */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tbo = mem->bo;\n\n\t\t/* Validate the BO if we got user pages */\n\t\tif (bo->tbo.ttm->pages[0]) {\n\t\t\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\n\t\t/* Update mapping. If the BO was not validated\n\t\t * (because we couldn't get user pages), this will\n\t\t * clear the page table entries, which will result in\n\t\t * VM faults if the GPU tries to access the invalid\n\t\t * memory.\n\t\t */\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: update PTE failed\\n\", __func__);\n\t\t\t\t/* make sure this gets validated again */\n\t\t\t\tmutex_lock(&process_info->notifier_lock);\n\t\t\t\tmem->invalid++;\n\t\t\t\tmutex_unlock(&process_info->notifier_lock);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync);\n\nunreserve_out:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_wait(&sync, false);\n\tamdgpu_sync_free(&sync);\n\n\treturn ret;\n}\n\n/* Confirm that all user pages are valid while holding the notifier lock\n *\n * Moves valid BOs from the userptr_inval_list back to userptr_val_list.\n */\nstatic int confirm_valid_user_pages_locked(struct amdkfd_process_info *process_info)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tbool valid;\n\n\t\t/* keep mem without hmm range at userptr_inval_list */\n\t\tif (!mem->range)\n\t\t\tcontinue;\n\n\t\t/* Only check mem with hmm range associated */\n\t\tvalid = amdgpu_ttm_tt_get_user_pages_done(\n\t\t\t\t\tmem->bo->tbo.ttm, mem->range);\n\n\t\tmem->range = NULL;\n\t\tif (!valid) {\n\t\t\tWARN(!mem->invalid, \"Invalid BO not marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem->invalid) {\n\t\t\tWARN(1, \"Valid BO is marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move_tail(&mem->validate_list,\n\t\t\t       &process_info->userptr_valid_list);\n\t}\n\n\treturn ret;\n}\n\n/* Worker callback to restore evicted userptr BOs\n *\n * Tries to update and validate all userptr BOs. If successful and no\n * concurrent evictions happened, the queues are restarted. Otherwise,\n * reschedule for another attempt later.\n */\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info =\n\t\tcontainer_of(dwork, struct amdkfd_process_info,\n\t\t\t     restore_userptr_work);\n\tstruct task_struct *usertask;\n\tstruct mm_struct *mm;\n\tuint32_t evicted_bos;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tevicted_bos = process_info->evicted_bos;\n\tmutex_unlock(&process_info->notifier_lock);\n\tif (!evicted_bos)\n\t\treturn;\n\n\t/* Reference task and mm in case of concurrent process termination */\n\tusertask = get_pid_task(process_info->pid, PIDTYPE_PID);\n\tif (!usertask)\n\t\treturn;\n\tmm = get_task_mm(usertask);\n\tif (!mm) {\n\t\tput_task_struct(usertask);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\n\tif (update_invalid_user_pages(process_info, mm))\n\t\tgoto unlock_out;\n\t/* userptr_inval_list can be empty if all evicted userptr BOs\n\t * have been freed. In that case there is nothing to validate\n\t * and we can just restart the queues.\n\t */\n\tif (!list_empty(&process_info->userptr_inval_list)) {\n\t\tif (validate_invalid_user_pages(process_info))\n\t\t\tgoto unlock_out;\n\t}\n\t/* Final check for concurrent evicton and atomic update. If\n\t * another eviction happens after successful update, it will\n\t * be a first eviction that calls quiesce_mm. The eviction\n\t * reference counting inside KFD will handle this case.\n\t */\n\tmutex_lock(&process_info->notifier_lock);\n\tif (process_info->evicted_bos != evicted_bos)\n\t\tgoto unlock_notifier_out;\n\n\tif (confirm_valid_user_pages_locked(process_info)) {\n\t\tWARN(1, \"User pages unexpectedly invalid\");\n\t\tgoto unlock_notifier_out;\n\t}\n\n\tprocess_info->evicted_bos = evicted_bos = 0;\n\n\tif (kgd2kfd_resume_mm(mm)) {\n\t\tpr_err(\"%s: Failed to resume KFD\\n\", __func__);\n\t\t/* No recovery from this failure. Probably the CP is\n\t\t * hanging. No point trying again.\n\t\t */\n\t}\n\nunlock_notifier_out:\n\tmutex_unlock(&process_info->notifier_lock);\nunlock_out:\n\tmutex_unlock(&process_info->lock);\n\n\t/* If validation failed, reschedule another attempt */\n\tif (evicted_bos) {\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n\tput_task_struct(usertask);\n}\n\nstatic void replace_eviction_fence(struct dma_fence __rcu **ef,\n\t\t\t\t   struct dma_fence *new_ef)\n{\n\tstruct dma_fence *old_ef = rcu_replace_pointer(*ef, new_ef, true\n\t\t/* protected by process_info->lock */);\n\n\t/* If we're replacing an unsignaled eviction fence, that fence will\n\t * never be signaled, and if anyone is still waiting on that fence,\n\t * they will hang forever. This should never happen. We should only\n\t * replace the fence in restore_work that only gets scheduled after\n\t * eviction work signaled the fence.\n\t */\n\tWARN_ONCE(!dma_fence_is_signaled(old_ef),\n\t\t  \"Replacing unsignaled eviction fence\");\n\tdma_fence_put(old_ef);\n}\n\n/** amdgpu_amdkfd_gpuvm_restore_process_bos - Restore all BOs for the given\n *   KFD process identified by process_info\n *\n * @process_info: amdkfd_process_info of the KFD process\n *\n * After memory eviction, restore thread calls this function. The function\n * should be called when the Process is still valid. BO restore involves -\n *\n * 1.  Release old eviction fence and create new one\n * 2.  Get two copies of PD BO list from all the VMs. Keep one copy as pd_list.\n * 3   Use the second PD list and kfd_bo_list to create a list (ctx.list) of\n *     BOs that need to be reserved.\n * 4.  Reserve all the BOs\n * 5.  Validate of PD and PT BOs.\n * 6.  Validate all KFD BOs using kfd_bo_list and Map them and add new fence\n * 7.  Add fence to all PD and PT BOs.\n * 8.  Unreserve all BOs\n */\nint amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct dma_fence __rcu **ef)\n{\n\tstruct amdkfd_process_info *process_info = info;\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem;\n\tstruct list_head duplicate_save;\n\tstruct amdgpu_sync sync_obj;\n\tunsigned long failed_size = 0;\n\tunsigned long total_size = 0;\n\tstruct drm_exec exec;\n\tint ret;\n\n\tINIT_LIST_HEAD(&duplicate_save);\n\n\tmutex_lock(&process_info->lock);\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"Locking VM PD failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Reserve all BOs and page tables/directory. Add all BOs from\n\t\t * kfd_bo_list to ctx.list\n\t\t */\n\t\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"drm_exec_prepare_obj failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tamdgpu_sync_create(&sync_obj);\n\n\t/* Validate BOs managed by KFD */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\n\t\tstruct amdgpu_bo *bo = mem->bo;\n\t\tuint32_t domain = mem->domain;\n\t\tstruct dma_resv_iter cursor;\n\t\tstruct dma_fence *fence;\n\n\t\ttotal_size += amdgpu_bo_size(bo);\n\n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: Validate BOs failed\\n\");\n\t\t\tfailed_size += amdgpu_bo_size(bo);\n\t\t\tret = amdgpu_amdkfd_bo_validate(bo,\n\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT, false);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tdma_resv_for_each_fence(&cursor, bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_KERNEL, fence) {\n\t\t\tret = amdgpu_sync_fence(&sync_obj, fence);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Sync BO fence failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (failed_size)\n\t\tpr_debug(\"0x%lx/0x%lx in system\\n\", failed_size, total_size);\n\n\t/* Validate PDs, PTs and evicted DMABuf imports last. Otherwise BO\n\t * validations above would invalidate DMABuf imports again.\n\t */\n\tret = process_validate_vms(process_info, &exec.ticket);\n\tif (ret) {\n\t\tpr_debug(\"Validating VMs failed, ret: %d\\n\", ret);\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Update mappings managed by KFD. */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync_obj);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: update PTE failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update mappings not managed by KFD */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\tvm_list_node) {\n\t\tstruct amdgpu_device *adev = amdgpu_ttm_adev(\n\t\t\tpeer_vm->root.bo->tbo.bdev);\n\n\t\tret = amdgpu_vm_handle_moved(adev, peer_vm, &exec.ticket);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: handle moved failed. Try again\\n\");\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: update PDs failed. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Sync with fences on all the page tables. They implicitly depend on any\n\t * move fences from amdgpu_vm_handle_moved above.\n\t */\n\tret = process_sync_pds_resv(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: Failed to sync to PD BO moving fence. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Wait for validate and PT updates to finish */\n\tamdgpu_sync_wait(&sync_obj, false);\n\n\t/* The old eviction fence may be unsignaled if restore happens\n\t * after a GPU reset or suspend/resume. Keep the old fence in that\n\t * case. Otherwise release the old eviction fence and create new\n\t * one, because fence only goes from unsignaled to signaled once\n\t * and cannot be reused. Use context and mm from the old fence.\n\t *\n\t * If an old eviction fence signals after this check, that's OK.\n\t * Anyone signaling an eviction fence must stop the queues first\n\t * and schedule another restore worker.\n\t */\n\tif (dma_fence_is_signaled(&process_info->eviction_fence->base)) {\n\t\tstruct amdgpu_amdkfd_fence *new_fence =\n\t\t\tamdgpu_amdkfd_fence_create(\n\t\t\t\tprocess_info->eviction_fence->base.context,\n\t\t\t\tprocess_info->eviction_fence->mm,\n\t\t\t\tNULL);\n\n\t\tif (!new_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tprocess_info->eviction_fence = new_fence;\n\t\treplace_eviction_fence(ef, dma_fence_get(&new_fence->base));\n\t} else {\n\t\tWARN_ONCE(*ef != &process_info->eviction_fence->base,\n\t\t\t  \"KFD eviction fence doesn't match KGD process_info\");\n\t}\n\n\t/* Attach new eviction fence to all BOs except pinned ones */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list, validate_list) {\n\t\tif (mem->bo->tbo.pin_count)\n\t\t\tcontinue;\n\n\t\tdma_resv_add_fence(mem->bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\t/* Attach eviction fence to PD / PT BOs and DMABuf imports */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *bo = peer_vm->root.bo;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\nvalidate_map_fail:\n\tamdgpu_sync_free(&sync_obj);\nttm_reserve_fail:\n\tdrm_exec_fini(&exec);\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_add_gws_to_process(void *info, void *gws, struct kgd_mem **mem)\n{\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct amdgpu_bo *gws_bo = (struct amdgpu_bo *)gws;\n\tint ret;\n\n\tif (!info || !gws)\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&(*mem)->lock);\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\t(*mem)->bo = amdgpu_bo_ref(gws_bo);\n\t(*mem)->domain = AMDGPU_GEM_DOMAIN_GWS;\n\t(*mem)->process_info = process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\n\t/* Validate gws bo the first time it is added to process */\n\tmutex_lock(&(*mem)->process_info->lock);\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\tgoto bo_reservation_failure;\n\t}\n\n\tret = amdgpu_amdkfd_bo_validate(gws_bo, AMDGPU_GEM_DOMAIN_GWS, true);\n\tif (ret) {\n\t\tpr_err(\"GWS BO validate failed %d\\n\", ret);\n\t\tgoto bo_validation_failure;\n\t}\n\t/* GWS resource is shared b/t amdgpu and amdkfd\n\t * Add process eviction fence to bo so they can\n\t * evict each other.\n\t */\n\tret = dma_resv_reserve_fences(gws_bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(gws_bo->tbo.base.resv,\n\t\t\t   &process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(gws_bo);\n\tmutex_unlock(&(*mem)->process_info->lock);\n\n\treturn ret;\n\nreserve_shared_fail:\nbo_validation_failure:\n\tamdgpu_bo_unreserve(gws_bo);\nbo_reservation_failure:\n\tmutex_unlock(&(*mem)->process_info->lock);\n\tamdgpu_sync_free(&(*mem)->sync);\n\tremove_kgd_mem_from_kfd_bo_list(*mem, process_info);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&(*mem)->lock);\n\tkfree(*mem);\n\t*mem = NULL;\n\treturn ret;\n}\n\nint amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)\n{\n\tint ret;\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\tstruct amdgpu_bo *gws_bo = kgd_mem->bo;\n\n\t/* Remove BO from process's validate list so restore worker won't touch\n\t * it anymore\n\t */\n\tremove_kgd_mem_from_kfd_bo_list(kgd_mem, process_info);\n\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\t//TODO add BO back to validate_list?\n\t\treturn ret;\n\t}\n\tamdgpu_amdkfd_remove_eviction_fence(gws_bo,\n\t\t\tprocess_info->eviction_fence);\n\tamdgpu_bo_unreserve(gws_bo);\n\tamdgpu_sync_free(&kgd_mem->sync);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&kgd_mem->lock);\n\tkfree(mem);\n\treturn 0;\n}\n\n/* Returns GPU-specific tiling mode information */\nint amdgpu_amdkfd_get_tile_config(struct amdgpu_device *adev,\n\t\t\t\tstruct tile_config *config)\n{\n\tconfig->gb_addr_config = adev->gfx.config.gb_addr_config;\n\tconfig->tile_config_ptr = adev->gfx.config.tile_mode_array;\n\tconfig->num_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.tile_mode_array);\n\tconfig->macro_tile_config_ptr =\n\t\t\tadev->gfx.config.macrotile_mode_array;\n\tconfig->num_macro_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.macrotile_mode_array);\n\n\t/* Those values are not set from GFX9 onwards */\n\tconfig->num_banks = adev->gfx.config.num_banks;\n\tconfig->num_ranks = adev->gfx.config.num_ranks;\n\n\treturn 0;\n}\n\nbool amdgpu_amdkfd_bo_mapped_to_dev(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->is_mapped && entry->adev == adev)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_kfd_mem_limits(struct seq_file *m, void *data)\n{\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tseq_printf(m, \"System mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.system_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_system_mem_limit >> 20));\n\tseq_printf(m, \"TTM mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.ttm_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_ttm_mem_limit >> 20));\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\treturn 0;\n}\n\n#endif\n", "source_code_path": "amdgpu_amdkfd_gpuvm.c", "line_number": 2464, "generated_diff": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_amdkfd_gpuvm.c\nLine: 2464\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\n====================================\nRead the following bug report.\n============ Bug Report ============\nFile: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription:The function amdgpu_amdkfd_gpuvm_import_dmabuf_fd may access memory that has already been freed. Specifically, the '*mem' pointer may have been released in the import_obj_create function, leaving it pointing to invalid memory. To prevent potential crashes or data corruption, a null check should be added to ensure '*mem' is valid before accessing it.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_amdkfd_gpuvm-bug.txt", "bug_report_text": "File: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription: A potential use-after-free vulnerability exists in the function amdgpu_amdkfd_gpuvm_import_dmabuf_fd. The '*mem' pointer may be accessed after it has been freed in the import_obj_create function, leading to undefined behavior. To mitigate this issue, a null check should be added to verify that '*mem' is valid before accessing its contents.\n", "diff_path": "amdgpu_amdkfd_gpuvm-diff.txt", "diff_text": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2014-2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include <linux/dma-buf.h>\n#include <linux/list.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/fdtable.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_object.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dma_buf.h\"\n#include <uapi/linux/kfd_ioctl.h>\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_smi_events.h\"\n\n/* Userptr restore delay, just long enough to allow consecutive VM\n * changes to accumulate\n */\n#define AMDGPU_USERPTR_RESTORE_DELAY_MS 1\n#define AMDGPU_RESERVE_MEM_LIMIT\t\t\t(3UL << 29)\n\n/*\n * Align VRAM availability to 2MB to avoid fragmentation caused by 4K allocations in the tail 2MB\n * BO chunk\n */\n#define VRAM_AVAILABLITY_ALIGN (1 << 21)\n\n/* Impose limit on how much memory KFD can use */\nstatic struct {\n\tuint64_t max_system_mem_limit;\n\tuint64_t max_ttm_mem_limit;\n\tint64_t system_mem_used;\n\tint64_t ttm_mem_used;\n\tspinlock_t mem_limit_lock;\n} kfd_mem_limit;\n\nstatic const char * const domain_bit_to_string[] = {\n\t\t\"CPU\",\n\t\t\"GTT\",\n\t\t\"VRAM\",\n\t\t\"GDS\",\n\t\t\"GWS\",\n\t\t\"OA\"\n};\n\n#define domain_string(domain) domain_bit_to_string[ffs(domain)-1]\n\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work);\n\nstatic bool kfd_mem_is_attached(struct amdgpu_vm *avm,\n\t\tstruct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list)\n\t\tif (entry->bo_va->base.vm == avm)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * reuse_dmamap() - Check whether adev can share the original\n * userptr BO\n *\n * If both adev and bo_adev are in direct mapping or\n * in the same iommu group, they can share the original BO.\n *\n * @adev: Device to which can or cannot share the original BO\n * @bo_adev: Device to which allocated BO belongs to\n *\n * Return: returns true if adev can share original userptr BO,\n * false otherwise.\n */\nstatic bool reuse_dmamap(struct amdgpu_device *adev, struct amdgpu_device *bo_adev)\n{\n\treturn (adev->ram_is_direct_mapped && bo_adev->ram_is_direct_mapped) ||\n\t\t\t(adev->dev->iommu_group == bo_adev->dev->iommu_group);\n}\n\n/* Set memory usage limits. Current, limits are\n *  System (TTM + userptr) memory - 15/16th System RAM\n *  TTM memory - 3/8th System RAM\n */\nvoid amdgpu_amdkfd_gpuvm_init_mem_limits(void)\n{\n\tstruct sysinfo si;\n\tuint64_t mem;\n\n\tif (kfd_mem_limit.max_system_mem_limit)\n\t\treturn;\n\n\tsi_meminfo(&si);\n\tmem = si.totalram - si.totalhigh;\n\tmem *= si.mem_unit;\n\n\tspin_lock_init(&kfd_mem_limit.mem_limit_lock);\n\tkfd_mem_limit.max_system_mem_limit = mem - (mem >> 6);\n\tif (kfd_mem_limit.max_system_mem_limit < 2 * AMDGPU_RESERVE_MEM_LIMIT)\n\t\tkfd_mem_limit.max_system_mem_limit >>= 1;\n\telse\n\t\tkfd_mem_limit.max_system_mem_limit -= AMDGPU_RESERVE_MEM_LIMIT;\n\n\tkfd_mem_limit.max_ttm_mem_limit = ttm_tt_pages_limit() << PAGE_SHIFT;\n\tpr_debug(\"Kernel memory limit %lluM, TTM limit %lluM\\n\",\n\t\t(kfd_mem_limit.max_system_mem_limit >> 20),\n\t\t(kfd_mem_limit.max_ttm_mem_limit >> 20));\n}\n\nvoid amdgpu_amdkfd_reserve_system_mem(uint64_t size)\n{\n\tkfd_mem_limit.system_mem_used += size;\n}\n\n/* Estimate page table size needed to represent a given memory size\n *\n * With 4KB pages, we need one 8 byte PTE for each 4KB of memory\n * (factor 512, >> 9). With 2MB pages, we need one 8 byte PTE for 2MB\n * of memory (factor 256K, >> 18). ROCm user mode tries to optimize\n * for 2MB pages for TLB efficiency. However, small allocations and\n * fragmented system memory still need some 4KB pages. We choose a\n * compromise that should work in most cases without reserving too\n * much memory for page tables unnecessarily (factor 16K, >> 14).\n */\n\n#define ESTIMATE_PT_SIZE(mem_size) max(((mem_size) >> 14), AMDGPU_VM_RESERVED_VRAM)\n\n/**\n * amdgpu_amdkfd_reserve_mem_limit() - Decrease available memory by size\n * of buffer.\n *\n * @adev: Device to which allocated BO belongs to\n * @size: Size of buffer, in bytes, encapsulated by B0. This should be\n * equivalent to amdgpu_bo_size(BO)\n * @alloc_flag: Flag used in allocating a BO as noted above\n * @xcp_id: xcp_id is used to get xcp from xcp manager, one xcp is\n * managed as one compute node in driver for app\n *\n * Return:\n *\treturns -ENOMEM in case of error, ZERO otherwise\n */\nint amdgpu_amdkfd_reserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tsize_t system_mem_needed, ttm_mem_needed, vram_needed;\n\tint ret = 0;\n\tuint64_t vram_size = 0;\n\n\tsystem_mem_needed = 0;\n\tttm_mem_needed = 0;\n\tvram_needed = 0;\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tsystem_mem_needed = size;\n\t\tttm_mem_needed = size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\t/*\n\t\t * Conservatively round up the allocation requirement to 2 MB\n\t\t * to avoid fragmentation caused by 4K allocations in the tail\n\t\t * 2M BO chunk.\n\t\t */\n\t\tvram_needed = size;\n\t\t/*\n\t\t * For GFX 9.4.3, get the VRAM size from XCP structs\n\t\t */\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\treturn -EINVAL;\n\n\t\tvram_size = KFD_XCP_MEMORY_SIZE(adev, xcp_id);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tsystem_mem_needed = size;\n\t\t\tttm_mem_needed = size;\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tsystem_mem_needed = size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (kfd_mem_limit.system_mem_used + system_mem_needed >\n\t    kfd_mem_limit.max_system_mem_limit)\n\t\tpr_debug(\"Set no_system_mem_limit=1 if using shared memory\\n\");\n\n\tif ((kfd_mem_limit.system_mem_used + system_mem_needed >\n\t     kfd_mem_limit.max_system_mem_limit && !no_system_mem_limit) ||\n\t    (kfd_mem_limit.ttm_mem_used + ttm_mem_needed >\n\t     kfd_mem_limit.max_ttm_mem_limit) ||\n\t    (adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] + vram_needed >\n\t     vram_size - reserved_for_pt - reserved_for_ras - atomic64_read(&adev->vram_pin_size))) {\n\t\tret = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\t/* Update memory accounting by decreasing available system\n\t * memory, TTM memory and GPU memory as computed above\n\t */\n\tWARN_ONCE(vram_needed && !adev,\n\t\t  \"adev reference can't be null when vram is used\");\n\tif (adev && xcp_id >= 0) {\n\t\tadev->kfd.vram_used[xcp_id] += vram_needed;\n\t\tadev->kfd.vram_used_aligned[xcp_id] +=\n\t\t\t\t(adev->flags & AMD_IS_APU) ?\n\t\t\t\tvram_needed :\n\t\t\t\tALIGN(vram_needed, VRAM_AVAILABLITY_ALIGN);\n\t}\n\tkfd_mem_limit.system_mem_used += system_mem_needed;\n\tkfd_mem_limit.ttm_mem_used += ttm_mem_needed;\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\treturn ret;\n}\n\nvoid amdgpu_amdkfd_unreserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tWARN_ONCE(!adev,\n\t\t\t  \"adev reference can't be null when alloc mem flags vram is set\");\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\tgoto release;\n\n\t\tif (adev) {\n\t\t\tadev->kfd.vram_used[xcp_id] -= size;\n\t\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -= size;\n\t\t\t\tkfd_mem_limit.system_mem_used -= size;\n\t\t\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t\t\t} else {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -=\n\t\t\t\t\tALIGN(size, VRAM_AVAILABLITY_ALIGN);\n\t\t\t}\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\tgoto release;\n\t}\n\tWARN_ONCE(adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] < 0,\n\t\t  \"KFD VRAM memory accounting unbalanced for xcp: %d\", xcp_id);\n\tWARN_ONCE(kfd_mem_limit.ttm_mem_used < 0,\n\t\t  \"KFD TTM memory accounting unbalanced\");\n\tWARN_ONCE(kfd_mem_limit.system_mem_used < 0,\n\t\t  \"KFD system memory accounting unbalanced\");\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n}\n\nvoid amdgpu_amdkfd_release_notify(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tu32 alloc_flags = bo->kfd_bo->alloc_flags;\n\tu64 size = amdgpu_bo_size(bo);\n\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, size, alloc_flags,\n\t\t\t\t\t  bo->xcp_id);\n\n\tkfree(bo->kfd_bo);\n}\n\n/**\n * create_dmamap_sg_bo() - Creates a amdgpu_bo object to reflect information\n * about USERPTR or DOOREBELL or MMIO BO.\n *\n * @adev: Device for which dmamap BO is being created\n * @mem: BO of peer device that is being DMA mapped. Provides parameters\n *\t in building the dmamap BO\n * @bo_out: Output parameter updated with handle of dmamap BO\n */\nstatic int\ncreate_dmamap_sg_bo(struct amdgpu_device *adev,\n\t\t struct kgd_mem *mem, struct amdgpu_bo **bo_out)\n{\n\tstruct drm_gem_object *gem_obj;\n\tint ret;\n\tuint64_t flags = 0;\n\n\tret = amdgpu_bo_reserve(mem->bo, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR)\n\t\tflags |= mem->bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t\t\tAMDGPU_GEM_CREATE_UNCACHED);\n\n\tret = amdgpu_gem_object_create(adev, mem->bo->tbo.base.size, 1,\n\t\t\tAMDGPU_GEM_DOMAIN_CPU, AMDGPU_GEM_CREATE_PREEMPTIBLE | flags,\n\t\t\tttm_bo_type_sg, mem->bo->tbo.base.resv, &gem_obj, 0);\n\n\tamdgpu_bo_unreserve(mem->bo);\n\n\tif (ret) {\n\t\tpr_err(\"Error in creating DMA mappable SG BO on domain: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\n\t*bo_out = gem_to_amdgpu_bo(gem_obj);\n\t(*bo_out)->parent = amdgpu_bo_ref(mem->bo);\n\treturn ret;\n}\n\n/* amdgpu_amdkfd_remove_eviction_fence - Removes eviction fence from BO's\n *  reservation object.\n *\n * @bo: [IN] Remove eviction fence(s) from this BO\n * @ef: [IN] This eviction fence is removed if it\n *  is present in the shared list.\n *\n * NOTE: Must be called with BO reserved i.e. bo->tbo.resv->lock held.\n */\nstatic int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tstruct amdgpu_amdkfd_fence *ef)\n{\n\tstruct dma_fence *replacement;\n\n\tif (!ef)\n\t\treturn -EINVAL;\n\n\t/* TODO: Instead of block before we should use the fence of the page\n\t * table update and TLB flush here directly.\n\t */\n\treplacement = dma_fence_get_stub();\n\tdma_resv_replace_fences(bo->tbo.base.resv, ef->base.context,\n\t\t\t\treplacement, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_fence_put(replacement);\n\treturn 0;\n}\n\nint amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo *root = bo;\n\tstruct amdgpu_vm_bo_base *vm_bo;\n\tstruct amdgpu_vm *vm;\n\tstruct amdkfd_process_info *info;\n\tstruct amdgpu_amdkfd_fence *ef;\n\tint ret;\n\n\t/* we can always get vm_bo from root PD bo.*/\n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tvm_bo = root->vm_bo;\n\tif (!vm_bo)\n\t\treturn 0;\n\n\tvm = vm_bo->vm;\n\tif (!vm)\n\t\treturn 0;\n\n\tinfo = vm->process_info;\n\tif (!info || !info->eviction_fence)\n\t\treturn 0;\n\n\tef = container_of(dma_fence_get(&info->eviction_fence->base),\n\t\t\tstruct amdgpu_amdkfd_fence, base);\n\n\tBUG_ON(!dma_resv_trylock(bo->tbo.base.resv));\n\tret = amdgpu_amdkfd_remove_eviction_fence(bo, ef);\n\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tdma_fence_put(&ef->base);\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,\n\t\t\t\t     bool wait)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint ret;\n\n\tif (WARN(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm),\n\t\t \"Called with userptr BO\"))\n\t\treturn -EINVAL;\n\n\t/* bo has been pinned, not need validate it */\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto validate_fail;\n\tif (wait)\n\t\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\nvalidate_fail:\n\treturn ret;\n}\n\nint amdgpu_amdkfd_bo_validate_and_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tuint32_t domain,\n\t\t\t\t\tstruct dma_fence *fence)\n{\n\tint ret = amdgpu_bo_reserve(bo, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = amdgpu_amdkfd_bo_validate(bo, domain, true);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tret = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tdma_resv_add_fence(bo->tbo.base.resv, fence,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\nunreserve_out:\n\tamdgpu_bo_unreserve(bo);\n\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_validate_vm_bo(void *_unused, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_amdkfd_bo_validate(bo, bo->allowed_domains, false);\n}\n\n/* vm_validate_pt_pd_bos - Validate page table and directory BOs\n *\n * Page directories are not updated here because huge page handling\n * during page table updates can invalidate page directory entries\n * again. Page directories are only updated after updating page\n * tables.\n */\nstatic int vm_validate_pt_pd_bos(struct amdgpu_vm *vm,\n\t\t\t\t struct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_validate(adev, vm, ticket,\n\t\t\t\t amdgpu_amdkfd_validate_vm_bo, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to validate PT BOs\\n\");\n\t\treturn ret;\n\t}\n\n\tvm->pd_phys_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\n\treturn 0;\n}\n\nstatic int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_sync_fence(sync, vm->last_update);\n}\n\nstatic uint64_t get_pte_flags(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tuint32_t mapping_flags = AMDGPU_VM_PAGE_READABLE |\n\t\t\t\t AMDGPU_VM_MTYPE_DEFAULT;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\treturn amdgpu_gem_va_map_flags(adev, mapping_flags);\n}\n\n/**\n * create_sg_table() - Create an sg_table for a contiguous DMA addr range\n * @addr: The starting address to point to\n * @size: Size of memory area in bytes being pointed to\n *\n * Allocates an instance of sg_table and initializes it to point to memory\n * area specified by input parameters. The address used to build is assumed\n * to be DMA mapped, if needed.\n *\n * DOORBELL or MMIO BOs use only one scatterlist node in their sg_table\n * because they are physically contiguous.\n *\n * Return: Initialized instance of SG Table or NULL\n */\nstatic struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n{\n\tstruct sg_table *sg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\n\tif (!sg)\n\t\treturn NULL;\n\tif (sg_alloc_table(sg, 1, GFP_KERNEL)) {\n\t\tkfree(sg);\n\t\treturn NULL;\n\t}\n\tsg_dma_address(sg->sgl) = addr;\n\tsg->sgl->length = size;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\tsg->sgl->dma_length = size;\n#endif\n\treturn sg;\n}\n\nstatic int\nkfd_mem_dmamap_userptr(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *src_ttm = mem->bo->tbo.ttm;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tint ret;\n\n\tif (WARN_ON(ttm->num_pages != src_ttm->num_pages))\n\t\treturn -EINVAL;\n\n\tttm->sg = kmalloc(sizeof(*ttm->sg), GFP_KERNEL);\n\tif (unlikely(!ttm->sg))\n\t\treturn -ENOMEM;\n\n\t/* Same sequence as in amdgpu_ttm_tt_pin_userptr */\n\tret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,\n\t\t\t\t\tttm->num_pages, 0,\n\t\t\t\t\t(u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\tret = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (unlikely(ret))\n\t\tgoto release_sg;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto unmap_sg;\n\n\treturn 0;\n\nunmap_sg:\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\nrelease_sg:\n\tpr_err(\"DMA map userptr failed: %d\\n\", ret);\n\tsg_free_table(ttm->sg);\nfree_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tint ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n/**\n * kfd_mem_dmamap_sg_bo() - Create DMA mapped sg_table to access DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * An access request from the device that owns DOORBELL does not require DMA mapping.\n * This is because the request doesn't go through PCIe root complex i.e. it instead\n * loops back. The need to DMA map arises only when accessing peer device's DOORBELL\n *\n * In contrast, all access requests for MMIO need to be DMA mapped without regard to\n * device ownership. This is because access requests for MMIO go through PCIe root\n * complex.\n *\n * This is accomplished in two steps:\n *   - Obtain DMA mapped address of DOORBELL or MMIO memory that could be used\n *         in updating requesting device's page table\n *   - Signal TTM to mark memory pointed to by requesting device's BO as GPU\n *         accessible. This allows an update of requesting device's page table\n *         with entries associated with DOOREBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *   - Mapping of DOORBELL or MMIO BO of same or peer device\n *   - Validating an evicted DOOREBELL or MMIO BO on device seeking access\n *\n * Return: ZERO if successful, NON-ZERO otherwise\n */\nstatic int\nkfd_mem_dmamap_sg_bo(struct kgd_mem *mem,\n\t\t     struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\tdma_addr_t dma_addr;\n\tbool mmio;\n\tint ret;\n\n\t/* Expect SG Table of dmapmap BO to be NULL */\n\tmmio = (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);\n\tif (unlikely(ttm->sg)) {\n\t\tpr_err(\"SG Table of %d BO for peer device is UNEXPECTEDLY NON-NULL\", mmio);\n\t\treturn -EINVAL;\n\t}\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_addr = mem->bo->tbo.sg->sgl->dma_address;\n\tpr_debug(\"%d BO size: %d\\n\", mmio, mem->bo->tbo.sg->sgl->length);\n\tpr_debug(\"%d BO address before DMA mapping: %llx\\n\", mmio, dma_addr);\n\tdma_addr = dma_map_resource(adev->dev, dma_addr,\n\t\t\tmem->bo->tbo.sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tret = dma_mapping_error(adev->dev, dma_addr);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tpr_debug(\"%d BO address after DMA mapping: %llx\\n\", mmio, dma_addr);\n\n\tttm->sg = create_sg_table(dma_addr, mem->bo->tbo.sg->sgl->length);\n\tif (unlikely(!ttm->sg)) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\treturn ret;\n\nfree_sg:\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\nunmap_sg:\n\tdma_unmap_resource(adev->dev, dma_addr, mem->bo->tbo.sg->sgl->length,\n\t\t\t   dir, DMA_ATTR_SKIP_CPU_SYNC);\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_attachment(struct kgd_mem *mem,\n\t\t\t  struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\treturn 0;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\treturn kfd_mem_dmamap_userptr(mem, attachment);\n\tcase KFD_MEM_ATT_DMABUF:\n\t\treturn kfd_mem_dmamap_dmabuf(attachment);\n\tcase KFD_MEM_ATT_SG:\n\t\treturn kfd_mem_dmamap_sg_bo(mem, attachment);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nkfd_mem_dmaunmap_userptr(struct kgd_mem *mem,\n\t\t\t struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = false};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\n\tif (unlikely(!ttm->sg))\n\t\treturn;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\t/* This is a no-op. We don't want to trigger eviction fences when\n\t * unmapping DMABufs. Therefore the invalidation (moving to system\n\t * domain) is done in kfd_mem_dmamap_dmabuf.\n\t */\n}\n\n/**\n * kfd_mem_dmaunmap_sg_bo() - Free DMA mapped sg_table of DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * The method performs following steps:\n *   - Signal TTM to mark memory pointed to by BO as GPU inaccessible\n *   - Free SG Table that is used to encapsulate DMA mapped memory of\n *          peer device's DOORBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *     UNMapping of DOORBELL or MMIO BO on a device having access to its memory\n *     Eviction of DOOREBELL or MMIO BO on device having access to its memory\n *\n * Return: void\n */\nstatic void\nkfd_mem_dmaunmap_sg_bo(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\n\tif (unlikely(!ttm->sg)) {\n\t\tpr_debug(\"SG Table of BO is NULL\");\n\t\treturn;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_unmap_resource(adev->dev, ttm->sg->sgl->dma_address,\n\t\t\tttm->sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\tbo->tbo.sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_attachment(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\tbreak;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\tkfd_mem_dmaunmap_userptr(mem, attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_DMABUF:\n\t\tkfd_mem_dmaunmap_dmabuf(attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_SG:\n\t\tkfd_mem_dmaunmap_sg_bo(mem, attachment);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic int kfd_mem_export_dmabuf(struct kgd_mem *mem)\n{\n\tif (!mem->dmabuf) {\n\t\tstruct amdgpu_device *bo_adev;\n\t\tstruct dma_buf *dmabuf;\n\t\tint r, fd;\n\n\t\tbo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\t\tr = drm_gem_prime_handle_to_fd(&bo_adev->ddev, bo_adev->kfd.client.file,\n\t\t\t\t\t       mem->gem_handle,\n\t\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\t\t       DRM_RDWR : 0, &fd);\n\t\tif (r)\n\t\t\treturn r;\n\t\tdmabuf = dma_buf_get(fd);\n\t\tclose_fd(fd);\n\t\tif (WARN_ON_ONCE(IS_ERR(dmabuf)))\n\t\t\treturn PTR_ERR(dmabuf);\n\t\tmem->dmabuf = dmabuf;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nkfd_mem_attach_dmabuf(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\t      struct amdgpu_bo **bo)\n{\n\tstruct drm_gem_object *gobj;\n\tint ret;\n\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tgobj = amdgpu_gem_prime_import(adev_to_drm(adev), mem->dmabuf);\n\tif (IS_ERR(gobj))\n\t\treturn PTR_ERR(gobj);\n\n\t*bo = gem_to_amdgpu_bo(gobj);\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\treturn 0;\n}\n\n/* kfd_mem_attach - Add a BO to a VM\n *\n * Everything that needs to bo done only once when a BO is first added\n * to a VM. It can later be mapped and unmapped many times without\n * repeating these steps.\n *\n * 0. Create BO for DMA mapping, if needed\n * 1. Allocate and initialize BO VA entry data structure\n * 2. Add BO to the VM\n * 3. Determine ASIC-specific PTE flags\n * 4. Alloc page tables and directories if needed\n * 4a.  Validate new page tables and directories\n */\nstatic int kfd_mem_attach(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tstruct amdgpu_vm *vm, bool is_aql)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tuint64_t va = mem->va;\n\tstruct kfd_mem_attachment *attachment[2] = {NULL, NULL};\n\tstruct amdgpu_bo *bo[2] = {NULL, NULL};\n\tstruct amdgpu_bo_va *bo_va;\n\tbool same_hive = false;\n\tint i, ret;\n\n\tif (!va) {\n\t\tpr_err(\"Invalid VA when adding BO to VM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Determine access to VRAM, MMIO and DOORBELL BOs of peer devices\n\t *\n\t * The access path of MMIO and DOORBELL BOs of is always over PCIe.\n\t * In contrast the access path of VRAM BOs depens upon the type of\n\t * link that connects the peer device. Access over PCIe is allowed\n\t * if peer device has large BAR. In contrast, access over xGMI is\n\t * allowed for both small and large BAR configurations of peer device\n\t */\n\tif ((adev != bo_adev && !(adev->flags & AMD_IS_APU)) &&\n\t    ((mem->domain == AMDGPU_GEM_DOMAIN_VRAM) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tif (mem->domain == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\tsame_hive = amdgpu_xgmi_same_hive(adev, bo_adev);\n\t\tif (!same_hive && !amdgpu_device_is_peer_accessible(bo_adev, adev))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i <= is_aql; i++) {\n\t\tattachment[i] = kzalloc(sizeof(*attachment[i]), GFP_KERNEL);\n\t\tif (unlikely(!attachment[i])) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tpr_debug(\"\\t add VA 0x%llx - 0x%llx to vm %p\\n\", va,\n\t\t\t va + bo_size, vm);\n\n\t\tif ((adev == bo_adev && !(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) ||\n\t\t    (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && reuse_dmamap(adev, bo_adev)) ||\n\t\t    (mem->domain == AMDGPU_GEM_DOMAIN_GTT && reuse_dmamap(adev, bo_adev)) ||\n\t\t    same_hive) {\n\t\t\t/* Mappings on the local GPU, or VRAM mappings in the\n\t\t\t * local hive, or userptr, or GTT mapping can reuse dma map\n\t\t\t * address space share the original BO\n\t\t\t */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = mem->bo;\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (i > 0) {\n\t\t\t/* Multiple mappings on the same GPU share the BO */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = bo[0];\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\t\t/* Create an SG BO to DMA-map userptrs on other GPUs */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_USERPTR;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Handle DOORBELL BOs of peer devices and MMIO BOs of local and peer devices */\n\t\t} else if (mem->bo->tbo.type == ttm_bo_type_sg) {\n\t\t\tWARN_ONCE(!(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL ||\n\t\t\t\t    mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP),\n\t\t\t\t  \"Handing invalid SG BO in ATTACH request\");\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SG;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Enable acces to GTT and VRAM BOs of peer devices */\n\t\t} else if (mem->domain == AMDGPU_GEM_DOMAIN_GTT ||\n\t\t\t   mem->domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tattachment[i]->type = KFD_MEM_ATT_DMABUF;\n\t\t\tret = kfd_mem_attach_dmabuf(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t\tpr_debug(\"Employ DMABUF mechanism to enable peer GPU access\\n\");\n\t\t} else {\n\t\t\tWARN_ONCE(true, \"Handling invalid ATTACH request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\t/* Add BO to VM internal data structures */\n\t\tret = amdgpu_bo_reserve(bo[i], false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Unable to reserve BO during memory attach\");\n\t\t\tgoto unwind;\n\t\t}\n\t\tbo_va = amdgpu_vm_bo_find(vm, bo[i]);\n\t\tif (!bo_va)\n\t\t\tbo_va = amdgpu_vm_bo_add(adev, vm, bo[i]);\n\t\telse\n\t\t\t++bo_va->ref_count;\n\t\tattachment[i]->bo_va = bo_va;\n\t\tamdgpu_bo_unreserve(bo[i]);\n\t\tif (unlikely(!attachment[i]->bo_va)) {\n\t\t\tret = -ENOMEM;\n\t\t\tpr_err(\"Failed to add BO object to VM. ret == %d\\n\",\n\t\t\t       ret);\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->va = va;\n\t\tattachment[i]->pte_flags = get_pte_flags(adev, mem);\n\t\tattachment[i]->adev = adev;\n\t\tlist_add(&attachment[i]->list, &mem->attachments);\n\n\t\tva += bo_size;\n\t}\n\n\treturn 0;\n\nunwind:\n\tfor (; i >= 0; i--) {\n\t\tif (!attachment[i])\n\t\t\tcontinue;\n\t\tif (attachment[i]->bo_va) {\n\t\t\tamdgpu_bo_reserve(bo[i], true);\n\t\t\tif (--attachment[i]->bo_va->ref_count == 0)\n\t\t\t\tamdgpu_vm_bo_del(adev, attachment[i]->bo_va);\n\t\t\tamdgpu_bo_unreserve(bo[i]);\n\t\t\tlist_del(&attachment[i]->list);\n\t\t}\n\t\tif (bo[i])\n\t\t\tdrm_gem_object_put(&bo[i]->tbo.base);\n\t\tkfree(attachment[i]);\n\t}\n\treturn ret;\n}\n\nstatic void kfd_mem_detach(struct kfd_mem_attachment *attachment)\n{\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\n\tpr_debug(\"\\t remove VA 0x%llx in entry %p\\n\",\n\t\t\tattachment->va, attachment);\n\tif (--attachment->bo_va->ref_count == 0)\n\t\tamdgpu_vm_bo_del(attachment->adev, attachment->bo_va);\n\tdrm_gem_object_put(&bo->tbo.base);\n\tlist_del(&attachment->list);\n\tkfree(attachment);\n}\n\nstatic void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,\n\t\t\t\tstruct amdkfd_process_info *process_info,\n\t\t\t\tbool userptr)\n{\n\tmutex_lock(&process_info->lock);\n\tif (userptr)\n\t\tlist_add_tail(&mem->validate_list,\n\t\t\t      &process_info->userptr_valid_list);\n\telse\n\t\tlist_add_tail(&mem->validate_list, &process_info->kfd_bo_list);\n\tmutex_unlock(&process_info->lock);\n}\n\nstatic void remove_kgd_mem_from_kfd_bo_list(struct kgd_mem *mem,\n\t\tstruct amdkfd_process_info *process_info)\n{\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n}\n\n/* Initializes user pages. It registers the MMU notifier and validates\n * the userptr BO in the GTT domain.\n *\n * The BO must already be on the userptr_valid_list. Otherwise an\n * eviction and restore may happen that leaves the new BO unmapped\n * with the user mode queues running.\n *\n * Takes the process_info->lock to protect against concurrent restore\n * workers.\n *\n * Returns 0 for success, negative errno for errors.\n */\nstatic int init_user_pages(struct kgd_mem *mem, uint64_t user_addr,\n\t\t\t   bool criu_resume)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct hmm_range *range;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->lock);\n\n\tret = amdgpu_ttm_tt_set_userptr(&bo->tbo, user_addr, 0);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to set userptr: %d\\n\", __func__, ret);\n\t\tgoto out;\n\t}\n\n\tret = amdgpu_hmm_register(bo, user_addr);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register MMU notifier: %d\\n\",\n\t\t       __func__, ret);\n\t\tgoto out;\n\t}\n\n\tif (criu_resume) {\n\t\t/*\n\t\t * During a CRIU restore operation, the userptr buffer objects\n\t\t * will be validated in the restore_userptr_work worker at a\n\t\t * later stage when it is scheduled by another ioctl called by\n\t\t * CRIU master process for the target pid for restore.\n\t\t */\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tmem->invalid++;\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t\tmutex_unlock(&process_info->lock);\n\t\treturn 0;\n\t}\n\n\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);\n\tif (ret) {\n\t\tif (ret == -EAGAIN)\n\t\t\tpr_debug(\"Failed to get user pages, try again\\n\");\n\t\telse\n\t\t\tpr_err(\"%s: Failed to get user pages: %d\\n\", __func__, ret);\n\t\tgoto unregister_out;\n\t}\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to reserve BO\\n\", __func__);\n\t\tgoto release_out;\n\t}\n\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\tamdgpu_bo_unreserve(bo);\n\nrelease_out:\n\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\nunregister_out:\n\tif (ret)\n\t\tamdgpu_hmm_unregister(bo);\nout:\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\n/* Reserving a BO and its page table BOs must happen atomically to\n * avoid deadlocks. Some operations update multiple VMs at once. Track\n * all the reservation info in a context structure. Optionally a sync\n * object can track VM updates.\n */\nstruct bo_vm_reservation_context {\n\t/* DRM execution context for the reservation */\n\tstruct drm_exec exec;\n\t/* Number of VMs reserved */\n\tunsigned int n_vms;\n\t/* Pointer to sync object */\n\tstruct amdgpu_sync *sync;\n};\n\nenum bo_vm_match {\n\tBO_VM_NOT_MAPPED = 0,\t/* Match VMs where a BO is not mapped */\n\tBO_VM_MAPPED,\t\t/* Match VMs where a BO is mapped     */\n\tBO_VM_ALL,\t\t/* Match all VMs a BO was added to    */\n};\n\n/**\n * reserve_bo_and_vm - reserve a BO and a VM unconditionally.\n * @mem: KFD BO structure.\n * @vm: the VM to reserve.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n */\nstatic int reserve_bo_and_vm(struct kgd_mem *mem,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct bo_vm_reservation_context *ctx)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tWARN_ON(!vm);\n\n\tctx->n_vms = 1;\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tret = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * reserve_bo_and_cond_vms - reserve a BO and some VMs conditionally\n * @mem: KFD BO structure.\n * @vm: the VM to reserve. If NULL, then all VMs associated with the BO\n * is used. Otherwise, a single VM associated with the BO.\n * @map_type: the mapping status that will be used to filter the VMs.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n *\n * Returns 0 for success, negative for failure.\n */\nstatic int reserve_bo_and_cond_vms(struct kgd_mem *mem,\n\t\t\t\tstruct amdgpu_vm *vm, enum bo_vm_match map_type,\n\t\t\t\tstruct bo_vm_reservation_context *ctx)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tctx->n_vms = 0;\n\t\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\t\tif ((vm && vm != entry->bo_va->base.vm) ||\n\t\t\t\t(entry->is_mapped != map_type\n\t\t\t\t&& map_type != BO_VM_ALL))\n\t\t\t\tcontinue;\n\n\t\t\tret = amdgpu_vm_lock_pd(entry->bo_va->base.vm,\n\t\t\t\t\t\t&ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto error;\n\t\t\t++ctx->n_vms;\n\t\t}\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * unreserve_bo_and_vms - Unreserve BO and VMs from a reservation context\n * @ctx: Reservation context to unreserve\n * @wait: Optionally wait for a sync object representing pending VM updates\n * @intr: Whether the wait is interruptible\n *\n * Also frees any resources allocated in\n * reserve_bo_and_(cond_)vm(s). Returns the status from\n * amdgpu_sync_wait.\n */\nstatic int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,\n\t\t\t\t bool wait, bool intr)\n{\n\tint ret = 0;\n\n\tif (wait)\n\t\tret = amdgpu_sync_wait(ctx->sync, intr);\n\n\tdrm_exec_fini(&ctx->exec);\n\tctx->sync = NULL;\n\treturn ret;\n}\n\nstatic void unmap_bo_from_gpuvm(struct kgd_mem *mem,\n\t\t\t\tstruct kfd_mem_attachment *entry,\n\t\t\t\tstruct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\n\tamdgpu_vm_bo_unmap(adev, bo_va, entry->va);\n\n\tamdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);\n\n\tamdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int update_gpuvm_pte(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *entry,\n\t\t\t    struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tint ret;\n\n\tret = kfd_mem_dmamap_attachment(mem, entry);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Update the page tables  */\n\tret = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (ret) {\n\t\tpr_err(\"amdgpu_vm_bo_update failed\\n\");\n\t\treturn ret;\n\t}\n\n\treturn amdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int map_bo_to_gpuvm(struct kgd_mem *mem,\n\t\t\t   struct kfd_mem_attachment *entry,\n\t\t\t   struct amdgpu_sync *sync,\n\t\t\t   bool no_update_pte)\n{\n\tint ret;\n\n\t/* Set virtual address for the allocation */\n\tret = amdgpu_vm_bo_map(entry->adev, entry->bo_va, entry->va, 0,\n\t\t\t       amdgpu_bo_size(entry->bo_va->base.bo),\n\t\t\t       entry->pte_flags);\n\tif (ret) {\n\t\tpr_err(\"Failed to map VA 0x%llx in vm. ret %d\\n\",\n\t\t\t\tentry->va, ret);\n\t\treturn ret;\n\t}\n\n\tif (no_update_pte)\n\t\treturn 0;\n\n\tret = update_gpuvm_pte(mem, entry, sync);\n\tif (ret) {\n\t\tpr_err(\"update_gpuvm_pte() failed\\n\");\n\t\tgoto update_gpuvm_pte_failed;\n\t}\n\n\treturn 0;\n\nupdate_gpuvm_pte_failed:\n\tunmap_bo_from_gpuvm(mem, entry, sync);\n\tkfd_mem_dmaunmap_attachment(mem, entry);\n\treturn ret;\n}\n\nstatic int process_validate_vms(struct amdkfd_process_info *process_info,\n\t\t\t\tstruct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_validate_pt_pd_bos(peer_vm, ticket);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_sync_pds_resv(struct amdkfd_process_info *process_info,\n\t\t\t\t struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *pd = peer_vm->root.bo;\n\n\t\tret = amdgpu_sync_resv(NULL, sync, pd->tbo.base.resv,\n\t\t\t\t       AMDGPU_SYNC_NE_OWNER,\n\t\t\t\t       AMDGPU_FENCE_OWNER_KFD);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_update_pds(struct amdkfd_process_info *process_info,\n\t\t\t      struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_update_pds(peer_vm, sync);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,\n\t\t       struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *info = NULL;\n\tint ret;\n\n\tif (!*process_info) {\n\t\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tmutex_init(&info->lock);\n\t\tmutex_init(&info->notifier_lock);\n\t\tINIT_LIST_HEAD(&info->vm_list_head);\n\t\tINIT_LIST_HEAD(&info->kfd_bo_list);\n\t\tINIT_LIST_HEAD(&info->userptr_valid_list);\n\t\tINIT_LIST_HEAD(&info->userptr_inval_list);\n\n\t\tinfo->eviction_fence =\n\t\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t\t   current->mm,\n\t\t\t\t\t\t   NULL);\n\t\tif (!info->eviction_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto create_evict_fence_fail;\n\t\t}\n\n\t\tinfo->pid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\t\tINIT_DELAYED_WORK(&info->restore_userptr_work,\n\t\t\t\t  amdgpu_amdkfd_restore_userptr_worker);\n\n\t\t*process_info = info;\n\t}\n\n\tvm->process_info = *process_info;\n\n\t/* Validate page directory and attach eviction fence */\n\tret = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (ret)\n\t\tgoto reserve_pd_fail;\n\tret = vm_validate_pt_pd_bos(vm, NULL);\n\tif (ret) {\n\t\tpr_err(\"validate_pt_pd_bos() failed\\n\");\n\t\tgoto validate_pd_fail;\n\t}\n\tret = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t  AMDGPU_FENCE_OWNER_KFD, false);\n\tif (ret)\n\t\tgoto wait_pd_fail;\n\tret = dma_resv_reserve_fences(vm->root.bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(vm->root.bo->tbo.base.resv,\n\t\t\t   &vm->process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\n\t/* Update process info */\n\tmutex_lock(&vm->process_info->lock);\n\tlist_add_tail(&vm->vm_list_node,\n\t\t\t&(vm->process_info->vm_list_head));\n\tvm->process_info->n_vms++;\n\n\t*ef = dma_fence_get(&vm->process_info->eviction_fence->base);\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn 0;\n\nreserve_shared_fail:\nwait_pd_fail:\nvalidate_pd_fail:\n\tamdgpu_bo_unreserve(vm->root.bo);\nreserve_pd_fail:\n\tvm->process_info = NULL;\n\tif (info) {\n\t\tdma_fence_put(&info->eviction_fence->base);\n\t\t*process_info = NULL;\n\t\tput_pid(info->pid);\ncreate_evict_fence_fail:\n\t\tmutex_destroy(&info->lock);\n\t\tmutex_destroy(&info->notifier_lock);\n\t\tkfree(info);\n\t}\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_pin_bo() - Pins a BO using following criteria\n * @bo: Handle of buffer object being pinned\n * @domain: Domain into which BO should be pinned\n *\n *   - USERPTR BOs are UNPINNABLE and will return error\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count incremented. It is valid to PIN a BO multiple times\n *\n * Return: ZERO if successful in pinning, Non-Zero in case of error.\n */\nstatic int amdgpu_amdkfd_gpuvm_pin_bo(struct amdgpu_bo *bo, u32 domain)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {\n\t\t/*\n\t\t * If bo is not contiguous on VRAM, move to system memory first to ensure\n\t\t * we can get contiguous VRAM space after evicting other BOs.\n\t\t */\n\t\tif (!(bo->tbo.resource->placement & TTM_PL_FLAG_CONTIGUOUS)) {\n\t\t\tstruct ttm_operation_ctx ctx = { true, false };\n\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_debug(\"validate bo 0x%p to GTT failed %d\\n\", &bo->tbo, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n\tif (ret)\n\t\tpr_err(\"Error in Pinning BO to domain: %d\\n\", domain);\n\n\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\nout:\n\tamdgpu_bo_unreserve(bo);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_unpin_bo() - Unpins BO using following criteria\n * @bo: Handle of buffer object being unpinned\n *\n *   - Is a illegal request for USERPTR BOs and is ignored\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count decremented. Calls to UNPIN must balance calls to PIN\n */\nstatic void amdgpu_amdkfd_gpuvm_unpin_bo(struct amdgpu_bo *bo)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn;\n\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_set_vm_pasid(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_vm *avm, u32 pasid)\n\n{\n\tint ret;\n\n\t/* Free the original amdgpu allocated pasid,\n\t * will be replaced with kfd allocated pasid.\n\t */\n\tif (avm->pasid) {\n\t\tamdgpu_pasid_free(avm->pasid);\n\t\tamdgpu_vm_set_pasid(adev, avm, 0);\n\t}\n\n\tret = amdgpu_vm_set_pasid(adev, avm, pasid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_acquire_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *avm,\n\t\t\t\t\t   void **process_info,\n\t\t\t\t\t   struct dma_fence **ef)\n{\n\tint ret;\n\n\t/* Already a compute VM? */\n\tif (avm->process_info)\n\t\treturn -EINVAL;\n\n\t/* Convert VM into a compute VM */\n\tret = amdgpu_vm_make_compute(adev, avm);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Initialize KFD part of the VM and process info */\n\tret = init_kfd_vm(avm, process_info, ef);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_vm_set_task_info(avm);\n\n\treturn 0;\n}\n\nvoid amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm)\n{\n\tstruct amdkfd_process_info *process_info = vm->process_info;\n\n\tif (!process_info)\n\t\treturn;\n\n\t/* Update process info */\n\tmutex_lock(&process_info->lock);\n\tprocess_info->n_vms--;\n\tlist_del(&vm->vm_list_node);\n\tmutex_unlock(&process_info->lock);\n\n\tvm->process_info = NULL;\n\n\t/* Release per-process resources when last compute VM is destroyed */\n\tif (!process_info->n_vms) {\n\t\tWARN_ON(!list_empty(&process_info->kfd_bo_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_valid_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_inval_list));\n\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tcancel_delayed_work_sync(&process_info->restore_userptr_work);\n\t\tput_pid(process_info->pid);\n\t\tmutex_destroy(&process_info->lock);\n\t\tmutex_destroy(&process_info->notifier_lock);\n\t\tkfree(process_info);\n\t}\n}\n\nvoid amdgpu_amdkfd_gpuvm_release_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t    void *drm_priv)\n{\n\tstruct amdgpu_vm *avm;\n\n\tif (WARN_ON(!adev || !drm_priv))\n\t\treturn;\n\n\tavm = drm_priv_to_vm(drm_priv);\n\n\tpr_debug(\"Releasing process vm %p\\n\", avm);\n\n\t/* The original pasid of amdgpu vm has already been\n\t * released during making a amdgpu vm to a compute vm\n\t * The current pasid is managed by kfd and will be\n\t * released on kfd process destroy. Set amdgpu pasid\n\t * to 0 to avoid duplicate release.\n\t */\n\tamdgpu_vm_release_compute(adev, avm);\n}\n\nuint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *pd = avm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\n\tif (adev->asic_type < CHIP_VEGA10)\n\t\treturn avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;\n\treturn avm->pd_phys_addr;\n}\n\nvoid amdgpu_amdkfd_block_mmu_notifications(void *p)\n{\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tWRITE_ONCE(pinfo->block_mmu_notifications, true);\n\tmutex_unlock(&pinfo->lock);\n}\n\nint amdgpu_amdkfd_criu_resume(void *p)\n{\n\tint ret = 0;\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tpr_debug(\"scheduling work\\n\");\n\tmutex_lock(&pinfo->notifier_lock);\n\tpinfo->evicted_bos++;\n\tmutex_unlock(&pinfo->notifier_lock);\n\tif (!READ_ONCE(pinfo->block_mmu_notifications)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tWRITE_ONCE(pinfo->block_mmu_notifications, false);\n\tqueue_delayed_work(system_freezable_wq,\n\t\t\t   &pinfo->restore_userptr_work, 0);\n\nout_unlock:\n\tmutex_unlock(&pinfo->lock);\n\treturn ret;\n}\n\nsize_t amdgpu_amdkfd_get_available_memory(struct amdgpu_device *adev,\n\t\t\t\t\t  uint8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tssize_t available;\n\tuint64_t vram_available, system_mem_available, ttm_mem_available;\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tvram_available = KFD_XCP_MEMORY_SIZE(adev, xcp_id)\n\t\t- adev->kfd.vram_used_aligned[xcp_id]\n\t\t- atomic64_read(&adev->vram_pin_size)\n\t\t- reserved_for_pt\n\t\t- reserved_for_ras;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tsystem_mem_available = no_system_mem_limit ?\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit :\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit -\n\t\t\t\t\tkfd_mem_limit.system_mem_used;\n\n\t\tttm_mem_available = kfd_mem_limit.max_ttm_mem_limit -\n\t\t\t\tkfd_mem_limit.ttm_mem_used;\n\n\t\tavailable = min3(system_mem_available, ttm_mem_available,\n\t\t\t\t vram_available);\n\t\tavailable = ALIGN_DOWN(available, PAGE_SIZE);\n\t} else {\n\t\tavailable = ALIGN_DOWN(vram_available, VRAM_AVAILABLITY_ALIGN);\n\t}\n\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\n\treturn available;\n}\n\nint amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, uint64_t va, uint64_t size,\n\t\tvoid *drm_priv, struct kgd_mem **mem,\n\t\tuint64_t *offset, uint32_t flags, bool criu_resume)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_fpriv *fpriv = container_of(avm, struct amdgpu_fpriv, vm);\n\tenum ttm_bo_type bo_type = ttm_bo_type_device;\n\tstruct sg_table *sg = NULL;\n\tuint64_t user_addr = 0;\n\tstruct amdgpu_bo *bo;\n\tstruct drm_gem_object *gobj = NULL;\n\tu32 domain, alloc_domain;\n\tuint64_t aligned_size;\n\tint8_t xcp_id = -1;\n\tu64 alloc_flags;\n\tint ret;\n\n\t/*\n\t * Check on which domain to allocate BO\n\t */\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_flags = 0;\n\t\t} else {\n\t\t\talloc_flags = AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\t\t\talloc_flags |= (flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) ?\n\t\t\tAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED : 0;\n\n\t\t\t/* For contiguous VRAM allocation */\n\t\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_CONTIGUOUS)\n\t\t\t\talloc_flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\t\t}\n\t\txcp_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ?\n\t\t\t\t\t0 : fpriv->xcp_id;\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_flags = 0;\n\t} else {\n\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_domain = AMDGPU_GEM_DOMAIN_CPU;\n\t\talloc_flags = AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\tif (!offset || !*offset)\n\t\t\t\treturn -EINVAL;\n\t\t\tuser_addr = untagged_addr(*offset);\n\t\t} else if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\t\tbo_type = ttm_bo_type_sg;\n\t\t\tif (size > UINT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\tsg = create_sg_table(*offset, size);\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_EXT_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_EXT_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_UNCACHED)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_UNCACHED;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\t(*mem)->aql_queue = !!(flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);\n\n\t/* Workaround for AQL queue wraparound bug. Map the same\n\t * memory twice. That means we only actually allocate half\n\t * the memory.\n\t */\n\tif ((*mem)->aql_queue)\n\t\tsize >>= 1;\n\taligned_size = PAGE_ALIGN(size);\n\n\t(*mem)->alloc_flags = flags;\n\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\tret = amdgpu_amdkfd_reserve_mem_limit(adev, aligned_size, flags,\n\t\t\t\t\t      xcp_id);\n\tif (ret) {\n\t\tpr_debug(\"Insufficient memory\\n\");\n\t\tgoto err_reserve_limit;\n\t}\n\n\tpr_debug(\"\\tcreate BO VA 0x%llx size 0x%llx domain %s xcp_id %d\\n\",\n\t\t va, (*mem)->aql_queue ? size << 1 : size,\n\t\t domain_string(alloc_domain), xcp_id);\n\n\tret = amdgpu_gem_object_create(adev, aligned_size, 1, alloc_domain, alloc_flags,\n\t\t\t\t       bo_type, NULL, &gobj, xcp_id + 1);\n\tif (ret) {\n\t\tpr_debug(\"Failed to create BO on domain %s. ret %d\\n\",\n\t\t\t domain_string(alloc_domain), ret);\n\t\tgoto err_bo_create;\n\t}\n\tret = drm_vma_node_allow(&gobj->vma_node, drm_priv);\n\tif (ret) {\n\t\tpr_debug(\"Failed to allow vma node access. ret %d\\n\", ret);\n\t\tgoto err_node_allow;\n\t}\n\tret = drm_gem_handle_create(adev->kfd.client.file, gobj, &(*mem)->gem_handle);\n\tif (ret)\n\t\tgoto err_gem_handle_create;\n\tbo = gem_to_amdgpu_bo(gobj);\n\tif (bo_type == ttm_bo_type_sg) {\n\t\tbo->tbo.sg = sg;\n\t\tbo->tbo.ttm->sg = sg;\n\t}\n\tbo->kfd_bo = *mem;\n\t(*mem)->bo = bo;\n\tif (user_addr)\n\t\tbo->flags |= AMDGPU_AMDKFD_CREATE_USERPTR_BO;\n\n\t(*mem)->va = va;\n\t(*mem)->domain = domain;\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);\n\n\tif (user_addr) {\n\t\tpr_debug(\"creating userptr BO for user_addr = %llx\\n\", user_addr);\n\t\tret = init_user_pages(*mem, user_addr, criu_resume);\n\t\tif (ret)\n\t\t\tgoto allocate_init_user_pages_failed;\n\t} else  if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tret = amdgpu_amdkfd_gpuvm_pin_bo(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (ret) {\n\t\t\tpr_err(\"Pinning MMIO/DOORBELL BO during ALLOC FAILED\\n\");\n\t\t\tgoto err_pin_bo;\n\t\t}\n\t\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t} else {\n\t\tmutex_lock(&avm->process_info->lock);\n\t\tif (avm->process_info->eviction_fence &&\n\t\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\t\tmutex_unlock(&avm->process_info->lock);\n\t\tif (ret)\n\t\t\tgoto err_validate_bo;\n\t}\n\n\tif (offset)\n\t\t*offset = amdgpu_bo_mmap_offset(bo);\n\n\treturn 0;\n\nallocate_init_user_pages_failed:\nerr_pin_bo:\nerr_validate_bo:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_gem_handle_delete(adev->kfd.client.file, (*mem)->gem_handle);\nerr_gem_handle_create:\n\tdrm_vma_node_revoke(&gobj->vma_node, drm_priv);\nerr_node_allow:\n\t/* Don't unreserve system mem limit twice */\n\tgoto err_reserve_limit;\nerr_bo_create:\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, aligned_size, flags, xcp_id);\nerr_reserve_limit:\n\tamdgpu_sync_free(&(*mem)->sync);\n\tmutex_destroy(&(*mem)->lock);\n\tif (gobj)\n\t\tdrm_gem_object_put(gobj);\n\telse\n\t\tkfree(*mem);\nerr:\n\tif (sg) {\n\t\tsg_free_table(sg);\n\t\tkfree(sg);\n\t}\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_free_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv,\n\t\tuint64_t *size)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tbool use_release_notifier = (mem->bo->kfd_bo == mem);\n\tstruct kfd_mem_attachment *entry, *tmp;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned int mapped_to_gpu_memory;\n\tint ret;\n\tbool is_imported = false;\n\n\tmutex_lock(&mem->lock);\n\n\t/* Unpin MMIO/DOORBELL BO's that were pinned during allocation */\n\tif (mem->alloc_flags &\n\t    (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t     KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tamdgpu_amdkfd_gpuvm_unpin_bo(mem->bo);\n\t}\n\n\tmapped_to_gpu_memory = mem->mapped_to_gpu_memory;\n\tis_imported = mem->is_imported;\n\tmutex_unlock(&mem->lock);\n\t/* lock is not needed after this, since mem is unused and will\n\t * be freed anyway\n\t */\n\n\tif (mapped_to_gpu_memory > 0) {\n\t\tpr_debug(\"BO VA 0x%llx size 0x%lx is still mapped.\\n\",\n\t\t\t\tmem->va, bo_size);\n\t\treturn -EBUSY;\n\t}\n\n\t/* Make sure restore workers don't access the BO any more */\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n\n\t/* Cleanup user pages and MMU notifiers */\n\tif (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\tamdgpu_hmm_unregister(mem->bo);\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tamdgpu_ttm_tt_discard_user_pages(mem->bo->tbo.ttm, mem->range);\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t}\n\n\tret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\tprocess_info->eviction_fence);\n\tpr_debug(\"Release VA 0x%llx - 0x%llx\\n\", mem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue));\n\n\t/* Remove from VM internal data structures */\n\tlist_for_each_entry_safe(entry, tmp, &mem->attachments, list) {\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t\tkfd_mem_detach(entry);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\t/* Free the sync object */\n\tamdgpu_sync_free(&mem->sync);\n\n\t/* If the SG is not NULL, it's one we created for a doorbell or mmio\n\t * remap BO. We need to free it.\n\t */\n\tif (mem->bo->tbo.sg) {\n\t\tsg_free_table(mem->bo->tbo.sg);\n\t\tkfree(mem->bo->tbo.sg);\n\t}\n\n\t/* Update the size of the BO being freed if it was allocated from\n\t * VRAM and is not imported. For APP APU VRAM allocations are done\n\t * in GTT domain\n\t */\n\tif (size) {\n\t\tif (!is_imported &&\n\t\t   (mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM ||\n\t\t   ((adev->flags & AMD_IS_APU) &&\n\t\t    mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT)))\n\t\t\t*size = bo_size;\n\t\telse\n\t\t\t*size = 0;\n\t}\n\n\t/* Free the BO*/\n\tdrm_vma_node_revoke(&mem->bo->tbo.base.vma_node, drm_priv);\n\tdrm_gem_handle_delete(adev->kfd.client.file, mem->gem_handle);\n\tif (mem->dmabuf) {\n\t\tdma_buf_put(mem->dmabuf);\n\t\tmem->dmabuf = NULL;\n\t}\n\tmutex_destroy(&mem->lock);\n\n\t/* If this releases the last reference, it will end up calling\n\t * amdgpu_amdkfd_release_notify and kfree the mem struct. That's why\n\t * this needs to be the last call here.\n\t */\n\tdrm_gem_object_put(&mem->bo->tbo.base);\n\n\t/*\n\t * For kgd_mem allocated in amdgpu_amdkfd_gpuvm_import_dmabuf(),\n\t * explicitly free it here.\n\t */\n\tif (!use_release_notifier)\n\t\tkfree(mem);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tvoid *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tint ret;\n\tstruct amdgpu_bo *bo;\n\tuint32_t domain;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned long bo_size;\n\tbool is_invalid_userptr = false;\n\n\tbo = mem->bo;\n\tif (!bo) {\n\t\tpr_err(\"Invalid BO when mapping memory to GPU\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Make sure restore is not running concurrently. Since we\n\t * don't map invalid userptr BOs, we rely on the next restore\n\t * worker to do the mapping\n\t */\n\tmutex_lock(&mem->process_info->lock);\n\n\t/* Lock notifier lock. If we find an invalid userptr BO, we can be\n\t * sure that the MMU notifier is no longer running\n\t * concurrently and the queues are actually stopped\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tmutex_lock(&mem->process_info->notifier_lock);\n\t\tis_invalid_userptr = !!mem->invalid;\n\t\tmutex_unlock(&mem->process_info->notifier_lock);\n\t}\n\n\tmutex_lock(&mem->lock);\n\n\tdomain = mem->domain;\n\tbo_size = bo->tbo.base.size;\n\n\tpr_debug(\"Map VA 0x%llx - 0x%llx to vm %p domain %s\\n\",\n\t\t\tmem->va,\n\t\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\t\tavm, domain_string(domain));\n\n\tif (!kfd_mem_is_attached(avm, mem)) {\n\t\tret = kfd_mem_attach(adev, mem, avm, mem->aql_queue);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = reserve_bo_and_vm(mem, avm, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\t/* Userptr can be marked as \"not invalid\", but not actually be\n\t * validated yet (still in the system domain). In that case\n\t * the queues are still stopped and we can leave mapping for\n\t * the next restore worker\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_SYSTEM)\n\t\tis_invalid_userptr = true;\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto out_unreserve;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t map VA 0x%llx - 0x%llx in entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = map_bo_to_gpuvm(mem, entry, ctx.sync,\n\t\t\t\t      is_invalid_userptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map bo to gpuvm\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tret = vm_update_pds(avm, ctx.sync);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to update page directories\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tentry->is_mapped = true;\n\t\tmem->mapped_to_gpu_memory++;\n\t\tpr_debug(\"\\t INC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\tgoto out;\n\nout_unreserve:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->process_info->lock);\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_dmaunmap_mem(struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_vm *vm;\n\tint ret;\n\n\tvm = drm_priv_to_vm(drm_priv);\n\n\tmutex_lock(&mem->lock);\n\n\tret = amdgpu_bo_reserve(mem->bo, true);\n\tif (ret)\n\t\tgoto out;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != vm)\n\t\t\tcontinue;\n\t\tif (entry->bo_va->base.bo->tbo.ttm &&\n\t\t    !entry->bo_va->base.bo->tbo.ttm->sg)\n\t\t\tcontinue;\n\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t}\n\n\tamdgpu_bo_unreserve(mem->bo);\nout:\n\tmutex_unlock(&mem->lock);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\n\tret = reserve_bo_and_cond_vms(mem, avm, BO_VM_MAPPED, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\t/* If no VMs were reserved, it means the BO wasn't actually mapped */\n\tif (ctx.n_vms == 0) {\n\t\tret = -EINVAL;\n\t\tgoto unreserve_out;\n\t}\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto unreserve_out;\n\n\tpr_debug(\"Unmap VA 0x%llx - 0x%llx from vm %p\\n\",\n\t\tmem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\tavm);\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || !entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t unmap VA 0x%llx - 0x%llx from entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tunmap_bo_from_gpuvm(mem, entry, ctx.sync);\n\t\tentry->is_mapped = false;\n\n\t\tmem->mapped_to_gpu_memory--;\n\t\tpr_debug(\"\\t DEC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\nunreserve_out:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_sync_memory(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tmutex_lock(&mem->lock);\n\tamdgpu_sync_clone(&mem->sync, &sync);\n\tmutex_unlock(&mem->lock);\n\n\tret = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_map_gtt_bo_to_gart - Map BO to GART and increment reference count\n * @bo: Buffer object to be mapped\n *\n * Before return, bo reference count is incremented. To release the reference and unpin/\n * unmap the BO, call amdgpu_amdkfd_free_gtt_mem.\n */\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo)\n{\n\tint ret;\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto err_reserve_bo_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto err_pin_bo_failed;\n\t}\n\n\tret = amdgpu_ttm_alloc_gart(&bo->tbo);\n\tif (ret) {\n\t\tpr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\n\t\tgoto err_map_bo_gart_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, bo->vm_bo->vm->process_info->eviction_fence);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tbo = amdgpu_bo_ref(bo);\n\n\treturn 0;\n\nerr_map_bo_gart_failed:\n\tamdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\n\tamdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Map a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be mapped for CPU access\n * @kptr[out]: pointer in kernel CPU address space\n * @size[out]: size of the buffer\n *\n * Pins the BO and maps it for kernel CPU access. The eviction fence is removed\n * from the BO, since pinned BOs cannot be evicted. The bo must remain on the\n * validate_list, so the GPU mapping can be restored after a page table was\n * evicted.\n *\n * Return: 0 on success, error code on failure\n */\nint amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_mem *mem,\n\t\t\t\t\t     void **kptr, uint64_t *size)\n{\n\tint ret;\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tpr_err(\"userptr can't be mapped to kernel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&mem->process_info->lock);\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto bo_reserve_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto pin_failed;\n\t}\n\n\tret = amdgpu_bo_kmap(bo, kptr);\n\tif (ret) {\n\t\tpr_err(\"Failed to map bo to kernel. ret %d\\n\", ret);\n\t\tgoto kmap_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, mem->process_info->eviction_fence);\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tmutex_unlock(&mem->process_info->lock);\n\treturn 0;\n\nkmap_failed:\n\tamdgpu_bo_unpin(bo);\npin_failed:\n\tamdgpu_bo_unreserve(bo);\nbo_reserve_failed:\n\tmutex_unlock(&mem->process_info->lock);\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Unmap a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be unmapped for CPU access\n *\n * Removes the kernel CPU mapping and unpins the BO. It does not restore the\n * eviction fence, so this function should only be used for cleanup before the\n * BO is destroyed.\n */\nvoid amdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(struct kgd_mem *mem)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct kfd_vm_fault_info *mem)\n{\n\tif (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {\n\t\t*mem = *adev->gmc.vm_fault_info;\n\t\tmb(); /* make sure read happened */\n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\t}\n\treturn 0;\n}\n\nstatic int import_obj_create(struct amdgpu_device *adev,\n\t\t\t     struct dma_buf *dma_buf,\n\t\t\t     struct drm_gem_object *obj,\n\t\t\t     uint64_t va, void *drm_priv,\n\t\t\t     struct kgd_mem **mem, uint64_t *size,\n\t\t\t     uint64_t *mmap_offset)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tbo = gem_to_amdgpu_bo(obj);\n\tif (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT)))\n\t\t/* Only VRAM and GTT BOs are supported */\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tret = drm_vma_node_allow(&obj->vma_node, drm_priv);\n\tif (ret)\n\t\tgoto err_free_mem;\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tif (mmap_offset)\n\t\t*mmap_offset = amdgpu_bo_mmap_offset(bo);\n\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\n\t(*mem)->alloc_flags =\n\t\t((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?\n\t\tKFD_IOC_ALLOC_MEM_FLAGS_VRAM : KFD_IOC_ALLOC_MEM_FLAGS_GTT)\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\n\tget_dma_buf(dma_buf);\n\t(*mem)->dmabuf = dma_buf;\n\t(*mem)->bo = bo;\n\t(*mem)->va = va;\n\t(*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&\n\t\t\t !(adev->flags & AMD_IS_APU) ?\n\t\t\t AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;\n\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\t(*mem)->is_imported = true;\n\n\tmutex_lock(&avm->process_info->lock);\n\tif (avm->process_info->eviction_fence &&\n\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, (*mem)->domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\tmutex_unlock(&avm->process_info->lock);\n\tif (ret)\n\t\tgoto err_remove_mem;\n\n\treturn 0;\n\nerr_remove_mem:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_vma_node_revoke(&obj->vma_node, drm_priv);\nerr_free_mem:\n\tkfree(*mem);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n\t\t\t\t\t uint64_t va, void *drm_priv,\n\t\t\t\t\t struct kgd_mem **mem, uint64_t *size,\n\t\t\t\t\t uint64_t *mmap_offset)\n{\n\tstruct drm_gem_object *obj;\n\tuint32_t handle;\n\tint ret;\n\n\tret = drm_gem_prime_fd_to_handle(&adev->ddev, adev->kfd.client.file, fd,\n\t\t\t\t\t &handle);\n\tif (ret)\n\t\treturn ret;\n\tobj = drm_gem_object_lookup(adev->kfd.client.file, handle);\n\tif (!obj) {\n\t\tret = -EINVAL;\n\t\tgoto err_release_handle;\n\t}\n\n\tret = import_obj_create(adev, obj->dma_buf, obj, va, drm_priv, mem, size,\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\nerr_release_handle:\n\tdrm_gem_handle_delete(adev->kfd.client.file, handle);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_mem *mem,\n\t\t\t\t      struct dma_buf **dma_buf)\n{\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\tgoto out;\n\n\tget_dma_buf(mem->dmabuf);\n\t*dma_buf = mem->dmabuf;\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\n/* Evict a userptr BO by stopping the queues if necessary\n *\n * Runs in MMU notifier, may be in RECLAIM_FS context. This means it\n * cannot do any memory allocations, and cannot take any locks that\n * are held elsewhere while allocating memory.\n *\n * It doesn't do anything to the BO itself. The real work happens in\n * restore, where we get updated page addresses. This function only\n * ensures that GPU access to the BO is stopped.\n */\nint amdgpu_amdkfd_evict_userptr(struct mmu_interval_notifier *mni,\n\t\t\t\tunsigned long cur_seq, struct kgd_mem *mem)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tint r = 0;\n\n\t/* Do not process MMU notifications during CRIU restore until\n\t * KFD_CRIU_OP_RESUME IOCTL is received\n\t */\n\tif (READ_ONCE(process_info->block_mmu_notifications))\n\t\treturn 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmem->invalid++;\n\tif (++process_info->evicted_bos == 1) {\n\t\t/* First eviction, stop the queues */\n\t\tr = kgd2kfd_quiesce_mm(mni->mm,\n\t\t\t\t       KFD_QUEUE_EVICTION_TRIGGER_USERPTR);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to quiesce KFD\\n\");\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\t}\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn r;\n}\n\n/* Update invalid userptr BOs\n *\n * Moves invalidated (evicted) userptr BOs from userptr_valid_list to\n * userptr_inval_list and updates user pages for all BOs that have\n * been invalidated since their last update.\n */\nstatic int update_invalid_user_pages(struct amdkfd_process_info *process_info,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tuint32_t invalid;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\n\t/* Move all invalidated BOs to the userptr_inval_list */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_valid_list,\n\t\t\t\t validate_list)\n\t\tif (mem->invalid)\n\t\t\tlist_move_tail(&mem->validate_list,\n\t\t\t\t       &process_info->userptr_inval_list);\n\n\t/* Go through userptr_inval_list and update any invalid user_pages */\n\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t    validate_list) {\n\t\tinvalid = mem->invalid;\n\t\tif (!invalid)\n\t\t\t/* BO hasn't been invalidated since the last\n\t\t\t * revalidation attempt. Keep its page list.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\tbo = mem->bo;\n\n\t\tamdgpu_ttm_tt_discard_user_pages(bo->tbo.ttm, mem->range);\n\t\tmem->range = NULL;\n\n\t\t/* BO reservations and getting user pages (hmm_range_fault)\n\t\t * must happen outside the notifier lock\n\t\t */\n\t\tmutex_unlock(&process_info->notifier_lock);\n\n\t\t/* Move the BO to system (CPU) domain if necessary to unmap\n\t\t * and free the SG table\n\t\t */\n\t\tif (bo->tbo.resource->mem_type != TTM_PL_SYSTEM) {\n\t\t\tif (amdgpu_bo_reserve(bo, true))\n\t\t\t\treturn -EAGAIN;\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Failed to invalidate userptr BO\\n\",\n\t\t\t\t       __func__);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\t/* Get updated user pages */\n\t\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t   &mem->range);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed %d to get user pages\\n\", ret);\n\n\t\t\t/* Return -EFAULT bad address error as success. It will\n\t\t\t * fail later with a VM fault if the GPU tries to access\n\t\t\t * it. Better than hanging indefinitely with stalled\n\t\t\t * user mode queues.\n\t\t\t *\n\t\t\t * Return other error -EBUSY or -ENOMEM to retry restore\n\t\t\t */\n\t\t\tif (ret != -EFAULT)\n\t\t\t\treturn ret;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\tmutex_lock(&process_info->notifier_lock);\n\n\t\t/* Mark the BO as valid unless it was invalidated\n\t\t * again concurrently.\n\t\t */\n\t\tif (mem->invalid != invalid) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t /* set mem valid if mem has hmm range associated */\n\t\tif (mem->range)\n\t\t\tmem->invalid = 0;\n\t}\n\nunlock_out:\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn ret;\n}\n\n/* Validate invalid userptr BOs\n *\n * Validates BOs on the userptr_inval_list. Also updates GPUVM page tables\n * with new page addresses and waits for the page table updates to complete.\n */\nstatic int validate_invalid_user_pages(struct amdkfd_process_info *process_info)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0, 0);\n\t/* Reserve all BOs and page tables for validation */\n\tdrm_exec_until_all_locked(&exec) {\n\t\t/* Reserve all the page directories */\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\t/* Reserve the userptr_inval_list entries to resv_list */\n\t\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\tret = process_validate_vms(process_info, NULL);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\t/* Validate BOs and update GPUVM page tables */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tbo = mem->bo;\n\n\t\t/* Validate the BO if we got user pages */\n\t\tif (bo->tbo.ttm->pages[0]) {\n\t\t\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\n\t\t/* Update mapping. If the BO was not validated\n\t\t * (because we couldn't get user pages), this will\n\t\t * clear the page table entries, which will result in\n\t\t * VM faults if the GPU tries to access the invalid\n\t\t * memory.\n\t\t */\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: update PTE failed\\n\", __func__);\n\t\t\t\t/* make sure this gets validated again */\n\t\t\t\tmutex_lock(&process_info->notifier_lock);\n\t\t\t\tmem->invalid++;\n\t\t\t\tmutex_unlock(&process_info->notifier_lock);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync);\n\nunreserve_out:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_wait(&sync, false);\n\tamdgpu_sync_free(&sync);\n\n\treturn ret;\n}\n\n/* Confirm that all user pages are valid while holding the notifier lock\n *\n * Moves valid BOs from the userptr_inval_list back to userptr_val_list.\n */\nstatic int confirm_valid_user_pages_locked(struct amdkfd_process_info *process_info)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tbool valid;\n\n\t\t/* keep mem without hmm range at userptr_inval_list */\n\t\tif (!mem->range)\n\t\t\tcontinue;\n\n\t\t/* Only check mem with hmm range associated */\n\t\tvalid = amdgpu_ttm_tt_get_user_pages_done(\n\t\t\t\t\tmem->bo->tbo.ttm, mem->range);\n\n\t\tmem->range = NULL;\n\t\tif (!valid) {\n\t\t\tWARN(!mem->invalid, \"Invalid BO not marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem->invalid) {\n\t\t\tWARN(1, \"Valid BO is marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move_tail(&mem->validate_list,\n\t\t\t       &process_info->userptr_valid_list);\n\t}\n\n\treturn ret;\n}\n\n/* Worker callback to restore evicted userptr BOs\n *\n * Tries to update and validate all userptr BOs. If successful and no\n * concurrent evictions happened, the queues are restarted. Otherwise,\n * reschedule for another attempt later.\n */\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info =\n\t\tcontainer_of(dwork, struct amdkfd_process_info,\n\t\t\t     restore_userptr_work);\n\tstruct task_struct *usertask;\n\tstruct mm_struct *mm;\n\tuint32_t evicted_bos;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tevicted_bos = process_info->evicted_bos;\n\tmutex_unlock(&process_info->notifier_lock);\n\tif (!evicted_bos)\n\t\treturn;\n\n\t/* Reference task and mm in case of concurrent process termination */\n\tusertask = get_pid_task(process_info->pid, PIDTYPE_PID);\n\tif (!usertask)\n\t\treturn;\n\tmm = get_task_mm(usertask);\n\tif (!mm) {\n\t\tput_task_struct(usertask);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\n\tif (update_invalid_user_pages(process_info, mm))\n\t\tgoto unlock_out;\n\t/* userptr_inval_list can be empty if all evicted userptr BOs\n\t * have been freed. In that case there is nothing to validate\n\t * and we can just restart the queues.\n\t */\n\tif (!list_empty(&process_info->userptr_inval_list)) {\n\t\tif (validate_invalid_user_pages(process_info))\n\t\t\tgoto unlock_out;\n\t}\n\t/* Final check for concurrent evicton and atomic update. If\n\t * another eviction happens after successful update, it will\n\t * be a first eviction that calls quiesce_mm. The eviction\n\t * reference counting inside KFD will handle this case.\n\t */\n\tmutex_lock(&process_info->notifier_lock);\n\tif (process_info->evicted_bos != evicted_bos)\n\t\tgoto unlock_notifier_out;\n\n\tif (confirm_valid_user_pages_locked(process_info)) {\n\t\tWARN(1, \"User pages unexpectedly invalid\");\n\t\tgoto unlock_notifier_out;\n\t}\n\n\tprocess_info->evicted_bos = evicted_bos = 0;\n\n\tif (kgd2kfd_resume_mm(mm)) {\n\t\tpr_err(\"%s: Failed to resume KFD\\n\", __func__);\n\t\t/* No recovery from this failure. Probably the CP is\n\t\t * hanging. No point trying again.\n\t\t */\n\t}\n\nunlock_notifier_out:\n\tmutex_unlock(&process_info->notifier_lock);\nunlock_out:\n\tmutex_unlock(&process_info->lock);\n\n\t/* If validation failed, reschedule another attempt */\n\tif (evicted_bos) {\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n\tput_task_struct(usertask);\n}\n\nstatic void replace_eviction_fence(struct dma_fence __rcu **ef,\n\t\t\t\t   struct dma_fence *new_ef)\n{\n\tstruct dma_fence *old_ef = rcu_replace_pointer(*ef, new_ef, true\n\t\t/* protected by process_info->lock */);\n\n\t/* If we're replacing an unsignaled eviction fence, that fence will\n\t * never be signaled, and if anyone is still waiting on that fence,\n\t * they will hang forever. This should never happen. We should only\n\t * replace the fence in restore_work that only gets scheduled after\n\t * eviction work signaled the fence.\n\t */\n\tWARN_ONCE(!dma_fence_is_signaled(old_ef),\n\t\t  \"Replacing unsignaled eviction fence\");\n\tdma_fence_put(old_ef);\n}\n\n/** amdgpu_amdkfd_gpuvm_restore_process_bos - Restore all BOs for the given\n *   KFD process identified by process_info\n *\n * @process_info: amdkfd_process_info of the KFD process\n *\n * After memory eviction, restore thread calls this function. The function\n * should be called when the Process is still valid. BO restore involves -\n *\n * 1.  Release old eviction fence and create new one\n * 2.  Get two copies of PD BO list from all the VMs. Keep one copy as pd_list.\n * 3   Use the second PD list and kfd_bo_list to create a list (ctx.list) of\n *     BOs that need to be reserved.\n * 4.  Reserve all the BOs\n * 5.  Validate of PD and PT BOs.\n * 6.  Validate all KFD BOs using kfd_bo_list and Map them and add new fence\n * 7.  Add fence to all PD and PT BOs.\n * 8.  Unreserve all BOs\n */\nint amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct dma_fence __rcu **ef)\n{\n\tstruct amdkfd_process_info *process_info = info;\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem;\n\tstruct list_head duplicate_save;\n\tstruct amdgpu_sync sync_obj;\n\tunsigned long failed_size = 0;\n\tunsigned long total_size = 0;\n\tstruct drm_exec exec;\n\tint ret;\n\n\tINIT_LIST_HEAD(&duplicate_save);\n\n\tmutex_lock(&process_info->lock);\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"Locking VM PD failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Reserve all BOs and page tables/directory. Add all BOs from\n\t\t * kfd_bo_list to ctx.list\n\t\t */\n\t\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"drm_exec_prepare_obj failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tamdgpu_sync_create(&sync_obj);\n\n\t/* Validate BOs managed by KFD */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\n\t\tstruct amdgpu_bo *bo = mem->bo;\n\t\tuint32_t domain = mem->domain;\n\t\tstruct dma_resv_iter cursor;\n\t\tstruct dma_fence *fence;\n\n\t\ttotal_size += amdgpu_bo_size(bo);\n\n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: Validate BOs failed\\n\");\n\t\t\tfailed_size += amdgpu_bo_size(bo);\n\t\t\tret = amdgpu_amdkfd_bo_validate(bo,\n\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT, false);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tdma_resv_for_each_fence(&cursor, bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_KERNEL, fence) {\n\t\t\tret = amdgpu_sync_fence(&sync_obj, fence);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Sync BO fence failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (failed_size)\n\t\tpr_debug(\"0x%lx/0x%lx in system\\n\", failed_size, total_size);\n\n\t/* Validate PDs, PTs and evicted DMABuf imports last. Otherwise BO\n\t * validations above would invalidate DMABuf imports again.\n\t */\n\tret = process_validate_vms(process_info, &exec.ticket);\n\tif (ret) {\n\t\tpr_debug(\"Validating VMs failed, ret: %d\\n\", ret);\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Update mappings managed by KFD. */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync_obj);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: update PTE failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update mappings not managed by KFD */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\tvm_list_node) {\n\t\tstruct amdgpu_device *adev = amdgpu_ttm_adev(\n\t\t\tpeer_vm->root.bo->tbo.bdev);\n\n\t\tret = amdgpu_vm_handle_moved(adev, peer_vm, &exec.ticket);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: handle moved failed. Try again\\n\");\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: update PDs failed. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Sync with fences on all the page tables. They implicitly depend on any\n\t * move fences from amdgpu_vm_handle_moved above.\n\t */\n\tret = process_sync_pds_resv(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: Failed to sync to PD BO moving fence. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Wait for validate and PT updates to finish */\n\tamdgpu_sync_wait(&sync_obj, false);\n\n\t/* The old eviction fence may be unsignaled if restore happens\n\t * after a GPU reset or suspend/resume. Keep the old fence in that\n\t * case. Otherwise release the old eviction fence and create new\n\t * one, because fence only goes from unsignaled to signaled once\n\t * and cannot be reused. Use context and mm from the old fence.\n\t *\n\t * If an old eviction fence signals after this check, that's OK.\n\t * Anyone signaling an eviction fence must stop the queues first\n\t * and schedule another restore worker.\n\t */\n\tif (dma_fence_is_signaled(&process_info->eviction_fence->base)) {\n\t\tstruct amdgpu_amdkfd_fence *new_fence =\n\t\t\tamdgpu_amdkfd_fence_create(\n\t\t\t\tprocess_info->eviction_fence->base.context,\n\t\t\t\tprocess_info->eviction_fence->mm,\n\t\t\t\tNULL);\n\n\t\tif (!new_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tprocess_info->eviction_fence = new_fence;\n\t\treplace_eviction_fence(ef, dma_fence_get(&new_fence->base));\n\t} else {\n\t\tWARN_ONCE(*ef != &process_info->eviction_fence->base,\n\t\t\t  \"KFD eviction fence doesn't match KGD process_info\");\n\t}\n\n\t/* Attach new eviction fence to all BOs except pinned ones */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list, validate_list) {\n\t\tif (mem->bo->tbo.pin_count)\n\t\t\tcontinue;\n\n\t\tdma_resv_add_fence(mem->bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\t/* Attach eviction fence to PD / PT BOs and DMABuf imports */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *bo = peer_vm->root.bo;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\nvalidate_map_fail:\n\tamdgpu_sync_free(&sync_obj);\nttm_reserve_fail:\n\tdrm_exec_fini(&exec);\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_add_gws_to_process(void *info, void *gws, struct kgd_mem **mem)\n{\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct amdgpu_bo *gws_bo = (struct amdgpu_bo *)gws;\n\tint ret;\n\n\tif (!info || !gws)\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&(*mem)->lock);\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\t(*mem)->bo = amdgpu_bo_ref(gws_bo);\n\t(*mem)->domain = AMDGPU_GEM_DOMAIN_GWS;\n\t(*mem)->process_info = process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\n\t/* Validate gws bo the first time it is added to process */\n\tmutex_lock(&(*mem)->process_info->lock);\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\tgoto bo_reservation_failure;\n\t}\n\n\tret = amdgpu_amdkfd_bo_validate(gws_bo, AMDGPU_GEM_DOMAIN_GWS, true);\n\tif (ret) {\n\t\tpr_err(\"GWS BO validate failed %d\\n\", ret);\n\t\tgoto bo_validation_failure;\n\t}\n\t/* GWS resource is shared b/t amdgpu and amdkfd\n\t * Add process eviction fence to bo so they can\n\t * evict each other.\n\t */\n\tret = dma_resv_reserve_fences(gws_bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(gws_bo->tbo.base.resv,\n\t\t\t   &process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(gws_bo);\n\tmutex_unlock(&(*mem)->process_info->lock);\n\n\treturn ret;\n\nreserve_shared_fail:\nbo_validation_failure:\n\tamdgpu_bo_unreserve(gws_bo);\nbo_reservation_failure:\n\tmutex_unlock(&(*mem)->process_info->lock);\n\tamdgpu_sync_free(&(*mem)->sync);\n\tremove_kgd_mem_from_kfd_bo_list(*mem, process_info);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&(*mem)->lock);\n\tkfree(*mem);\n\t*mem = NULL;\n\treturn ret;\n}\n\nint amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)\n{\n\tint ret;\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\tstruct amdgpu_bo *gws_bo = kgd_mem->bo;\n\n\t/* Remove BO from process's validate list so restore worker won't touch\n\t * it anymore\n\t */\n\tremove_kgd_mem_from_kfd_bo_list(kgd_mem, process_info);\n\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\t//TODO add BO back to validate_list?\n\t\treturn ret;\n\t}\n\tamdgpu_amdkfd_remove_eviction_fence(gws_bo,\n\t\t\tprocess_info->eviction_fence);\n\tamdgpu_bo_unreserve(gws_bo);\n\tamdgpu_sync_free(&kgd_mem->sync);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&kgd_mem->lock);\n\tkfree(mem);\n\treturn 0;\n}\n\n/* Returns GPU-specific tiling mode information */\nint amdgpu_amdkfd_get_tile_config(struct amdgpu_device *adev,\n\t\t\t\tstruct tile_config *config)\n{\n\tconfig->gb_addr_config = adev->gfx.config.gb_addr_config;\n\tconfig->tile_config_ptr = adev->gfx.config.tile_mode_array;\n\tconfig->num_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.tile_mode_array);\n\tconfig->macro_tile_config_ptr =\n\t\t\tadev->gfx.config.macrotile_mode_array;\n\tconfig->num_macro_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.macrotile_mode_array);\n\n\t/* Those values are not set from GFX9 onwards */\n\tconfig->num_banks = adev->gfx.config.num_banks;\n\tconfig->num_ranks = adev->gfx.config.num_ranks;\n\n\treturn 0;\n}\n\nbool amdgpu_amdkfd_bo_mapped_to_dev(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->is_mapped && entry->adev == adev)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_kfd_mem_limits(struct seq_file *m, void *data)\n{\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tseq_printf(m, \"System mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.system_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_system_mem_limit >> 20));\n\tseq_printf(m, \"TTM mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.ttm_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_ttm_mem_limit >> 20));\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\treturn 0;\n}\n\n#endif\n", "source_code_path": "amdgpu_amdkfd_gpuvm.c", "line_number": 2464, "generated_diff": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_amdkfd_gpuvm.c\nLine: 2464\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\n====================================\nRead the following bug report.\n============ Bug Report ============\nFile: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription: A potential use-after-free vulnerability exists in the function amdgpu_amdkfd_gpuvm_import_dmabuf_fd. The '*mem' pointer may be accessed after it has been freed in the import_obj_create function, leading to undefined behavior. To mitigate this issue, a null check should be added to verify that '*mem' is valid before accessing its contents.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_amdkfd_gpuvm-bug.txt", "bug_report_text": "File: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription: The function amdgpu_amdkfd_gpuvm_import_dmabuf_fd is at risk of accessing freed memory. The '*mem' pointer may be released in the import_obj_create function, leaving it pointing to invalid memory. To prevent potential crashes or data corruption, a null check should be added to ensure '*mem' is valid before accessing its contents.\n", "diff_path": "amdgpu_amdkfd_gpuvm-diff.txt", "diff_text": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n", "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2014-2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n#include <linux/dma-buf.h>\n#include <linux/list.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/fdtable.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_object.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dma_buf.h\"\n#include <uapi/linux/kfd_ioctl.h>\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_smi_events.h\"\n\n/* Userptr restore delay, just long enough to allow consecutive VM\n * changes to accumulate\n */\n#define AMDGPU_USERPTR_RESTORE_DELAY_MS 1\n#define AMDGPU_RESERVE_MEM_LIMIT\t\t\t(3UL << 29)\n\n/*\n * Align VRAM availability to 2MB to avoid fragmentation caused by 4K allocations in the tail 2MB\n * BO chunk\n */\n#define VRAM_AVAILABLITY_ALIGN (1 << 21)\n\n/* Impose limit on how much memory KFD can use */\nstatic struct {\n\tuint64_t max_system_mem_limit;\n\tuint64_t max_ttm_mem_limit;\n\tint64_t system_mem_used;\n\tint64_t ttm_mem_used;\n\tspinlock_t mem_limit_lock;\n} kfd_mem_limit;\n\nstatic const char * const domain_bit_to_string[] = {\n\t\t\"CPU\",\n\t\t\"GTT\",\n\t\t\"VRAM\",\n\t\t\"GDS\",\n\t\t\"GWS\",\n\t\t\"OA\"\n};\n\n#define domain_string(domain) domain_bit_to_string[ffs(domain)-1]\n\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work);\n\nstatic bool kfd_mem_is_attached(struct amdgpu_vm *avm,\n\t\tstruct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list)\n\t\tif (entry->bo_va->base.vm == avm)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * reuse_dmamap() - Check whether adev can share the original\n * userptr BO\n *\n * If both adev and bo_adev are in direct mapping or\n * in the same iommu group, they can share the original BO.\n *\n * @adev: Device to which can or cannot share the original BO\n * @bo_adev: Device to which allocated BO belongs to\n *\n * Return: returns true if adev can share original userptr BO,\n * false otherwise.\n */\nstatic bool reuse_dmamap(struct amdgpu_device *adev, struct amdgpu_device *bo_adev)\n{\n\treturn (adev->ram_is_direct_mapped && bo_adev->ram_is_direct_mapped) ||\n\t\t\t(adev->dev->iommu_group == bo_adev->dev->iommu_group);\n}\n\n/* Set memory usage limits. Current, limits are\n *  System (TTM + userptr) memory - 15/16th System RAM\n *  TTM memory - 3/8th System RAM\n */\nvoid amdgpu_amdkfd_gpuvm_init_mem_limits(void)\n{\n\tstruct sysinfo si;\n\tuint64_t mem;\n\n\tif (kfd_mem_limit.max_system_mem_limit)\n\t\treturn;\n\n\tsi_meminfo(&si);\n\tmem = si.totalram - si.totalhigh;\n\tmem *= si.mem_unit;\n\n\tspin_lock_init(&kfd_mem_limit.mem_limit_lock);\n\tkfd_mem_limit.max_system_mem_limit = mem - (mem >> 6);\n\tif (kfd_mem_limit.max_system_mem_limit < 2 * AMDGPU_RESERVE_MEM_LIMIT)\n\t\tkfd_mem_limit.max_system_mem_limit >>= 1;\n\telse\n\t\tkfd_mem_limit.max_system_mem_limit -= AMDGPU_RESERVE_MEM_LIMIT;\n\n\tkfd_mem_limit.max_ttm_mem_limit = ttm_tt_pages_limit() << PAGE_SHIFT;\n\tpr_debug(\"Kernel memory limit %lluM, TTM limit %lluM\\n\",\n\t\t(kfd_mem_limit.max_system_mem_limit >> 20),\n\t\t(kfd_mem_limit.max_ttm_mem_limit >> 20));\n}\n\nvoid amdgpu_amdkfd_reserve_system_mem(uint64_t size)\n{\n\tkfd_mem_limit.system_mem_used += size;\n}\n\n/* Estimate page table size needed to represent a given memory size\n *\n * With 4KB pages, we need one 8 byte PTE for each 4KB of memory\n * (factor 512, >> 9). With 2MB pages, we need one 8 byte PTE for 2MB\n * of memory (factor 256K, >> 18). ROCm user mode tries to optimize\n * for 2MB pages for TLB efficiency. However, small allocations and\n * fragmented system memory still need some 4KB pages. We choose a\n * compromise that should work in most cases without reserving too\n * much memory for page tables unnecessarily (factor 16K, >> 14).\n */\n\n#define ESTIMATE_PT_SIZE(mem_size) max(((mem_size) >> 14), AMDGPU_VM_RESERVED_VRAM)\n\n/**\n * amdgpu_amdkfd_reserve_mem_limit() - Decrease available memory by size\n * of buffer.\n *\n * @adev: Device to which allocated BO belongs to\n * @size: Size of buffer, in bytes, encapsulated by B0. This should be\n * equivalent to amdgpu_bo_size(BO)\n * @alloc_flag: Flag used in allocating a BO as noted above\n * @xcp_id: xcp_id is used to get xcp from xcp manager, one xcp is\n * managed as one compute node in driver for app\n *\n * Return:\n *\treturns -ENOMEM in case of error, ZERO otherwise\n */\nint amdgpu_amdkfd_reserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tsize_t system_mem_needed, ttm_mem_needed, vram_needed;\n\tint ret = 0;\n\tuint64_t vram_size = 0;\n\n\tsystem_mem_needed = 0;\n\tttm_mem_needed = 0;\n\tvram_needed = 0;\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tsystem_mem_needed = size;\n\t\tttm_mem_needed = size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\t/*\n\t\t * Conservatively round up the allocation requirement to 2 MB\n\t\t * to avoid fragmentation caused by 4K allocations in the tail\n\t\t * 2M BO chunk.\n\t\t */\n\t\tvram_needed = size;\n\t\t/*\n\t\t * For GFX 9.4.3, get the VRAM size from XCP structs\n\t\t */\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\treturn -EINVAL;\n\n\t\tvram_size = KFD_XCP_MEMORY_SIZE(adev, xcp_id);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tsystem_mem_needed = size;\n\t\t\tttm_mem_needed = size;\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tsystem_mem_needed = size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (kfd_mem_limit.system_mem_used + system_mem_needed >\n\t    kfd_mem_limit.max_system_mem_limit)\n\t\tpr_debug(\"Set no_system_mem_limit=1 if using shared memory\\n\");\n\n\tif ((kfd_mem_limit.system_mem_used + system_mem_needed >\n\t     kfd_mem_limit.max_system_mem_limit && !no_system_mem_limit) ||\n\t    (kfd_mem_limit.ttm_mem_used + ttm_mem_needed >\n\t     kfd_mem_limit.max_ttm_mem_limit) ||\n\t    (adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] + vram_needed >\n\t     vram_size - reserved_for_pt - reserved_for_ras - atomic64_read(&adev->vram_pin_size))) {\n\t\tret = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\t/* Update memory accounting by decreasing available system\n\t * memory, TTM memory and GPU memory as computed above\n\t */\n\tWARN_ONCE(vram_needed && !adev,\n\t\t  \"adev reference can't be null when vram is used\");\n\tif (adev && xcp_id >= 0) {\n\t\tadev->kfd.vram_used[xcp_id] += vram_needed;\n\t\tadev->kfd.vram_used_aligned[xcp_id] +=\n\t\t\t\t(adev->flags & AMD_IS_APU) ?\n\t\t\t\tvram_needed :\n\t\t\t\tALIGN(vram_needed, VRAM_AVAILABLITY_ALIGN);\n\t}\n\tkfd_mem_limit.system_mem_used += system_mem_needed;\n\tkfd_mem_limit.ttm_mem_used += ttm_mem_needed;\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\treturn ret;\n}\n\nvoid amdgpu_amdkfd_unreserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tWARN_ONCE(!adev,\n\t\t\t  \"adev reference can't be null when alloc mem flags vram is set\");\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\tgoto release;\n\n\t\tif (adev) {\n\t\t\tadev->kfd.vram_used[xcp_id] -= size;\n\t\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -= size;\n\t\t\t\tkfd_mem_limit.system_mem_used -= size;\n\t\t\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t\t\t} else {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -=\n\t\t\t\t\tALIGN(size, VRAM_AVAILABLITY_ALIGN);\n\t\t\t}\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\tgoto release;\n\t}\n\tWARN_ONCE(adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] < 0,\n\t\t  \"KFD VRAM memory accounting unbalanced for xcp: %d\", xcp_id);\n\tWARN_ONCE(kfd_mem_limit.ttm_mem_used < 0,\n\t\t  \"KFD TTM memory accounting unbalanced\");\n\tWARN_ONCE(kfd_mem_limit.system_mem_used < 0,\n\t\t  \"KFD system memory accounting unbalanced\");\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n}\n\nvoid amdgpu_amdkfd_release_notify(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tu32 alloc_flags = bo->kfd_bo->alloc_flags;\n\tu64 size = amdgpu_bo_size(bo);\n\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, size, alloc_flags,\n\t\t\t\t\t  bo->xcp_id);\n\n\tkfree(bo->kfd_bo);\n}\n\n/**\n * create_dmamap_sg_bo() - Creates a amdgpu_bo object to reflect information\n * about USERPTR or DOOREBELL or MMIO BO.\n *\n * @adev: Device for which dmamap BO is being created\n * @mem: BO of peer device that is being DMA mapped. Provides parameters\n *\t in building the dmamap BO\n * @bo_out: Output parameter updated with handle of dmamap BO\n */\nstatic int\ncreate_dmamap_sg_bo(struct amdgpu_device *adev,\n\t\t struct kgd_mem *mem, struct amdgpu_bo **bo_out)\n{\n\tstruct drm_gem_object *gem_obj;\n\tint ret;\n\tuint64_t flags = 0;\n\n\tret = amdgpu_bo_reserve(mem->bo, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR)\n\t\tflags |= mem->bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t\t\tAMDGPU_GEM_CREATE_UNCACHED);\n\n\tret = amdgpu_gem_object_create(adev, mem->bo->tbo.base.size, 1,\n\t\t\tAMDGPU_GEM_DOMAIN_CPU, AMDGPU_GEM_CREATE_PREEMPTIBLE | flags,\n\t\t\tttm_bo_type_sg, mem->bo->tbo.base.resv, &gem_obj, 0);\n\n\tamdgpu_bo_unreserve(mem->bo);\n\n\tif (ret) {\n\t\tpr_err(\"Error in creating DMA mappable SG BO on domain: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\n\t*bo_out = gem_to_amdgpu_bo(gem_obj);\n\t(*bo_out)->parent = amdgpu_bo_ref(mem->bo);\n\treturn ret;\n}\n\n/* amdgpu_amdkfd_remove_eviction_fence - Removes eviction fence from BO's\n *  reservation object.\n *\n * @bo: [IN] Remove eviction fence(s) from this BO\n * @ef: [IN] This eviction fence is removed if it\n *  is present in the shared list.\n *\n * NOTE: Must be called with BO reserved i.e. bo->tbo.resv->lock held.\n */\nstatic int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tstruct amdgpu_amdkfd_fence *ef)\n{\n\tstruct dma_fence *replacement;\n\n\tif (!ef)\n\t\treturn -EINVAL;\n\n\t/* TODO: Instead of block before we should use the fence of the page\n\t * table update and TLB flush here directly.\n\t */\n\treplacement = dma_fence_get_stub();\n\tdma_resv_replace_fences(bo->tbo.base.resv, ef->base.context,\n\t\t\t\treplacement, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_fence_put(replacement);\n\treturn 0;\n}\n\nint amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo *root = bo;\n\tstruct amdgpu_vm_bo_base *vm_bo;\n\tstruct amdgpu_vm *vm;\n\tstruct amdkfd_process_info *info;\n\tstruct amdgpu_amdkfd_fence *ef;\n\tint ret;\n\n\t/* we can always get vm_bo from root PD bo.*/\n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tvm_bo = root->vm_bo;\n\tif (!vm_bo)\n\t\treturn 0;\n\n\tvm = vm_bo->vm;\n\tif (!vm)\n\t\treturn 0;\n\n\tinfo = vm->process_info;\n\tif (!info || !info->eviction_fence)\n\t\treturn 0;\n\n\tef = container_of(dma_fence_get(&info->eviction_fence->base),\n\t\t\tstruct amdgpu_amdkfd_fence, base);\n\n\tBUG_ON(!dma_resv_trylock(bo->tbo.base.resv));\n\tret = amdgpu_amdkfd_remove_eviction_fence(bo, ef);\n\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tdma_fence_put(&ef->base);\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,\n\t\t\t\t     bool wait)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint ret;\n\n\tif (WARN(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm),\n\t\t \"Called with userptr BO\"))\n\t\treturn -EINVAL;\n\n\t/* bo has been pinned, not need validate it */\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto validate_fail;\n\tif (wait)\n\t\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\nvalidate_fail:\n\treturn ret;\n}\n\nint amdgpu_amdkfd_bo_validate_and_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tuint32_t domain,\n\t\t\t\t\tstruct dma_fence *fence)\n{\n\tint ret = amdgpu_bo_reserve(bo, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = amdgpu_amdkfd_bo_validate(bo, domain, true);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tret = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\tdma_resv_add_fence(bo->tbo.base.resv, fence,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\nunreserve_out:\n\tamdgpu_bo_unreserve(bo);\n\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_validate_vm_bo(void *_unused, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_amdkfd_bo_validate(bo, bo->allowed_domains, false);\n}\n\n/* vm_validate_pt_pd_bos - Validate page table and directory BOs\n *\n * Page directories are not updated here because huge page handling\n * during page table updates can invalidate page directory entries\n * again. Page directories are only updated after updating page\n * tables.\n */\nstatic int vm_validate_pt_pd_bos(struct amdgpu_vm *vm,\n\t\t\t\t struct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_validate(adev, vm, ticket,\n\t\t\t\t amdgpu_amdkfd_validate_vm_bo, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to validate PT BOs\\n\");\n\t\treturn ret;\n\t}\n\n\tvm->pd_phys_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\n\treturn 0;\n}\n\nstatic int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_sync_fence(sync, vm->last_update);\n}\n\nstatic uint64_t get_pte_flags(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tuint32_t mapping_flags = AMDGPU_VM_PAGE_READABLE |\n\t\t\t\t AMDGPU_VM_MTYPE_DEFAULT;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\treturn amdgpu_gem_va_map_flags(adev, mapping_flags);\n}\n\n/**\n * create_sg_table() - Create an sg_table for a contiguous DMA addr range\n * @addr: The starting address to point to\n * @size: Size of memory area in bytes being pointed to\n *\n * Allocates an instance of sg_table and initializes it to point to memory\n * area specified by input parameters. The address used to build is assumed\n * to be DMA mapped, if needed.\n *\n * DOORBELL or MMIO BOs use only one scatterlist node in their sg_table\n * because they are physically contiguous.\n *\n * Return: Initialized instance of SG Table or NULL\n */\nstatic struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n{\n\tstruct sg_table *sg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\n\tif (!sg)\n\t\treturn NULL;\n\tif (sg_alloc_table(sg, 1, GFP_KERNEL)) {\n\t\tkfree(sg);\n\t\treturn NULL;\n\t}\n\tsg_dma_address(sg->sgl) = addr;\n\tsg->sgl->length = size;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\tsg->sgl->dma_length = size;\n#endif\n\treturn sg;\n}\n\nstatic int\nkfd_mem_dmamap_userptr(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *src_ttm = mem->bo->tbo.ttm;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tint ret;\n\n\tif (WARN_ON(ttm->num_pages != src_ttm->num_pages))\n\t\treturn -EINVAL;\n\n\tttm->sg = kmalloc(sizeof(*ttm->sg), GFP_KERNEL);\n\tif (unlikely(!ttm->sg))\n\t\treturn -ENOMEM;\n\n\t/* Same sequence as in amdgpu_ttm_tt_pin_userptr */\n\tret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,\n\t\t\t\t\tttm->num_pages, 0,\n\t\t\t\t\t(u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\tret = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (unlikely(ret))\n\t\tgoto release_sg;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto unmap_sg;\n\n\treturn 0;\n\nunmap_sg:\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\nrelease_sg:\n\tpr_err(\"DMA map userptr failed: %d\\n\", ret);\n\tsg_free_table(ttm->sg);\nfree_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tint ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n/**\n * kfd_mem_dmamap_sg_bo() - Create DMA mapped sg_table to access DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * An access request from the device that owns DOORBELL does not require DMA mapping.\n * This is because the request doesn't go through PCIe root complex i.e. it instead\n * loops back. The need to DMA map arises only when accessing peer device's DOORBELL\n *\n * In contrast, all access requests for MMIO need to be DMA mapped without regard to\n * device ownership. This is because access requests for MMIO go through PCIe root\n * complex.\n *\n * This is accomplished in two steps:\n *   - Obtain DMA mapped address of DOORBELL or MMIO memory that could be used\n *         in updating requesting device's page table\n *   - Signal TTM to mark memory pointed to by requesting device's BO as GPU\n *         accessible. This allows an update of requesting device's page table\n *         with entries associated with DOOREBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *   - Mapping of DOORBELL or MMIO BO of same or peer device\n *   - Validating an evicted DOOREBELL or MMIO BO on device seeking access\n *\n * Return: ZERO if successful, NON-ZERO otherwise\n */\nstatic int\nkfd_mem_dmamap_sg_bo(struct kgd_mem *mem,\n\t\t     struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\tdma_addr_t dma_addr;\n\tbool mmio;\n\tint ret;\n\n\t/* Expect SG Table of dmapmap BO to be NULL */\n\tmmio = (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);\n\tif (unlikely(ttm->sg)) {\n\t\tpr_err(\"SG Table of %d BO for peer device is UNEXPECTEDLY NON-NULL\", mmio);\n\t\treturn -EINVAL;\n\t}\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_addr = mem->bo->tbo.sg->sgl->dma_address;\n\tpr_debug(\"%d BO size: %d\\n\", mmio, mem->bo->tbo.sg->sgl->length);\n\tpr_debug(\"%d BO address before DMA mapping: %llx\\n\", mmio, dma_addr);\n\tdma_addr = dma_map_resource(adev->dev, dma_addr,\n\t\t\tmem->bo->tbo.sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tret = dma_mapping_error(adev->dev, dma_addr);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tpr_debug(\"%d BO address after DMA mapping: %llx\\n\", mmio, dma_addr);\n\n\tttm->sg = create_sg_table(dma_addr, mem->bo->tbo.sg->sgl->length);\n\tif (unlikely(!ttm->sg)) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\treturn ret;\n\nfree_sg:\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\nunmap_sg:\n\tdma_unmap_resource(adev->dev, dma_addr, mem->bo->tbo.sg->sgl->length,\n\t\t\t   dir, DMA_ATTR_SKIP_CPU_SYNC);\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_attachment(struct kgd_mem *mem,\n\t\t\t  struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\treturn 0;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\treturn kfd_mem_dmamap_userptr(mem, attachment);\n\tcase KFD_MEM_ATT_DMABUF:\n\t\treturn kfd_mem_dmamap_dmabuf(attachment);\n\tcase KFD_MEM_ATT_SG:\n\t\treturn kfd_mem_dmamap_sg_bo(mem, attachment);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nkfd_mem_dmaunmap_userptr(struct kgd_mem *mem,\n\t\t\t struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = false};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\n\tif (unlikely(!ttm->sg))\n\t\treturn;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\t/* This is a no-op. We don't want to trigger eviction fences when\n\t * unmapping DMABufs. Therefore the invalidation (moving to system\n\t * domain) is done in kfd_mem_dmamap_dmabuf.\n\t */\n}\n\n/**\n * kfd_mem_dmaunmap_sg_bo() - Free DMA mapped sg_table of DOORBELL or MMIO BO\n * @mem: SG BO of the DOORBELL or MMIO resource on the owning device\n * @attachment: Virtual address attachment of the BO on accessing device\n *\n * The method performs following steps:\n *   - Signal TTM to mark memory pointed to by BO as GPU inaccessible\n *   - Free SG Table that is used to encapsulate DMA mapped memory of\n *          peer device's DOORBELL or MMIO memory\n *\n * This method is invoked in the following contexts:\n *     UNMapping of DOORBELL or MMIO BO on a device having access to its memory\n *     Eviction of DOOREBELL or MMIO BO on device having access to its memory\n *\n * Return: void\n */\nstatic void\nkfd_mem_dmaunmap_sg_bo(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\n\tif (unlikely(!ttm->sg)) {\n\t\tpr_debug(\"SG Table of BO is NULL\");\n\t\treturn;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_unmap_resource(adev->dev, ttm->sg->sgl->dma_address,\n\t\t\tttm->sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\tbo->tbo.sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_attachment(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\tbreak;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\tkfd_mem_dmaunmap_userptr(mem, attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_DMABUF:\n\t\tkfd_mem_dmaunmap_dmabuf(attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_SG:\n\t\tkfd_mem_dmaunmap_sg_bo(mem, attachment);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic int kfd_mem_export_dmabuf(struct kgd_mem *mem)\n{\n\tif (!mem->dmabuf) {\n\t\tstruct amdgpu_device *bo_adev;\n\t\tstruct dma_buf *dmabuf;\n\t\tint r, fd;\n\n\t\tbo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\t\tr = drm_gem_prime_handle_to_fd(&bo_adev->ddev, bo_adev->kfd.client.file,\n\t\t\t\t\t       mem->gem_handle,\n\t\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\t\t       DRM_RDWR : 0, &fd);\n\t\tif (r)\n\t\t\treturn r;\n\t\tdmabuf = dma_buf_get(fd);\n\t\tclose_fd(fd);\n\t\tif (WARN_ON_ONCE(IS_ERR(dmabuf)))\n\t\t\treturn PTR_ERR(dmabuf);\n\t\tmem->dmabuf = dmabuf;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nkfd_mem_attach_dmabuf(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\t      struct amdgpu_bo **bo)\n{\n\tstruct drm_gem_object *gobj;\n\tint ret;\n\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tgobj = amdgpu_gem_prime_import(adev_to_drm(adev), mem->dmabuf);\n\tif (IS_ERR(gobj))\n\t\treturn PTR_ERR(gobj);\n\n\t*bo = gem_to_amdgpu_bo(gobj);\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\treturn 0;\n}\n\n/* kfd_mem_attach - Add a BO to a VM\n *\n * Everything that needs to bo done only once when a BO is first added\n * to a VM. It can later be mapped and unmapped many times without\n * repeating these steps.\n *\n * 0. Create BO for DMA mapping, if needed\n * 1. Allocate and initialize BO VA entry data structure\n * 2. Add BO to the VM\n * 3. Determine ASIC-specific PTE flags\n * 4. Alloc page tables and directories if needed\n * 4a.  Validate new page tables and directories\n */\nstatic int kfd_mem_attach(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tstruct amdgpu_vm *vm, bool is_aql)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tuint64_t va = mem->va;\n\tstruct kfd_mem_attachment *attachment[2] = {NULL, NULL};\n\tstruct amdgpu_bo *bo[2] = {NULL, NULL};\n\tstruct amdgpu_bo_va *bo_va;\n\tbool same_hive = false;\n\tint i, ret;\n\n\tif (!va) {\n\t\tpr_err(\"Invalid VA when adding BO to VM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Determine access to VRAM, MMIO and DOORBELL BOs of peer devices\n\t *\n\t * The access path of MMIO and DOORBELL BOs of is always over PCIe.\n\t * In contrast the access path of VRAM BOs depens upon the type of\n\t * link that connects the peer device. Access over PCIe is allowed\n\t * if peer device has large BAR. In contrast, access over xGMI is\n\t * allowed for both small and large BAR configurations of peer device\n\t */\n\tif ((adev != bo_adev && !(adev->flags & AMD_IS_APU)) &&\n\t    ((mem->domain == AMDGPU_GEM_DOMAIN_VRAM) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tif (mem->domain == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\tsame_hive = amdgpu_xgmi_same_hive(adev, bo_adev);\n\t\tif (!same_hive && !amdgpu_device_is_peer_accessible(bo_adev, adev))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i <= is_aql; i++) {\n\t\tattachment[i] = kzalloc(sizeof(*attachment[i]), GFP_KERNEL);\n\t\tif (unlikely(!attachment[i])) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tpr_debug(\"\\t add VA 0x%llx - 0x%llx to vm %p\\n\", va,\n\t\t\t va + bo_size, vm);\n\n\t\tif ((adev == bo_adev && !(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) ||\n\t\t    (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && reuse_dmamap(adev, bo_adev)) ||\n\t\t    (mem->domain == AMDGPU_GEM_DOMAIN_GTT && reuse_dmamap(adev, bo_adev)) ||\n\t\t    same_hive) {\n\t\t\t/* Mappings on the local GPU, or VRAM mappings in the\n\t\t\t * local hive, or userptr, or GTT mapping can reuse dma map\n\t\t\t * address space share the original BO\n\t\t\t */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = mem->bo;\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (i > 0) {\n\t\t\t/* Multiple mappings on the same GPU share the BO */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = bo[0];\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\t\t/* Create an SG BO to DMA-map userptrs on other GPUs */\n\t\t\tattachment[i]->type = KFD_MEM_ATT_USERPTR;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Handle DOORBELL BOs of peer devices and MMIO BOs of local and peer devices */\n\t\t} else if (mem->bo->tbo.type == ttm_bo_type_sg) {\n\t\t\tWARN_ONCE(!(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL ||\n\t\t\t\t    mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP),\n\t\t\t\t  \"Handing invalid SG BO in ATTACH request\");\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SG;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t/* Enable acces to GTT and VRAM BOs of peer devices */\n\t\t} else if (mem->domain == AMDGPU_GEM_DOMAIN_GTT ||\n\t\t\t   mem->domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tattachment[i]->type = KFD_MEM_ATT_DMABUF;\n\t\t\tret = kfd_mem_attach_dmabuf(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t\tpr_debug(\"Employ DMABUF mechanism to enable peer GPU access\\n\");\n\t\t} else {\n\t\t\tWARN_ONCE(true, \"Handling invalid ATTACH request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\t/* Add BO to VM internal data structures */\n\t\tret = amdgpu_bo_reserve(bo[i], false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Unable to reserve BO during memory attach\");\n\t\t\tgoto unwind;\n\t\t}\n\t\tbo_va = amdgpu_vm_bo_find(vm, bo[i]);\n\t\tif (!bo_va)\n\t\t\tbo_va = amdgpu_vm_bo_add(adev, vm, bo[i]);\n\t\telse\n\t\t\t++bo_va->ref_count;\n\t\tattachment[i]->bo_va = bo_va;\n\t\tamdgpu_bo_unreserve(bo[i]);\n\t\tif (unlikely(!attachment[i]->bo_va)) {\n\t\t\tret = -ENOMEM;\n\t\t\tpr_err(\"Failed to add BO object to VM. ret == %d\\n\",\n\t\t\t       ret);\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->va = va;\n\t\tattachment[i]->pte_flags = get_pte_flags(adev, mem);\n\t\tattachment[i]->adev = adev;\n\t\tlist_add(&attachment[i]->list, &mem->attachments);\n\n\t\tva += bo_size;\n\t}\n\n\treturn 0;\n\nunwind:\n\tfor (; i >= 0; i--) {\n\t\tif (!attachment[i])\n\t\t\tcontinue;\n\t\tif (attachment[i]->bo_va) {\n\t\t\tamdgpu_bo_reserve(bo[i], true);\n\t\t\tif (--attachment[i]->bo_va->ref_count == 0)\n\t\t\t\tamdgpu_vm_bo_del(adev, attachment[i]->bo_va);\n\t\t\tamdgpu_bo_unreserve(bo[i]);\n\t\t\tlist_del(&attachment[i]->list);\n\t\t}\n\t\tif (bo[i])\n\t\t\tdrm_gem_object_put(&bo[i]->tbo.base);\n\t\tkfree(attachment[i]);\n\t}\n\treturn ret;\n}\n\nstatic void kfd_mem_detach(struct kfd_mem_attachment *attachment)\n{\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\n\tpr_debug(\"\\t remove VA 0x%llx in entry %p\\n\",\n\t\t\tattachment->va, attachment);\n\tif (--attachment->bo_va->ref_count == 0)\n\t\tamdgpu_vm_bo_del(attachment->adev, attachment->bo_va);\n\tdrm_gem_object_put(&bo->tbo.base);\n\tlist_del(&attachment->list);\n\tkfree(attachment);\n}\n\nstatic void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,\n\t\t\t\tstruct amdkfd_process_info *process_info,\n\t\t\t\tbool userptr)\n{\n\tmutex_lock(&process_info->lock);\n\tif (userptr)\n\t\tlist_add_tail(&mem->validate_list,\n\t\t\t      &process_info->userptr_valid_list);\n\telse\n\t\tlist_add_tail(&mem->validate_list, &process_info->kfd_bo_list);\n\tmutex_unlock(&process_info->lock);\n}\n\nstatic void remove_kgd_mem_from_kfd_bo_list(struct kgd_mem *mem,\n\t\tstruct amdkfd_process_info *process_info)\n{\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n}\n\n/* Initializes user pages. It registers the MMU notifier and validates\n * the userptr BO in the GTT domain.\n *\n * The BO must already be on the userptr_valid_list. Otherwise an\n * eviction and restore may happen that leaves the new BO unmapped\n * with the user mode queues running.\n *\n * Takes the process_info->lock to protect against concurrent restore\n * workers.\n *\n * Returns 0 for success, negative errno for errors.\n */\nstatic int init_user_pages(struct kgd_mem *mem, uint64_t user_addr,\n\t\t\t   bool criu_resume)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct hmm_range *range;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->lock);\n\n\tret = amdgpu_ttm_tt_set_userptr(&bo->tbo, user_addr, 0);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to set userptr: %d\\n\", __func__, ret);\n\t\tgoto out;\n\t}\n\n\tret = amdgpu_hmm_register(bo, user_addr);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register MMU notifier: %d\\n\",\n\t\t       __func__, ret);\n\t\tgoto out;\n\t}\n\n\tif (criu_resume) {\n\t\t/*\n\t\t * During a CRIU restore operation, the userptr buffer objects\n\t\t * will be validated in the restore_userptr_work worker at a\n\t\t * later stage when it is scheduled by another ioctl called by\n\t\t * CRIU master process for the target pid for restore.\n\t\t */\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tmem->invalid++;\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t\tmutex_unlock(&process_info->lock);\n\t\treturn 0;\n\t}\n\n\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);\n\tif (ret) {\n\t\tif (ret == -EAGAIN)\n\t\t\tpr_debug(\"Failed to get user pages, try again\\n\");\n\t\telse\n\t\t\tpr_err(\"%s: Failed to get user pages: %d\\n\", __func__, ret);\n\t\tgoto unregister_out;\n\t}\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to reserve BO\\n\", __func__);\n\t\tgoto release_out;\n\t}\n\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\tamdgpu_bo_unreserve(bo);\n\nrelease_out:\n\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\nunregister_out:\n\tif (ret)\n\t\tamdgpu_hmm_unregister(bo);\nout:\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\n/* Reserving a BO and its page table BOs must happen atomically to\n * avoid deadlocks. Some operations update multiple VMs at once. Track\n * all the reservation info in a context structure. Optionally a sync\n * object can track VM updates.\n */\nstruct bo_vm_reservation_context {\n\t/* DRM execution context for the reservation */\n\tstruct drm_exec exec;\n\t/* Number of VMs reserved */\n\tunsigned int n_vms;\n\t/* Pointer to sync object */\n\tstruct amdgpu_sync *sync;\n};\n\nenum bo_vm_match {\n\tBO_VM_NOT_MAPPED = 0,\t/* Match VMs where a BO is not mapped */\n\tBO_VM_MAPPED,\t\t/* Match VMs where a BO is mapped     */\n\tBO_VM_ALL,\t\t/* Match all VMs a BO was added to    */\n};\n\n/**\n * reserve_bo_and_vm - reserve a BO and a VM unconditionally.\n * @mem: KFD BO structure.\n * @vm: the VM to reserve.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n */\nstatic int reserve_bo_and_vm(struct kgd_mem *mem,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct bo_vm_reservation_context *ctx)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tWARN_ON(!vm);\n\n\tctx->n_vms = 1;\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tret = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * reserve_bo_and_cond_vms - reserve a BO and some VMs conditionally\n * @mem: KFD BO structure.\n * @vm: the VM to reserve. If NULL, then all VMs associated with the BO\n * is used. Otherwise, a single VM associated with the BO.\n * @map_type: the mapping status that will be used to filter the VMs.\n * @ctx: the struct that will be used in unreserve_bo_and_vms().\n *\n * Returns 0 for success, negative for failure.\n */\nstatic int reserve_bo_and_cond_vms(struct kgd_mem *mem,\n\t\t\t\tstruct amdgpu_vm *vm, enum bo_vm_match map_type,\n\t\t\t\tstruct bo_vm_reservation_context *ctx)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tctx->n_vms = 0;\n\t\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\t\tif ((vm && vm != entry->bo_va->base.vm) ||\n\t\t\t\t(entry->is_mapped != map_type\n\t\t\t\t&& map_type != BO_VM_ALL))\n\t\t\t\tcontinue;\n\n\t\t\tret = amdgpu_vm_lock_pd(entry->bo_va->base.vm,\n\t\t\t\t\t\t&ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto error;\n\t\t\t++ctx->n_vms;\n\t\t}\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n/**\n * unreserve_bo_and_vms - Unreserve BO and VMs from a reservation context\n * @ctx: Reservation context to unreserve\n * @wait: Optionally wait for a sync object representing pending VM updates\n * @intr: Whether the wait is interruptible\n *\n * Also frees any resources allocated in\n * reserve_bo_and_(cond_)vm(s). Returns the status from\n * amdgpu_sync_wait.\n */\nstatic int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,\n\t\t\t\t bool wait, bool intr)\n{\n\tint ret = 0;\n\n\tif (wait)\n\t\tret = amdgpu_sync_wait(ctx->sync, intr);\n\n\tdrm_exec_fini(&ctx->exec);\n\tctx->sync = NULL;\n\treturn ret;\n}\n\nstatic void unmap_bo_from_gpuvm(struct kgd_mem *mem,\n\t\t\t\tstruct kfd_mem_attachment *entry,\n\t\t\t\tstruct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\n\tamdgpu_vm_bo_unmap(adev, bo_va, entry->va);\n\n\tamdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);\n\n\tamdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int update_gpuvm_pte(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *entry,\n\t\t\t    struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tint ret;\n\n\tret = kfd_mem_dmamap_attachment(mem, entry);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Update the page tables  */\n\tret = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (ret) {\n\t\tpr_err(\"amdgpu_vm_bo_update failed\\n\");\n\t\treturn ret;\n\t}\n\n\treturn amdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int map_bo_to_gpuvm(struct kgd_mem *mem,\n\t\t\t   struct kfd_mem_attachment *entry,\n\t\t\t   struct amdgpu_sync *sync,\n\t\t\t   bool no_update_pte)\n{\n\tint ret;\n\n\t/* Set virtual address for the allocation */\n\tret = amdgpu_vm_bo_map(entry->adev, entry->bo_va, entry->va, 0,\n\t\t\t       amdgpu_bo_size(entry->bo_va->base.bo),\n\t\t\t       entry->pte_flags);\n\tif (ret) {\n\t\tpr_err(\"Failed to map VA 0x%llx in vm. ret %d\\n\",\n\t\t\t\tentry->va, ret);\n\t\treturn ret;\n\t}\n\n\tif (no_update_pte)\n\t\treturn 0;\n\n\tret = update_gpuvm_pte(mem, entry, sync);\n\tif (ret) {\n\t\tpr_err(\"update_gpuvm_pte() failed\\n\");\n\t\tgoto update_gpuvm_pte_failed;\n\t}\n\n\treturn 0;\n\nupdate_gpuvm_pte_failed:\n\tunmap_bo_from_gpuvm(mem, entry, sync);\n\tkfd_mem_dmaunmap_attachment(mem, entry);\n\treturn ret;\n}\n\nstatic int process_validate_vms(struct amdkfd_process_info *process_info,\n\t\t\t\tstruct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_validate_pt_pd_bos(peer_vm, ticket);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_sync_pds_resv(struct amdkfd_process_info *process_info,\n\t\t\t\t struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *pd = peer_vm->root.bo;\n\n\t\tret = amdgpu_sync_resv(NULL, sync, pd->tbo.base.resv,\n\t\t\t\t       AMDGPU_SYNC_NE_OWNER,\n\t\t\t\t       AMDGPU_FENCE_OWNER_KFD);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_update_pds(struct amdkfd_process_info *process_info,\n\t\t\t      struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_update_pds(peer_vm, sync);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,\n\t\t       struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *info = NULL;\n\tint ret;\n\n\tif (!*process_info) {\n\t\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tmutex_init(&info->lock);\n\t\tmutex_init(&info->notifier_lock);\n\t\tINIT_LIST_HEAD(&info->vm_list_head);\n\t\tINIT_LIST_HEAD(&info->kfd_bo_list);\n\t\tINIT_LIST_HEAD(&info->userptr_valid_list);\n\t\tINIT_LIST_HEAD(&info->userptr_inval_list);\n\n\t\tinfo->eviction_fence =\n\t\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t\t   current->mm,\n\t\t\t\t\t\t   NULL);\n\t\tif (!info->eviction_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto create_evict_fence_fail;\n\t\t}\n\n\t\tinfo->pid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\t\tINIT_DELAYED_WORK(&info->restore_userptr_work,\n\t\t\t\t  amdgpu_amdkfd_restore_userptr_worker);\n\n\t\t*process_info = info;\n\t}\n\n\tvm->process_info = *process_info;\n\n\t/* Validate page directory and attach eviction fence */\n\tret = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (ret)\n\t\tgoto reserve_pd_fail;\n\tret = vm_validate_pt_pd_bos(vm, NULL);\n\tif (ret) {\n\t\tpr_err(\"validate_pt_pd_bos() failed\\n\");\n\t\tgoto validate_pd_fail;\n\t}\n\tret = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t  AMDGPU_FENCE_OWNER_KFD, false);\n\tif (ret)\n\t\tgoto wait_pd_fail;\n\tret = dma_resv_reserve_fences(vm->root.bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(vm->root.bo->tbo.base.resv,\n\t\t\t   &vm->process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\n\t/* Update process info */\n\tmutex_lock(&vm->process_info->lock);\n\tlist_add_tail(&vm->vm_list_node,\n\t\t\t&(vm->process_info->vm_list_head));\n\tvm->process_info->n_vms++;\n\n\t*ef = dma_fence_get(&vm->process_info->eviction_fence->base);\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn 0;\n\nreserve_shared_fail:\nwait_pd_fail:\nvalidate_pd_fail:\n\tamdgpu_bo_unreserve(vm->root.bo);\nreserve_pd_fail:\n\tvm->process_info = NULL;\n\tif (info) {\n\t\tdma_fence_put(&info->eviction_fence->base);\n\t\t*process_info = NULL;\n\t\tput_pid(info->pid);\ncreate_evict_fence_fail:\n\t\tmutex_destroy(&info->lock);\n\t\tmutex_destroy(&info->notifier_lock);\n\t\tkfree(info);\n\t}\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_pin_bo() - Pins a BO using following criteria\n * @bo: Handle of buffer object being pinned\n * @domain: Domain into which BO should be pinned\n *\n *   - USERPTR BOs are UNPINNABLE and will return error\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count incremented. It is valid to PIN a BO multiple times\n *\n * Return: ZERO if successful in pinning, Non-Zero in case of error.\n */\nstatic int amdgpu_amdkfd_gpuvm_pin_bo(struct amdgpu_bo *bo, u32 domain)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) {\n\t\t/*\n\t\t * If bo is not contiguous on VRAM, move to system memory first to ensure\n\t\t * we can get contiguous VRAM space after evicting other BOs.\n\t\t */\n\t\tif (!(bo->tbo.resource->placement & TTM_PL_FLAG_CONTIGUOUS)) {\n\t\t\tstruct ttm_operation_ctx ctx = { true, false };\n\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_debug(\"validate bo 0x%p to GTT failed %d\\n\", &bo->tbo, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n\tif (ret)\n\t\tpr_err(\"Error in Pinning BO to domain: %d\\n\", domain);\n\n\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\nout:\n\tamdgpu_bo_unreserve(bo);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_gpuvm_unpin_bo() - Unpins BO using following criteria\n * @bo: Handle of buffer object being unpinned\n *\n *   - Is a illegal request for USERPTR BOs and is ignored\n *   - All other BO types (GTT, VRAM, MMIO and DOORBELL) will have their\n *     PIN count decremented. Calls to UNPIN must balance calls to PIN\n */\nstatic void amdgpu_amdkfd_gpuvm_unpin_bo(struct amdgpu_bo *bo)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn;\n\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_set_vm_pasid(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_vm *avm, u32 pasid)\n\n{\n\tint ret;\n\n\t/* Free the original amdgpu allocated pasid,\n\t * will be replaced with kfd allocated pasid.\n\t */\n\tif (avm->pasid) {\n\t\tamdgpu_pasid_free(avm->pasid);\n\t\tamdgpu_vm_set_pasid(adev, avm, 0);\n\t}\n\n\tret = amdgpu_vm_set_pasid(adev, avm, pasid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_acquire_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *avm,\n\t\t\t\t\t   void **process_info,\n\t\t\t\t\t   struct dma_fence **ef)\n{\n\tint ret;\n\n\t/* Already a compute VM? */\n\tif (avm->process_info)\n\t\treturn -EINVAL;\n\n\t/* Convert VM into a compute VM */\n\tret = amdgpu_vm_make_compute(adev, avm);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Initialize KFD part of the VM and process info */\n\tret = init_kfd_vm(avm, process_info, ef);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_vm_set_task_info(avm);\n\n\treturn 0;\n}\n\nvoid amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm)\n{\n\tstruct amdkfd_process_info *process_info = vm->process_info;\n\n\tif (!process_info)\n\t\treturn;\n\n\t/* Update process info */\n\tmutex_lock(&process_info->lock);\n\tprocess_info->n_vms--;\n\tlist_del(&vm->vm_list_node);\n\tmutex_unlock(&process_info->lock);\n\n\tvm->process_info = NULL;\n\n\t/* Release per-process resources when last compute VM is destroyed */\n\tif (!process_info->n_vms) {\n\t\tWARN_ON(!list_empty(&process_info->kfd_bo_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_valid_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_inval_list));\n\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tcancel_delayed_work_sync(&process_info->restore_userptr_work);\n\t\tput_pid(process_info->pid);\n\t\tmutex_destroy(&process_info->lock);\n\t\tmutex_destroy(&process_info->notifier_lock);\n\t\tkfree(process_info);\n\t}\n}\n\nvoid amdgpu_amdkfd_gpuvm_release_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t    void *drm_priv)\n{\n\tstruct amdgpu_vm *avm;\n\n\tif (WARN_ON(!adev || !drm_priv))\n\t\treturn;\n\n\tavm = drm_priv_to_vm(drm_priv);\n\n\tpr_debug(\"Releasing process vm %p\\n\", avm);\n\n\t/* The original pasid of amdgpu vm has already been\n\t * released during making a amdgpu vm to a compute vm\n\t * The current pasid is managed by kfd and will be\n\t * released on kfd process destroy. Set amdgpu pasid\n\t * to 0 to avoid duplicate release.\n\t */\n\tamdgpu_vm_release_compute(adev, avm);\n}\n\nuint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *pd = avm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\n\tif (adev->asic_type < CHIP_VEGA10)\n\t\treturn avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;\n\treturn avm->pd_phys_addr;\n}\n\nvoid amdgpu_amdkfd_block_mmu_notifications(void *p)\n{\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tWRITE_ONCE(pinfo->block_mmu_notifications, true);\n\tmutex_unlock(&pinfo->lock);\n}\n\nint amdgpu_amdkfd_criu_resume(void *p)\n{\n\tint ret = 0;\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tpr_debug(\"scheduling work\\n\");\n\tmutex_lock(&pinfo->notifier_lock);\n\tpinfo->evicted_bos++;\n\tmutex_unlock(&pinfo->notifier_lock);\n\tif (!READ_ONCE(pinfo->block_mmu_notifications)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tWRITE_ONCE(pinfo->block_mmu_notifications, false);\n\tqueue_delayed_work(system_freezable_wq,\n\t\t\t   &pinfo->restore_userptr_work, 0);\n\nout_unlock:\n\tmutex_unlock(&pinfo->lock);\n\treturn ret;\n}\n\nsize_t amdgpu_amdkfd_get_available_memory(struct amdgpu_device *adev,\n\t\t\t\t\t  uint8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tuint64_t reserved_for_ras = (con ? con->reserved_pages_in_bytes : 0);\n\tssize_t available;\n\tuint64_t vram_available, system_mem_available, ttm_mem_available;\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tvram_available = KFD_XCP_MEMORY_SIZE(adev, xcp_id)\n\t\t- adev->kfd.vram_used_aligned[xcp_id]\n\t\t- atomic64_read(&adev->vram_pin_size)\n\t\t- reserved_for_pt\n\t\t- reserved_for_ras;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tsystem_mem_available = no_system_mem_limit ?\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit :\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit -\n\t\t\t\t\tkfd_mem_limit.system_mem_used;\n\n\t\tttm_mem_available = kfd_mem_limit.max_ttm_mem_limit -\n\t\t\t\tkfd_mem_limit.ttm_mem_used;\n\n\t\tavailable = min3(system_mem_available, ttm_mem_available,\n\t\t\t\t vram_available);\n\t\tavailable = ALIGN_DOWN(available, PAGE_SIZE);\n\t} else {\n\t\tavailable = ALIGN_DOWN(vram_available, VRAM_AVAILABLITY_ALIGN);\n\t}\n\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\n\treturn available;\n}\n\nint amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, uint64_t va, uint64_t size,\n\t\tvoid *drm_priv, struct kgd_mem **mem,\n\t\tuint64_t *offset, uint32_t flags, bool criu_resume)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_fpriv *fpriv = container_of(avm, struct amdgpu_fpriv, vm);\n\tenum ttm_bo_type bo_type = ttm_bo_type_device;\n\tstruct sg_table *sg = NULL;\n\tuint64_t user_addr = 0;\n\tstruct amdgpu_bo *bo;\n\tstruct drm_gem_object *gobj = NULL;\n\tu32 domain, alloc_domain;\n\tuint64_t aligned_size;\n\tint8_t xcp_id = -1;\n\tu64 alloc_flags;\n\tint ret;\n\n\t/*\n\t * Check on which domain to allocate BO\n\t */\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_flags = 0;\n\t\t} else {\n\t\t\talloc_flags = AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\t\t\talloc_flags |= (flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) ?\n\t\t\tAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED : 0;\n\n\t\t\t/* For contiguous VRAM allocation */\n\t\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_CONTIGUOUS)\n\t\t\t\talloc_flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\t\t}\n\t\txcp_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ?\n\t\t\t\t\t0 : fpriv->xcp_id;\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_flags = 0;\n\t} else {\n\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_domain = AMDGPU_GEM_DOMAIN_CPU;\n\t\talloc_flags = AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\tif (!offset || !*offset)\n\t\t\t\treturn -EINVAL;\n\t\t\tuser_addr = untagged_addr(*offset);\n\t\t} else if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\t\tbo_type = ttm_bo_type_sg;\n\t\t\tif (size > UINT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\tsg = create_sg_table(*offset, size);\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_EXT_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_EXT_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_UNCACHED)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_UNCACHED;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\t(*mem)->aql_queue = !!(flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);\n\n\t/* Workaround for AQL queue wraparound bug. Map the same\n\t * memory twice. That means we only actually allocate half\n\t * the memory.\n\t */\n\tif ((*mem)->aql_queue)\n\t\tsize >>= 1;\n\taligned_size = PAGE_ALIGN(size);\n\n\t(*mem)->alloc_flags = flags;\n\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\tret = amdgpu_amdkfd_reserve_mem_limit(adev, aligned_size, flags,\n\t\t\t\t\t      xcp_id);\n\tif (ret) {\n\t\tpr_debug(\"Insufficient memory\\n\");\n\t\tgoto err_reserve_limit;\n\t}\n\n\tpr_debug(\"\\tcreate BO VA 0x%llx size 0x%llx domain %s xcp_id %d\\n\",\n\t\t va, (*mem)->aql_queue ? size << 1 : size,\n\t\t domain_string(alloc_domain), xcp_id);\n\n\tret = amdgpu_gem_object_create(adev, aligned_size, 1, alloc_domain, alloc_flags,\n\t\t\t\t       bo_type, NULL, &gobj, xcp_id + 1);\n\tif (ret) {\n\t\tpr_debug(\"Failed to create BO on domain %s. ret %d\\n\",\n\t\t\t domain_string(alloc_domain), ret);\n\t\tgoto err_bo_create;\n\t}\n\tret = drm_vma_node_allow(&gobj->vma_node, drm_priv);\n\tif (ret) {\n\t\tpr_debug(\"Failed to allow vma node access. ret %d\\n\", ret);\n\t\tgoto err_node_allow;\n\t}\n\tret = drm_gem_handle_create(adev->kfd.client.file, gobj, &(*mem)->gem_handle);\n\tif (ret)\n\t\tgoto err_gem_handle_create;\n\tbo = gem_to_amdgpu_bo(gobj);\n\tif (bo_type == ttm_bo_type_sg) {\n\t\tbo->tbo.sg = sg;\n\t\tbo->tbo.ttm->sg = sg;\n\t}\n\tbo->kfd_bo = *mem;\n\t(*mem)->bo = bo;\n\tif (user_addr)\n\t\tbo->flags |= AMDGPU_AMDKFD_CREATE_USERPTR_BO;\n\n\t(*mem)->va = va;\n\t(*mem)->domain = domain;\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);\n\n\tif (user_addr) {\n\t\tpr_debug(\"creating userptr BO for user_addr = %llx\\n\", user_addr);\n\t\tret = init_user_pages(*mem, user_addr, criu_resume);\n\t\tif (ret)\n\t\t\tgoto allocate_init_user_pages_failed;\n\t} else  if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tret = amdgpu_amdkfd_gpuvm_pin_bo(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (ret) {\n\t\t\tpr_err(\"Pinning MMIO/DOORBELL BO during ALLOC FAILED\\n\");\n\t\t\tgoto err_pin_bo;\n\t\t}\n\t\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t} else {\n\t\tmutex_lock(&avm->process_info->lock);\n\t\tif (avm->process_info->eviction_fence &&\n\t\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\t\tmutex_unlock(&avm->process_info->lock);\n\t\tif (ret)\n\t\t\tgoto err_validate_bo;\n\t}\n\n\tif (offset)\n\t\t*offset = amdgpu_bo_mmap_offset(bo);\n\n\treturn 0;\n\nallocate_init_user_pages_failed:\nerr_pin_bo:\nerr_validate_bo:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_gem_handle_delete(adev->kfd.client.file, (*mem)->gem_handle);\nerr_gem_handle_create:\n\tdrm_vma_node_revoke(&gobj->vma_node, drm_priv);\nerr_node_allow:\n\t/* Don't unreserve system mem limit twice */\n\tgoto err_reserve_limit;\nerr_bo_create:\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, aligned_size, flags, xcp_id);\nerr_reserve_limit:\n\tamdgpu_sync_free(&(*mem)->sync);\n\tmutex_destroy(&(*mem)->lock);\n\tif (gobj)\n\t\tdrm_gem_object_put(gobj);\n\telse\n\t\tkfree(*mem);\nerr:\n\tif (sg) {\n\t\tsg_free_table(sg);\n\t\tkfree(sg);\n\t}\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_free_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv,\n\t\tuint64_t *size)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tbool use_release_notifier = (mem->bo->kfd_bo == mem);\n\tstruct kfd_mem_attachment *entry, *tmp;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned int mapped_to_gpu_memory;\n\tint ret;\n\tbool is_imported = false;\n\n\tmutex_lock(&mem->lock);\n\n\t/* Unpin MMIO/DOORBELL BO's that were pinned during allocation */\n\tif (mem->alloc_flags &\n\t    (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t     KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tamdgpu_amdkfd_gpuvm_unpin_bo(mem->bo);\n\t}\n\n\tmapped_to_gpu_memory = mem->mapped_to_gpu_memory;\n\tis_imported = mem->is_imported;\n\tmutex_unlock(&mem->lock);\n\t/* lock is not needed after this, since mem is unused and will\n\t * be freed anyway\n\t */\n\n\tif (mapped_to_gpu_memory > 0) {\n\t\tpr_debug(\"BO VA 0x%llx size 0x%lx is still mapped.\\n\",\n\t\t\t\tmem->va, bo_size);\n\t\treturn -EBUSY;\n\t}\n\n\t/* Make sure restore workers don't access the BO any more */\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n\n\t/* Cleanup user pages and MMU notifiers */\n\tif (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\tamdgpu_hmm_unregister(mem->bo);\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tamdgpu_ttm_tt_discard_user_pages(mem->bo->tbo.ttm, mem->range);\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t}\n\n\tret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\tprocess_info->eviction_fence);\n\tpr_debug(\"Release VA 0x%llx - 0x%llx\\n\", mem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue));\n\n\t/* Remove from VM internal data structures */\n\tlist_for_each_entry_safe(entry, tmp, &mem->attachments, list) {\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t\tkfd_mem_detach(entry);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\t/* Free the sync object */\n\tamdgpu_sync_free(&mem->sync);\n\n\t/* If the SG is not NULL, it's one we created for a doorbell or mmio\n\t * remap BO. We need to free it.\n\t */\n\tif (mem->bo->tbo.sg) {\n\t\tsg_free_table(mem->bo->tbo.sg);\n\t\tkfree(mem->bo->tbo.sg);\n\t}\n\n\t/* Update the size of the BO being freed if it was allocated from\n\t * VRAM and is not imported. For APP APU VRAM allocations are done\n\t * in GTT domain\n\t */\n\tif (size) {\n\t\tif (!is_imported &&\n\t\t   (mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM ||\n\t\t   ((adev->flags & AMD_IS_APU) &&\n\t\t    mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT)))\n\t\t\t*size = bo_size;\n\t\telse\n\t\t\t*size = 0;\n\t}\n\n\t/* Free the BO*/\n\tdrm_vma_node_revoke(&mem->bo->tbo.base.vma_node, drm_priv);\n\tdrm_gem_handle_delete(adev->kfd.client.file, mem->gem_handle);\n\tif (mem->dmabuf) {\n\t\tdma_buf_put(mem->dmabuf);\n\t\tmem->dmabuf = NULL;\n\t}\n\tmutex_destroy(&mem->lock);\n\n\t/* If this releases the last reference, it will end up calling\n\t * amdgpu_amdkfd_release_notify and kfree the mem struct. That's why\n\t * this needs to be the last call here.\n\t */\n\tdrm_gem_object_put(&mem->bo->tbo.base);\n\n\t/*\n\t * For kgd_mem allocated in amdgpu_amdkfd_gpuvm_import_dmabuf(),\n\t * explicitly free it here.\n\t */\n\tif (!use_release_notifier)\n\t\tkfree(mem);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tvoid *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tint ret;\n\tstruct amdgpu_bo *bo;\n\tuint32_t domain;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned long bo_size;\n\tbool is_invalid_userptr = false;\n\n\tbo = mem->bo;\n\tif (!bo) {\n\t\tpr_err(\"Invalid BO when mapping memory to GPU\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Make sure restore is not running concurrently. Since we\n\t * don't map invalid userptr BOs, we rely on the next restore\n\t * worker to do the mapping\n\t */\n\tmutex_lock(&mem->process_info->lock);\n\n\t/* Lock notifier lock. If we find an invalid userptr BO, we can be\n\t * sure that the MMU notifier is no longer running\n\t * concurrently and the queues are actually stopped\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tmutex_lock(&mem->process_info->notifier_lock);\n\t\tis_invalid_userptr = !!mem->invalid;\n\t\tmutex_unlock(&mem->process_info->notifier_lock);\n\t}\n\n\tmutex_lock(&mem->lock);\n\n\tdomain = mem->domain;\n\tbo_size = bo->tbo.base.size;\n\n\tpr_debug(\"Map VA 0x%llx - 0x%llx to vm %p domain %s\\n\",\n\t\t\tmem->va,\n\t\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\t\tavm, domain_string(domain));\n\n\tif (!kfd_mem_is_attached(avm, mem)) {\n\t\tret = kfd_mem_attach(adev, mem, avm, mem->aql_queue);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = reserve_bo_and_vm(mem, avm, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\t/* Userptr can be marked as \"not invalid\", but not actually be\n\t * validated yet (still in the system domain). In that case\n\t * the queues are still stopped and we can leave mapping for\n\t * the next restore worker\n\t */\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_SYSTEM)\n\t\tis_invalid_userptr = true;\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto out_unreserve;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t map VA 0x%llx - 0x%llx in entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = map_bo_to_gpuvm(mem, entry, ctx.sync,\n\t\t\t\t      is_invalid_userptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map bo to gpuvm\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tret = vm_update_pds(avm, ctx.sync);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to update page directories\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tentry->is_mapped = true;\n\t\tmem->mapped_to_gpu_memory++;\n\t\tpr_debug(\"\\t INC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\tgoto out;\n\nout_unreserve:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->process_info->lock);\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_dmaunmap_mem(struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_vm *vm;\n\tint ret;\n\n\tvm = drm_priv_to_vm(drm_priv);\n\n\tmutex_lock(&mem->lock);\n\n\tret = amdgpu_bo_reserve(mem->bo, true);\n\tif (ret)\n\t\tgoto out;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != vm)\n\t\t\tcontinue;\n\t\tif (entry->bo_va->base.bo->tbo.ttm &&\n\t\t    !entry->bo_va->base.bo->tbo.ttm->sg)\n\t\t\tcontinue;\n\n\t\tkfd_mem_dmaunmap_attachment(mem, entry);\n\t}\n\n\tamdgpu_bo_unreserve(mem->bo);\nout:\n\tmutex_unlock(&mem->lock);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\n\tret = reserve_bo_and_cond_vms(mem, avm, BO_VM_MAPPED, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\t/* If no VMs were reserved, it means the BO wasn't actually mapped */\n\tif (ctx.n_vms == 0) {\n\t\tret = -EINVAL;\n\t\tgoto unreserve_out;\n\t}\n\n\tret = vm_validate_pt_pd_bos(avm, NULL);\n\tif (unlikely(ret))\n\t\tgoto unreserve_out;\n\n\tpr_debug(\"Unmap VA 0x%llx - 0x%llx from vm %p\\n\",\n\t\tmem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\tavm);\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || !entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t unmap VA 0x%llx - 0x%llx from entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tunmap_bo_from_gpuvm(mem, entry, ctx.sync);\n\t\tentry->is_mapped = false;\n\n\t\tmem->mapped_to_gpu_memory--;\n\t\tpr_debug(\"\\t DEC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\nunreserve_out:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_sync_memory(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tmutex_lock(&mem->lock);\n\tamdgpu_sync_clone(&mem->sync, &sync);\n\tmutex_unlock(&mem->lock);\n\n\tret = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn ret;\n}\n\n/**\n * amdgpu_amdkfd_map_gtt_bo_to_gart - Map BO to GART and increment reference count\n * @bo: Buffer object to be mapped\n *\n * Before return, bo reference count is incremented. To release the reference and unpin/\n * unmap the BO, call amdgpu_amdkfd_free_gtt_mem.\n */\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo)\n{\n\tint ret;\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto err_reserve_bo_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto err_pin_bo_failed;\n\t}\n\n\tret = amdgpu_ttm_alloc_gart(&bo->tbo);\n\tif (ret) {\n\t\tpr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\n\t\tgoto err_map_bo_gart_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, bo->vm_bo->vm->process_info->eviction_fence);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tbo = amdgpu_bo_ref(bo);\n\n\treturn 0;\n\nerr_map_bo_gart_failed:\n\tamdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\n\tamdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Map a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be mapped for CPU access\n * @kptr[out]: pointer in kernel CPU address space\n * @size[out]: size of the buffer\n *\n * Pins the BO and maps it for kernel CPU access. The eviction fence is removed\n * from the BO, since pinned BOs cannot be evicted. The bo must remain on the\n * validate_list, so the GPU mapping can be restored after a page table was\n * evicted.\n *\n * Return: 0 on success, error code on failure\n */\nint amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_mem *mem,\n\t\t\t\t\t     void **kptr, uint64_t *size)\n{\n\tint ret;\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tpr_err(\"userptr can't be mapped to kernel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&mem->process_info->lock);\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto bo_reserve_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto pin_failed;\n\t}\n\n\tret = amdgpu_bo_kmap(bo, kptr);\n\tif (ret) {\n\t\tpr_err(\"Failed to map bo to kernel. ret %d\\n\", ret);\n\t\tgoto kmap_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, mem->process_info->eviction_fence);\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tmutex_unlock(&mem->process_info->lock);\n\treturn 0;\n\nkmap_failed:\n\tamdgpu_bo_unpin(bo);\npin_failed:\n\tamdgpu_bo_unreserve(bo);\nbo_reserve_failed:\n\tmutex_unlock(&mem->process_info->lock);\n\n\treturn ret;\n}\n\n/** amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel() - Unmap a GTT BO for kernel CPU access\n *\n * @mem: Buffer object to be unmapped for CPU access\n *\n * Removes the kernel CPU mapping and unpins the BO. It does not restore the\n * eviction fence, so this function should only be used for cleanup before the\n * BO is destroyed.\n */\nvoid amdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(struct kgd_mem *mem)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct kfd_vm_fault_info *mem)\n{\n\tif (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {\n\t\t*mem = *adev->gmc.vm_fault_info;\n\t\tmb(); /* make sure read happened */\n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\t}\n\treturn 0;\n}\n\nstatic int import_obj_create(struct amdgpu_device *adev,\n\t\t\t     struct dma_buf *dma_buf,\n\t\t\t     struct drm_gem_object *obj,\n\t\t\t     uint64_t va, void *drm_priv,\n\t\t\t     struct kgd_mem **mem, uint64_t *size,\n\t\t\t     uint64_t *mmap_offset)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tbo = gem_to_amdgpu_bo(obj);\n\tif (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT)))\n\t\t/* Only VRAM and GTT BOs are supported */\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tret = drm_vma_node_allow(&obj->vma_node, drm_priv);\n\tif (ret)\n\t\tgoto err_free_mem;\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tif (mmap_offset)\n\t\t*mmap_offset = amdgpu_bo_mmap_offset(bo);\n\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\n\t(*mem)->alloc_flags =\n\t\t((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?\n\t\tKFD_IOC_ALLOC_MEM_FLAGS_VRAM : KFD_IOC_ALLOC_MEM_FLAGS_GTT)\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\n\tget_dma_buf(dma_buf);\n\t(*mem)->dmabuf = dma_buf;\n\t(*mem)->bo = bo;\n\t(*mem)->va = va;\n\t(*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) &&\n\t\t\t !(adev->flags & AMD_IS_APU) ?\n\t\t\t AMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;\n\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\t(*mem)->is_imported = true;\n\n\tmutex_lock(&avm->process_info->lock);\n\tif (avm->process_info->eviction_fence &&\n\t    !dma_fence_is_signaled(&avm->process_info->eviction_fence->base))\n\t\tret = amdgpu_amdkfd_bo_validate_and_fence(bo, (*mem)->domain,\n\t\t\t\t&avm->process_info->eviction_fence->base);\n\tmutex_unlock(&avm->process_info->lock);\n\tif (ret)\n\t\tgoto err_remove_mem;\n\n\treturn 0;\n\nerr_remove_mem:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_vma_node_revoke(&obj->vma_node, drm_priv);\nerr_free_mem:\n\tkfree(*mem);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n\t\t\t\t\t uint64_t va, void *drm_priv,\n\t\t\t\t\t struct kgd_mem **mem, uint64_t *size,\n\t\t\t\t\t uint64_t *mmap_offset)\n{\n\tstruct drm_gem_object *obj;\n\tuint32_t handle;\n\tint ret;\n\n\tret = drm_gem_prime_fd_to_handle(&adev->ddev, adev->kfd.client.file, fd,\n\t\t\t\t\t &handle);\n\tif (ret)\n\t\treturn ret;\n\tobj = drm_gem_object_lookup(adev->kfd.client.file, handle);\n\tif (!obj) {\n\t\tret = -EINVAL;\n\t\tgoto err_release_handle;\n\t}\n\n\tret = import_obj_create(adev, obj->dma_buf, obj, va, drm_priv, mem, size,\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\nerr_release_handle:\n\tdrm_gem_handle_delete(adev->kfd.client.file, handle);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_mem *mem,\n\t\t\t\t      struct dma_buf **dma_buf)\n{\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\tgoto out;\n\n\tget_dma_buf(mem->dmabuf);\n\t*dma_buf = mem->dmabuf;\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\n/* Evict a userptr BO by stopping the queues if necessary\n *\n * Runs in MMU notifier, may be in RECLAIM_FS context. This means it\n * cannot do any memory allocations, and cannot take any locks that\n * are held elsewhere while allocating memory.\n *\n * It doesn't do anything to the BO itself. The real work happens in\n * restore, where we get updated page addresses. This function only\n * ensures that GPU access to the BO is stopped.\n */\nint amdgpu_amdkfd_evict_userptr(struct mmu_interval_notifier *mni,\n\t\t\t\tunsigned long cur_seq, struct kgd_mem *mem)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tint r = 0;\n\n\t/* Do not process MMU notifications during CRIU restore until\n\t * KFD_CRIU_OP_RESUME IOCTL is received\n\t */\n\tif (READ_ONCE(process_info->block_mmu_notifications))\n\t\treturn 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmem->invalid++;\n\tif (++process_info->evicted_bos == 1) {\n\t\t/* First eviction, stop the queues */\n\t\tr = kgd2kfd_quiesce_mm(mni->mm,\n\t\t\t\t       KFD_QUEUE_EVICTION_TRIGGER_USERPTR);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to quiesce KFD\\n\");\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\t}\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn r;\n}\n\n/* Update invalid userptr BOs\n *\n * Moves invalidated (evicted) userptr BOs from userptr_valid_list to\n * userptr_inval_list and updates user pages for all BOs that have\n * been invalidated since their last update.\n */\nstatic int update_invalid_user_pages(struct amdkfd_process_info *process_info,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tuint32_t invalid;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\n\t/* Move all invalidated BOs to the userptr_inval_list */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_valid_list,\n\t\t\t\t validate_list)\n\t\tif (mem->invalid)\n\t\t\tlist_move_tail(&mem->validate_list,\n\t\t\t\t       &process_info->userptr_inval_list);\n\n\t/* Go through userptr_inval_list and update any invalid user_pages */\n\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t    validate_list) {\n\t\tinvalid = mem->invalid;\n\t\tif (!invalid)\n\t\t\t/* BO hasn't been invalidated since the last\n\t\t\t * revalidation attempt. Keep its page list.\n\t\t\t */\n\t\t\tcontinue;\n\n\t\tbo = mem->bo;\n\n\t\tamdgpu_ttm_tt_discard_user_pages(bo->tbo.ttm, mem->range);\n\t\tmem->range = NULL;\n\n\t\t/* BO reservations and getting user pages (hmm_range_fault)\n\t\t * must happen outside the notifier lock\n\t\t */\n\t\tmutex_unlock(&process_info->notifier_lock);\n\n\t\t/* Move the BO to system (CPU) domain if necessary to unmap\n\t\t * and free the SG table\n\t\t */\n\t\tif (bo->tbo.resource->mem_type != TTM_PL_SYSTEM) {\n\t\t\tif (amdgpu_bo_reserve(bo, true))\n\t\t\t\treturn -EAGAIN;\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Failed to invalidate userptr BO\\n\",\n\t\t\t\t       __func__);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\t/* Get updated user pages */\n\t\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t   &mem->range);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed %d to get user pages\\n\", ret);\n\n\t\t\t/* Return -EFAULT bad address error as success. It will\n\t\t\t * fail later with a VM fault if the GPU tries to access\n\t\t\t * it. Better than hanging indefinitely with stalled\n\t\t\t * user mode queues.\n\t\t\t *\n\t\t\t * Return other error -EBUSY or -ENOMEM to retry restore\n\t\t\t */\n\t\t\tif (ret != -EFAULT)\n\t\t\t\treturn ret;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\tmutex_lock(&process_info->notifier_lock);\n\n\t\t/* Mark the BO as valid unless it was invalidated\n\t\t * again concurrently.\n\t\t */\n\t\tif (mem->invalid != invalid) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t /* set mem valid if mem has hmm range associated */\n\t\tif (mem->range)\n\t\t\tmem->invalid = 0;\n\t}\n\nunlock_out:\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn ret;\n}\n\n/* Validate invalid userptr BOs\n *\n * Validates BOs on the userptr_inval_list. Also updates GPUVM page tables\n * with new page addresses and waits for the page table updates to complete.\n */\nstatic int validate_invalid_user_pages(struct amdkfd_process_info *process_info)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0, 0);\n\t/* Reserve all BOs and page tables for validation */\n\tdrm_exec_until_all_locked(&exec) {\n\t\t/* Reserve all the page directories */\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\t/* Reserve the userptr_inval_list entries to resv_list */\n\t\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\tret = process_validate_vms(process_info, NULL);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\t/* Validate BOs and update GPUVM page tables */\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tbo = mem->bo;\n\n\t\t/* Validate the BO if we got user pages */\n\t\tif (bo->tbo.ttm->pages[0]) {\n\t\t\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\n\t\t/* Update mapping. If the BO was not validated\n\t\t * (because we couldn't get user pages), this will\n\t\t * clear the page table entries, which will result in\n\t\t * VM faults if the GPU tries to access the invalid\n\t\t * memory.\n\t\t */\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: update PTE failed\\n\", __func__);\n\t\t\t\t/* make sure this gets validated again */\n\t\t\t\tmutex_lock(&process_info->notifier_lock);\n\t\t\t\tmem->invalid++;\n\t\t\t\tmutex_unlock(&process_info->notifier_lock);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync);\n\nunreserve_out:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_wait(&sync, false);\n\tamdgpu_sync_free(&sync);\n\n\treturn ret;\n}\n\n/* Confirm that all user pages are valid while holding the notifier lock\n *\n * Moves valid BOs from the userptr_inval_list back to userptr_val_list.\n */\nstatic int confirm_valid_user_pages_locked(struct amdkfd_process_info *process_info)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tbool valid;\n\n\t\t/* keep mem without hmm range at userptr_inval_list */\n\t\tif (!mem->range)\n\t\t\tcontinue;\n\n\t\t/* Only check mem with hmm range associated */\n\t\tvalid = amdgpu_ttm_tt_get_user_pages_done(\n\t\t\t\t\tmem->bo->tbo.ttm, mem->range);\n\n\t\tmem->range = NULL;\n\t\tif (!valid) {\n\t\t\tWARN(!mem->invalid, \"Invalid BO not marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem->invalid) {\n\t\t\tWARN(1, \"Valid BO is marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move_tail(&mem->validate_list,\n\t\t\t       &process_info->userptr_valid_list);\n\t}\n\n\treturn ret;\n}\n\n/* Worker callback to restore evicted userptr BOs\n *\n * Tries to update and validate all userptr BOs. If successful and no\n * concurrent evictions happened, the queues are restarted. Otherwise,\n * reschedule for another attempt later.\n */\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info =\n\t\tcontainer_of(dwork, struct amdkfd_process_info,\n\t\t\t     restore_userptr_work);\n\tstruct task_struct *usertask;\n\tstruct mm_struct *mm;\n\tuint32_t evicted_bos;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tevicted_bos = process_info->evicted_bos;\n\tmutex_unlock(&process_info->notifier_lock);\n\tif (!evicted_bos)\n\t\treturn;\n\n\t/* Reference task and mm in case of concurrent process termination */\n\tusertask = get_pid_task(process_info->pid, PIDTYPE_PID);\n\tif (!usertask)\n\t\treturn;\n\tmm = get_task_mm(usertask);\n\tif (!mm) {\n\t\tput_task_struct(usertask);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\n\tif (update_invalid_user_pages(process_info, mm))\n\t\tgoto unlock_out;\n\t/* userptr_inval_list can be empty if all evicted userptr BOs\n\t * have been freed. In that case there is nothing to validate\n\t * and we can just restart the queues.\n\t */\n\tif (!list_empty(&process_info->userptr_inval_list)) {\n\t\tif (validate_invalid_user_pages(process_info))\n\t\t\tgoto unlock_out;\n\t}\n\t/* Final check for concurrent evicton and atomic update. If\n\t * another eviction happens after successful update, it will\n\t * be a first eviction that calls quiesce_mm. The eviction\n\t * reference counting inside KFD will handle this case.\n\t */\n\tmutex_lock(&process_info->notifier_lock);\n\tif (process_info->evicted_bos != evicted_bos)\n\t\tgoto unlock_notifier_out;\n\n\tif (confirm_valid_user_pages_locked(process_info)) {\n\t\tWARN(1, \"User pages unexpectedly invalid\");\n\t\tgoto unlock_notifier_out;\n\t}\n\n\tprocess_info->evicted_bos = evicted_bos = 0;\n\n\tif (kgd2kfd_resume_mm(mm)) {\n\t\tpr_err(\"%s: Failed to resume KFD\\n\", __func__);\n\t\t/* No recovery from this failure. Probably the CP is\n\t\t * hanging. No point trying again.\n\t\t */\n\t}\n\nunlock_notifier_out:\n\tmutex_unlock(&process_info->notifier_lock);\nunlock_out:\n\tmutex_unlock(&process_info->lock);\n\n\t/* If validation failed, reschedule another attempt */\n\tif (evicted_bos) {\n\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n\tput_task_struct(usertask);\n}\n\nstatic void replace_eviction_fence(struct dma_fence __rcu **ef,\n\t\t\t\t   struct dma_fence *new_ef)\n{\n\tstruct dma_fence *old_ef = rcu_replace_pointer(*ef, new_ef, true\n\t\t/* protected by process_info->lock */);\n\n\t/* If we're replacing an unsignaled eviction fence, that fence will\n\t * never be signaled, and if anyone is still waiting on that fence,\n\t * they will hang forever. This should never happen. We should only\n\t * replace the fence in restore_work that only gets scheduled after\n\t * eviction work signaled the fence.\n\t */\n\tWARN_ONCE(!dma_fence_is_signaled(old_ef),\n\t\t  \"Replacing unsignaled eviction fence\");\n\tdma_fence_put(old_ef);\n}\n\n/** amdgpu_amdkfd_gpuvm_restore_process_bos - Restore all BOs for the given\n *   KFD process identified by process_info\n *\n * @process_info: amdkfd_process_info of the KFD process\n *\n * After memory eviction, restore thread calls this function. The function\n * should be called when the Process is still valid. BO restore involves -\n *\n * 1.  Release old eviction fence and create new one\n * 2.  Get two copies of PD BO list from all the VMs. Keep one copy as pd_list.\n * 3   Use the second PD list and kfd_bo_list to create a list (ctx.list) of\n *     BOs that need to be reserved.\n * 4.  Reserve all the BOs\n * 5.  Validate of PD and PT BOs.\n * 6.  Validate all KFD BOs using kfd_bo_list and Map them and add new fence\n * 7.  Add fence to all PD and PT BOs.\n * 8.  Unreserve all BOs\n */\nint amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct dma_fence __rcu **ef)\n{\n\tstruct amdkfd_process_info *process_info = info;\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem;\n\tstruct list_head duplicate_save;\n\tstruct amdgpu_sync sync_obj;\n\tunsigned long failed_size = 0;\n\tunsigned long total_size = 0;\n\tstruct drm_exec exec;\n\tint ret;\n\n\tINIT_LIST_HEAD(&duplicate_save);\n\n\tmutex_lock(&process_info->lock);\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"Locking VM PD failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\n\t\t/* Reserve all BOs and page tables/directory. Add all BOs from\n\t\t * kfd_bo_list to ctx.list\n\t\t */\n\t\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tpr_err(\"drm_exec_prepare_obj failed, ret: %d\\n\", ret);\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tamdgpu_sync_create(&sync_obj);\n\n\t/* Validate BOs managed by KFD */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\n\t\tstruct amdgpu_bo *bo = mem->bo;\n\t\tuint32_t domain = mem->domain;\n\t\tstruct dma_resv_iter cursor;\n\t\tstruct dma_fence *fence;\n\n\t\ttotal_size += amdgpu_bo_size(bo);\n\n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: Validate BOs failed\\n\");\n\t\t\tfailed_size += amdgpu_bo_size(bo);\n\t\t\tret = amdgpu_amdkfd_bo_validate(bo,\n\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT, false);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tdma_resv_for_each_fence(&cursor, bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_KERNEL, fence) {\n\t\t\tret = amdgpu_sync_fence(&sync_obj, fence);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Sync BO fence failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (failed_size)\n\t\tpr_debug(\"0x%lx/0x%lx in system\\n\", failed_size, total_size);\n\n\t/* Validate PDs, PTs and evicted DMABuf imports last. Otherwise BO\n\t * validations above would invalidate DMABuf imports again.\n\t */\n\tret = process_validate_vms(process_info, &exec.ticket);\n\tif (ret) {\n\t\tpr_debug(\"Validating VMs failed, ret: %d\\n\", ret);\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Update mappings managed by KFD. */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync_obj);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: update PTE failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Update mappings not managed by KFD */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\tvm_list_node) {\n\t\tstruct amdgpu_device *adev = amdgpu_ttm_adev(\n\t\t\tpeer_vm->root.bo->tbo.bdev);\n\n\t\tret = amdgpu_vm_handle_moved(adev, peer_vm, &exec.ticket);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: handle moved failed. Try again\\n\");\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t}\n\n\t/* Update page directories */\n\tret = process_update_pds(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: update PDs failed. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Sync with fences on all the page tables. They implicitly depend on any\n\t * move fences from amdgpu_vm_handle_moved above.\n\t */\n\tret = process_sync_pds_resv(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: Failed to sync to PD BO moving fence. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t/* Wait for validate and PT updates to finish */\n\tamdgpu_sync_wait(&sync_obj, false);\n\n\t/* The old eviction fence may be unsignaled if restore happens\n\t * after a GPU reset or suspend/resume. Keep the old fence in that\n\t * case. Otherwise release the old eviction fence and create new\n\t * one, because fence only goes from unsignaled to signaled once\n\t * and cannot be reused. Use context and mm from the old fence.\n\t *\n\t * If an old eviction fence signals after this check, that's OK.\n\t * Anyone signaling an eviction fence must stop the queues first\n\t * and schedule another restore worker.\n\t */\n\tif (dma_fence_is_signaled(&process_info->eviction_fence->base)) {\n\t\tstruct amdgpu_amdkfd_fence *new_fence =\n\t\t\tamdgpu_amdkfd_fence_create(\n\t\t\t\tprocess_info->eviction_fence->base.context,\n\t\t\t\tprocess_info->eviction_fence->mm,\n\t\t\t\tNULL);\n\n\t\tif (!new_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto validate_map_fail;\n\t\t}\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tprocess_info->eviction_fence = new_fence;\n\t\treplace_eviction_fence(ef, dma_fence_get(&new_fence->base));\n\t} else {\n\t\tWARN_ONCE(*ef != &process_info->eviction_fence->base,\n\t\t\t  \"KFD eviction fence doesn't match KGD process_info\");\n\t}\n\n\t/* Attach new eviction fence to all BOs except pinned ones */\n\tlist_for_each_entry(mem, &process_info->kfd_bo_list, validate_list) {\n\t\tif (mem->bo->tbo.pin_count)\n\t\t\tcontinue;\n\n\t\tdma_resv_add_fence(mem->bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\t/* Attach eviction fence to PD / PT BOs and DMABuf imports */\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *bo = peer_vm->root.bo;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\nvalidate_map_fail:\n\tamdgpu_sync_free(&sync_obj);\nttm_reserve_fail:\n\tdrm_exec_fini(&exec);\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_add_gws_to_process(void *info, void *gws, struct kgd_mem **mem)\n{\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct amdgpu_bo *gws_bo = (struct amdgpu_bo *)gws;\n\tint ret;\n\n\tif (!info || !gws)\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&(*mem)->lock);\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\t(*mem)->bo = amdgpu_bo_ref(gws_bo);\n\t(*mem)->domain = AMDGPU_GEM_DOMAIN_GWS;\n\t(*mem)->process_info = process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\n\t/* Validate gws bo the first time it is added to process */\n\tmutex_lock(&(*mem)->process_info->lock);\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\tgoto bo_reservation_failure;\n\t}\n\n\tret = amdgpu_amdkfd_bo_validate(gws_bo, AMDGPU_GEM_DOMAIN_GWS, true);\n\tif (ret) {\n\t\tpr_err(\"GWS BO validate failed %d\\n\", ret);\n\t\tgoto bo_validation_failure;\n\t}\n\t/* GWS resource is shared b/t amdgpu and amdkfd\n\t * Add process eviction fence to bo so they can\n\t * evict each other.\n\t */\n\tret = dma_resv_reserve_fences(gws_bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(gws_bo->tbo.base.resv,\n\t\t\t   &process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(gws_bo);\n\tmutex_unlock(&(*mem)->process_info->lock);\n\n\treturn ret;\n\nreserve_shared_fail:\nbo_validation_failure:\n\tamdgpu_bo_unreserve(gws_bo);\nbo_reservation_failure:\n\tmutex_unlock(&(*mem)->process_info->lock);\n\tamdgpu_sync_free(&(*mem)->sync);\n\tremove_kgd_mem_from_kfd_bo_list(*mem, process_info);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&(*mem)->lock);\n\tkfree(*mem);\n\t*mem = NULL;\n\treturn ret;\n}\n\nint amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)\n{\n\tint ret;\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\tstruct amdgpu_bo *gws_bo = kgd_mem->bo;\n\n\t/* Remove BO from process's validate list so restore worker won't touch\n\t * it anymore\n\t */\n\tremove_kgd_mem_from_kfd_bo_list(kgd_mem, process_info);\n\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\t//TODO add BO back to validate_list?\n\t\treturn ret;\n\t}\n\tamdgpu_amdkfd_remove_eviction_fence(gws_bo,\n\t\t\tprocess_info->eviction_fence);\n\tamdgpu_bo_unreserve(gws_bo);\n\tamdgpu_sync_free(&kgd_mem->sync);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&kgd_mem->lock);\n\tkfree(mem);\n\treturn 0;\n}\n\n/* Returns GPU-specific tiling mode information */\nint amdgpu_amdkfd_get_tile_config(struct amdgpu_device *adev,\n\t\t\t\tstruct tile_config *config)\n{\n\tconfig->gb_addr_config = adev->gfx.config.gb_addr_config;\n\tconfig->tile_config_ptr = adev->gfx.config.tile_mode_array;\n\tconfig->num_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.tile_mode_array);\n\tconfig->macro_tile_config_ptr =\n\t\t\tadev->gfx.config.macrotile_mode_array;\n\tconfig->num_macro_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.macrotile_mode_array);\n\n\t/* Those values are not set from GFX9 onwards */\n\tconfig->num_banks = adev->gfx.config.num_banks;\n\tconfig->num_ranks = adev->gfx.config.num_ranks;\n\n\treturn 0;\n}\n\nbool amdgpu_amdkfd_bo_mapped_to_dev(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->is_mapped && entry->adev == adev)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_kfd_mem_limits(struct seq_file *m, void *data)\n{\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tseq_printf(m, \"System mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.system_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_system_mem_limit >> 20));\n\tseq_printf(m, \"TTM mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.ttm_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_ttm_mem_limit >> 20));\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\treturn 0;\n}\n\n#endif\n", "source_code_path": "amdgpu_amdkfd_gpuvm.c", "line_number": 2464, "generated_diff": "--- a/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n+++ b/data/raw-data/code/amdgpu_amdkfd_gpuvm.c\n@@ -2461,7 +2461,9 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf_fd(struct amdgpu_device *adev, int fd,\n        if (ret)\n                goto err_put_obj;\n\n-       (*mem)->gem_handle = handle;\n+    if (*mem) {\n+           (*mem)->gem_handle = handle;\n+    }\n\n        return 0;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_amdkfd_gpuvm.c\nLine: 2464\n\t\t\t\tmmap_offset);\n\tif (ret)\n\t\tgoto err_put_obj;\n\n\t(*mem)->gem_handle = handle;\n\n\treturn 0;\n\nerr_put_obj:\n\tdrm_gem_object_put(obj);\n====================================\nRead the following bug report.\n============ Bug Report ============\nFile: amdgpu_amdkfd_gpuvm.c\n\nLine: 2469\n\nFunction: amdgpu_amdkfd_gpuvm_import_dmabuf_fd\n\nDescription: The function amdgpu_amdkfd_gpuvm_import_dmabuf_fd is at risk of accessing freed memory. The '*mem' pointer may be released in the import_obj_create function, leaving it pointing to invalid memory. To prevent potential crashes or data corruption, a null check should be added to ensure '*mem' is valid before accessing its contents.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-bug.txt", "bug_report_text": "Issue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nA memory resource allocated using kvmalloc_array() may not be freed if an error occurs during execution.\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(). However, if an error occurs after this allocation but before the function successfully completes, the allocated memory may not be freed, resulting in a resource leak.\n\n", "diff_path": "amdgpu_cs-diff.txt", "diff_text": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 188, "generated_diff": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 188\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nA memory resource allocated using kvmalloc_array() may not be freed if an error occurs during execution.\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(). However, if an error occurs after this allocation but before the function successfully completes, the allocated memory may not be freed, resulting in a resource leak.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-bug.txt", "bug_report_text": "Issue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(), but fails to ensure its release in the event of an error. If an exception occurs after the allocation but before the function completes, the allocated memory may remain unfreed, leading to a resource leak.\n\n", "diff_path": "amdgpu_cs-diff.txt", "diff_text": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 188, "generated_diff": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 188\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(), but fails to ensure its release in the event of an error. If an exception occurs after the allocation but before the function completes, the allocated memory may remain unfreed, leading to a resource leak.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-bug.txt", "bug_report_text": "Issue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nA resource leak vulnerability exists in the function amdgpu_cs_pass1, which allocates memory for chunk_array using kvmalloc_array() but neglects to free it in the event of an error. This can result in memory being retained unnecessarily, leading to resource waste and potential system instability.\n\n", "diff_path": "amdgpu_cs-diff.txt", "diff_text": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 188, "generated_diff": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 188\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nA resource leak vulnerability exists in the function amdgpu_cs_pass1, which allocates memory for chunk_array using kvmalloc_array() but neglects to free it in the event of an error. This can result in memory being retained unnecessarily, leading to resource waste and potential system instability.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-bug.txt", "bug_report_text": "Issue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(), but fails to properly handle the case where an error occurs after allocation but before the function completes. As a result, the allocated memory may not be released, leading to a memory leak and potential system resource exhaustion.\n\n", "diff_path": "amdgpu_cs-diff.txt", "diff_text": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 188, "generated_diff": "diff --git a/data/raw-data/code/amdgpu_cs.c b/data/raw-data/code/amdgpu_cs.c\nindex af43fd5..6dfdff5 100644\n--- a/data/raw-data/code/amdgpu_cs.c\n+++ b/data/raw-data/code/amdgpu_cs.c\n@@ -215,6 +215,7 @@ static int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n                if (copy_from_user(&user_chunk, chunk_ptr,\n                                       sizeof(struct drm_amdgpu_cs_chunk))) {\n                        ret = -EFAULT;\n+                       i--;\n                        goto free_partial_kdata;\n                }\n                p->chunks[i].chunk_id = user_chunk.chunk_id;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 188\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Resource Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_pass1\nLine: 188\n\nDescription:\nThe function amdgpu_cs_pass1 allocates memory for chunk_array using kvmalloc_array(), but fails to properly handle the case where an error occurs after allocation but before the function completes. As a result, the allocated memory may not be released, leading to a memory leak and potential system resource exhaustion.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-2-bug.txt", "bug_report_text": "Issue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nThe function 'amdgpu_cs_parser_init' dereferences 'cs' without first checking if it is NULL. This can lead to a null pointer dereference if 'cs' is NULL when the function is called.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n", "diff_path": "amdgpu_cs-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 51, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 51\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nThe function 'amdgpu_cs_parser_init' dereferences 'cs' without first checking if it is NULL. This can lead to a null pointer dereference if 'cs' is NULL when the function is called.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-2-bug.txt", "bug_report_text": "Issue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nThe amdgpu_cs_parser_init function does not validate the 'cs' pointer before accessing its members. This can result in a null pointer dereference if 'cs' is null when the function is called, leading to a potential crash or unexpected behavior.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n", "diff_path": "amdgpu_cs-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 51, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 51\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nThe amdgpu_cs_parser_init function does not validate the 'cs' pointer before accessing its members. This can result in a null pointer dereference if 'cs' is null when the function is called, leading to a potential crash or unexpected behavior.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-2-bug.txt", "bug_report_text": "Issue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nA null pointer dereference vulnerability exists in the amdgpu_cs_parser_init function, which fails to check if the 'cs' pointer is valid before accessing its members. This can lead to a crash or unexpected behavior if 'cs' is null when the function is called.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n", "diff_path": "amdgpu_cs-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 51, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 51\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nA null pointer dereference vulnerability exists in the amdgpu_cs_parser_init function, which fails to check if the 'cs' pointer is valid before accessing its members. This can lead to a crash or unexpected behavior if 'cs' is null when the function is called.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_cs-2-bug.txt", "bug_report_text": "Issue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nThe amdgpu_cs_parser_init function is susceptible to a null pointer dereference if the 'cs' pointer is null when the function is called. This is because the function does not perform a null check before accessing the 'cs' pointer's members.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n", "diff_path": "amdgpu_cs-2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n", "code": "/*\n * Copyright 2008 Jerome Glisse.\n * All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors:\n *    Jerome Glisse <glisse@freedesktop.org>\n */\n\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/sync_file.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_syncobj.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu_cs.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_ras.h\"\n\nstatic int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n\t\t\t\t struct amdgpu_device *adev,\n\t\t\t\t struct drm_file *filp,\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n\n\tp->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);\n\tif (!p->ctx)\n\t\treturn -EINVAL;\n\n\tif (atomic_read(&p->ctx->guilty)) {\n\t\tamdgpu_ctx_put(p->ctx);\n\t\treturn -ECANCELED;\n\t}\n\n\tamdgpu_sync_create(&p->sync);\n\tdrm_exec_init(&p->exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\treturn 0;\n}\n\nstatic int amdgpu_cs_job_idx(struct amdgpu_cs_parser *p,\n\t\t\t     struct drm_amdgpu_cs_chunk_ib *chunk_ib)\n{\n\tstruct drm_sched_entity *entity;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_get_entity(p->ctx, chunk_ib->ip_type,\n\t\t\t\t  chunk_ib->ip_instance,\n\t\t\t\t  chunk_ib->ring, &entity);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Abort if there is no run queue associated with this entity.\n\t * Possibly because of disabled HW IP.\n\t */\n\tif (entity->rq == NULL)\n\t\treturn -EINVAL;\n\n\t/* Check if we can add this IB to some existing job */\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tif (p->entities[i] == entity)\n\t\t\treturn i;\n\n\t/* If not increase the gang size if possible */\n\tif (i == AMDGPU_CS_GANG_SIZE)\n\t\treturn -EINVAL;\n\n\tp->entities[i] = entity;\n\tp->gang_size = i + 1;\n\treturn i;\n}\n\nstatic int amdgpu_cs_p1_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct drm_amdgpu_cs_chunk_ib *chunk_ib,\n\t\t\t   unsigned int *num_ibs)\n{\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (num_ibs[r] >= amdgpu_ring_max_ibs(chunk_ib->ip_type))\n\t\treturn -EINVAL;\n\n\t++(num_ibs[r]);\n\tp->gang_leader_idx = r;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_user_fence(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_cs_chunk_fence *data,\n\t\t\t\t   uint32_t *offset)\n{\n\tstruct drm_gem_object *gobj;\n\tunsigned long size;\n\n\tgobj = drm_gem_object_lookup(p->filp, data->handle);\n\tif (gobj == NULL)\n\t\treturn -EINVAL;\n\n\tp->uf_bo = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\n\tdrm_gem_object_put(gobj);\n\n\tsize = amdgpu_bo_size(p->uf_bo);\n\tif (size != PAGE_SIZE || data->offset > (size - 8))\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ttm_tt_get_usermm(p->uf_bo->tbo.ttm))\n\t\treturn -EINVAL;\n\n\t*offset = data->offset;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p1_bo_handles(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct drm_amdgpu_bo_list_in *data)\n{\n\tstruct drm_amdgpu_bo_list_entry *info;\n\tint r;\n\n\tr = amdgpu_bo_create_list_entry_array(data, &info);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,\n\t\t\t\t  &p->bo_list);\n\tif (r)\n\t\tgoto error_free;\n\n\tkvfree(info);\n\treturn 0;\n\nerror_free:\n\tkvfree(info);\n\n\treturn r;\n}\n\n/* Copy the data from userspace and go over it the first time */\nstatic int amdgpu_cs_pass1(struct amdgpu_cs_parser *p,\n\t\t\t   union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_ibs[AMDGPU_CS_GANG_SIZE] = { };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tuint64_t *chunk_array_user;\n\tuint64_t *chunk_array;\n\tuint32_t uf_offset = 0;\n\tsize_t size;\n\tint ret;\n\tint i;\n\n\tchunk_array = kvmalloc_array(cs->in.num_chunks, sizeof(uint64_t),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_array)\n\t\treturn -ENOMEM;\n\n\t/* get chunks */\n\tchunk_array_user = u64_to_user_ptr(cs->in.chunks);\n\tif (copy_from_user(chunk_array, chunk_array_user,\n\t\t\t   sizeof(uint64_t)*cs->in.num_chunks)) {\n\t\tret = -EFAULT;\n\t\tgoto free_chunk;\n\t}\n\n\tp->nchunks = cs->in.num_chunks;\n\tp->chunks = kvmalloc_array(p->nchunks, sizeof(struct amdgpu_cs_chunk),\n\t\t\t    GFP_KERNEL);\n\tif (!p->chunks) {\n\t\tret = -ENOMEM;\n\t\tgoto free_chunk;\n\t}\n\n\tfor (i = 0; i < p->nchunks; i++) {\n\t\tstruct drm_amdgpu_cs_chunk __user *chunk_ptr = NULL;\n\t\tstruct drm_amdgpu_cs_chunk user_chunk;\n\t\tuint32_t __user *cdata;\n\n\t\tchunk_ptr = u64_to_user_ptr(chunk_array[i]);\n\t\tif (copy_from_user(&user_chunk, chunk_ptr,\n\t\t\t\t       sizeof(struct drm_amdgpu_cs_chunk))) {\n\t\t\tret = -EFAULT;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tp->chunks[i].chunk_id = user_chunk.chunk_id;\n\t\tp->chunks[i].length_dw = user_chunk.length_dw;\n\n\t\tsize = p->chunks[i].length_dw;\n\t\tcdata = u64_to_user_ptr(user_chunk.chunk_data);\n\n\t\tp->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (p->chunks[i].kdata == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\ti--;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t\tsize *= sizeof(uint32_t);\n\t\tif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\n\t\t/* Assume the worst on the following checks */\n\t\tret = -EINVAL;\n\t\tswitch (p->chunks[i].chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_ib))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_ib(p, p->chunks[i].kdata, num_ibs);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_FENCE:\n\t\t\tif (size < sizeof(struct drm_amdgpu_cs_chunk_fence))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_user_fence(p, p->chunks[i].kdata,\n\t\t\t\t\t\t      &uf_offset);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_BO_HANDLES:\n\t\t\tif (size < sizeof(struct drm_amdgpu_bo_list_in))\n\t\t\t\tgoto free_partial_kdata;\n\n\t\t\tret = amdgpu_cs_p1_bo_handles(p, p->chunks[i].kdata);\n\t\t\tif (ret)\n\t\t\t\tgoto free_partial_kdata;\n\t\t\tbreak;\n\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto free_partial_kdata;\n\t\t}\n\t}\n\n\tif (!p->gang_size) {\n\t\tret = -EINVAL;\n\t\tgoto free_all_kdata;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tret = amdgpu_job_alloc(p->adev, vm, p->entities[i], vm,\n\t\t\t\t       num_ibs[i], &p->jobs[i]);\n\t\tif (ret)\n\t\t\tgoto free_all_kdata;\n\t}\n\tp->gang_leader = p->jobs[p->gang_leader_idx];\n\n\tif (p->ctx->generation != p->gang_leader->generation) {\n\t\tret = -ECANCELED;\n\t\tgoto free_all_kdata;\n\t}\n\n\tif (p->uf_bo)\n\t\tp->gang_leader->uf_addr = uf_offset;\n\tkvfree(chunk_array);\n\n\t/* Use this opportunity to fill in task info for the vm */\n\tamdgpu_vm_set_task_info(vm);\n\n\treturn 0;\n\nfree_all_kdata:\n\ti = p->nchunks - 1;\nfree_partial_kdata:\n\tfor (; i >= 0; i--)\n\t\tkvfree(p->chunks[i].kdata);\n\tkvfree(p->chunks);\n\tp->chunks = NULL;\n\tp->nchunks = 0;\nfree_chunk:\n\tkvfree(chunk_array);\n\n\treturn ret;\n}\n\nstatic int amdgpu_cs_p2_ib(struct amdgpu_cs_parser *p,\n\t\t\t   struct amdgpu_cs_chunk *chunk,\n\t\t\t   unsigned int *ce_preempt,\n\t\t\t   unsigned int *de_preempt)\n{\n\tstruct drm_amdgpu_cs_chunk_ib *chunk_ib = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint r;\n\n\tr = amdgpu_cs_job_idx(p, chunk_ib);\n\tif (r < 0)\n\t\treturn r;\n\n\tjob = p->jobs[r];\n\tring = amdgpu_job_ring(job);\n\tib = &job->ibs[job->num_ibs++];\n\n\t/* MM engine doesn't support user fences */\n\tif (p->uf_bo && ring->funcs->no_user_fence)\n\t\treturn -EINVAL;\n\n\tif (chunk_ib->ip_type == AMDGPU_HW_IP_GFX &&\n\t    chunk_ib->flags & AMDGPU_IB_FLAG_PREEMPT) {\n\t\tif (chunk_ib->flags & AMDGPU_IB_FLAG_CE)\n\t\t\t(*ce_preempt)++;\n\t\telse\n\t\t\t(*de_preempt)++;\n\n\t\t/* Each GFX command submit allows only 1 IB max\n\t\t * preemptible for CE & DE */\n\t\tif (*ce_preempt > 1 || *de_preempt > 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (chunk_ib->flags & AMDGPU_IB_FLAG_PREAMBLE)\n\t\tjob->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT;\n\n\tr =  amdgpu_ib_get(p->adev, vm, ring->funcs->parse_cs ?\n\t\t\t   chunk_ib->ib_bytes : 0,\n\t\t\t   AMDGPU_IB_POOL_DELAYED, ib);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to get ib !\\n\");\n\t\treturn r;\n\t}\n\n\tib->gpu_addr = chunk_ib->va_start;\n\tib->length_dw = chunk_ib->ib_bytes / 4;\n\tib->flags = chunk_ib->flags;\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_dependencies(struct amdgpu_cs_parser *p,\n\t\t\t\t     struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_dep *deps = chunk->kdata;\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_dep);\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_ctx *ctx;\n\t\tstruct drm_sched_entity *entity;\n\t\tstruct dma_fence *fence;\n\n\t\tctx = amdgpu_ctx_get(fpriv, deps[i].ctx_id);\n\t\tif (ctx == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_ctx_get_entity(ctx, deps[i].ip_type,\n\t\t\t\t\t  deps[i].ip_instance,\n\t\t\t\t\t  deps[i].ring, &entity);\n\t\tif (r) {\n\t\t\tamdgpu_ctx_put(ctx);\n\t\t\treturn r;\n\t\t}\n\n\t\tfence = amdgpu_ctx_get_fence(ctx, entity, deps[i].handle);\n\t\tamdgpu_ctx_put(ctx);\n\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tif (chunk->chunk_id == AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES) {\n\t\t\tstruct drm_sched_fence *s_fence;\n\t\t\tstruct dma_fence *old = fence;\n\n\t\t\ts_fence = to_drm_sched_fence(fence);\n\t\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\t\tdma_fence_put(old);\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_syncobj_lookup_and_add(struct amdgpu_cs_parser *p,\n\t\t\t\t\t uint32_t handle, u64 point,\n\t\t\t\t\t u64 flags)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = drm_syncobj_find_fence(p->filp, handle, point, flags, &fence);\n\tif (r) {\n\t\tDRM_ERROR(\"syncobj %u failed to find fence @ %llu (%d)!\\n\",\n\t\t\t  handle, point, r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_sync_fence(&p->sync, fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nstatic int amdgpu_cs_p2_syncobj_in(struct amdgpu_cs_parser *p,\n\t\t\t\t   struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, deps[i].handle, 0, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_wait(struct amdgpu_cs_parser *p,\n\t\t\t\t\t      struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i, r;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tr = amdgpu_syncobj_lookup_and_add(p, syncobj_deps[i].handle,\n\t\t\t\t\t\t  syncobj_deps[i].point,\n\t\t\t\t\t\t  syncobj_deps[i].flags);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_out(struct amdgpu_cs_parser *p,\n\t\t\t\t    struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_sem *deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_sem);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tp->post_deps[i].syncobj =\n\t\t\tdrm_syncobj_find(p->filp, deps[i].handle);\n\t\tif (!p->post_deps[i].syncobj)\n\t\t\treturn -EINVAL;\n\t\tp->post_deps[i].chain = NULL;\n\t\tp->post_deps[i].point = 0;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_syncobj_timeline_signal(struct amdgpu_cs_parser *p,\n\t\t\t\t\t\tstruct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_syncobj *syncobj_deps = chunk->kdata;\n\tunsigned int num_deps;\n\tint i;\n\n\tnum_deps = chunk->length_dw * 4 /\n\t\tsizeof(struct drm_amdgpu_cs_chunk_syncobj);\n\n\tif (p->post_deps)\n\t\treturn -EINVAL;\n\n\tp->post_deps = kmalloc_array(num_deps, sizeof(*p->post_deps),\n\t\t\t\t     GFP_KERNEL);\n\tp->num_post_deps = 0;\n\n\tif (!p->post_deps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_deps; ++i) {\n\t\tstruct amdgpu_cs_post_dep *dep = &p->post_deps[i];\n\n\t\tdep->chain = NULL;\n\t\tif (syncobj_deps[i].point) {\n\t\t\tdep->chain = dma_fence_chain_alloc();\n\t\t\tif (!dep->chain)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdep->syncobj = drm_syncobj_find(p->filp,\n\t\t\t\t\t\tsyncobj_deps[i].handle);\n\t\tif (!dep->syncobj) {\n\t\t\tdma_fence_chain_free(dep->chain);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdep->point = syncobj_deps[i].point;\n\t\tp->num_post_deps++;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_p2_shadow(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_cs_chunk *chunk)\n{\n\tstruct drm_amdgpu_cs_chunk_cp_gfx_shadow *shadow = chunk->kdata;\n\tint i;\n\n\tif (shadow->flags & ~AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tp->jobs[i]->shadow_va = shadow->shadow_va;\n\t\tp->jobs[i]->csa_va = shadow->csa_va;\n\t\tp->jobs[i]->gds_va = shadow->gds_va;\n\t\tp->jobs[i]->init_shadow =\n\t\t\tshadow->flags & AMDGPU_CS_CHUNK_CP_GFX_SHADOW_FLAGS_INIT_SHADOW;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_pass2(struct amdgpu_cs_parser *p)\n{\n\tunsigned int ce_preempt = 0, de_preempt = 0;\n\tint i, r;\n\n\tfor (i = 0; i < p->nchunks; ++i) {\n\t\tstruct amdgpu_cs_chunk *chunk;\n\n\t\tchunk = &p->chunks[i];\n\n\t\tswitch (chunk->chunk_id) {\n\t\tcase AMDGPU_CHUNK_ID_IB:\n\t\t\tr = amdgpu_cs_p2_ib(p, chunk, &ce_preempt, &de_preempt);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_DEPENDENCIES:\n\t\tcase AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES:\n\t\t\tr = amdgpu_cs_p2_dependencies(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_IN:\n\t\t\tr = amdgpu_cs_p2_syncobj_in(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_OUT:\n\t\t\tr = amdgpu_cs_p2_syncobj_out(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_wait(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL:\n\t\t\tr = amdgpu_cs_p2_syncobj_timeline_signal(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase AMDGPU_CHUNK_ID_CP_GFX_SHADOW:\n\t\t\tr = amdgpu_cs_p2_shadow(p, chunk);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* Convert microseconds to bytes. */\nstatic u64 us_to_bytes(struct amdgpu_device *adev, s64 us)\n{\n\tif (us <= 0 || !adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\t/* Since accum_us is incremented by a million per second, just\n\t * multiply it by the number of MB/s to get the number of bytes.\n\t */\n\treturn us << adev->mm_stats.log2_max_MBps;\n}\n\nstatic s64 bytes_to_us(struct amdgpu_device *adev, u64 bytes)\n{\n\tif (!adev->mm_stats.log2_max_MBps)\n\t\treturn 0;\n\n\treturn bytes >> adev->mm_stats.log2_max_MBps;\n}\n\n/* Returns how many bytes TTM can move right now. If no bytes can be moved,\n * it returns 0. If it returns non-zero, it's OK to move at least one buffer,\n * which means it can go over the threshold once. If that happens, the driver\n * will be in debt and no other buffer migrations can be done until that debt\n * is repaid.\n *\n * This approach allows moving a buffer of any size (it's important to allow\n * that).\n *\n * The currency is simply time in microseconds and it increases as the clock\n * ticks. The accumulated microseconds (us) are converted to bytes and\n * returned.\n */\nstatic void amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev,\n\t\t\t\t\t      u64 *max_bytes,\n\t\t\t\t\t      u64 *max_vis_bytes)\n{\n\ts64 time_us, increment_us;\n\tu64 free_vram, total_vram, used_vram;\n\t/* Allow a maximum of 200 accumulated ms. This is basically per-IB\n\t * throttling.\n\t *\n\t * It means that in order to get full max MBps, at least 5 IBs per\n\t * second must be submitted and not more than 200ms apart from each\n\t * other.\n\t */\n\tconst s64 us_upper_bound = 200000;\n\n\tif (!adev->mm_stats.log2_max_MBps) {\n\t\t*max_bytes = 0;\n\t\t*max_vis_bytes = 0;\n\t\treturn;\n\t}\n\n\ttotal_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);\n\tused_vram = ttm_resource_manager_usage(&adev->mman.vram_mgr.manager);\n\tfree_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;\n\n\tspin_lock(&adev->mm_stats.lock);\n\n\t/* Increase the amount of accumulated us. */\n\ttime_us = ktime_to_us(ktime_get());\n\tincrement_us = time_us - adev->mm_stats.last_update_us;\n\tadev->mm_stats.last_update_us = time_us;\n\tadev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,\n\t\t\t\t      us_upper_bound);\n\n\t/* This prevents the short period of low performance when the VRAM\n\t * usage is low and the driver is in debt or doesn't have enough\n\t * accumulated us to fill VRAM quickly.\n\t *\n\t * The situation can occur in these cases:\n\t * - a lot of VRAM is freed by userspace\n\t * - the presence of a big buffer causes a lot of evictions\n\t *   (solution: split buffers into smaller ones)\n\t *\n\t * If 128 MB or 1/8th of VRAM is free, start filling it now by setting\n\t * accum_us to a positive number.\n\t */\n\tif (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {\n\t\ts64 min_us;\n\n\t\t/* Be more aggressive on dGPUs. Try to fill a portion of free\n\t\t * VRAM now.\n\t\t */\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tmin_us = bytes_to_us(adev, free_vram / 4);\n\t\telse\n\t\t\tmin_us = 0; /* Reset accum_us on APUs. */\n\n\t\tadev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);\n\t}\n\n\t/* This is set to 0 if the driver is in debt to disallow (optional)\n\t * buffer moves.\n\t */\n\t*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);\n\n\t/* Do the same for visible VRAM if half of it is free */\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc)) {\n\t\tu64 total_vis_vram = adev->gmc.visible_vram_size;\n\t\tu64 used_vis_vram =\n\t\t  amdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr);\n\n\t\tif (used_vis_vram < total_vis_vram) {\n\t\t\tu64 free_vis_vram = total_vis_vram - used_vis_vram;\n\n\t\t\tadev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +\n\t\t\t\t\t\t\t  increment_us, us_upper_bound);\n\n\t\t\tif (free_vis_vram >= total_vis_vram / 2)\n\t\t\t\tadev->mm_stats.accum_us_vis =\n\t\t\t\t\tmax(bytes_to_us(adev, free_vis_vram / 2),\n\t\t\t\t\t    adev->mm_stats.accum_us_vis);\n\t\t}\n\n\t\t*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);\n\t} else {\n\t\t*max_vis_bytes = 0;\n\t}\n\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\n/* Report how many bytes have really been moved for the last command\n * submission. This can result in a debt that can stop buffer migrations\n * temporarily.\n */\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes)\n{\n\tspin_lock(&adev->mm_stats.lock);\n\tadev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);\n\tadev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);\n\tspin_unlock(&adev->mm_stats.lock);\n}\n\nstatic int amdgpu_cs_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_cs_parser *p = param;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t\t.resv = bo->tbo.base.resv\n\t};\n\tuint32_t domain;\n\tint r;\n\n\tif (bo->tbo.pin_count)\n\t\treturn 0;\n\n\t/* Don't move this buffer if we have depleted our allowance\n\t * to move it. Don't move anything if the threshold is zero.\n\t */\n\tif (p->bytes_moved < p->bytes_moved_threshold &&\n\t    (!bo->tbo.base.dma_buf ||\n\t    list_empty(&bo->tbo.base.dma_buf->attachments))) {\n\t\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {\n\t\t\t/* And don't move a CPU_ACCESS_REQUIRED BO to limited\n\t\t\t * visible VRAM if we've depleted our allowance to do\n\t\t\t * that.\n\t\t\t */\n\t\t\tif (p->bytes_moved_vis < p->bytes_moved_vis_threshold)\n\t\t\t\tdomain = bo->preferred_domains;\n\t\t\telse\n\t\t\t\tdomain = bo->allowed_domains;\n\t\t} else {\n\t\t\tdomain = bo->preferred_domains;\n\t\t}\n\t} else {\n\t\tdomain = bo->allowed_domains;\n\t}\n\nretry:\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tp->bytes_moved += ctx.bytes_moved;\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    amdgpu_res_cpu_visible(adev, bo->tbo.resource))\n\t\tp->bytes_moved_vis += ctx.bytes_moved;\n\n\tif (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {\n\t\tdomain = bo->allowed_domains;\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,\n\t\t\t\tunion drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\t/* p->bo_list could already be assigned if AMDGPU_CHUNK_ID_BO_HANDLES is present */\n\tif (cs->in.bo_list_handle) {\n\t\tif (p->bo_list)\n\t\t\treturn -EINVAL;\n\n\t\tr = amdgpu_bo_list_get(fpriv, cs->in.bo_list_handle,\n\t\t\t\t       &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!p->bo_list) {\n\t\t/* Create a empty bo_list when no handle is provided */\n\t\tr = amdgpu_bo_list_create(p->adev, p->filp, NULL, 0,\n\t\t\t\t\t  &p->bo_list);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmutex_lock(&p->bo_list->bo_list_mutex);\n\n\t/* Get userptr backing pages. If pages are updated after registered\n\t * in amdgpu_gem_userptr_ioctl(), amdgpu_cs_list_validate() will do\n\t * amdgpu_ttm_backend_bind() to flush and invalidate new pages\n\t */\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tbool userpage_invalidated = false;\n\t\tstruct amdgpu_bo *bo = e->bo;\n\t\tint i;\n\n\t\te->user_pages = kvcalloc(bo->tbo.ttm->num_pages,\n\t\t\t\t\t sizeof(struct page *),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!e->user_pages) {\n\t\t\tDRM_ERROR(\"kvmalloc_array failure\\n\");\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);\n\t\tif (r) {\n\t\t\tkvfree(e->user_pages);\n\t\t\te->user_pages = NULL;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tfor (i = 0; i < bo->tbo.ttm->num_pages; i++) {\n\t\t\tif (bo->tbo.ttm->pages[i] != e->user_pages[i]) {\n\t\t\t\tuserpage_invalidated = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\te->user_invalidated = userpage_invalidated;\n\t}\n\n\tdrm_exec_until_all_locked(&p->exec) {\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &p->exec, 1 + p->gang_size);\n\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\t/* One fence for TTM and one for each CS job */\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &e->bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\te->bo_va = amdgpu_vm_bo_find(vm, e->bo);\n\t\t}\n\n\t\tif (p->uf_bo) {\n\t\t\tr = drm_exec_prepare_obj(&p->exec, &p->uf_bo->tbo.base,\n\t\t\t\t\t\t 1 + p->gang_size);\n\t\t\tdrm_exec_retry_on_contention(&p->exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out_free_user_pages;\n\t\t}\n\t}\n\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct mm_struct *usermm;\n\n\t\tusermm = amdgpu_ttm_tt_get_usermm(e->bo->tbo.ttm);\n\t\tif (usermm && usermm != current->mm) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_free_user_pages;\n\t\t}\n\n\t\tif (amdgpu_ttm_tt_is_userptr(e->bo->tbo.ttm) &&\n\t\t    e->user_invalidated && e->user_pages) {\n\t\t\tamdgpu_bo_placement_from_domain(e->bo,\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tr = ttm_bo_validate(&e->bo->tbo, &e->bo->placement,\n\t\t\t\t\t    &ctx);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_user_pages;\n\n\t\t\tamdgpu_ttm_tt_set_user_pages(e->bo->tbo.ttm,\n\t\t\t\t\t\t     e->user_pages);\n\t\t}\n\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t}\n\n\tamdgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,\n\t\t\t\t\t  &p->bytes_moved_vis_threshold);\n\tp->bytes_moved = 0;\n\tp->bytes_moved_vis = 0;\n\n\tr = amdgpu_vm_validate(p->adev, &fpriv->vm, NULL,\n\t\t\t       amdgpu_cs_bo_validate, p);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_vm_validate() failed.\\n\");\n\t\tgoto out_free_user_pages;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tr = amdgpu_cs_bo_validate(p, gem_to_amdgpu_bo(obj));\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\t}\n\n\tif (p->uf_bo) {\n\t\tr = amdgpu_ttm_alloc_gart(&p->uf_bo->tbo);\n\t\tif (unlikely(r))\n\t\t\tgoto out_free_user_pages;\n\n\t\tp->gang_leader->uf_addr += amdgpu_bo_gpu_offset(p->uf_bo);\n\t}\n\n\tamdgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,\n\t\t\t\t     p->bytes_moved_vis);\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tamdgpu_job_set_resources(p->jobs[i], p->bo_list->gds_obj,\n\t\t\t\t\t p->bo_list->gws_obj,\n\t\t\t\t\t p->bo_list->oa_obj);\n\treturn 0;\n\nout_free_user_pages:\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\tif (!e->user_pages)\n\t\t\tcontinue;\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, e->range);\n\t\tkvfree(e->user_pages);\n\t\te->user_pages = NULL;\n\t\te->range = NULL;\n\t}\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn r;\n}\n\nstatic void trace_amdgpu_cs_ibs(struct amdgpu_cs_parser *p)\n{\n\tint i, j;\n\n\tif (!trace_amdgpu_cs_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct amdgpu_job *job = p->jobs[i];\n\n\t\tfor (j = 0; j < job->num_ibs; ++j)\n\t\t\ttrace_amdgpu_cs(p, job, &job->ibs[j]);\n\t}\n}\n\nstatic int amdgpu_cs_patch_ibs(struct amdgpu_cs_parser *p,\n\t\t\t       struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = amdgpu_job_ring(job);\n\tunsigned int i;\n\tint r;\n\n\t/* Only for UVD/VCE VM emulation */\n\tif (!ring->funcs->parse_cs && !ring->funcs->patch_cs_in_place)\n\t\treturn 0;\n\n\tfor (i = 0; i < job->num_ibs; ++i) {\n\t\tstruct amdgpu_ib *ib = &job->ibs[i];\n\t\tstruct amdgpu_bo_va_mapping *m;\n\t\tstruct amdgpu_bo *aobj;\n\t\tuint64_t va_start;\n\t\tuint8_t *kptr;\n\n\t\tva_start = ib->gpu_addr & AMDGPU_GMC_HOLE_MASK;\n\t\tr = amdgpu_cs_find_mapping(p, va_start, &aobj, &m);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"IB va_start is invalid\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tif ((va_start + ib->length_dw * 4) >\n\t\t    (m->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\t\tDRM_ERROR(\"IB va_start+ib_bytes is invalid\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* the IB should be reserved at this point */\n\t\tr = amdgpu_bo_kmap(aobj, (void **)&kptr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkptr += va_start - (m->start * AMDGPU_GPU_PAGE_SIZE);\n\n\t\tif (ring->funcs->parse_cs) {\n\t\t\tmemcpy(ib->ptr, kptr, ib->length_dw * 4);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\n\t\t\tr = amdgpu_ring_parse_cs(ring, p, job, ib);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (ib->sa_bo)\n\t\t\t\tib->gpu_addr =  amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t\t} else {\n\t\t\tib->ptr = (uint32_t *)kptr;\n\t\t\tr = amdgpu_ring_patch_cs_in_place(ring, p, job, ib);\n\t\t\tamdgpu_bo_kunmap(aobj);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_patch_jobs(struct amdgpu_cs_parser *p)\n{\n\tunsigned int i;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_cs_patch_ibs(p, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_cs_vm_handling(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *job = p->gang_leader;\n\tstruct amdgpu_device *adev = p->adev;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct amdgpu_bo_va *bo_va;\n\tunsigned int i;\n\tint r;\n\n\t/*\n\t * We can't use gang submit on with reserved VMIDs when the VM changes\n\t * can't be invalidated by more than one engine at the same time.\n\t */\n\tif (p->gang_size > 1 && !p->adev->vm_manager.concurrent_flush) {\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tstruct drm_sched_entity *entity = p->entities[i];\n\t\t\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\t\t\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched);\n\n\t\t\tif (amdgpu_vmid_uses_reserved(vm, ring->vm_hub))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_bo_update(adev, fpriv->prt_va, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, fpriv->prt_va->last_pt_update);\n\tif (r)\n\t\treturn r;\n\n\tif (fpriv->csa_va) {\n\t\tbo_va = fpriv->csa_va;\n\t\tBUG_ON(!bo_va);\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t/* FIXME: In theory this loop shouldn't be needed any more when\n\t * amdgpu_vm_handle_moved handles all moved BOs that are reserved\n\t * with p->ticket. But removing it caused test regressions, so I'm\n\t * leaving it here for now.\n\t */\n\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\tbo_va = e->bo_va;\n\t\tif (bo_va == NULL)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_sync_fence(&p->sync, bo_va->last_pt_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vm_handle_moved(adev, vm, &p->exec.ticket);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sync_fence(&p->sync, vm->last_update);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tjob = p->jobs[i];\n\n\t\tif (!job->vm)\n\t\t\tcontinue;\n\n\t\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\t}\n\n\tif (adev->debug_vm) {\n\t\t/* Invalidate all BOs to test for userspace bugs */\n\t\tamdgpu_bo_list_for_each_entry(e, p->bo_list) {\n\t\t\tstruct amdgpu_bo *bo = e->bo;\n\n\t\t\t/* ignore duplicates */\n\t\t\tif (!bo)\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_vm_bo_invalidate(adev, bo, false);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_gem_object *obj;\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\tunsigned int i;\n\tint r;\n\n\tr = amdgpu_ctx_wait_prev_fence(p->ctx, p->entities[p->gang_leader_idx]);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"amdgpu_ctx_wait_prev_fence failed.\\n\");\n\t\treturn r;\n\t}\n\n\tdrm_exec_for_each_locked_object(&p->exec, index, obj) {\n\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\t\tstruct dma_resv *resv = bo->tbo.base.resv;\n\t\tenum amdgpu_sync_mode sync_mode;\n\n\t\tsync_mode = amdgpu_bo_explicit_sync(bo) ?\n\t\t\tAMDGPU_SYNC_EXPLICIT : AMDGPU_SYNC_NE_OWNER;\n\t\tr = amdgpu_sync_resv(p->adev, &p->sync, resv, sync_mode,\n\t\t\t\t     &fpriv->vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tr = amdgpu_sync_push_to_job(&p->sync, p->jobs[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tsched = p->gang_leader->base.entity->rq->sched;\n\twhile ((fence = amdgpu_sync_get_fence(&p->sync))) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(fence);\n\n\t\t/*\n\t\t * When we have an dependency it might be necessary to insert a\n\t\t * pipeline sync to make sure that all caches etc are flushed and the\n\t\t * next job actually sees the results from the previous one\n\t\t * before we start executing on the same scheduler ring.\n\t\t */\n\t\tif (!s_fence || s_fence->sched != sched) {\n\t\t\tdma_fence_put(fence);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = amdgpu_sync_fence(&p->gang_leader->explicit_sync, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_cs_post_dependencies(struct amdgpu_cs_parser *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->num_post_deps; ++i) {\n\t\tif (p->post_deps[i].chain && p->post_deps[i].point) {\n\t\t\tdrm_syncobj_add_point(p->post_deps[i].syncobj,\n\t\t\t\t\t      p->post_deps[i].chain,\n\t\t\t\t\t      p->fence, p->post_deps[i].point);\n\t\t\tp->post_deps[i].chain = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(p->post_deps[i].syncobj,\n\t\t\t\t\t\t  p->fence);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_cs_submit(struct amdgpu_cs_parser *p,\n\t\t\t    union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = p->filp->driver_priv;\n\tstruct amdgpu_job *leader = p->gang_leader;\n\tstruct amdgpu_bo_list_entry *e;\n\tstruct drm_gem_object *gobj;\n\tunsigned long index;\n\tunsigned int i;\n\tuint64_t seq;\n\tint r;\n\n\tfor (i = 0; i < p->gang_size; ++i)\n\t\tdrm_sched_job_arm(&p->jobs[i]->base);\n\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tif (p->jobs[i] == leader)\n\t\t\tcontinue;\n\n\t\tfence = &p->jobs[i]->base.s_fence->scheduled;\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&leader->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (p->gang_size > 1) {\n\t\tfor (i = 0; i < p->gang_size; ++i)\n\t\t\tamdgpu_job_set_gang_leader(p->jobs[i], leader);\n\t}\n\n\t/* No memory allocation is allowed while holding the notifier lock.\n\t * The lock is held until amdgpu_cs_submit is finished and fence is\n\t * added to BOs.\n\t */\n\tmutex_lock(&p->adev->notifier_lock);\n\n\t/* If userptr are invalidated after amdgpu_cs_parser_bos(), return\n\t * -EAGAIN, drmIoctl in libdrm will restart the amdgpu_cs_ioctl.\n\t */\n\tr = 0;\n\tamdgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {\n\t\tr |= !amdgpu_ttm_tt_get_user_pages_done(e->bo->tbo.ttm,\n\t\t\t\t\t\t\te->range);\n\t\te->range = NULL;\n\t}\n\tif (r) {\n\t\tr = -EAGAIN;\n\t\tmutex_unlock(&p->adev->notifier_lock);\n\t\treturn r;\n\t}\n\n\tp->fence = dma_fence_get(&leader->base.s_fence->finished);\n\tdrm_exec_for_each_locked_object(&p->exec, index, gobj) {\n\n\t\tttm_bo_move_to_lru_tail_unlocked(&gem_to_amdgpu_bo(gobj)->tbo);\n\n\t\t/* Everybody except for the gang leader uses READ */\n\t\tfor (i = 0; i < p->gang_size; ++i) {\n\t\t\tif (p->jobs[i] == leader)\n\t\t\t\tcontinue;\n\n\t\t\tdma_resv_add_fence(gobj->resv,\n\t\t\t\t\t   &p->jobs[i]->base.s_fence->finished,\n\t\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\t}\n\n\t\t/* The gang leader as remembered as writer */\n\t\tdma_resv_add_fence(gobj->resv, p->fence, DMA_RESV_USAGE_WRITE);\n\t}\n\n\tseq = amdgpu_ctx_add_fence(p->ctx, p->entities[p->gang_leader_idx],\n\t\t\t\t   p->fence);\n\tamdgpu_cs_post_dependencies(p);\n\n\tif ((leader->preamble_status & AMDGPU_PREAMBLE_IB_PRESENT) &&\n\t    !p->ctx->preamble_presented) {\n\t\tleader->preamble_status |= AMDGPU_PREAMBLE_IB_PRESENT_FIRST;\n\t\tp->ctx->preamble_presented = true;\n\t}\n\n\tcs->out.handle = seq;\n\tleader->uf_sequence = seq;\n\n\tamdgpu_vm_bo_trace_cs(&fpriv->vm, &p->exec.ticket);\n\tfor (i = 0; i < p->gang_size; ++i) {\n\t\tamdgpu_job_free_resources(p->jobs[i]);\n\t\ttrace_amdgpu_cs_ioctl(p->jobs[i]);\n\t\tdrm_sched_entity_push_job(&p->jobs[i]->base);\n\t\tp->jobs[i] = NULL;\n\t}\n\n\tamdgpu_vm_move_to_lru_tail(p->adev, &fpriv->vm);\n\n\tmutex_unlock(&p->adev->notifier_lock);\n\tmutex_unlock(&p->bo_list->bo_list_mutex);\n\treturn 0;\n}\n\n/* Cleanup the parser structure */\nstatic void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser)\n{\n\tunsigned int i;\n\n\tamdgpu_sync_free(&parser->sync);\n\tdrm_exec_fini(&parser->exec);\n\n\tfor (i = 0; i < parser->num_post_deps; i++) {\n\t\tdrm_syncobj_put(parser->post_deps[i].syncobj);\n\t\tkfree(parser->post_deps[i].chain);\n\t}\n\tkfree(parser->post_deps);\n\n\tdma_fence_put(parser->fence);\n\n\tif (parser->ctx)\n\t\tamdgpu_ctx_put(parser->ctx);\n\tif (parser->bo_list)\n\t\tamdgpu_bo_list_put(parser->bo_list);\n\n\tfor (i = 0; i < parser->nchunks; i++)\n\t\tkvfree(parser->chunks[i].kdata);\n\tkvfree(parser->chunks);\n\tfor (i = 0; i < parser->gang_size; ++i) {\n\t\tif (parser->jobs[i])\n\t\t\tamdgpu_job_free(parser->jobs[i]);\n\t}\n\tamdgpu_bo_unref(&parser->uf_bo);\n}\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_cs_parser parser;\n\tint r;\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn -EHWPOISON;\n\n\tif (!adev->accel_working)\n\t\treturn -EBUSY;\n\n\tr = amdgpu_cs_parser_init(&parser, adev, filp, data);\n\tif (r) {\n\t\tDRM_ERROR_RATELIMITED(\"Failed to initialize parser %d!\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_cs_pass1(&parser, data);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_pass2(&parser);\n\tif (r)\n\t\tgoto error_fini;\n\n\tr = amdgpu_cs_parser_bos(&parser, data);\n\tif (r) {\n\t\tif (r == -ENOMEM)\n\t\t\tDRM_ERROR(\"Not enough memory for command submission!\\n\");\n\t\telse if (r != -ERESTARTSYS && r != -EAGAIN)\n\t\t\tDRM_DEBUG(\"Failed to process the buffer list %d!\\n\", r);\n\t\tgoto error_fini;\n\t}\n\n\tr = amdgpu_cs_patch_jobs(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_vm_handling(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tr = amdgpu_cs_sync_rings(&parser);\n\tif (r)\n\t\tgoto error_backoff;\n\n\ttrace_amdgpu_cs_ibs(&parser);\n\n\tr = amdgpu_cs_submit(&parser, data);\n\tif (r)\n\t\tgoto error_backoff;\n\n\tamdgpu_cs_parser_fini(&parser);\n\treturn 0;\n\nerror_backoff:\n\tmutex_unlock(&parser.bo_list->bo_list_mutex);\n\nerror_fini:\n\tamdgpu_cs_parser_fini(&parser);\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_ioctl - wait for a command submission to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n *\n * Wait for the command submission identified by handle to finish.\n */\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *filp)\n{\n\tunion drm_amdgpu_wait_cs *wait = data;\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout);\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_ctx_get_entity(ctx, wait->in.ip_type, wait->in.ip_instance,\n\t\t\t\t  wait->in.ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn r;\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, wait->in.handle);\n\tif (IS_ERR(fence))\n\t\tr = PTR_ERR(fence);\n\telse if (fence) {\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\t\tdma_fence_put(fence);\n\t} else\n\t\tr = 1;\n\n\tamdgpu_ctx_put(ctx);\n\tif (r < 0)\n\t\treturn r;\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r == 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_get_fence - helper to get fence from drm_amdgpu_fence\n *\n * @adev: amdgpu device\n * @filp: file private\n * @user: drm_amdgpu_fence copied from user space\n */\nstatic struct dma_fence *amdgpu_cs_get_fence(struct amdgpu_device *adev,\n\t\t\t\t\t     struct drm_file *filp,\n\t\t\t\t\t     struct drm_amdgpu_fence *user)\n{\n\tstruct drm_sched_entity *entity;\n\tstruct amdgpu_ctx *ctx;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tctx = amdgpu_ctx_get(filp->driver_priv, user->ctx_id);\n\tif (ctx == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tr = amdgpu_ctx_get_entity(ctx, user->ip_type, user->ip_instance,\n\t\t\t\t  user->ring, &entity);\n\tif (r) {\n\t\tamdgpu_ctx_put(ctx);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tfence = amdgpu_ctx_get_fence(ctx, entity, user->seq_no);\n\tamdgpu_ctx_put(ctx);\n\n\treturn fence;\n}\n\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_fence_to_handle *info = data;\n\tstruct dma_fence *fence;\n\tstruct drm_syncobj *syncobj;\n\tstruct sync_file *sync_file;\n\tint fd, r;\n\n\tfence = amdgpu_cs_get_fence(adev, filp, &info->in.fence);\n\tif (IS_ERR(fence))\n\t\treturn PTR_ERR(fence);\n\n\tif (!fence)\n\t\tfence = dma_fence_get_stub();\n\n\tswitch (info->in.what) {\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:\n\t\tr = drm_syncobj_create(&syncobj, 0, fence);\n\t\tdma_fence_put(fence);\n\t\tif (r)\n\t\t\treturn r;\n\t\tr = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);\n\t\tdrm_syncobj_put(syncobj);\n\t\treturn r;\n\n\tcase AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:\n\t\tfd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (fd < 0) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn fd;\n\t\t}\n\n\t\tsync_file = sync_file_create(fence);\n\t\tdma_fence_put(fence);\n\t\tif (!sync_file) {\n\t\t\tput_unused_fd(fd);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfd_install(fd, sync_file->file);\n\t\tinfo->out.handle = fd;\n\t\treturn 0;\n\n\tdefault:\n\t\tdma_fence_put(fence);\n\t\treturn -EINVAL;\n\t}\n}\n\n/**\n * amdgpu_cs_wait_all_fences - wait on all fences to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_all_fences(struct amdgpu_device *adev,\n\t\t\t\t     struct drm_file *filp,\n\t\t\t\t     union drm_amdgpu_wait_fences *wait,\n\t\t\t\t     struct drm_amdgpu_fence *fences)\n{\n\tuint32_t fence_count = wait->in.fence_count;\n\tunsigned int i;\n\tlong r = 1;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\t\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence))\n\t\t\treturn PTR_ERR(fence);\n\t\telse if (!fence)\n\t\t\tcontinue;\n\n\t\tr = dma_fence_wait_timeout(fence, true, timeout);\n\t\tif (r > 0 && fence->error)\n\t\t\tr = fence->error;\n\n\t\tdma_fence_put(fence);\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tif (r == 0)\n\t\t\tbreak;\n\t}\n\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_cs_wait_any_fence - wait on any fence to signal\n *\n * @adev: amdgpu device\n * @filp: file private\n * @wait: wait parameters\n * @fences: array of drm_amdgpu_fence\n */\nstatic int amdgpu_cs_wait_any_fence(struct amdgpu_device *adev,\n\t\t\t\t    struct drm_file *filp,\n\t\t\t\t    union drm_amdgpu_wait_fences *wait,\n\t\t\t\t    struct drm_amdgpu_fence *fences)\n{\n\tunsigned long timeout = amdgpu_gem_timeout(wait->in.timeout_ns);\n\tuint32_t fence_count = wait->in.fence_count;\n\tuint32_t first = ~0;\n\tstruct dma_fence **array;\n\tunsigned int i;\n\tlong r;\n\n\t/* Prepare the fence array */\n\tarray = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);\n\n\tif (array == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < fence_count; i++) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = amdgpu_cs_get_fence(adev, filp, &fences[i]);\n\t\tif (IS_ERR(fence)) {\n\t\t\tr = PTR_ERR(fence);\n\t\t\tgoto err_free_fence_array;\n\t\t} else if (fence) {\n\t\t\tarray[i] = fence;\n\t\t} else { /* NULL, the fence has been already signaled */\n\t\t\tr = 1;\n\t\t\tfirst = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tr = dma_fence_wait_any_timeout(array, fence_count, true, timeout,\n\t\t\t\t       &first);\n\tif (r < 0)\n\t\tgoto err_free_fence_array;\n\nout:\n\tmemset(wait, 0, sizeof(*wait));\n\twait->out.status = (r > 0);\n\twait->out.first_signaled = first;\n\n\tif (first < fence_count && array[first])\n\t\tr = array[first]->error;\n\telse\n\t\tr = 0;\n\nerr_free_fence_array:\n\tfor (i = 0; i < fence_count; i++)\n\t\tdma_fence_put(array[i]);\n\tkfree(array);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish\n *\n * @dev: drm device\n * @data: data from userspace\n * @filp: file private\n */\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tunion drm_amdgpu_wait_fences *wait = data;\n\tuint32_t fence_count = wait->in.fence_count;\n\tstruct drm_amdgpu_fence *fences_user;\n\tstruct drm_amdgpu_fence *fences;\n\tint r;\n\n\t/* Get the fences from userspace */\n\tfences = kmalloc_array(fence_count, sizeof(struct drm_amdgpu_fence),\n\t\t\tGFP_KERNEL);\n\tif (fences == NULL)\n\t\treturn -ENOMEM;\n\n\tfences_user = u64_to_user_ptr(wait->in.fences);\n\tif (copy_from_user(fences, fences_user,\n\t\tsizeof(struct drm_amdgpu_fence) * fence_count)) {\n\t\tr = -EFAULT;\n\t\tgoto err_free_fences;\n\t}\n\n\tif (wait->in.wait_all)\n\t\tr = amdgpu_cs_wait_all_fences(adev, filp, wait, fences);\n\telse\n\t\tr = amdgpu_cs_wait_any_fence(adev, filp, wait, fences);\n\nerr_free_fences:\n\tkfree(fences);\n\n\treturn r;\n}\n\n/**\n * amdgpu_cs_find_mapping - find bo_va for VM address\n *\n * @parser: command submission parser context\n * @addr: VM address\n * @bo: resulting BO of the mapping found\n * @map: Placeholder to return found BO mapping\n *\n * Search the buffer objects in the command submission context for a certain\n * virtual memory address. Returns allocation structure when found, NULL\n * otherwise.\n */\nint amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser,\n\t\t\t   uint64_t addr, struct amdgpu_bo **bo,\n\t\t\t   struct amdgpu_bo_va_mapping **map)\n{\n\tstruct amdgpu_fpriv *fpriv = parser->filp->driver_priv;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tint i, r;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping = amdgpu_vm_bo_lookup_mapping(vm, addr);\n\tif (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)\n\t\treturn -EINVAL;\n\n\t*bo = mapping->bo_va->base.bo;\n\t*map = mapping;\n\n\t/* Double check that the BO is reserved by this CS */\n\tif (dma_resv_locking_ctx((*bo)->tbo.base.resv) != &parser->exec.ticket)\n\t\treturn -EINVAL;\n\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tamdgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);\n\tfor (i = 0; i < (*bo)->placement.num_placement; i++)\n\t\t(*bo)->placements[i].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tr = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_ttm_alloc_gart(&(*bo)->tbo);\n}\n", "source_code_path": "amdgpu_cs.c", "line_number": 51, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nindex 1e475eb01417..d3db17084e3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\n@@ -46,6 +46,8 @@ static int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p,\n                                 struct drm_file *filp,\n                                 union drm_amdgpu_cs *cs)\n {\n+       if (p == NULL || adev == NULL || filp == NULL || cs == NULL)\n+               return -EINVAL;\n        struct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n        if (cs->in.num_chunks == 0)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_cs.c\nLine: 51\n\t\t\t\t union drm_amdgpu_cs *cs)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tif (cs->in.num_chunks == 0)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->adev = adev;\n\tp->filp = filp;\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Null Pointer Dereference\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c\nFunction: amdgpu_cs_parser_init\nLine: 51\n\nDescription:\nThe amdgpu_cs_parser_init function is susceptible to a null pointer dereference if the 'cs' pointer is null when the function is called. This is because the function does not perform a null check before accessing the 'cs' pointer's members.\nCode Path:\n\nFunction 'amdgpu_cs_parser_init' is called with 'cs' as an argument\n'cs' is dereferenced at line 51 without a NULL check: if (cs->in.num_chunks == 0)\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nThe function sprintf is used to write to the buffer stmp without any length checking. This could lead to a buffer overflow if the formatted string exceeds the size of stmp.\n\n", "diff_path": "amdgpu_atombios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 147, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 147\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nThe function sprintf is used to write to the buffer stmp without any length checking. This could lead to a buffer overflow if the formatted string exceeds the size of stmp.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nThe function sprintf is used to write to the buffer stmp without verifying the length of the formatted string. This can lead to a buffer overflow if the string exceeds the buffer's capacity, potentially causing data corruption or system instability.\n\n", "diff_path": "amdgpu_atombios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 147, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 147\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nThe function sprintf is used to write to the buffer stmp without verifying the length of the formatted string. This can lead to a buffer overflow if the string exceeds the buffer's capacity, potentially causing data corruption or system instability.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nA buffer overflow vulnerability exists in the use of sprintf, which writes to the stmp buffer without checking the length of the formatted string. This can result in data being written beyond the buffer's bounds, potentially leading to system crashes or data corruption.\n\n", "diff_path": "amdgpu_atombios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 147, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 147\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nA buffer overflow vulnerability exists in the use of sprintf, which writes to the stmp buffer without checking the length of the formatted string. This can result in data being written beyond the buffer's bounds, potentially leading to system crashes or data corruption.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atombios-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nThe function sprintf is used to write to the stmp buffer without ensuring that the formatted string fits within the buffer's capacity. This can lead to a buffer overflow, where data is written beyond the buffer's bounds, potentially causing system instability or data corruption.\n\n", "diff_path": "amdgpu_atombios-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n", "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\t\tsizeof(args));\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\t\tsizeof(args));\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n", "source_code_path": "amdgpu_atombios.c", "line_number": 147, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..e215b5e2d183 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp), \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atombios.c\nLine: 147\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in sprintf\n\nCID: 1001\nType: Buffer Overflow\nSeverity: High\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nDescription:\nThe function sprintf is used to write to the stmp buffer without ensuring that the formatted string fits within the buffer's capacity. This can lead to a buffer overflow, where data is written beyond the buffer's bounds, potentially causing system instability or data corruption.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "alderaban-bug.txt", "bug_report_text": "CID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nThe function 'aldebaran_get_reset_handler' dereferences 'reset_ctl->handle' without first checking if 'reset_ctl' or 'reset_ctl->handle' is NULL. This could lead to a null pointer dereference if either 'reset_ctl' or 'reset_ctl->handle' is NULL when the function is called.\n\nExecution path:\n  1. Function aldebaran_get_reset_handler is called\n  2. 'reset_ctl->handle' is dereferenced without a null check\n\n", "diff_path": "alderaban-diff.txt", "diff_text": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl || !reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n", "code": "/*\n * Copyright 2021 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include \"aldebaran.h\"\n#include \"amdgpu_reset.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dpm.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_psp.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic bool aldebaran_is_mode2_default(struct amdgpu_reset_control *reset_ctl)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tif ((amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t     adev->gmc.xgmi.connected_to_cpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct amdgpu_reset_handler *\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n\t\telse\n\t\t\treset_context->method = amdgpu_asic_reset_method(adev);\n\t}\n\n\tif (reset_context->method != AMD_RESET_METHOD_NONE) {\n\t\tdev_dbg(adev->dev, \"Getting reset handler for method %d\\n\",\n\t\t\treset_context->method);\n\t\tfor_each_handler(i, handler, reset_ctl) {\n\t\t\tif (handler->reset_method == reset_context->method)\n\t\t\t\treturn handler;\n\t\t}\n\t}\n\n\tdev_dbg(adev->dev, \"Reset handler not found!\\n\");\n\n\treturn NULL;\n}\n\nstatic int aldebaran_mode2_suspend_ip(struct amdgpu_device *adev)\n{\n\tint r, i;\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int\naldebaran_mode2_prepare_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tint r = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tdev_dbg(adev->dev, \"Aldebaran prepare hw context\\n\");\n\t/* Don't suspend on bare metal if we are not going to HW reset the ASIC */\n\tif (!amdgpu_sriov_vf(adev))\n\t\tr = aldebaran_mode2_suspend_ip(adev);\n\n\treturn r;\n}\n\nstatic void aldebaran_async_reset(struct work_struct *work)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_reset_control *reset_ctl =\n\t\tcontainer_of(work, struct amdgpu_reset_control, reset_work);\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tfor_each_handler(i, handler, reset_ctl)\t{\n\t\tif (handler->reset_method == reset_ctl->active_reset) {\n\t\t\tdev_dbg(adev->dev, \"Resetting device\\n\");\n\t\t\thandler->do_reset(adev);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int aldebaran_mode2_reset(struct amdgpu_device *adev)\n{\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\tadev->asic_reset_res = amdgpu_dpm_mode2_reset(adev);\n\treturn adev->asic_reset_res;\n}\n\nstatic int\naldebaran_mode2_perform_reset(struct amdgpu_reset_control *reset_ctl,\n\t\t\t      struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tint r = 0;\n\n\tdev_dbg(adev->dev, \"aldebaran perform hw reset\\n\");\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_lock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_MODE2;\n\t}\n\t/*\n\t * Mode2 reset doesn't need any sync between nodes in XGMI hive, instead launch\n\t * them together so that they can be completed asynchronously on multiple nodes\n\t */\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t/* For XGMI run all resets in parallel to speed up the process */\n\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\tif (!queue_work(system_unbound_wq,\n\t\t\t\t\t&tmp_adev->reset_cntl->reset_work))\n\t\t\t\tr = -EALREADY;\n\t\t} else\n\t\t\tr = aldebaran_mode2_reset(tmp_adev);\n\t\tif (r) {\n\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t\tr, adev_to_drm(tmp_adev)->unique);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* For XGMI wait for all resets to complete before proceed */\n\tif (!r) {\n\t\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\tflush_work(&tmp_adev->reset_cntl->reset_work);\n\t\t\t\tr = tmp_adev->asic_reset_res;\n\t\t\t\tif (r)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_unlock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_NONE;\n\t}\n\n\treturn r;\n}\n\nstatic int aldebaran_mode2_restore_ip(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_firmware_info *ucode_list[AMDGPU_UCODE_ID_MAXIMUM];\n\tstruct amdgpu_firmware_info *ucode;\n\tstruct amdgpu_ip_block *cmn_block;\n\tint ucode_count = 0;\n\tint i, r;\n\n\tdev_dbg(adev->dev, \"Reloading ucodes after reset\\n\");\n\tfor (i = 0; i < adev->firmware.max_ucodes; i++) {\n\t\tucode = &adev->firmware.ucode[i];\n\t\tif (!ucode->fw)\n\t\t\tcontinue;\n\t\tswitch (ucode->ucode_id) {\n\t\tcase AMDGPU_UCODE_ID_SDMA0:\n\t\tcase AMDGPU_UCODE_ID_SDMA1:\n\t\tcase AMDGPU_UCODE_ID_SDMA2:\n\t\tcase AMDGPU_UCODE_ID_SDMA3:\n\t\tcase AMDGPU_UCODE_ID_SDMA4:\n\t\tcase AMDGPU_UCODE_ID_SDMA5:\n\t\tcase AMDGPU_UCODE_ID_SDMA6:\n\t\tcase AMDGPU_UCODE_ID_SDMA7:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1_JT:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_G:\n\t\t\tucode_list[ucode_count++] = ucode;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Reinit NBIF block */\n\tcmn_block =\n\t\tamdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_COMMON);\n\tif (unlikely(!cmn_block)) {\n\t\tdev_err(adev->dev, \"Failed to get BIF handle\\n\");\n\t\treturn -EINVAL;\n\t}\n\tr = cmn_block->version->funcs->resume(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Reinit GFXHUB */\n\tadev->gfxhub.funcs->init(adev);\n\tr = adev->gfxhub.funcs->gart_enable(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFXHUB gart reenable failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Reload GFX firmware */\n\tr = psp_load_fw_list(&adev->psp, ucode_list, ucode_count);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFX ucode load failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Resume RLC, FW needs RLC alive to complete reset process */\n\tadev->gfx.rlc.funcs->resume(adev);\n\n\t/* Wait for FW reset event complete */\n\tr = amdgpu_dpm_wait_for_event(adev, SMU_EVENT_RESET_COMPLETE, 0);\n\tif (r) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Failed to get response from firmware after reset\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_COMMON))\n\t\t\tcontinue;\n\n\t\tif (adev->ip_blocks[i].version->funcs->late_init) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->late_init(\n\t\t\t\t(void *)adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\"late_init of IP block <%s> failed %d after reset\\n\",\n\t\t\t\t\tadev->ip_blocks[i].version->funcs->name,\n\t\t\t\t\tr);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tadev->ip_blocks[i].status.late_initialized = true;\n\t}\n\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_GATE);\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_GATE);\n\n\treturn r;\n}\n\nstatic int\naldebaran_mode2_restore_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tstruct amdgpu_ras *con;\n\tint r;\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(reset_context->reset_req_dev, MP1_HWIP, 0) ==\n\t\t    IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tdev_info(tmp_adev->dev,\n\t\t\t \"GPU reset succeeded, trying to resume\\n\");\n\t\tr = aldebaran_mode2_restore_ip(tmp_adev);\n\t\tif (r)\n\t\t\tgoto end;\n\n\t\t/*\n\t\t * Add this ASIC as tracked as reset was already\n\t\t * complete successfully.\n\t\t */\n\t\tamdgpu_register_gpu_instance(tmp_adev);\n\n\t\t/* Resume RAS, ecc_irq */\n\t\tcon = amdgpu_ras_get_context(tmp_adev);\n\t\tif (!amdgpu_sriov_vf(tmp_adev) && con) {\n\t\t\tif (tmp_adev->sdma.ras &&\n\t\t\t\ttmp_adev->sdma.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->sdma.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->sdma.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"SDMA failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (tmp_adev->gfx.ras &&\n\t\t\t\ttmp_adev->gfx.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->gfx.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->gfx.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"GFX failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_ras_resume(tmp_adev);\n\n\t\t/* Update PSP FW topology after reset */\n\t\tif (reset_context->hive &&\n\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\tr = amdgpu_xgmi_update_topology(reset_context->hive,\n\t\t\t\t\t\t\ttmp_adev);\n\n\t\tif (!r) {\n\t\t\tamdgpu_irq_gpu_reset_resume_helper(tmp_adev);\n\n\t\t\tr = amdgpu_ib_ring_tests(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\"ib ring test failed (%d).\\n\", r);\n\t\t\t\tr = -EAGAIN;\n\t\t\t\ttmp_adev->asic_reset_res = r;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\t}\n\nend:\n\treturn r;\n}\n\nstatic struct amdgpu_reset_handler aldebaran_mode2_handler = {\n\t.reset_method\t\t= AMD_RESET_METHOD_MODE2,\n\t.prepare_env\t\t= NULL,\n\t.prepare_hwcontext\t= aldebaran_mode2_prepare_hwcontext,\n\t.perform_reset\t\t= aldebaran_mode2_perform_reset,\n\t.restore_hwcontext\t= aldebaran_mode2_restore_hwcontext,\n\t.restore_env\t\t= NULL,\n\t.do_reset\t\t= aldebaran_mode2_reset,\n};\n\nstatic struct amdgpu_reset_handler\n\t*aldebaran_rst_handlers[AMDGPU_RESET_MAX_HANDLERS] = {\n\t\t&aldebaran_mode2_handler,\n\t};\n\nint aldebaran_reset_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_reset_control *reset_ctl;\n\n\treset_ctl = kzalloc(sizeof(*reset_ctl), GFP_KERNEL);\n\tif (!reset_ctl)\n\t\treturn -ENOMEM;\n\n\treset_ctl->handle = adev;\n\treset_ctl->async_reset = aldebaran_async_reset;\n\treset_ctl->active_reset = AMD_RESET_METHOD_NONE;\n\treset_ctl->get_reset_handler = aldebaran_get_reset_handler;\n\n\tINIT_WORK(&reset_ctl->reset_work, reset_ctl->async_reset);\n\t/* Only mode2 is handled through reset control now */\n\treset_ctl->reset_handlers = &aldebaran_rst_handlers;\n\n\tadev->reset_cntl = reset_ctl;\n\n\treturn 0;\n}\n\nint aldebaran_reset_fini(struct amdgpu_device *adev)\n{\n\tkfree(adev->reset_cntl);\n\tadev->reset_cntl = NULL;\n\treturn 0;\n}\n", "source_code_path": "aldebaran.c", "line_number": 50, "generated_diff": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl ||!reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: aldebaran.c\nLine: 50\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n====================================\nRead the following bug report.\n============ Bug Report ============\nCID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nThe function 'aldebaran_get_reset_handler' dereferences 'reset_ctl->handle' without first checking if 'reset_ctl' or 'reset_ctl->handle' is NULL. This could lead to a null pointer dereference if either 'reset_ctl' or 'reset_ctl->handle' is NULL when the function is called.\n\nExecution path:\n  1. Function aldebaran_get_reset_handler is called\n  2. 'reset_ctl->handle' is dereferenced without a null check\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "alderaban-bug.txt", "bug_report_text": "CID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nThe function aldebaran_get_reset_handler fails to validate the'reset_ctl' and'reset_ctl->handle' pointers before dereferencing them. This can lead to a null pointer dereference if either pointer is null when the function is called, resulting in potential system instability or crashes.\n\n", "diff_path": "alderaban-diff.txt", "diff_text": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl || !reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n", "code": "/*\n * Copyright 2021 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include \"aldebaran.h\"\n#include \"amdgpu_reset.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dpm.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_psp.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic bool aldebaran_is_mode2_default(struct amdgpu_reset_control *reset_ctl)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tif ((amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t     adev->gmc.xgmi.connected_to_cpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct amdgpu_reset_handler *\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n\t\telse\n\t\t\treset_context->method = amdgpu_asic_reset_method(adev);\n\t}\n\n\tif (reset_context->method != AMD_RESET_METHOD_NONE) {\n\t\tdev_dbg(adev->dev, \"Getting reset handler for method %d\\n\",\n\t\t\treset_context->method);\n\t\tfor_each_handler(i, handler, reset_ctl) {\n\t\t\tif (handler->reset_method == reset_context->method)\n\t\t\t\treturn handler;\n\t\t}\n\t}\n\n\tdev_dbg(adev->dev, \"Reset handler not found!\\n\");\n\n\treturn NULL;\n}\n\nstatic int aldebaran_mode2_suspend_ip(struct amdgpu_device *adev)\n{\n\tint r, i;\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\treturn 0;\n}\n\nstatic int\naldebaran_mode2_prepare_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tint r = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\n\tdev_dbg(adev->dev, \"Aldebaran prepare hw context\\n\");\n\t/* Don't suspend on bare metal if we are not going to HW reset the ASIC */\n\tif (!amdgpu_sriov_vf(adev))\n\t\tr = aldebaran_mode2_suspend_ip(adev);\n\n\treturn r;\n}\n\nstatic void aldebaran_async_reset(struct work_struct *work)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_reset_control *reset_ctl =\n\t\tcontainer_of(work, struct amdgpu_reset_control, reset_work);\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tfor_each_handler(i, handler, reset_ctl)\t{\n\t\tif (handler->reset_method == reset_ctl->active_reset) {\n\t\t\tdev_dbg(adev->dev, \"Resetting device\\n\");\n\t\t\thandler->do_reset(adev);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int aldebaran_mode2_reset(struct amdgpu_device *adev)\n{\n\t/* disable BM */\n\tpci_clear_master(adev->pdev);\n\tadev->asic_reset_res = amdgpu_dpm_mode2_reset(adev);\n\treturn adev->asic_reset_res;\n}\n\nstatic int\naldebaran_mode2_perform_reset(struct amdgpu_reset_control *reset_ctl,\n\t\t\t      struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tint r = 0;\n\n\tdev_dbg(adev->dev, \"aldebaran perform hw reset\\n\");\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(adev, MP1_HWIP, 0) == IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_lock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_MODE2;\n\t}\n\t/*\n\t * Mode2 reset doesn't need any sync between nodes in XGMI hive, instead launch\n\t * them together so that they can be completed asynchronously on multiple nodes\n\t */\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t/* For XGMI run all resets in parallel to speed up the process */\n\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\tif (!queue_work(system_unbound_wq,\n\t\t\t\t\t&tmp_adev->reset_cntl->reset_work))\n\t\t\t\tr = -EALREADY;\n\t\t} else\n\t\t\tr = aldebaran_mode2_reset(tmp_adev);\n\t\tif (r) {\n\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t\tr, adev_to_drm(tmp_adev)->unique);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* For XGMI wait for all resets to complete before proceed */\n\tif (!r) {\n\t\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\tflush_work(&tmp_adev->reset_cntl->reset_work);\n\t\t\t\tr = tmp_adev->asic_reset_res;\n\t\t\t\tif (r)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tmutex_unlock(&tmp_adev->reset_cntl->reset_lock);\n\t\ttmp_adev->reset_cntl->active_reset = AMD_RESET_METHOD_NONE;\n\t}\n\n\treturn r;\n}\n\nstatic int aldebaran_mode2_restore_ip(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_firmware_info *ucode_list[AMDGPU_UCODE_ID_MAXIMUM];\n\tstruct amdgpu_firmware_info *ucode;\n\tstruct amdgpu_ip_block *cmn_block;\n\tint ucode_count = 0;\n\tint i, r;\n\n\tdev_dbg(adev->dev, \"Reloading ucodes after reset\\n\");\n\tfor (i = 0; i < adev->firmware.max_ucodes; i++) {\n\t\tucode = &adev->firmware.ucode[i];\n\t\tif (!ucode->fw)\n\t\t\tcontinue;\n\t\tswitch (ucode->ucode_id) {\n\t\tcase AMDGPU_UCODE_ID_SDMA0:\n\t\tcase AMDGPU_UCODE_ID_SDMA1:\n\t\tcase AMDGPU_UCODE_ID_SDMA2:\n\t\tcase AMDGPU_UCODE_ID_SDMA3:\n\t\tcase AMDGPU_UCODE_ID_SDMA4:\n\t\tcase AMDGPU_UCODE_ID_SDMA5:\n\t\tcase AMDGPU_UCODE_ID_SDMA6:\n\t\tcase AMDGPU_UCODE_ID_SDMA7:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1:\n\t\tcase AMDGPU_UCODE_ID_CP_MEC1_JT:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM:\n\t\tcase AMDGPU_UCODE_ID_RLC_G:\n\t\t\tucode_list[ucode_count++] = ucode;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Reinit NBIF block */\n\tcmn_block =\n\t\tamdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_COMMON);\n\tif (unlikely(!cmn_block)) {\n\t\tdev_err(adev->dev, \"Failed to get BIF handle\\n\");\n\t\treturn -EINVAL;\n\t}\n\tr = cmn_block->version->funcs->resume(adev);\n\tif (r)\n\t\treturn r;\n\n\t/* Reinit GFXHUB */\n\tadev->gfxhub.funcs->init(adev);\n\tr = adev->gfxhub.funcs->gart_enable(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFXHUB gart reenable failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Reload GFX firmware */\n\tr = psp_load_fw_list(&adev->psp, ucode_list, ucode_count);\n\tif (r) {\n\t\tdev_err(adev->dev, \"GFX ucode load failed after reset\\n\");\n\t\treturn r;\n\t}\n\n\t/* Resume RLC, FW needs RLC alive to complete reset process */\n\tadev->gfx.rlc.funcs->resume(adev);\n\n\t/* Wait for FW reset event complete */\n\tr = amdgpu_dpm_wait_for_event(adev, SMU_EVENT_RESET_COMPLETE, 0);\n\tif (r) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Failed to get response from firmware after reset\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\tadev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!(adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_GFX ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_SDMA ||\n\t\t      adev->ip_blocks[i].version->type ==\n\t\t\t      AMD_IP_BLOCK_TYPE_COMMON))\n\t\t\tcontinue;\n\n\t\tif (adev->ip_blocks[i].version->funcs->late_init) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->late_init(\n\t\t\t\t(void *)adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\"late_init of IP block <%s> failed %d after reset\\n\",\n\t\t\t\t\tadev->ip_blocks[i].version->funcs->name,\n\t\t\t\t\tr);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tadev->ip_blocks[i].status.late_initialized = true;\n\t}\n\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_GATE);\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_GATE);\n\n\treturn r;\n}\n\nstatic int\naldebaran_mode2_restore_hwcontext(struct amdgpu_reset_control *reset_ctl,\n\t\t\t\t  struct amdgpu_reset_context *reset_context)\n{\n\tstruct list_head *reset_device_list = reset_context->reset_device_list;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tstruct amdgpu_ras *con;\n\tint r;\n\n\tif (reset_device_list == NULL)\n\t\treturn -EINVAL;\n\n\tif (amdgpu_ip_version(reset_context->reset_req_dev, MP1_HWIP, 0) ==\n\t\t    IP_VERSION(13, 0, 2) &&\n\t    reset_context->hive == NULL) {\n\t\t/* Wrong context, return error */\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(tmp_adev, reset_device_list, reset_list) {\n\t\tdev_info(tmp_adev->dev,\n\t\t\t \"GPU reset succeeded, trying to resume\\n\");\n\t\tr = aldebaran_mode2_restore_ip(tmp_adev);\n\t\tif (r)\n\t\t\tgoto end;\n\n\t\t/*\n\t\t * Add this ASIC as tracked as reset was already\n\t\t * complete successfully.\n\t\t */\n\t\tamdgpu_register_gpu_instance(tmp_adev);\n\n\t\t/* Resume RAS, ecc_irq */\n\t\tcon = amdgpu_ras_get_context(tmp_adev);\n\t\tif (!amdgpu_sriov_vf(tmp_adev) && con) {\n\t\t\tif (tmp_adev->sdma.ras &&\n\t\t\t\ttmp_adev->sdma.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->sdma.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->sdma.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"SDMA failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (tmp_adev->gfx.ras &&\n\t\t\t\ttmp_adev->gfx.ras->ras_block.ras_late_init) {\n\t\t\t\tr = tmp_adev->gfx.ras->ras_block.ras_late_init(tmp_adev,\n\t\t\t\t\t\t&tmp_adev->gfx.ras->ras_block.ras_comm);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(tmp_adev->dev, \"GFX failed to execute ras_late_init! ret:%d\\n\", r);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_ras_resume(tmp_adev);\n\n\t\t/* Update PSP FW topology after reset */\n\t\tif (reset_context->hive &&\n\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\tr = amdgpu_xgmi_update_topology(reset_context->hive,\n\t\t\t\t\t\t\ttmp_adev);\n\n\t\tif (!r) {\n\t\t\tamdgpu_irq_gpu_reset_resume_helper(tmp_adev);\n\n\t\t\tr = amdgpu_ib_ring_tests(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\"ib ring test failed (%d).\\n\", r);\n\t\t\t\tr = -EAGAIN;\n\t\t\t\ttmp_adev->asic_reset_res = r;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\t}\n\nend:\n\treturn r;\n}\n\nstatic struct amdgpu_reset_handler aldebaran_mode2_handler = {\n\t.reset_method\t\t= AMD_RESET_METHOD_MODE2,\n\t.prepare_env\t\t= NULL,\n\t.prepare_hwcontext\t= aldebaran_mode2_prepare_hwcontext,\n\t.perform_reset\t\t= aldebaran_mode2_perform_reset,\n\t.restore_hwcontext\t= aldebaran_mode2_restore_hwcontext,\n\t.restore_env\t\t= NULL,\n\t.do_reset\t\t= aldebaran_mode2_reset,\n};\n\nstatic struct amdgpu_reset_handler\n\t*aldebaran_rst_handlers[AMDGPU_RESET_MAX_HANDLERS] = {\n\t\t&aldebaran_mode2_handler,\n\t};\n\nint aldebaran_reset_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_reset_control *reset_ctl;\n\n\treset_ctl = kzalloc(sizeof(*reset_ctl), GFP_KERNEL);\n\tif (!reset_ctl)\n\t\treturn -ENOMEM;\n\n\treset_ctl->handle = adev;\n\treset_ctl->async_reset = aldebaran_async_reset;\n\treset_ctl->active_reset = AMD_RESET_METHOD_NONE;\n\treset_ctl->get_reset_handler = aldebaran_get_reset_handler;\n\n\tINIT_WORK(&reset_ctl->reset_work, reset_ctl->async_reset);\n\t/* Only mode2 is handled through reset control now */\n\treset_ctl->reset_handlers = &aldebaran_rst_handlers;\n\n\tadev->reset_cntl = reset_ctl;\n\n\treturn 0;\n}\n\nint aldebaran_reset_fini(struct amdgpu_device *adev)\n{\n\tkfree(adev->reset_cntl);\n\tadev->reset_cntl = NULL;\n\treturn 0;\n}\n", "source_code_path": "aldebaran.c", "line_number": 50, "generated_diff": "--- a/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n+++ b/drivers/gpu/drm/amd/amdgpu/aldebaran.c\n@@ -46,11 +46,16 @@ aldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n \t\t\t    struct amdgpu_reset_context *reset_context)\n {\n \tstruct amdgpu_reset_handler *handler;\n-\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n+\tstruct amdgpu_device *adev;\n \tint i;\n\n+\tif (!reset_ctl ||!reset_ctl->handle) {\n+\t\tpr_err(\"Invalid reset control or handle\\n\");\n+\t\treturn NULL;\n+\t}\n+\n+\tadev = (struct amdgpu_device *)reset_ctl->handle;\n+\n \tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n \t\tif (aldebaran_is_mode2_default(reset_ctl))\n \t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: aldebaran.c\nLine: 50\naldebaran_get_reset_handler(struct amdgpu_reset_control *reset_ctl,\n\t\t\t    struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_reset_handler *handler;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)reset_ctl->handle;\n\tint i;\n\n\tif (reset_context->method == AMD_RESET_METHOD_NONE) {\n\t\tif (aldebaran_is_mode2_default(reset_ctl))\n\t\t\treset_context->method = AMD_RESET_METHOD_MODE2;\n====================================\nRead the following bug report.\n============ Bug Report ============\nCID: 12345\nClassification: NULL_RETURNS\nFunction: aldebaran_get_reset_handler\nFile: drivers/gpu/drm/amd/amdgpu/aldebaran.c\nLine: 50\n\nDescription:\nThe function aldebaran_get_reset_handler fails to validate the'reset_ctl' and'reset_ctl->handle' pointers before dereferencing them. This can lead to a null pointer dereference if either pointer is null when the function is called, resulting in potential system instability or crashes.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atpx_handler-bug.txt", "bug_report_text": "Issue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nThe function amdgpu_atpx_call allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER, but does not free this memory in all code paths. Specifically, when the function succeeds (i.e., when acpi_evaluate_object does not fail), the allocated memory is returned to the caller without being freed.\n\n", "diff_path": "amdgpu_atpx_handler-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n", "code": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright (c) 2010 Red Hat Inc.\n * Author : Dave Airlie <airlied@redhat.com>\n *\n * ATPX support for both Intel/ATI\n */\n#include <linux/vga_switcheroo.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n\n#include \"amdgpu.h\"\n#include \"amd_acpi.h\"\n\n#define AMDGPU_PX_QUIRK_FORCE_ATPX  (1 << 0)\n\nstruct amdgpu_px_quirk {\n\tu32 chip_vendor;\n\tu32 chip_device;\n\tu32 subsys_vendor;\n\tu32 subsys_device;\n\tu32 px_quirk_flags;\n};\n\nstruct amdgpu_atpx_functions {\n\tbool px_params;\n\tbool power_cntl;\n\tbool disp_mux_cntl;\n\tbool i2c_mux_cntl;\n\tbool switch_start;\n\tbool switch_end;\n\tbool disp_connectors_mapping;\n\tbool disp_detection_ports;\n};\n\nstruct amdgpu_atpx {\n\tacpi_handle handle;\n\tstruct amdgpu_atpx_functions functions;\n\tbool is_hybrid;\n\tbool dgpu_req_power_for_displays;\n};\n\nstatic struct amdgpu_atpx_priv {\n\tbool atpx_detected;\n\tbool bridge_pm_usable;\n\tunsigned int quirks;\n\t/* handle for device - and atpx */\n\tacpi_handle dhandle;\n\tacpi_handle other_handle;\n\tstruct amdgpu_atpx atpx;\n} amdgpu_atpx_priv;\n\nstruct atpx_verify_interface {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu16 version;\t\t/* version */\n\tu32 function_bits;\t/* supported functions bit vector */\n} __packed;\n\nstruct atpx_px_params {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu32 valid_flags;\t/* which flags are valid */\n\tu32 flags;\t\t/* flags */\n} __packed;\n\nstruct atpx_power_control {\n\tu16 size;\n\tu8 dgpu_state;\n} __packed;\n\nstruct atpx_mux {\n\tu16 size;\n\tu16 mux;\n} __packed;\n\nbool amdgpu_has_atpx(void)\n{\n\treturn amdgpu_atpx_priv.atpx_detected;\n}\n\nbool amdgpu_has_atpx_dgpu_power_cntl(void)\n{\n\treturn amdgpu_atpx_priv.atpx.functions.power_cntl;\n}\n\nbool amdgpu_is_atpx_hybrid(void)\n{\n\treturn amdgpu_atpx_priv.atpx.is_hybrid;\n}\n\nbool amdgpu_atpx_dgpu_req_power_for_displays(void)\n{\n\treturn amdgpu_atpx_priv.atpx.dgpu_req_power_for_displays;\n}\n\n#if defined(CONFIG_ACPI)\nvoid *amdgpu_atpx_get_dhandle(void)\n{\n\treturn amdgpu_atpx_priv.dhandle;\n}\n#endif\n\n/**\n * amdgpu_atpx_call - call an ATPX method\n *\n * @handle: acpi handle\n * @function: the ATPX function to execute\n * @params: ATPX function params\n *\n * Executes the requested ATPX function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atpx_arg_elements[2];\n\tstruct acpi_object_list atpx_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatpx_arg.count = 2;\n\tatpx_arg.pointer = &atpx_arg_elements[0];\n\n\tatpx_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatpx_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatpx_arg_elements[1].buffer.length = params->length;\n\t\tatpx_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatpx_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(handle, NULL, &atpx_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATPX is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atpx_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATPX\n *\n * Use the supported functions mask from ATPX function\n * ATPX_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atpx_parse_functions(struct amdgpu_atpx_functions *f, u32 mask)\n{\n\tf->px_params = mask & ATPX_GET_PX_PARAMETERS_SUPPORTED;\n\tf->power_cntl = mask & ATPX_POWER_CONTROL_SUPPORTED;\n\tf->disp_mux_cntl = mask & ATPX_DISPLAY_MUX_CONTROL_SUPPORTED;\n\tf->i2c_mux_cntl = mask & ATPX_I2C_MUX_CONTROL_SUPPORTED;\n\tf->switch_start = mask & ATPX_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION_SUPPORTED;\n\tf->switch_end = mask & ATPX_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION_SUPPORTED;\n\tf->disp_connectors_mapping = mask & ATPX_GET_DISPLAY_CONNECTORS_MAPPING_SUPPORTED;\n\tf->disp_detection_ports = mask & ATPX_GET_DISPLAY_DETECTION_PORTS_SUPPORTED;\n}\n\n/**\n * amdgpu_atpx_validate - validate ATPX functions\n *\n * @atpx: amdgpu atpx struct\n *\n * Validate that required functions are enabled (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)\n{\n\tu32 valid_bits = 0;\n\n\tif (atpx->functions.px_params) {\n\t\tunion acpi_object *info;\n\t\tstruct atpx_px_params output;\n\t\tsize_t size;\n\n\t\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_GET_PX_PARAMETERS, NULL);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&output, 0, sizeof(output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 10) {\n\t\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(output), size);\n\n\t\tmemcpy(&output, info->buffer.pointer, size);\n\n\t\tvalid_bits = output.flags & output.valid_flags;\n\n\t\tkfree(info);\n\t}\n\n\t/* if separate mux flag is set, mux controls are required */\n\tif (valid_bits & ATPX_SEPARATE_MUX_FOR_I2C) {\n\t\tatpx->functions.i2c_mux_cntl = true;\n\t\tatpx->functions.disp_mux_cntl = true;\n\t}\n\t/* if any outputs are muxed, mux controls are required */\n\tif (valid_bits & (ATPX_CRT1_RGB_SIGNAL_MUXED |\n\t\t\t  ATPX_TV_SIGNAL_MUXED |\n\t\t\t  ATPX_DFP_SIGNAL_MUXED))\n\t\tatpx->functions.disp_mux_cntl = true;\n\n\n\t/* some bioses set these bits rather than flagging power_cntl as supported */\n\tif (valid_bits & (ATPX_DYNAMIC_PX_SUPPORTED |\n\t\t\t  ATPX_DYNAMIC_DGPU_POWER_OFF_SUPPORTED))\n\t\tatpx->functions.power_cntl = true;\n\n\tatpx->is_hybrid = false;\n\tif (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {\n\t\tif (amdgpu_atpx_priv.quirks & AMDGPU_PX_QUIRK_FORCE_ATPX) {\n\t\t\tpr_warn(\"ATPX Hybrid Graphics, forcing to ATPX\\n\");\n\t\t\tatpx->functions.power_cntl = true;\n\t\t\tatpx->is_hybrid = false;\n\t\t} else {\n\t\t\tpr_notice(\"ATPX Hybrid Graphics\\n\");\n\t\t\t/*\n\t\t\t * Disable legacy PM methods only when pcie port PM is usable,\n\t\t\t * otherwise the device might fail to power off or power on.\n\t\t\t */\n\t\t\tatpx->functions.power_cntl = !amdgpu_atpx_priv.bridge_pm_usable;\n\t\t\tatpx->is_hybrid = true;\n\t\t}\n\t}\n\n\tatpx->dgpu_req_power_for_displays = false;\n\tif (valid_bits & ATPX_DGPU_REQ_POWER_FOR_DISPLAYS)\n\t\tatpx->dgpu_req_power_for_displays = true;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_verify_interface - verify ATPX\n *\n * @atpx: amdgpu atpx struct\n *\n * Execute the ATPX_FUNCTION_VERIFY_INTERFACE ATPX function\n * to initialize ATPX and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_verify_interface(struct amdgpu_atpx *atpx)\n{\n\tunion acpi_object *info;\n\tstruct atpx_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tpr_notice(\"ATPX version %u, functions 0x%08x\\n\",\n\t\t  output.version, output.function_bits);\n\n\tamdgpu_atpx_parse_functions(&atpx->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atpx_set_discrete_state - power up/down discrete GPU\n *\n * @atpx: atpx info struct\n * @state: discrete GPU state (0 = power down, 1 = power up)\n *\n * Execute the ATPX_FUNCTION_POWER_CONTROL ATPX function to\n * power down/up the discrete GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_set_discrete_state(struct amdgpu_atpx *atpx, u8 state)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_power_control input;\n\n\tif (atpx->functions.power_cntl) {\n\t\tinput.size = 3;\n\t\tinput.dgpu_state = state;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_POWER_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\n\t\t/* 200ms delay is required after off */\n\t\tif (state == 0)\n\t\t\tmsleep(200);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_disp_mux - switch display mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_DISPLAY_MUX_CONTROL ATPX function to\n * switch the display mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_disp_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.disp_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_DISPLAY_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_i2c_mux - switch i2c/hpd mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_I2C_MUX_CONTROL ATPX function to\n * switch the i2c/hpd mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_i2c_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.i2c_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_I2C_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_start - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has begun (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_start(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_start) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_end - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has ended (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_end(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_end) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switchto - switch to the requested GPU\n *\n * @id: GPU to switch to\n *\n * Execute the necessary ATPX functions to switch between the discrete GPU and\n * integrated GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switchto(enum vga_switcheroo_client_id id)\n{\n\tu16 gpu_id;\n\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\tgpu_id = ATPX_INTEGRATED_GPU;\n\telse\n\t\tgpu_id = ATPX_DISCRETE_GPU;\n\n\tamdgpu_atpx_switch_start(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_disp_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_i2c_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_end(&amdgpu_atpx_priv.atpx, gpu_id);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_power_state - power down/up the requested GPU\n *\n * @id: GPU to power down/up\n * @state: requested power state (0 = off, 1 = on)\n *\n * Execute the necessary ATPX function to power down/up the discrete GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_power_state(enum vga_switcheroo_client_id id,\n\t\t\t\t   enum vga_switcheroo_state state)\n{\n\t/* on w500 ACPI can't change intel gpu state */\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\treturn 0;\n\n\tamdgpu_atpx_set_discrete_state(&amdgpu_atpx_priv.atpx, state);\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_pci_probe_handle - look up the ATPX handle\n *\n * @pdev: pci device\n *\n * Look up the ATPX handles (all asics).\n * Returns true if the handles are found, false if not.\n */\nstatic bool amdgpu_atpx_pci_probe_handle(struct pci_dev *pdev)\n{\n\tacpi_handle dhandle, atpx_handle;\n\tacpi_status status;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATPX\", &atpx_handle);\n\tif (ACPI_FAILURE(status)) {\n\t\tamdgpu_atpx_priv.other_handle = dhandle;\n\t\treturn false;\n\t}\n\tamdgpu_atpx_priv.dhandle = dhandle;\n\tamdgpu_atpx_priv.atpx.handle = atpx_handle;\n\treturn true;\n}\n\n/**\n * amdgpu_atpx_init - verify the ATPX interface\n *\n * Verify the ATPX interface (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_init(void)\n{\n\tint r;\n\n\t/* set up the ATPX handle */\n\tr = amdgpu_atpx_verify_interface(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\t/* validate the atpx setup */\n\tr = amdgpu_atpx_validate(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_get_client_id - get the client id\n *\n * @pdev: pci device\n *\n * look up whether we are the integrated or discrete GPU (all asics).\n * Returns the client id.\n */\nstatic enum vga_switcheroo_client_id amdgpu_atpx_get_client_id(struct pci_dev *pdev)\n{\n\tif (amdgpu_atpx_priv.dhandle == ACPI_HANDLE(&pdev->dev))\n\t\treturn VGA_SWITCHEROO_IGD;\n\telse\n\t\treturn VGA_SWITCHEROO_DIS;\n}\n\nstatic const struct vga_switcheroo_handler amdgpu_atpx_handler = {\n\t.switchto = amdgpu_atpx_switchto,\n\t.power_state = amdgpu_atpx_power_state,\n\t.get_client_id = amdgpu_atpx_get_client_id,\n};\n\nstatic const struct amdgpu_px_quirk amdgpu_px_quirk_list[] = {\n\t/* HG _PR3 doesn't seem to work on this A+A weston board */\n\t{ 0x1002, 0x6900, 0x1002, 0x0124, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0812, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0813, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x699f, 0x1028, 0x0814, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1025, 0x125A, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x17AA, 0x3806, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0, 0, 0, 0, 0 },\n};\n\nstatic void amdgpu_atpx_get_quirks(struct pci_dev *pdev)\n{\n\tconst struct amdgpu_px_quirk *p = amdgpu_px_quirk_list;\n\n\t/* Apply PX quirks */\n\twhile (p && p->chip_device != 0) {\n\t\tif (pdev->vendor == p->chip_vendor &&\n\t\t    pdev->device == p->chip_device &&\n\t\t    pdev->subsystem_vendor == p->subsys_vendor &&\n\t\t    pdev->subsystem_device == p->subsys_device) {\n\t\t\tamdgpu_atpx_priv.quirks |= p->px_quirk_flags;\n\t\t\tbreak;\n\t\t}\n\t\t++p;\n\t}\n}\n\n/**\n * amdgpu_atpx_detect - detect whether we have PX\n *\n * Check if we have a PX system (all asics).\n * Returns true if we have a PX system, false if not.\n */\nstatic bool amdgpu_atpx_detect(void)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tstruct pci_dev *pdev = NULL;\n\tbool has_atpx = false;\n\tint vga_count = 0;\n\tbool d3_supported = false;\n\tstruct pci_dev *parent_pdev;\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_OTHER << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\tif (has_atpx && vga_count == 2) {\n\t\tacpi_get_name(amdgpu_atpx_priv.atpx.handle, ACPI_FULL_PATHNAME, &buffer);\n\t\tpr_info(\"vga_switcheroo: detected switching method %s handle\\n\",\n\t\t\tacpi_method_name);\n\t\tamdgpu_atpx_priv.atpx_detected = true;\n\t\tamdgpu_atpx_priv.bridge_pm_usable = d3_supported;\n\t\tamdgpu_atpx_init();\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * amdgpu_register_atpx_handler - register with vga_switcheroo\n *\n * Register the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_register_atpx_handler(void)\n{\n\tbool r;\n\tenum vga_switcheroo_handler_flags_t handler_flags = 0;\n\n\t/* detect if we have any ATPX + 2 VGA in the system */\n\tr = amdgpu_atpx_detect();\n\tif (!r)\n\t\treturn;\n\n\tvga_switcheroo_register_handler(&amdgpu_atpx_handler, handler_flags);\n}\n\n/**\n * amdgpu_unregister_atpx_handler - unregister with vga_switcheroo\n *\n * Unregister the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_unregister_atpx_handler(void)\n{\n\tvga_switcheroo_unregister_handler();\n}\n", "source_code_path": "amdgpu_atpx_handler.c", "line_number": 145, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atpx_handler.c\nLine: 145\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nThe function amdgpu_atpx_call allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER, but does not free this memory in all code paths. Specifically, when the function succeeds (i.e., when acpi_evaluate_object does not fail), the allocated memory is returned to the caller without being freed.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atpx_handler-bug.txt", "bug_report_text": "Issue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nThe function amdgpu_atpx_call allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER, but fails to release it in all scenarios. When the function succeeds, the allocated memory is returned to the caller without being freed, leading to a potential memory leak..\n\n", "diff_path": "amdgpu_atpx_handler-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n", "code": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright (c) 2010 Red Hat Inc.\n * Author : Dave Airlie <airlied@redhat.com>\n *\n * ATPX support for both Intel/ATI\n */\n#include <linux/vga_switcheroo.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n\n#include \"amdgpu.h\"\n#include \"amd_acpi.h\"\n\n#define AMDGPU_PX_QUIRK_FORCE_ATPX  (1 << 0)\n\nstruct amdgpu_px_quirk {\n\tu32 chip_vendor;\n\tu32 chip_device;\n\tu32 subsys_vendor;\n\tu32 subsys_device;\n\tu32 px_quirk_flags;\n};\n\nstruct amdgpu_atpx_functions {\n\tbool px_params;\n\tbool power_cntl;\n\tbool disp_mux_cntl;\n\tbool i2c_mux_cntl;\n\tbool switch_start;\n\tbool switch_end;\n\tbool disp_connectors_mapping;\n\tbool disp_detection_ports;\n};\n\nstruct amdgpu_atpx {\n\tacpi_handle handle;\n\tstruct amdgpu_atpx_functions functions;\n\tbool is_hybrid;\n\tbool dgpu_req_power_for_displays;\n};\n\nstatic struct amdgpu_atpx_priv {\n\tbool atpx_detected;\n\tbool bridge_pm_usable;\n\tunsigned int quirks;\n\t/* handle for device - and atpx */\n\tacpi_handle dhandle;\n\tacpi_handle other_handle;\n\tstruct amdgpu_atpx atpx;\n} amdgpu_atpx_priv;\n\nstruct atpx_verify_interface {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu16 version;\t\t/* version */\n\tu32 function_bits;\t/* supported functions bit vector */\n} __packed;\n\nstruct atpx_px_params {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu32 valid_flags;\t/* which flags are valid */\n\tu32 flags;\t\t/* flags */\n} __packed;\n\nstruct atpx_power_control {\n\tu16 size;\n\tu8 dgpu_state;\n} __packed;\n\nstruct atpx_mux {\n\tu16 size;\n\tu16 mux;\n} __packed;\n\nbool amdgpu_has_atpx(void)\n{\n\treturn amdgpu_atpx_priv.atpx_detected;\n}\n\nbool amdgpu_has_atpx_dgpu_power_cntl(void)\n{\n\treturn amdgpu_atpx_priv.atpx.functions.power_cntl;\n}\n\nbool amdgpu_is_atpx_hybrid(void)\n{\n\treturn amdgpu_atpx_priv.atpx.is_hybrid;\n}\n\nbool amdgpu_atpx_dgpu_req_power_for_displays(void)\n{\n\treturn amdgpu_atpx_priv.atpx.dgpu_req_power_for_displays;\n}\n\n#if defined(CONFIG_ACPI)\nvoid *amdgpu_atpx_get_dhandle(void)\n{\n\treturn amdgpu_atpx_priv.dhandle;\n}\n#endif\n\n/**\n * amdgpu_atpx_call - call an ATPX method\n *\n * @handle: acpi handle\n * @function: the ATPX function to execute\n * @params: ATPX function params\n *\n * Executes the requested ATPX function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atpx_arg_elements[2];\n\tstruct acpi_object_list atpx_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatpx_arg.count = 2;\n\tatpx_arg.pointer = &atpx_arg_elements[0];\n\n\tatpx_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatpx_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatpx_arg_elements[1].buffer.length = params->length;\n\t\tatpx_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatpx_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(handle, NULL, &atpx_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATPX is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atpx_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATPX\n *\n * Use the supported functions mask from ATPX function\n * ATPX_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atpx_parse_functions(struct amdgpu_atpx_functions *f, u32 mask)\n{\n\tf->px_params = mask & ATPX_GET_PX_PARAMETERS_SUPPORTED;\n\tf->power_cntl = mask & ATPX_POWER_CONTROL_SUPPORTED;\n\tf->disp_mux_cntl = mask & ATPX_DISPLAY_MUX_CONTROL_SUPPORTED;\n\tf->i2c_mux_cntl = mask & ATPX_I2C_MUX_CONTROL_SUPPORTED;\n\tf->switch_start = mask & ATPX_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION_SUPPORTED;\n\tf->switch_end = mask & ATPX_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION_SUPPORTED;\n\tf->disp_connectors_mapping = mask & ATPX_GET_DISPLAY_CONNECTORS_MAPPING_SUPPORTED;\n\tf->disp_detection_ports = mask & ATPX_GET_DISPLAY_DETECTION_PORTS_SUPPORTED;\n}\n\n/**\n * amdgpu_atpx_validate - validate ATPX functions\n *\n * @atpx: amdgpu atpx struct\n *\n * Validate that required functions are enabled (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)\n{\n\tu32 valid_bits = 0;\n\n\tif (atpx->functions.px_params) {\n\t\tunion acpi_object *info;\n\t\tstruct atpx_px_params output;\n\t\tsize_t size;\n\n\t\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_GET_PX_PARAMETERS, NULL);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&output, 0, sizeof(output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 10) {\n\t\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(output), size);\n\n\t\tmemcpy(&output, info->buffer.pointer, size);\n\n\t\tvalid_bits = output.flags & output.valid_flags;\n\n\t\tkfree(info);\n\t}\n\n\t/* if separate mux flag is set, mux controls are required */\n\tif (valid_bits & ATPX_SEPARATE_MUX_FOR_I2C) {\n\t\tatpx->functions.i2c_mux_cntl = true;\n\t\tatpx->functions.disp_mux_cntl = true;\n\t}\n\t/* if any outputs are muxed, mux controls are required */\n\tif (valid_bits & (ATPX_CRT1_RGB_SIGNAL_MUXED |\n\t\t\t  ATPX_TV_SIGNAL_MUXED |\n\t\t\t  ATPX_DFP_SIGNAL_MUXED))\n\t\tatpx->functions.disp_mux_cntl = true;\n\n\n\t/* some bioses set these bits rather than flagging power_cntl as supported */\n\tif (valid_bits & (ATPX_DYNAMIC_PX_SUPPORTED |\n\t\t\t  ATPX_DYNAMIC_DGPU_POWER_OFF_SUPPORTED))\n\t\tatpx->functions.power_cntl = true;\n\n\tatpx->is_hybrid = false;\n\tif (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {\n\t\tif (amdgpu_atpx_priv.quirks & AMDGPU_PX_QUIRK_FORCE_ATPX) {\n\t\t\tpr_warn(\"ATPX Hybrid Graphics, forcing to ATPX\\n\");\n\t\t\tatpx->functions.power_cntl = true;\n\t\t\tatpx->is_hybrid = false;\n\t\t} else {\n\t\t\tpr_notice(\"ATPX Hybrid Graphics\\n\");\n\t\t\t/*\n\t\t\t * Disable legacy PM methods only when pcie port PM is usable,\n\t\t\t * otherwise the device might fail to power off or power on.\n\t\t\t */\n\t\t\tatpx->functions.power_cntl = !amdgpu_atpx_priv.bridge_pm_usable;\n\t\t\tatpx->is_hybrid = true;\n\t\t}\n\t}\n\n\tatpx->dgpu_req_power_for_displays = false;\n\tif (valid_bits & ATPX_DGPU_REQ_POWER_FOR_DISPLAYS)\n\t\tatpx->dgpu_req_power_for_displays = true;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_verify_interface - verify ATPX\n *\n * @atpx: amdgpu atpx struct\n *\n * Execute the ATPX_FUNCTION_VERIFY_INTERFACE ATPX function\n * to initialize ATPX and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_verify_interface(struct amdgpu_atpx *atpx)\n{\n\tunion acpi_object *info;\n\tstruct atpx_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tpr_notice(\"ATPX version %u, functions 0x%08x\\n\",\n\t\t  output.version, output.function_bits);\n\n\tamdgpu_atpx_parse_functions(&atpx->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atpx_set_discrete_state - power up/down discrete GPU\n *\n * @atpx: atpx info struct\n * @state: discrete GPU state (0 = power down, 1 = power up)\n *\n * Execute the ATPX_FUNCTION_POWER_CONTROL ATPX function to\n * power down/up the discrete GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_set_discrete_state(struct amdgpu_atpx *atpx, u8 state)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_power_control input;\n\n\tif (atpx->functions.power_cntl) {\n\t\tinput.size = 3;\n\t\tinput.dgpu_state = state;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_POWER_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\n\t\t/* 200ms delay is required after off */\n\t\tif (state == 0)\n\t\t\tmsleep(200);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_disp_mux - switch display mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_DISPLAY_MUX_CONTROL ATPX function to\n * switch the display mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_disp_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.disp_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_DISPLAY_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_i2c_mux - switch i2c/hpd mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_I2C_MUX_CONTROL ATPX function to\n * switch the i2c/hpd mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_i2c_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.i2c_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_I2C_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_start - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has begun (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_start(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_start) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_end - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has ended (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_end(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_end) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switchto - switch to the requested GPU\n *\n * @id: GPU to switch to\n *\n * Execute the necessary ATPX functions to switch between the discrete GPU and\n * integrated GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switchto(enum vga_switcheroo_client_id id)\n{\n\tu16 gpu_id;\n\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\tgpu_id = ATPX_INTEGRATED_GPU;\n\telse\n\t\tgpu_id = ATPX_DISCRETE_GPU;\n\n\tamdgpu_atpx_switch_start(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_disp_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_i2c_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_end(&amdgpu_atpx_priv.atpx, gpu_id);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_power_state - power down/up the requested GPU\n *\n * @id: GPU to power down/up\n * @state: requested power state (0 = off, 1 = on)\n *\n * Execute the necessary ATPX function to power down/up the discrete GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_power_state(enum vga_switcheroo_client_id id,\n\t\t\t\t   enum vga_switcheroo_state state)\n{\n\t/* on w500 ACPI can't change intel gpu state */\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\treturn 0;\n\n\tamdgpu_atpx_set_discrete_state(&amdgpu_atpx_priv.atpx, state);\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_pci_probe_handle - look up the ATPX handle\n *\n * @pdev: pci device\n *\n * Look up the ATPX handles (all asics).\n * Returns true if the handles are found, false if not.\n */\nstatic bool amdgpu_atpx_pci_probe_handle(struct pci_dev *pdev)\n{\n\tacpi_handle dhandle, atpx_handle;\n\tacpi_status status;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATPX\", &atpx_handle);\n\tif (ACPI_FAILURE(status)) {\n\t\tamdgpu_atpx_priv.other_handle = dhandle;\n\t\treturn false;\n\t}\n\tamdgpu_atpx_priv.dhandle = dhandle;\n\tamdgpu_atpx_priv.atpx.handle = atpx_handle;\n\treturn true;\n}\n\n/**\n * amdgpu_atpx_init - verify the ATPX interface\n *\n * Verify the ATPX interface (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_init(void)\n{\n\tint r;\n\n\t/* set up the ATPX handle */\n\tr = amdgpu_atpx_verify_interface(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\t/* validate the atpx setup */\n\tr = amdgpu_atpx_validate(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_get_client_id - get the client id\n *\n * @pdev: pci device\n *\n * look up whether we are the integrated or discrete GPU (all asics).\n * Returns the client id.\n */\nstatic enum vga_switcheroo_client_id amdgpu_atpx_get_client_id(struct pci_dev *pdev)\n{\n\tif (amdgpu_atpx_priv.dhandle == ACPI_HANDLE(&pdev->dev))\n\t\treturn VGA_SWITCHEROO_IGD;\n\telse\n\t\treturn VGA_SWITCHEROO_DIS;\n}\n\nstatic const struct vga_switcheroo_handler amdgpu_atpx_handler = {\n\t.switchto = amdgpu_atpx_switchto,\n\t.power_state = amdgpu_atpx_power_state,\n\t.get_client_id = amdgpu_atpx_get_client_id,\n};\n\nstatic const struct amdgpu_px_quirk amdgpu_px_quirk_list[] = {\n\t/* HG _PR3 doesn't seem to work on this A+A weston board */\n\t{ 0x1002, 0x6900, 0x1002, 0x0124, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0812, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0813, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x699f, 0x1028, 0x0814, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1025, 0x125A, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x17AA, 0x3806, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0, 0, 0, 0, 0 },\n};\n\nstatic void amdgpu_atpx_get_quirks(struct pci_dev *pdev)\n{\n\tconst struct amdgpu_px_quirk *p = amdgpu_px_quirk_list;\n\n\t/* Apply PX quirks */\n\twhile (p && p->chip_device != 0) {\n\t\tif (pdev->vendor == p->chip_vendor &&\n\t\t    pdev->device == p->chip_device &&\n\t\t    pdev->subsystem_vendor == p->subsys_vendor &&\n\t\t    pdev->subsystem_device == p->subsys_device) {\n\t\t\tamdgpu_atpx_priv.quirks |= p->px_quirk_flags;\n\t\t\tbreak;\n\t\t}\n\t\t++p;\n\t}\n}\n\n/**\n * amdgpu_atpx_detect - detect whether we have PX\n *\n * Check if we have a PX system (all asics).\n * Returns true if we have a PX system, false if not.\n */\nstatic bool amdgpu_atpx_detect(void)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tstruct pci_dev *pdev = NULL;\n\tbool has_atpx = false;\n\tint vga_count = 0;\n\tbool d3_supported = false;\n\tstruct pci_dev *parent_pdev;\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_OTHER << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\tif (has_atpx && vga_count == 2) {\n\t\tacpi_get_name(amdgpu_atpx_priv.atpx.handle, ACPI_FULL_PATHNAME, &buffer);\n\t\tpr_info(\"vga_switcheroo: detected switching method %s handle\\n\",\n\t\t\tacpi_method_name);\n\t\tamdgpu_atpx_priv.atpx_detected = true;\n\t\tamdgpu_atpx_priv.bridge_pm_usable = d3_supported;\n\t\tamdgpu_atpx_init();\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * amdgpu_register_atpx_handler - register with vga_switcheroo\n *\n * Register the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_register_atpx_handler(void)\n{\n\tbool r;\n\tenum vga_switcheroo_handler_flags_t handler_flags = 0;\n\n\t/* detect if we have any ATPX + 2 VGA in the system */\n\tr = amdgpu_atpx_detect();\n\tif (!r)\n\t\treturn;\n\n\tvga_switcheroo_register_handler(&amdgpu_atpx_handler, handler_flags);\n}\n\n/**\n * amdgpu_unregister_atpx_handler - unregister with vga_switcheroo\n *\n * Unregister the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_unregister_atpx_handler(void)\n{\n\tvga_switcheroo_unregister_handler();\n}\n", "source_code_path": "amdgpu_atpx_handler.c", "line_number": 145, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atpx_handler.c\nLine: 145\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nThe function amdgpu_atpx_call allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER, but fails to release it in all scenarios. When the function succeeds, the allocated memory is returned to the caller without being freed, leading to a potential memory leak..\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atpx_handler-bug.txt", "bug_report_text": "Issue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nA memory leak vulnerability exists in the function amdgpu_atpx_call, which allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER but neglects to free it in all code paths. Specifically, when the function succeeds, the allocated memory is not released, resulting in a potential memory leak.\n\n", "diff_path": "amdgpu_atpx_handler-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n", "code": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright (c) 2010 Red Hat Inc.\n * Author : Dave Airlie <airlied@redhat.com>\n *\n * ATPX support for both Intel/ATI\n */\n#include <linux/vga_switcheroo.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n\n#include \"amdgpu.h\"\n#include \"amd_acpi.h\"\n\n#define AMDGPU_PX_QUIRK_FORCE_ATPX  (1 << 0)\n\nstruct amdgpu_px_quirk {\n\tu32 chip_vendor;\n\tu32 chip_device;\n\tu32 subsys_vendor;\n\tu32 subsys_device;\n\tu32 px_quirk_flags;\n};\n\nstruct amdgpu_atpx_functions {\n\tbool px_params;\n\tbool power_cntl;\n\tbool disp_mux_cntl;\n\tbool i2c_mux_cntl;\n\tbool switch_start;\n\tbool switch_end;\n\tbool disp_connectors_mapping;\n\tbool disp_detection_ports;\n};\n\nstruct amdgpu_atpx {\n\tacpi_handle handle;\n\tstruct amdgpu_atpx_functions functions;\n\tbool is_hybrid;\n\tbool dgpu_req_power_for_displays;\n};\n\nstatic struct amdgpu_atpx_priv {\n\tbool atpx_detected;\n\tbool bridge_pm_usable;\n\tunsigned int quirks;\n\t/* handle for device - and atpx */\n\tacpi_handle dhandle;\n\tacpi_handle other_handle;\n\tstruct amdgpu_atpx atpx;\n} amdgpu_atpx_priv;\n\nstruct atpx_verify_interface {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu16 version;\t\t/* version */\n\tu32 function_bits;\t/* supported functions bit vector */\n} __packed;\n\nstruct atpx_px_params {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu32 valid_flags;\t/* which flags are valid */\n\tu32 flags;\t\t/* flags */\n} __packed;\n\nstruct atpx_power_control {\n\tu16 size;\n\tu8 dgpu_state;\n} __packed;\n\nstruct atpx_mux {\n\tu16 size;\n\tu16 mux;\n} __packed;\n\nbool amdgpu_has_atpx(void)\n{\n\treturn amdgpu_atpx_priv.atpx_detected;\n}\n\nbool amdgpu_has_atpx_dgpu_power_cntl(void)\n{\n\treturn amdgpu_atpx_priv.atpx.functions.power_cntl;\n}\n\nbool amdgpu_is_atpx_hybrid(void)\n{\n\treturn amdgpu_atpx_priv.atpx.is_hybrid;\n}\n\nbool amdgpu_atpx_dgpu_req_power_for_displays(void)\n{\n\treturn amdgpu_atpx_priv.atpx.dgpu_req_power_for_displays;\n}\n\n#if defined(CONFIG_ACPI)\nvoid *amdgpu_atpx_get_dhandle(void)\n{\n\treturn amdgpu_atpx_priv.dhandle;\n}\n#endif\n\n/**\n * amdgpu_atpx_call - call an ATPX method\n *\n * @handle: acpi handle\n * @function: the ATPX function to execute\n * @params: ATPX function params\n *\n * Executes the requested ATPX function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atpx_arg_elements[2];\n\tstruct acpi_object_list atpx_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatpx_arg.count = 2;\n\tatpx_arg.pointer = &atpx_arg_elements[0];\n\n\tatpx_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatpx_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatpx_arg_elements[1].buffer.length = params->length;\n\t\tatpx_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatpx_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(handle, NULL, &atpx_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATPX is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atpx_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATPX\n *\n * Use the supported functions mask from ATPX function\n * ATPX_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atpx_parse_functions(struct amdgpu_atpx_functions *f, u32 mask)\n{\n\tf->px_params = mask & ATPX_GET_PX_PARAMETERS_SUPPORTED;\n\tf->power_cntl = mask & ATPX_POWER_CONTROL_SUPPORTED;\n\tf->disp_mux_cntl = mask & ATPX_DISPLAY_MUX_CONTROL_SUPPORTED;\n\tf->i2c_mux_cntl = mask & ATPX_I2C_MUX_CONTROL_SUPPORTED;\n\tf->switch_start = mask & ATPX_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION_SUPPORTED;\n\tf->switch_end = mask & ATPX_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION_SUPPORTED;\n\tf->disp_connectors_mapping = mask & ATPX_GET_DISPLAY_CONNECTORS_MAPPING_SUPPORTED;\n\tf->disp_detection_ports = mask & ATPX_GET_DISPLAY_DETECTION_PORTS_SUPPORTED;\n}\n\n/**\n * amdgpu_atpx_validate - validate ATPX functions\n *\n * @atpx: amdgpu atpx struct\n *\n * Validate that required functions are enabled (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)\n{\n\tu32 valid_bits = 0;\n\n\tif (atpx->functions.px_params) {\n\t\tunion acpi_object *info;\n\t\tstruct atpx_px_params output;\n\t\tsize_t size;\n\n\t\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_GET_PX_PARAMETERS, NULL);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&output, 0, sizeof(output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 10) {\n\t\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(output), size);\n\n\t\tmemcpy(&output, info->buffer.pointer, size);\n\n\t\tvalid_bits = output.flags & output.valid_flags;\n\n\t\tkfree(info);\n\t}\n\n\t/* if separate mux flag is set, mux controls are required */\n\tif (valid_bits & ATPX_SEPARATE_MUX_FOR_I2C) {\n\t\tatpx->functions.i2c_mux_cntl = true;\n\t\tatpx->functions.disp_mux_cntl = true;\n\t}\n\t/* if any outputs are muxed, mux controls are required */\n\tif (valid_bits & (ATPX_CRT1_RGB_SIGNAL_MUXED |\n\t\t\t  ATPX_TV_SIGNAL_MUXED |\n\t\t\t  ATPX_DFP_SIGNAL_MUXED))\n\t\tatpx->functions.disp_mux_cntl = true;\n\n\n\t/* some bioses set these bits rather than flagging power_cntl as supported */\n\tif (valid_bits & (ATPX_DYNAMIC_PX_SUPPORTED |\n\t\t\t  ATPX_DYNAMIC_DGPU_POWER_OFF_SUPPORTED))\n\t\tatpx->functions.power_cntl = true;\n\n\tatpx->is_hybrid = false;\n\tif (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {\n\t\tif (amdgpu_atpx_priv.quirks & AMDGPU_PX_QUIRK_FORCE_ATPX) {\n\t\t\tpr_warn(\"ATPX Hybrid Graphics, forcing to ATPX\\n\");\n\t\t\tatpx->functions.power_cntl = true;\n\t\t\tatpx->is_hybrid = false;\n\t\t} else {\n\t\t\tpr_notice(\"ATPX Hybrid Graphics\\n\");\n\t\t\t/*\n\t\t\t * Disable legacy PM methods only when pcie port PM is usable,\n\t\t\t * otherwise the device might fail to power off or power on.\n\t\t\t */\n\t\t\tatpx->functions.power_cntl = !amdgpu_atpx_priv.bridge_pm_usable;\n\t\t\tatpx->is_hybrid = true;\n\t\t}\n\t}\n\n\tatpx->dgpu_req_power_for_displays = false;\n\tif (valid_bits & ATPX_DGPU_REQ_POWER_FOR_DISPLAYS)\n\t\tatpx->dgpu_req_power_for_displays = true;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_verify_interface - verify ATPX\n *\n * @atpx: amdgpu atpx struct\n *\n * Execute the ATPX_FUNCTION_VERIFY_INTERFACE ATPX function\n * to initialize ATPX and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_verify_interface(struct amdgpu_atpx *atpx)\n{\n\tunion acpi_object *info;\n\tstruct atpx_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tpr_notice(\"ATPX version %u, functions 0x%08x\\n\",\n\t\t  output.version, output.function_bits);\n\n\tamdgpu_atpx_parse_functions(&atpx->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atpx_set_discrete_state - power up/down discrete GPU\n *\n * @atpx: atpx info struct\n * @state: discrete GPU state (0 = power down, 1 = power up)\n *\n * Execute the ATPX_FUNCTION_POWER_CONTROL ATPX function to\n * power down/up the discrete GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_set_discrete_state(struct amdgpu_atpx *atpx, u8 state)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_power_control input;\n\n\tif (atpx->functions.power_cntl) {\n\t\tinput.size = 3;\n\t\tinput.dgpu_state = state;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_POWER_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\n\t\t/* 200ms delay is required after off */\n\t\tif (state == 0)\n\t\t\tmsleep(200);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_disp_mux - switch display mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_DISPLAY_MUX_CONTROL ATPX function to\n * switch the display mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_disp_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.disp_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_DISPLAY_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_i2c_mux - switch i2c/hpd mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_I2C_MUX_CONTROL ATPX function to\n * switch the i2c/hpd mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_i2c_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.i2c_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_I2C_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_start - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has begun (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_start(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_start) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_end - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has ended (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_end(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_end) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switchto - switch to the requested GPU\n *\n * @id: GPU to switch to\n *\n * Execute the necessary ATPX functions to switch between the discrete GPU and\n * integrated GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switchto(enum vga_switcheroo_client_id id)\n{\n\tu16 gpu_id;\n\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\tgpu_id = ATPX_INTEGRATED_GPU;\n\telse\n\t\tgpu_id = ATPX_DISCRETE_GPU;\n\n\tamdgpu_atpx_switch_start(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_disp_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_i2c_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_end(&amdgpu_atpx_priv.atpx, gpu_id);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_power_state - power down/up the requested GPU\n *\n * @id: GPU to power down/up\n * @state: requested power state (0 = off, 1 = on)\n *\n * Execute the necessary ATPX function to power down/up the discrete GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_power_state(enum vga_switcheroo_client_id id,\n\t\t\t\t   enum vga_switcheroo_state state)\n{\n\t/* on w500 ACPI can't change intel gpu state */\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\treturn 0;\n\n\tamdgpu_atpx_set_discrete_state(&amdgpu_atpx_priv.atpx, state);\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_pci_probe_handle - look up the ATPX handle\n *\n * @pdev: pci device\n *\n * Look up the ATPX handles (all asics).\n * Returns true if the handles are found, false if not.\n */\nstatic bool amdgpu_atpx_pci_probe_handle(struct pci_dev *pdev)\n{\n\tacpi_handle dhandle, atpx_handle;\n\tacpi_status status;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATPX\", &atpx_handle);\n\tif (ACPI_FAILURE(status)) {\n\t\tamdgpu_atpx_priv.other_handle = dhandle;\n\t\treturn false;\n\t}\n\tamdgpu_atpx_priv.dhandle = dhandle;\n\tamdgpu_atpx_priv.atpx.handle = atpx_handle;\n\treturn true;\n}\n\n/**\n * amdgpu_atpx_init - verify the ATPX interface\n *\n * Verify the ATPX interface (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_init(void)\n{\n\tint r;\n\n\t/* set up the ATPX handle */\n\tr = amdgpu_atpx_verify_interface(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\t/* validate the atpx setup */\n\tr = amdgpu_atpx_validate(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_get_client_id - get the client id\n *\n * @pdev: pci device\n *\n * look up whether we are the integrated or discrete GPU (all asics).\n * Returns the client id.\n */\nstatic enum vga_switcheroo_client_id amdgpu_atpx_get_client_id(struct pci_dev *pdev)\n{\n\tif (amdgpu_atpx_priv.dhandle == ACPI_HANDLE(&pdev->dev))\n\t\treturn VGA_SWITCHEROO_IGD;\n\telse\n\t\treturn VGA_SWITCHEROO_DIS;\n}\n\nstatic const struct vga_switcheroo_handler amdgpu_atpx_handler = {\n\t.switchto = amdgpu_atpx_switchto,\n\t.power_state = amdgpu_atpx_power_state,\n\t.get_client_id = amdgpu_atpx_get_client_id,\n};\n\nstatic const struct amdgpu_px_quirk amdgpu_px_quirk_list[] = {\n\t/* HG _PR3 doesn't seem to work on this A+A weston board */\n\t{ 0x1002, 0x6900, 0x1002, 0x0124, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0812, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0813, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x699f, 0x1028, 0x0814, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1025, 0x125A, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x17AA, 0x3806, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0, 0, 0, 0, 0 },\n};\n\nstatic void amdgpu_atpx_get_quirks(struct pci_dev *pdev)\n{\n\tconst struct amdgpu_px_quirk *p = amdgpu_px_quirk_list;\n\n\t/* Apply PX quirks */\n\twhile (p && p->chip_device != 0) {\n\t\tif (pdev->vendor == p->chip_vendor &&\n\t\t    pdev->device == p->chip_device &&\n\t\t    pdev->subsystem_vendor == p->subsys_vendor &&\n\t\t    pdev->subsystem_device == p->subsys_device) {\n\t\t\tamdgpu_atpx_priv.quirks |= p->px_quirk_flags;\n\t\t\tbreak;\n\t\t}\n\t\t++p;\n\t}\n}\n\n/**\n * amdgpu_atpx_detect - detect whether we have PX\n *\n * Check if we have a PX system (all asics).\n * Returns true if we have a PX system, false if not.\n */\nstatic bool amdgpu_atpx_detect(void)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tstruct pci_dev *pdev = NULL;\n\tbool has_atpx = false;\n\tint vga_count = 0;\n\tbool d3_supported = false;\n\tstruct pci_dev *parent_pdev;\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_OTHER << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\tif (has_atpx && vga_count == 2) {\n\t\tacpi_get_name(amdgpu_atpx_priv.atpx.handle, ACPI_FULL_PATHNAME, &buffer);\n\t\tpr_info(\"vga_switcheroo: detected switching method %s handle\\n\",\n\t\t\tacpi_method_name);\n\t\tamdgpu_atpx_priv.atpx_detected = true;\n\t\tamdgpu_atpx_priv.bridge_pm_usable = d3_supported;\n\t\tamdgpu_atpx_init();\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * amdgpu_register_atpx_handler - register with vga_switcheroo\n *\n * Register the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_register_atpx_handler(void)\n{\n\tbool r;\n\tenum vga_switcheroo_handler_flags_t handler_flags = 0;\n\n\t/* detect if we have any ATPX + 2 VGA in the system */\n\tr = amdgpu_atpx_detect();\n\tif (!r)\n\t\treturn;\n\n\tvga_switcheroo_register_handler(&amdgpu_atpx_handler, handler_flags);\n}\n\n/**\n * amdgpu_unregister_atpx_handler - unregister with vga_switcheroo\n *\n * Unregister the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_unregister_atpx_handler(void)\n{\n\tvga_switcheroo_unregister_handler();\n}\n", "source_code_path": "amdgpu_atpx_handler.c", "line_number": 145, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atpx_handler.c\nLine: 145\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nA memory leak vulnerability exists in the function amdgpu_atpx_call, which allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER but neglects to free it in all code paths. Specifically, when the function succeeds, the allocated memory is not released, resulting in a potential memory leak.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_atpx_handler-bug.txt", "bug_report_text": "Issue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nThe function amdgpu_atpx_call allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER, but fails to properly handle memory deallocation in all cases. When the function succeeds, the allocated memory is not released, leading to a memory leak and potential system resource exhaustion.\n\n", "diff_path": "amdgpu_atpx_handler-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n", "code": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright (c) 2010 Red Hat Inc.\n * Author : Dave Airlie <airlied@redhat.com>\n *\n * ATPX support for both Intel/ATI\n */\n#include <linux/vga_switcheroo.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n\n#include \"amdgpu.h\"\n#include \"amd_acpi.h\"\n\n#define AMDGPU_PX_QUIRK_FORCE_ATPX  (1 << 0)\n\nstruct amdgpu_px_quirk {\n\tu32 chip_vendor;\n\tu32 chip_device;\n\tu32 subsys_vendor;\n\tu32 subsys_device;\n\tu32 px_quirk_flags;\n};\n\nstruct amdgpu_atpx_functions {\n\tbool px_params;\n\tbool power_cntl;\n\tbool disp_mux_cntl;\n\tbool i2c_mux_cntl;\n\tbool switch_start;\n\tbool switch_end;\n\tbool disp_connectors_mapping;\n\tbool disp_detection_ports;\n};\n\nstruct amdgpu_atpx {\n\tacpi_handle handle;\n\tstruct amdgpu_atpx_functions functions;\n\tbool is_hybrid;\n\tbool dgpu_req_power_for_displays;\n};\n\nstatic struct amdgpu_atpx_priv {\n\tbool atpx_detected;\n\tbool bridge_pm_usable;\n\tunsigned int quirks;\n\t/* handle for device - and atpx */\n\tacpi_handle dhandle;\n\tacpi_handle other_handle;\n\tstruct amdgpu_atpx atpx;\n} amdgpu_atpx_priv;\n\nstruct atpx_verify_interface {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu16 version;\t\t/* version */\n\tu32 function_bits;\t/* supported functions bit vector */\n} __packed;\n\nstruct atpx_px_params {\n\tu16 size;\t\t/* structure size in bytes (includes size field) */\n\tu32 valid_flags;\t/* which flags are valid */\n\tu32 flags;\t\t/* flags */\n} __packed;\n\nstruct atpx_power_control {\n\tu16 size;\n\tu8 dgpu_state;\n} __packed;\n\nstruct atpx_mux {\n\tu16 size;\n\tu16 mux;\n} __packed;\n\nbool amdgpu_has_atpx(void)\n{\n\treturn amdgpu_atpx_priv.atpx_detected;\n}\n\nbool amdgpu_has_atpx_dgpu_power_cntl(void)\n{\n\treturn amdgpu_atpx_priv.atpx.functions.power_cntl;\n}\n\nbool amdgpu_is_atpx_hybrid(void)\n{\n\treturn amdgpu_atpx_priv.atpx.is_hybrid;\n}\n\nbool amdgpu_atpx_dgpu_req_power_for_displays(void)\n{\n\treturn amdgpu_atpx_priv.atpx.dgpu_req_power_for_displays;\n}\n\n#if defined(CONFIG_ACPI)\nvoid *amdgpu_atpx_get_dhandle(void)\n{\n\treturn amdgpu_atpx_priv.dhandle;\n}\n#endif\n\n/**\n * amdgpu_atpx_call - call an ATPX method\n *\n * @handle: acpi handle\n * @function: the ATPX function to execute\n * @params: ATPX function params\n *\n * Executes the requested ATPX function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atpx_arg_elements[2];\n\tstruct acpi_object_list atpx_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatpx_arg.count = 2;\n\tatpx_arg.pointer = &atpx_arg_elements[0];\n\n\tatpx_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatpx_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatpx_arg_elements[1].buffer.length = params->length;\n\t\tatpx_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatpx_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatpx_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(handle, NULL, &atpx_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATPX is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atpx_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATPX\n *\n * Use the supported functions mask from ATPX function\n * ATPX_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atpx_parse_functions(struct amdgpu_atpx_functions *f, u32 mask)\n{\n\tf->px_params = mask & ATPX_GET_PX_PARAMETERS_SUPPORTED;\n\tf->power_cntl = mask & ATPX_POWER_CONTROL_SUPPORTED;\n\tf->disp_mux_cntl = mask & ATPX_DISPLAY_MUX_CONTROL_SUPPORTED;\n\tf->i2c_mux_cntl = mask & ATPX_I2C_MUX_CONTROL_SUPPORTED;\n\tf->switch_start = mask & ATPX_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION_SUPPORTED;\n\tf->switch_end = mask & ATPX_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION_SUPPORTED;\n\tf->disp_connectors_mapping = mask & ATPX_GET_DISPLAY_CONNECTORS_MAPPING_SUPPORTED;\n\tf->disp_detection_ports = mask & ATPX_GET_DISPLAY_DETECTION_PORTS_SUPPORTED;\n}\n\n/**\n * amdgpu_atpx_validate - validate ATPX functions\n *\n * @atpx: amdgpu atpx struct\n *\n * Validate that required functions are enabled (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_validate(struct amdgpu_atpx *atpx)\n{\n\tu32 valid_bits = 0;\n\n\tif (atpx->functions.px_params) {\n\t\tunion acpi_object *info;\n\t\tstruct atpx_px_params output;\n\t\tsize_t size;\n\n\t\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_GET_PX_PARAMETERS, NULL);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&output, 0, sizeof(output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 10) {\n\t\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(output), size);\n\n\t\tmemcpy(&output, info->buffer.pointer, size);\n\n\t\tvalid_bits = output.flags & output.valid_flags;\n\n\t\tkfree(info);\n\t}\n\n\t/* if separate mux flag is set, mux controls are required */\n\tif (valid_bits & ATPX_SEPARATE_MUX_FOR_I2C) {\n\t\tatpx->functions.i2c_mux_cntl = true;\n\t\tatpx->functions.disp_mux_cntl = true;\n\t}\n\t/* if any outputs are muxed, mux controls are required */\n\tif (valid_bits & (ATPX_CRT1_RGB_SIGNAL_MUXED |\n\t\t\t  ATPX_TV_SIGNAL_MUXED |\n\t\t\t  ATPX_DFP_SIGNAL_MUXED))\n\t\tatpx->functions.disp_mux_cntl = true;\n\n\n\t/* some bioses set these bits rather than flagging power_cntl as supported */\n\tif (valid_bits & (ATPX_DYNAMIC_PX_SUPPORTED |\n\t\t\t  ATPX_DYNAMIC_DGPU_POWER_OFF_SUPPORTED))\n\t\tatpx->functions.power_cntl = true;\n\n\tatpx->is_hybrid = false;\n\tif (valid_bits & ATPX_MS_HYBRID_GFX_SUPPORTED) {\n\t\tif (amdgpu_atpx_priv.quirks & AMDGPU_PX_QUIRK_FORCE_ATPX) {\n\t\t\tpr_warn(\"ATPX Hybrid Graphics, forcing to ATPX\\n\");\n\t\t\tatpx->functions.power_cntl = true;\n\t\t\tatpx->is_hybrid = false;\n\t\t} else {\n\t\t\tpr_notice(\"ATPX Hybrid Graphics\\n\");\n\t\t\t/*\n\t\t\t * Disable legacy PM methods only when pcie port PM is usable,\n\t\t\t * otherwise the device might fail to power off or power on.\n\t\t\t */\n\t\t\tatpx->functions.power_cntl = !amdgpu_atpx_priv.bridge_pm_usable;\n\t\t\tatpx->is_hybrid = true;\n\t\t}\n\t}\n\n\tatpx->dgpu_req_power_for_displays = false;\n\tif (valid_bits & ATPX_DGPU_REQ_POWER_FOR_DISPLAYS)\n\t\tatpx->dgpu_req_power_for_displays = true;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_verify_interface - verify ATPX\n *\n * @atpx: amdgpu atpx struct\n *\n * Execute the ATPX_FUNCTION_VERIFY_INTERFACE ATPX function\n * to initialize ATPX and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_verify_interface(struct amdgpu_atpx *atpx)\n{\n\tunion acpi_object *info;\n\tstruct atpx_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atpx_call(atpx->handle, ATPX_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tpr_err(\"ATPX buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tpr_notice(\"ATPX version %u, functions 0x%08x\\n\",\n\t\t  output.version, output.function_bits);\n\n\tamdgpu_atpx_parse_functions(&atpx->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atpx_set_discrete_state - power up/down discrete GPU\n *\n * @atpx: atpx info struct\n * @state: discrete GPU state (0 = power down, 1 = power up)\n *\n * Execute the ATPX_FUNCTION_POWER_CONTROL ATPX function to\n * power down/up the discrete GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_set_discrete_state(struct amdgpu_atpx *atpx, u8 state)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_power_control input;\n\n\tif (atpx->functions.power_cntl) {\n\t\tinput.size = 3;\n\t\tinput.dgpu_state = state;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_POWER_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\n\t\t/* 200ms delay is required after off */\n\t\tif (state == 0)\n\t\t\tmsleep(200);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_disp_mux - switch display mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_DISPLAY_MUX_CONTROL ATPX function to\n * switch the display mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_disp_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.disp_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_DISPLAY_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_i2c_mux - switch i2c/hpd mux\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_I2C_MUX_CONTROL ATPX function to\n * switch the i2c/hpd mux between the discrete GPU and integrated GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_i2c_mux(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.i2c_mux_cntl) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_I2C_MUX_CONTROL,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_start - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has begun (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_start(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_start) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_START_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switch_end - notify the sbios of a GPU switch\n *\n * @atpx: atpx info struct\n * @mux_id: mux state (0 = integrated GPU, 1 = discrete GPU)\n *\n * Execute the ATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION ATPX\n * function to notify the sbios that a switch between the discrete GPU and\n * integrated GPU has ended (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switch_end(struct amdgpu_atpx *atpx, u16 mux_id)\n{\n\tstruct acpi_buffer params;\n\tunion acpi_object *info;\n\tstruct atpx_mux input;\n\n\tif (atpx->functions.switch_end) {\n\t\tinput.size = 4;\n\t\tinput.mux = mux_id;\n\t\tparams.length = input.size;\n\t\tparams.pointer = &input;\n\t\tinfo = amdgpu_atpx_call(atpx->handle,\n\t\t\t\t\tATPX_FUNCTION_GRAPHICS_DEVICE_SWITCH_END_NOTIFICATION,\n\t\t\t\t\t&params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\t\tkfree(info);\n\t}\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_switchto - switch to the requested GPU\n *\n * @id: GPU to switch to\n *\n * Execute the necessary ATPX functions to switch between the discrete GPU and\n * integrated GPU (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_switchto(enum vga_switcheroo_client_id id)\n{\n\tu16 gpu_id;\n\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\tgpu_id = ATPX_INTEGRATED_GPU;\n\telse\n\t\tgpu_id = ATPX_DISCRETE_GPU;\n\n\tamdgpu_atpx_switch_start(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_disp_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_i2c_mux(&amdgpu_atpx_priv.atpx, gpu_id);\n\tamdgpu_atpx_switch_end(&amdgpu_atpx_priv.atpx, gpu_id);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_power_state - power down/up the requested GPU\n *\n * @id: GPU to power down/up\n * @state: requested power state (0 = off, 1 = on)\n *\n * Execute the necessary ATPX function to power down/up the discrete GPU\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_power_state(enum vga_switcheroo_client_id id,\n\t\t\t\t   enum vga_switcheroo_state state)\n{\n\t/* on w500 ACPI can't change intel gpu state */\n\tif (id == VGA_SWITCHEROO_IGD)\n\t\treturn 0;\n\n\tamdgpu_atpx_set_discrete_state(&amdgpu_atpx_priv.atpx, state);\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_pci_probe_handle - look up the ATPX handle\n *\n * @pdev: pci device\n *\n * Look up the ATPX handles (all asics).\n * Returns true if the handles are found, false if not.\n */\nstatic bool amdgpu_atpx_pci_probe_handle(struct pci_dev *pdev)\n{\n\tacpi_handle dhandle, atpx_handle;\n\tacpi_status status;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATPX\", &atpx_handle);\n\tif (ACPI_FAILURE(status)) {\n\t\tamdgpu_atpx_priv.other_handle = dhandle;\n\t\treturn false;\n\t}\n\tamdgpu_atpx_priv.dhandle = dhandle;\n\tamdgpu_atpx_priv.atpx.handle = atpx_handle;\n\treturn true;\n}\n\n/**\n * amdgpu_atpx_init - verify the ATPX interface\n *\n * Verify the ATPX interface (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atpx_init(void)\n{\n\tint r;\n\n\t/* set up the ATPX handle */\n\tr = amdgpu_atpx_verify_interface(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\t/* validate the atpx setup */\n\tr = amdgpu_atpx_validate(&amdgpu_atpx_priv.atpx);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atpx_get_client_id - get the client id\n *\n * @pdev: pci device\n *\n * look up whether we are the integrated or discrete GPU (all asics).\n * Returns the client id.\n */\nstatic enum vga_switcheroo_client_id amdgpu_atpx_get_client_id(struct pci_dev *pdev)\n{\n\tif (amdgpu_atpx_priv.dhandle == ACPI_HANDLE(&pdev->dev))\n\t\treturn VGA_SWITCHEROO_IGD;\n\telse\n\t\treturn VGA_SWITCHEROO_DIS;\n}\n\nstatic const struct vga_switcheroo_handler amdgpu_atpx_handler = {\n\t.switchto = amdgpu_atpx_switchto,\n\t.power_state = amdgpu_atpx_power_state,\n\t.get_client_id = amdgpu_atpx_get_client_id,\n};\n\nstatic const struct amdgpu_px_quirk amdgpu_px_quirk_list[] = {\n\t/* HG _PR3 doesn't seem to work on this A+A weston board */\n\t{ 0x1002, 0x6900, 0x1002, 0x0124, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0812, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1028, 0x0813, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x699f, 0x1028, 0x0814, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x1025, 0x125A, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0x1002, 0x6900, 0x17AA, 0x3806, AMDGPU_PX_QUIRK_FORCE_ATPX },\n\t{ 0, 0, 0, 0, 0 },\n};\n\nstatic void amdgpu_atpx_get_quirks(struct pci_dev *pdev)\n{\n\tconst struct amdgpu_px_quirk *p = amdgpu_px_quirk_list;\n\n\t/* Apply PX quirks */\n\twhile (p && p->chip_device != 0) {\n\t\tif (pdev->vendor == p->chip_vendor &&\n\t\t    pdev->device == p->chip_device &&\n\t\t    pdev->subsystem_vendor == p->subsys_vendor &&\n\t\t    pdev->subsystem_device == p->subsys_device) {\n\t\t\tamdgpu_atpx_priv.quirks |= p->px_quirk_flags;\n\t\t\tbreak;\n\t\t}\n\t\t++p;\n\t}\n}\n\n/**\n * amdgpu_atpx_detect - detect whether we have PX\n *\n * Check if we have a PX system (all asics).\n * Returns true if we have a PX system, false if not.\n */\nstatic bool amdgpu_atpx_detect(void)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tstruct pci_dev *pdev = NULL;\n\tbool has_atpx = false;\n\tint vga_count = 0;\n\tbool d3_supported = false;\n\tstruct pci_dev *parent_pdev;\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\twhile ((pdev = pci_get_class(PCI_CLASS_DISPLAY_OTHER << 8, pdev)) != NULL) {\n\t\tvga_count++;\n\n\t\thas_atpx |= amdgpu_atpx_pci_probe_handle(pdev);\n\n\t\tparent_pdev = pci_upstream_bridge(pdev);\n\t\td3_supported |= parent_pdev && parent_pdev->bridge_d3;\n\t\tamdgpu_atpx_get_quirks(pdev);\n\t}\n\n\tif (has_atpx && vga_count == 2) {\n\t\tacpi_get_name(amdgpu_atpx_priv.atpx.handle, ACPI_FULL_PATHNAME, &buffer);\n\t\tpr_info(\"vga_switcheroo: detected switching method %s handle\\n\",\n\t\t\tacpi_method_name);\n\t\tamdgpu_atpx_priv.atpx_detected = true;\n\t\tamdgpu_atpx_priv.bridge_pm_usable = d3_supported;\n\t\tamdgpu_atpx_init();\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * amdgpu_register_atpx_handler - register with vga_switcheroo\n *\n * Register the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_register_atpx_handler(void)\n{\n\tbool r;\n\tenum vga_switcheroo_handler_flags_t handler_flags = 0;\n\n\t/* detect if we have any ATPX + 2 VGA in the system */\n\tr = amdgpu_atpx_detect();\n\tif (!r)\n\t\treturn;\n\n\tvga_switcheroo_register_handler(&amdgpu_atpx_handler, handler_flags);\n}\n\n/**\n * amdgpu_unregister_atpx_handler - unregister with vga_switcheroo\n *\n * Unregister the PX callbacks with vga_switcheroo (all asics).\n */\nvoid amdgpu_unregister_atpx_handler(void)\n{\n\tvga_switcheroo_unregister_handler();\n}\n", "source_code_path": "amdgpu_atpx_handler.c", "line_number": 145, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nindex 375f02002579..0e717d89b3e7 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\n@@ -144,7 +144,13 @@ static union acpi_object *amdgpu_atpx_call(acpi_handle handle, int function,\n                kfree(buffer.pointer);\n                return NULL;\n        }\n+   /* If status is AE_NOT_FOUND, buffer.pointer will be NULL */\n+   if (!buffer.pointer) {\n+       pr_warn(\"ATPX buffer is NULL\\n\");\n+       return NULL;\n+   }\n\n+   /* Caller must free the returned buffer */\n        return buffer.pointer;\n }\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_atpx_handler.c\nLine: 145\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tpr_err(\"failed to evaluate ATPX got %s\\n\",\n\t\t       acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Memory Leak\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atpx_handler.c\nLine: 148\n\nDescription:\nThe function amdgpu_atpx_call allocates memory for buffer.pointer using ACPI_ALLOCATE_BUFFER, but fails to properly handle memory deallocation in all cases. When the function succeeds, the allocated memory is not released, leading to a memory leak and potential system resource exhaustion.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "gfx_v9_4_2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nA potential buffer overflow has been detected in the function gfx_v9_4_2_log_wave_assignment(). The function uses a fixed-size buffer of 256 bytes to store a formatted string, but does not implement proper bounds checking. This could lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size.\n", "diff_path": "gfx_v9_4_2-diff.txt", "diff_text": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n", "code": "/*\n * Copyright 2020 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include \"amdgpu.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n\n#include \"gc/gc_9_4_2_offset.h\"\n#include \"gc/gc_9_4_2_sh_mask.h\"\n#include \"gfx_v9_0.h\"\n\n#include \"gfx_v9_4_2.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_gfx.h\"\n\n#define SE_ID_MAX 8\n#define CU_ID_MAX 16\n#define SIMD_ID_MAX 4\n#define WAVE_ID_MAX 10\n\nenum gfx_v9_4_2_utc_type {\n\tVML2_MEM,\n\tVML2_WALKER_MEM,\n\tUTCL2_MEM,\n\tATC_L2_CACHE_2M,\n\tATC_L2_CACHE_32K,\n\tATC_L2_CACHE_4K\n};\n\nstruct gfx_v9_4_2_utc_block {\n\tenum gfx_v9_4_2_utc_type type;\n\tuint32_t num_banks;\n\tuint32_t num_ways;\n\tuint32_t num_mem_blocks;\n\tstruct soc15_reg idx_reg;\n\tstruct soc15_reg data_reg;\n\tuint32_t sec_count_mask;\n\tuint32_t sec_count_shift;\n\tuint32_t ded_count_mask;\n\tuint32_t ded_count_shift;\n\tuint32_t clear;\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x141dc920),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0x3b458b93),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x1a4f5583),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0x317717f6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x107cc1e6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x351),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_1[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x2591aa38),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0xac9e88b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x2bc3369b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0xfb74ee),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x21f0a2fe),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x49),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regGB_ADDR_CONFIG, 0xffff77ff, 0x2a114042),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTA_CNTL_AUX, 0xfffffeef, 0x10b0000),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_UTCL1_CNTL1, 0xffffffff, 0x30800400),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCI_CNTL_3, 0xff, 0x20),\n};\n\n/*\n * This shader is used to clear VGPRS and LDS, and also write the input\n * pattern into the write back buffer, which will be used by driver to\n * check whether all SIMDs have been covered.\n*/\nstatic const u32 vgpr_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xd3d94000,\n\t0x18000080, 0xd3d94001, 0x18000080, 0xd3d94002, 0x18000080, 0xd3d94003,\n\t0x18000080, 0xd3d94004, 0x18000080, 0xd3d94005, 0x18000080, 0xd3d94006,\n\t0x18000080, 0xd3d94007, 0x18000080, 0xd3d94008, 0x18000080, 0xd3d94009,\n\t0x18000080, 0xd3d9400a, 0x18000080, 0xd3d9400b, 0x18000080, 0xd3d9400c,\n\t0x18000080, 0xd3d9400d, 0x18000080, 0xd3d9400e, 0x18000080, 0xd3d9400f,\n\t0x18000080, 0xd3d94010, 0x18000080, 0xd3d94011, 0x18000080, 0xd3d94012,\n\t0x18000080, 0xd3d94013, 0x18000080, 0xd3d94014, 0x18000080, 0xd3d94015,\n\t0x18000080, 0xd3d94016, 0x18000080, 0xd3d94017, 0x18000080, 0xd3d94018,\n\t0x18000080, 0xd3d94019, 0x18000080, 0xd3d9401a, 0x18000080, 0xd3d9401b,\n\t0x18000080, 0xd3d9401c, 0x18000080, 0xd3d9401d, 0x18000080, 0xd3d9401e,\n\t0x18000080, 0xd3d9401f, 0x18000080, 0xd3d94020, 0x18000080, 0xd3d94021,\n\t0x18000080, 0xd3d94022, 0x18000080, 0xd3d94023, 0x18000080, 0xd3d94024,\n\t0x18000080, 0xd3d94025, 0x18000080, 0xd3d94026, 0x18000080, 0xd3d94027,\n\t0x18000080, 0xd3d94028, 0x18000080, 0xd3d94029, 0x18000080, 0xd3d9402a,\n\t0x18000080, 0xd3d9402b, 0x18000080, 0xd3d9402c, 0x18000080, 0xd3d9402d,\n\t0x18000080, 0xd3d9402e, 0x18000080, 0xd3d9402f, 0x18000080, 0xd3d94030,\n\t0x18000080, 0xd3d94031, 0x18000080, 0xd3d94032, 0x18000080, 0xd3d94033,\n\t0x18000080, 0xd3d94034, 0x18000080, 0xd3d94035, 0x18000080, 0xd3d94036,\n\t0x18000080, 0xd3d94037, 0x18000080, 0xd3d94038, 0x18000080, 0xd3d94039,\n\t0x18000080, 0xd3d9403a, 0x18000080, 0xd3d9403b, 0x18000080, 0xd3d9403c,\n\t0x18000080, 0xd3d9403d, 0x18000080, 0xd3d9403e, 0x18000080, 0xd3d9403f,\n\t0x18000080, 0xd3d94040, 0x18000080, 0xd3d94041, 0x18000080, 0xd3d94042,\n\t0x18000080, 0xd3d94043, 0x18000080, 0xd3d94044, 0x18000080, 0xd3d94045,\n\t0x18000080, 0xd3d94046, 0x18000080, 0xd3d94047, 0x18000080, 0xd3d94048,\n\t0x18000080, 0xd3d94049, 0x18000080, 0xd3d9404a, 0x18000080, 0xd3d9404b,\n\t0x18000080, 0xd3d9404c, 0x18000080, 0xd3d9404d, 0x18000080, 0xd3d9404e,\n\t0x18000080, 0xd3d9404f, 0x18000080, 0xd3d94050, 0x18000080, 0xd3d94051,\n\t0x18000080, 0xd3d94052, 0x18000080, 0xd3d94053, 0x18000080, 0xd3d94054,\n\t0x18000080, 0xd3d94055, 0x18000080, 0xd3d94056, 0x18000080, 0xd3d94057,\n\t0x18000080, 0xd3d94058, 0x18000080, 0xd3d94059, 0x18000080, 0xd3d9405a,\n\t0x18000080, 0xd3d9405b, 0x18000080, 0xd3d9405c, 0x18000080, 0xd3d9405d,\n\t0x18000080, 0xd3d9405e, 0x18000080, 0xd3d9405f, 0x18000080, 0xd3d94060,\n\t0x18000080, 0xd3d94061, 0x18000080, 0xd3d94062, 0x18000080, 0xd3d94063,\n\t0x18000080, 0xd3d94064, 0x18000080, 0xd3d94065, 0x18000080, 0xd3d94066,\n\t0x18000080, 0xd3d94067, 0x18000080, 0xd3d94068, 0x18000080, 0xd3d94069,\n\t0x18000080, 0xd3d9406a, 0x18000080, 0xd3d9406b, 0x18000080, 0xd3d9406c,\n\t0x18000080, 0xd3d9406d, 0x18000080, 0xd3d9406e, 0x18000080, 0xd3d9406f,\n\t0x18000080, 0xd3d94070, 0x18000080, 0xd3d94071, 0x18000080, 0xd3d94072,\n\t0x18000080, 0xd3d94073, 0x18000080, 0xd3d94074, 0x18000080, 0xd3d94075,\n\t0x18000080, 0xd3d94076, 0x18000080, 0xd3d94077, 0x18000080, 0xd3d94078,\n\t0x18000080, 0xd3d94079, 0x18000080, 0xd3d9407a, 0x18000080, 0xd3d9407b,\n\t0x18000080, 0xd3d9407c, 0x18000080, 0xd3d9407d, 0x18000080, 0xd3d9407e,\n\t0x18000080, 0xd3d9407f, 0x18000080, 0xd3d94080, 0x18000080, 0xd3d94081,\n\t0x18000080, 0xd3d94082, 0x18000080, 0xd3d94083, 0x18000080, 0xd3d94084,\n\t0x18000080, 0xd3d94085, 0x18000080, 0xd3d94086, 0x18000080, 0xd3d94087,\n\t0x18000080, 0xd3d94088, 0x18000080, 0xd3d94089, 0x18000080, 0xd3d9408a,\n\t0x18000080, 0xd3d9408b, 0x18000080, 0xd3d9408c, 0x18000080, 0xd3d9408d,\n\t0x18000080, 0xd3d9408e, 0x18000080, 0xd3d9408f, 0x18000080, 0xd3d94090,\n\t0x18000080, 0xd3d94091, 0x18000080, 0xd3d94092, 0x18000080, 0xd3d94093,\n\t0x18000080, 0xd3d94094, 0x18000080, 0xd3d94095, 0x18000080, 0xd3d94096,\n\t0x18000080, 0xd3d94097, 0x18000080, 0xd3d94098, 0x18000080, 0xd3d94099,\n\t0x18000080, 0xd3d9409a, 0x18000080, 0xd3d9409b, 0x18000080, 0xd3d9409c,\n\t0x18000080, 0xd3d9409d, 0x18000080, 0xd3d9409e, 0x18000080, 0xd3d9409f,\n\t0x18000080, 0xd3d940a0, 0x18000080, 0xd3d940a1, 0x18000080, 0xd3d940a2,\n\t0x18000080, 0xd3d940a3, 0x18000080, 0xd3d940a4, 0x18000080, 0xd3d940a5,\n\t0x18000080, 0xd3d940a6, 0x18000080, 0xd3d940a7, 0x18000080, 0xd3d940a8,\n\t0x18000080, 0xd3d940a9, 0x18000080, 0xd3d940aa, 0x18000080, 0xd3d940ab,\n\t0x18000080, 0xd3d940ac, 0x18000080, 0xd3d940ad, 0x18000080, 0xd3d940ae,\n\t0x18000080, 0xd3d940af, 0x18000080, 0xd3d940b0, 0x18000080, 0xd3d940b1,\n\t0x18000080, 0xd3d940b2, 0x18000080, 0xd3d940b3, 0x18000080, 0xd3d940b4,\n\t0x18000080, 0xd3d940b5, 0x18000080, 0xd3d940b6, 0x18000080, 0xd3d940b7,\n\t0x18000080, 0xd3d940b8, 0x18000080, 0xd3d940b9, 0x18000080, 0xd3d940ba,\n\t0x18000080, 0xd3d940bb, 0x18000080, 0xd3d940bc, 0x18000080, 0xd3d940bd,\n\t0x18000080, 0xd3d940be, 0x18000080, 0xd3d940bf, 0x18000080, 0xd3d940c0,\n\t0x18000080, 0xd3d940c1, 0x18000080, 0xd3d940c2, 0x18000080, 0xd3d940c3,\n\t0x18000080, 0xd3d940c4, 0x18000080, 0xd3d940c5, 0x18000080, 0xd3d940c6,\n\t0x18000080, 0xd3d940c7, 0x18000080, 0xd3d940c8, 0x18000080, 0xd3d940c9,\n\t0x18000080, 0xd3d940ca, 0x18000080, 0xd3d940cb, 0x18000080, 0xd3d940cc,\n\t0x18000080, 0xd3d940cd, 0x18000080, 0xd3d940ce, 0x18000080, 0xd3d940cf,\n\t0x18000080, 0xd3d940d0, 0x18000080, 0xd3d940d1, 0x18000080, 0xd3d940d2,\n\t0x18000080, 0xd3d940d3, 0x18000080, 0xd3d940d4, 0x18000080, 0xd3d940d5,\n\t0x18000080, 0xd3d940d6, 0x18000080, 0xd3d940d7, 0x18000080, 0xd3d940d8,\n\t0x18000080, 0xd3d940d9, 0x18000080, 0xd3d940da, 0x18000080, 0xd3d940db,\n\t0x18000080, 0xd3d940dc, 0x18000080, 0xd3d940dd, 0x18000080, 0xd3d940de,\n\t0x18000080, 0xd3d940df, 0x18000080, 0xd3d940e0, 0x18000080, 0xd3d940e1,\n\t0x18000080, 0xd3d940e2, 0x18000080, 0xd3d940e3, 0x18000080, 0xd3d940e4,\n\t0x18000080, 0xd3d940e5, 0x18000080, 0xd3d940e6, 0x18000080, 0xd3d940e7,\n\t0x18000080, 0xd3d940e8, 0x18000080, 0xd3d940e9, 0x18000080, 0xd3d940ea,\n\t0x18000080, 0xd3d940eb, 0x18000080, 0xd3d940ec, 0x18000080, 0xd3d940ed,\n\t0x18000080, 0xd3d940ee, 0x18000080, 0xd3d940ef, 0x18000080, 0xd3d940f0,\n\t0x18000080, 0xd3d940f1, 0x18000080, 0xd3d940f2, 0x18000080, 0xd3d940f3,\n\t0x18000080, 0xd3d940f4, 0x18000080, 0xd3d940f5, 0x18000080, 0xd3d940f6,\n\t0x18000080, 0xd3d940f7, 0x18000080, 0xd3d940f8, 0x18000080, 0xd3d940f9,\n\t0x18000080, 0xd3d940fa, 0x18000080, 0xd3d940fb, 0x18000080, 0xd3d940fc,\n\t0x18000080, 0xd3d940fd, 0x18000080, 0xd3d940fe, 0x18000080, 0xd3d940ff,\n\t0x18000080, 0xb07c0000, 0xbe8a00ff, 0x000000f8, 0xbf11080a, 0x7e000280,\n\t0x7e020280, 0x7e040280, 0x7e060280, 0x7e080280, 0x7e0a0280, 0x7e0c0280,\n\t0x7e0e0280, 0x808a880a, 0xbe80320a, 0xbf84fff5, 0xbf9c0000, 0xd28c0001,\n\t0x0001007f, 0xd28d0001, 0x0002027e, 0x10020288, 0xbe8b0004, 0xb78b4000,\n\t0xd1196a01, 0x00001701, 0xbe8a0087, 0xbefc00c1, 0xd89c4000, 0x00020201,\n\t0xd89cc080, 0x00040401, 0x320202ff, 0x00000800, 0x808a810a, 0xbf84fff8,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry vgpr_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 4 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0xbf },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x400006 },  /* 64KB LDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x3F }, /*  63 - accum-offset = 256 */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * The below shaders are used to clear SGPRS, and also write the input\n * pattern into the write back buffer. The first two dispatch should be\n * scheduled simultaneously which make sure that all SGPRS could be\n * allocated, so the dispatch 1 need check write back buffer before scheduled,\n * make sure that waves of dispatch 0 are all dispacthed to all simds\n * balanced. both dispatch 0 and dispatch 1 should be halted until all waves\n * are dispatched, and then driver write a pattern to the shared memory to make\n * all waves continue.\n*/\nstatic const u32 sgpr112_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbeda0080, 0xbedb0080, 0xbedc0080, 0xbedd0080, 0xbede0080, 0xbedf0080,\n\t0xbee00080, 0xbee10080, 0xbee20080, 0xbee30080, 0xbee40080, 0xbee50080,\n\t0xbf810000\n};\n\nconst struct soc15_reg_entry sgpr112_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 8 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x340 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic const u32 sgpr96_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr96_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0xc },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x2c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * This shader is used to clear the uninitiated sgprs after the above\n * two dispatches, because of hardware feature, dispath 0 couldn't clear\n * top hole sgprs. Therefore need 4 waves per SIMD to cover these sgprs\n*/\nstatic const u32 sgpr64_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr64_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0x10 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x1c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic int gfx_v9_4_2_run_shader(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_ib *ib,\n\t\t\t\t const u32 *shader_ptr, u32 shader_size,\n\t\t\t\t const struct soc15_reg_entry *init_regs, u32 regs_size,\n\t\t\t\t u32 compute_dim_x, u64 wb_gpu_addr, u32 pattern,\n\t\t\t\t struct dma_fence **fence_ptr)\n{\n\tint r, i;\n\tuint32_t total_size, shader_offset;\n\tu64 gpu_addr;\n\n\ttotal_size = (regs_size * 3 + 4 + 5 + 5) * 4;\n\ttotal_size = ALIGN(total_size, 256);\n\tshader_offset = total_size;\n\ttotal_size += ALIGN(shader_size, 256);\n\n\t/* allocate an indirect buffer to put the commands in */\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, total_size,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\t/* load the compute shaders */\n\tfor (i = 0; i < shader_size/sizeof(u32); i++)\n\t\tib->ptr[i + (shader_offset / 4)] = shader_ptr[i];\n\n\t/* init the ib length to 0 */\n\tib->length_dw = 0;\n\n\t/* write the register state for the compute dispatch */\n\tfor (i = 0; i < regs_size; i++) {\n\t\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);\n\t\tib->ptr[ib->length_dw++] = SOC15_REG_ENTRY_OFFSET(init_regs[i])\n\t\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\t\tib->ptr[ib->length_dw++] = init_regs[i].reg_value;\n\t}\n\n\t/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */\n\tgpu_addr = (ib->gpu_addr + (u64)shader_offset) >> 8;\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_PGM_LO)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(gpu_addr);\n\n\t/* write the wb buffer address */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 3);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_USER_DATA_0)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = pattern;\n\n\t/* write dispatch packet */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);\n\tib->ptr[ib->length_dw++] = compute_dim_x; /* x */\n\tib->ptr[ib->length_dw++] = 1; /* y */\n\tib->ptr[ib->length_dw++] = 1; /* z */\n\tib->ptr[ib->length_dw++] =\n\t\tREG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);\n\n\t/* shedule the ib on the ring */\n\tr = amdgpu_ib_schedule(ring, 1, ib, NULL, fence_ptr);\n\tif (r) {\n\t\tdev_err(adev->dev, \"ib submit failed (%d).\\n\", r);\n\t\tamdgpu_ib_free(adev, ib, NULL);\n\t}\n\treturn r;\n}\n\nstatic void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t *wb_ptr)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n\t\tfor (cu = 0; cu < CU_ID_MAX; cu++) {\n\t\t\tmemset(str, 0, 256);\n\t\t\tsize = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++) {\n\t\t\t\tsize += sprintf(str + size, \"[\");\n\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\tsize += sprintf(str + size, \"%x\", wb_ptr[offset]);\n\t\t\t\t\toffset++;\n\t\t\t\t}\n\t\t\t\tsize += sprintf(str + size, \"]  \");\n\t\t\t}\n\t\t\tdev_dbg(adev->dev, \"%s\\n\", str);\n\t\t}\n\t}\n\n\tkfree(str);\n}\n\nstatic int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t *wb_ptr, uint32_t mask,\n\t\t\t\t\t      uint32_t pattern, uint32_t num_wave, bool wait)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t loop = 0;\n\tuint32_t wave_cnt;\n\tuint32_t offset;\n\n\tdo {\n\t\twave_cnt = 0;\n\t\toffset = 0;\n\n\t\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++)\n\t\t\tfor (cu = 0; cu < CU_ID_MAX; cu++)\n\t\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++)\n\t\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\t\tif (((1 << wave) & mask) &&\n\t\t\t\t\t\t    (wb_ptr[offset] == pattern))\n\t\t\t\t\t\t\twave_cnt++;\n\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t}\n\n\t\tif (wave_cnt == num_wave)\n\t\t\treturn 0;\n\n\t\tmdelay(1);\n\t} while (++loop < 2000 && wait);\n\n\tdev_err(adev->dev, \"actual wave num: %d, expected wave num: %d\\n\",\n\t\twave_cnt, num_wave);\n\n\tgfx_v9_4_2_log_wave_assignment(adev, wb_ptr);\n\n\treturn -EBADSLT;\n}\n\nstatic int gfx_v9_4_2_do_sgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ibs[3];\n\tstruct dma_fence *fences[3];\n\tu32 pattern[3] = { 0x1, 0x5, 0xa };\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready ||\n\t\t !adev->gfx.compute_ring[1].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[0],\n\t\t\tsgpr112_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr112_init_compute_shader_aldebaran),\n\t\t\tsgpr112_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr112_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[0], &fences[0]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 224 sgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11,\n\t\t\tpattern[0],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 2,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 224 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[1],\n\t\t\t&disp_ibs[1],\n\t\t\tsgpr96_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr96_init_compute_shader_aldebaran),\n\t\t\tsgpr96_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr96_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number * 2,\n\t\t\twb_ib.gpu_addr, pattern[1], &fences[1]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear next 576 sgprs\\n\");\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11111100,\n\t\t\tpattern[1], adev->gfx.cu_info.number * SIMD_ID_MAX * 6,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 576 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp1_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fences[0], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 224 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = dma_fence_wait(fences[1], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 576 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[2],\n\t\t\tsgpr64_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr64_init_compute_shader_aldebaran),\n\t\t\tsgpr64_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr64_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[2], &fences[2]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 256 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1111,\n\t\t\tpattern[2],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 4,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 256 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp2_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\tr = dma_fence_wait(fences[2], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 256 sgprs\\n\");\n\t\tgoto disp2_failed;\n\t}\n\ndisp2_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[2], NULL);\n\tdma_fence_put(fences[2]);\ndisp1_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[1], NULL);\n\tdma_fence_put(fences[1]);\ndisp0_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[0], NULL);\n\tdma_fence_put(fences[0]);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init SGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init SGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_2_do_vgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\t/* CU_ID: 0~15, SIMD_ID: 0~3, WAVE_ID: 0 ~ 9 */\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ib;\n\tstruct dma_fence *fence;\n\tu32 pattern = 0xa;\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb.\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ib,\n\t\t\tvgpr_init_compute_shader_aldebaran,\n\t\t\tsizeof(vgpr_init_compute_shader_aldebaran),\n\t\t\tvgpr_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(vgpr_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern, &fence);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear vgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fence, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1,\n\t\t\tpattern,\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX,\n\t\t\tfalse);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to cover all simds when clearing vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\ndisp_failed:\n\tamdgpu_ib_free(adev, &disp_ib, NULL);\n\tdma_fence_put(fence);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init VGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init VGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nint gfx_v9_4_2_do_edc_gpr_workarounds(struct amdgpu_device *adev)\n{\n\t/* only support when RAS is enabled */\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn 0;\n\n\t/* Workaround for ALDEBARAN, skip GPRs init in GPU reset.\n\t   Will remove it once GPRs init algorithm works for all CU settings. */\n\tif (amdgpu_in_reset(adev))\n\t\treturn 0;\n\n\tgfx_v9_4_2_do_sgprs_init(adev);\n\n\tgfx_v9_4_2_do_vgprs_init(adev);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev);\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev);\n\nvoid gfx_v9_4_2_init_golden_registers(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t die_id)\n{\n\tsoc15_program_register_sequence(adev,\n\t\t\t\t\tgolden_settings_gc_9_4_2_alde,\n\t\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde));\n\n\t/* apply golden settings per die */\n\tswitch (die_id) {\n\tcase 0:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_0,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_0));\n\t\tbreak;\n\tcase 1:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_1,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_1));\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(adev->dev,\n\t\t\t \"invalid die id %d, ignore channel fabricid remap settings\\n\",\n\t\t\t die_id);\n\t\tbreak;\n\t}\n}\n\nvoid gfx_v9_4_2_debug_trap_config_init(struct amdgpu_device *adev,\n\t\t\t\tuint32_t first_vmid,\n\t\t\t\tuint32_t last_vmid)\n{\n\tuint32_t data;\n\tint i;\n\n\tmutex_lock(&adev->srbm_mutex);\n\n\tfor (i = first_vmid; i < last_vmid; i++) {\n\t\tdata = 0;\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, TRAP_EN, 1);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_EN, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_REPLACE,\n\t\t\t\t\t0);\n\t\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_PER_VMID_CNTL), data);\n\t}\n\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, 0);\n\tmutex_unlock(&adev->srbm_mutex);\n\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA0), 0);\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA1), 0);\n}\n\nvoid gfx_v9_4_2_set_power_brake_sequence(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tgfx_v9_0_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL, PATTERN_MODE, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL, tmp);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL1, PWRBRK_STALL_EN, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL1, tmp);\n\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_INDEX, ixPWRBRK_STALL_PATTERN_CTRL);\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, PWRBRK_STALL_PATTERN_CTRL, PWRBRK_END_STEP, 0x12);\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_DATA, tmp);\n}\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_edc_counter_regs[] = {\n\t/* CPF */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT), 0, 1, 1 },\n\t/* CPC */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT), 0, 1, 1 },\n\t/* GDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT), 0, 1, 1 },\n\t/* RLC */\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2), 0, 1, 1 },\n\t/* SPI */\n\t{ SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT), 0, 8, 1 },\n\t/* SQC */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3), 0, 8, 7 },\n\t/* SQ */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT), 0, 8, 14 },\n\t/* TCP */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW), 0, 8, 14 },\n\t/* TCI */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT), 0, 1, 69 },\n\t/* TCC */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2), 0, 1, 16 },\n\t/* TCA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT), 0, 1, 2 },\n\t/* TCX */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT), 0, 1, 2 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2), 0, 1, 2 },\n\t/* TD */\n\t{ SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT), 0, 8, 14 },\n\t/* TA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT), 0, 8, 14 },\n\t/* GCEA */\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 1, 16 },\n};\n\nstatic void gfx_v9_4_2_select_se_sh(struct amdgpu_device *adev, u32 se_num,\n\t\t\t\t  u32 sh_num, u32 instance)\n{\n\tu32 data;\n\n\tif (instance == 0xffffffff)\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX,\n\t\t\t\t     instance);\n\n\tif (se_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);\n\n\tif (sh_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);\n\n\tWREG32_SOC15_RLC_SHADOW_EX(reg, GC, 0, regGRBM_GFX_INDEX, data);\n}\n\nstatic const struct soc15_ras_field_entry gfx_v9_4_2_ras_fields[] = {\n\t/* CPF */\n\t{ \"CPF_ROQ_ME2\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME2),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME2) },\n\t{ \"CPF_ROQ_ME1\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME1) },\n\t{ \"CPF_TCIU_TAG\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT) },\n\n\t/* CPC */\n\t{ \"CPC_SCRATCH\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT) },\n\t{ \"CPC_UCODE\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT) },\n\t{ \"CPC_DC_STATE_RAM_ME1\", SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT1_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT1_ME1) },\n\n\t/* GDS */\n\t{ \"GDS_GRBM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, DED) },\n\t{ \"GDS_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED) },\n\t{ \"GDS_PHY_CMD_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED) },\n\t{ \"GDS_PHY_DATA_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_DED) },\n\t{ \"GDS_ME0_CS_PIPE_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE0_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE1_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE2_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE3_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED) },\n\t{ \"GDS_ME0_GFXHP3D_PIX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_PIX_DED) },\n\t{ \"GDS_ME0_GFXHP3D_VTX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_VTX_DED) },\n\t{ \"GDS_ME0_CS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_CS_DED) },\n\t{ \"GDS_ME0_GFXHP3D_GS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_GS_DED) },\n\t{ \"GDS_ME1_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE0_DED) },\n\t{ \"GDS_ME1_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE1_DED) },\n\t{ \"GDS_ME1_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE2_DED) },\n\t{ \"GDS_ME1_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE3_DED) },\n\t{ \"GDS_ME2_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE0_DED) },\n\t{ \"GDS_ME2_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE1_DED) },\n\t{ \"GDS_ME2_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE2_DED) },\n\t{ \"GDS_ME2_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE3_DED) },\n\n\t/* RLC */\n\t{ \"RLCG_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCG_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLCV_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCV_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_TCTAG_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_DATA_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_ADDR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE0_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE1_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE2_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE3_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE4_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE5_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE6_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE7_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_DED_COUNT) },\n\n\t/* SPI */\n\t{ \"SPI_SR_MEM\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_DED_COUNT) },\n\t{ \"SPI_GDS_EXPREQ\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_DED_COUNT) },\n\t{ \"SPI_WB_GRANT_30\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_DED_COUNT) },\n\t{ \"SPI_LIFE_CNT\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT */\n\t{ \"SQC_DATA_CU0_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU0_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT2 */\n\t{ \"SQC_INST_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_INST_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT3 */\n\t{ \"SQC_INST_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_PARITY_CNT3 */\n\t{ \"SQC_INST_BANKA_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_DED_COUNT) },\n\n\t/* SQ */\n\t{ \"SQ_LDS_D\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT) },\n\t{ \"SQ_LDS_I\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT) },\n\t{ \"SQ_SGPR\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT) },\n\t{ \"SQ_VGPR0\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT) },\n\t{ \"SQ_VGPR1\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT) },\n\t{ \"SQ_VGPR2\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT) },\n\t{ \"SQ_VGPR3\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT) },\n\n\t/* TCP */\n\t{ \"TCP_CACHE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT) },\n\t{ \"TCP_LFIFO_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT) },\n\t{ \"TCP_CMD_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_DED_COUNT) },\n\t{ \"TCP_VM_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_DED_COUNT) },\n\t{ \"TCP_DB_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO0\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO1\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT) },\n\n\t/* TCI */\n\t{ \"TCI_WRITE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_DED_COUNT) },\n\n\t/* TCC */\n\t{ \"TCC_CACHE_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT) },\n\t{ \"TCC_CACHE_DIRTY\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT) },\n\t{ \"TCC_HIGH_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_LOW_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_SRC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO_NEXT_RAM\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_DED_COUNT) },\n\t{ \"TCC_CACHE_TAG_PROBE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_DED_COUNT) },\n\t{ \"TCC_UC_ATOMIC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_DED_COUNT) },\n\t{ \"TCC_WRITE_CACHE_READ\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_DED_COUNT) },\n\t{ \"TCC_RETURN_CONTROL\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_DED_COUNT) },\n\t{ \"TCC_IN_USE_TRANSFER\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_DED_COUNT) },\n\t{ \"TCC_IN_USE_DEC\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_DED_COUNT) },\n\t{ \"TCC_WRITE_RETURN\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_DED_COUNT) },\n\t{ \"TCC_RETURN_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_DED_COUNT) },\n\n\t/* TCA */\n\t{ \"TCA_HOLE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_DED_COUNT) },\n\t{ \"TCA_REQ_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_DED_COUNT) },\n\n\t/* TCX */\n\t{ \"TCX_GROUP0\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_DED_COUNT) },\n\t{ \"TCX_GROUP1\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_DED_COUNT) },\n\t{ \"TCX_GROUP2\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_DED_COUNT) },\n\t{ \"TCX_GROUP3\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_DED_COUNT) },\n\t{ \"TCX_GROUP4\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_DED_COUNT) },\n\t{ \"TCX_GROUP5\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP5_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP6\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP6_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP7\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP7_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP8\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP8_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP9\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP9_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP10\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP10_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP11\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP11_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP12\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP12_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP13\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP13_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP14\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP14_SED_COUNT), 0, 0 },\n\n\t/* TD */\n\t{ \"TD_SS_FIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT) },\n\t{ \"TD_SS_FIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT) },\n\t{ \"TD_CS_FIFO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_DED_COUNT) },\n\n\t/* TA */\n\t{ \"TA_FS_DFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_DED_COUNT) },\n\t{ \"TA_FL_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_DED_COUNT) },\n\t{ \"TA_FX_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_DED_COUNT) },\n\t{ \"TA_FS_CFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT */\n\t{ \"EA_DRAMRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_RRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_WRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_IOWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT), 0, 0 },\n\n\t/* EA - regGCEA_EDC_CNT2 */\n\t{ \"EA_GMIRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_MAM_D0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_DED_COUNT) },\n\t{ \"EA_MAM_D1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_DED_COUNT) },\n\t{ \"EA_MAM_D2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_DED_COUNT) },\n\t{ \"EA_MAM_D3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT3 */\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IORD_CMDMEM_DED_COUNT) },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IOWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_MAM_A0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_DED_COUNT) },\n\t{ \"EA_MAM_A1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_DED_COUNT) },\n\t{ \"EA_MAM_A2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_DED_COUNT) },\n\t{ \"EA_MAM_A3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_DED_COUNT) },\n\t{ \"EA_MAM_AFMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_DED_COUNT) },\n};\n\nstatic const char * const vml2_walker_mems[] = {\n\t\"UTC_VML2_CACHE_PDE0_MEM0\",\n\t\"UTC_VML2_CACHE_PDE0_MEM1\",\n\t\"UTC_VML2_CACHE_PDE1_MEM0\",\n\t\"UTC_VML2_CACHE_PDE1_MEM1\",\n\t\"UTC_VML2_CACHE_PDE2_MEM0\",\n\t\"UTC_VML2_CACHE_PDE2_MEM1\",\n\t\"UTC_VML2_RDIF_ARADDRS\",\n\t\"UTC_VML2_RDIF_LOG_FIFO\",\n\t\"UTC_VML2_QUEUE_REQ\",\n\t\"UTC_VML2_QUEUE_RET\",\n};\n\nstatic struct gfx_v9_4_2_utc_block gfx_v9_4_2_utc_blocks[] = {\n\t{ VML2_MEM, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ VML2_WALKER_MEM, ARRAY_SIZE(vml2_walker_mems), 1, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_WALKER_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ UTCL2_MEM, 18, 1, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, UTCL2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_2M, 8, 2, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_2M_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_32K, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_32K_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_4K, 8, 2, 8,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_4K_DSM_CNTL, WRITE_COUNTERS, 1) },\n};\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_ea_err_status_regs = {\n\tSOC15_REG_ENTRY(GC, 0, regGCEA_ERR_STATUS), 0, 1, 16\n};\n\nstatic int gfx_v9_4_2_get_reg_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t  const struct soc15_reg_entry *reg,\n\t\t\t\t\t  uint32_t se_id, uint32_t inst_id,\n\t\t\t\t\t  uint32_t value, uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i;\n\tuint32_t sec_cnt, ded_cnt;\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_ras_fields); i++) {\n\t\tif (gfx_v9_4_2_ras_fields[i].reg_offset != reg->reg_offset ||\n\t\t    gfx_v9_4_2_ras_fields[i].seg != reg->seg ||\n\t\t    gfx_v9_4_2_ras_fields[i].inst != reg->inst)\n\t\t\tcontinue;\n\n\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], sec);\n\t\tif (sec_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], SEC %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t sec_cnt);\n\t\t\t*sec_count += sec_cnt;\n\t\t}\n\n\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], ded);\n\t\tif (ded_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], DED %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t ded_cnt);\n\t\t\t*ded_count += ded_cnt;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_2_query_sram_edc_count(struct amdgpu_device *adev,\n\t\t\t\tuint32_t *sec_count, uint32_t *ded_count)\n{\n\tuint32_t i, j, k, data;\n\tuint32_t sec_cnt = 0, ded_cnt = 0;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_edc_counter_regs); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_edc_counter_regs[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_2_edc_counter_regs[i].instance;\n\t\t\t     k++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, j, 0, k);\n\n\t\t\t\t/* if sec/ded_count is null, just clear counter */\n\t\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]));\n\n\t\t\t\tif (!data)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tgfx_v9_4_2_get_reg_error_count(adev,\n\t\t\t\t\t&gfx_v9_4_2_edc_counter_regs[i],\n\t\t\t\t\tj, k, data, &sec_cnt, &ded_cnt);\n\n\t\t\t\t/* clear counter after read */\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count += sec_cnt;\n\t\t*ded_count += ded_cnt;\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_log_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t struct gfx_v9_4_2_utc_block *blk,\n\t\t\t\t\t uint32_t instance, uint32_t sec_cnt,\n\t\t\t\t\t uint32_t ded_cnt)\n{\n\tuint32_t bank, way, mem;\n\tstatic const char * const vml2_way_str[] = { \"BIGK\", \"4K\" };\n\tstatic const char * const utcl2_rounter_str[] = { \"VMC\", \"APT\" };\n\n\tmem = instance % blk->num_mem_blocks;\n\tway = (instance / blk->num_mem_blocks) % blk->num_ways;\n\tbank = instance / (blk->num_mem_blocks * blk->num_ways);\n\n\tswitch (blk->type) {\n\tcase VML2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_VML2_BANK_CACHE_%d_%s_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, vml2_way_str[way], mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase VML2_WALKER_MEM:\n\t\tdev_info(adev->dev, \"GFX SubBlock %s, SED %d, DED %d\\n\",\n\t\t\t vml2_walker_mems[bank], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase UTCL2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTCL2_ROUTER_IFIF%d_GROUP0_%s, SED %d, DED %d\\n\",\n\t\t\tbank, utcl2_rounter_str[mem], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_2M:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_2M_BANK%d_WAY%d_MEM, SED %d, DED %d\\n\",\n\t\t\tbank, way, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_32K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_32K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_4K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_4K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_2_query_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i, j, data;\n\tuint32_t sec_cnt, ded_cnt;\n\tuint32_t num_instances;\n\tstruct gfx_v9_4_2_utc_block *blk;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_utc_blocks); i++) {\n\t\tblk = &gfx_v9_4_2_utc_blocks[i];\n\t\tnum_instances =\n\t\t\tblk->num_banks * blk->num_ways * blk->num_mem_blocks;\n\t\tfor (j = 0; j < num_instances; j++) {\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->idx_reg), j);\n\n\t\t\t/* if sec/ded_count is NULL, just clear counter */\n\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t\t       blk->clear);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg));\n\t\t\tif (!data)\n\t\t\t\tcontinue;\n\n\t\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, sec);\n\t\t\t*sec_count += sec_cnt;\n\t\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, ded);\n\t\t\t*ded_count += ded_cnt;\n\n\t\t\t/* clear counter after read */\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t       blk->clear);\n\n\t\t\t/* print the edc count */\n\t\t\tif (sec_cnt || ded_cnt)\n\t\t\t\tgfx_v9_4_2_log_utc_edc_count(adev, blk, j, sec_cnt,\n\t\t\t\t\t\t\t     ded_cnt);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t    void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tuint32_t sec_count = 0, ded_count = 0;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\terr_data->ue_count = 0;\n\terr_data->ce_count = 0;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n\tgfx_v9_4_2_query_utc_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n}\n\nstatic void gfx_v9_4_2_reset_utc_err_status(struct amdgpu_device *adev)\n{\n\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n}\n\nstatic void gfx_v9_4_2_reset_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\tvalue = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\t\t\tvalue = REG_SET_FIELD(value, GCEA_ERR_STATUS, CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), value);\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, NULL, NULL);\n\tgfx_v9_4_2_query_utc_edc_count(adev, NULL, NULL);\n}\n\nstatic void gfx_v9_4_2_query_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t reg_value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\treg_value = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\n\t\t\tif (REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_WRRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_DATAPARITY_ERROR)) {\n\t\t\t\tdev_warn(adev->dev, \"GCEA err detected at instance: %d, status: 0x%x!\\n\",\n\t\t\t\t\t\tj, reg_value);\n\t\t\t}\n\t\t\t/* clear after read */\n\t\t\treg_value = REG_SET_FIELD(reg_value, GCEA_ERR_STATUS,\n\t\t\t\t\t\t  CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), reg_value);\n\t\t}\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_query_utc_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX UTCL2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Walker Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_ea_err_status(adev);\n\tgfx_v9_4_2_query_utc_err_status(adev);\n\tgfx_v9_4_2_query_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_reset_utc_err_status(adev);\n\tgfx_v9_4_2_reset_ea_err_status(adev);\n\tgfx_v9_4_2_reset_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_enable_watchdog_timer(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\tuint32_t data;\n\n\tdata = REG_SET_FIELD(0, SQ_TIMEOUT_CONFIG, TIMEOUT_FATAL_DISABLE,\n\t\t\t     amdgpu_watchdog_timer.timeout_fatal_disable ? 1 :\n\t\t\t\t\t\t\t\t\t   0);\n\n\tif (amdgpu_watchdog_timer.timeout_fatal_disable &&\n\t    (amdgpu_watchdog_timer.period < 1 ||\n\t     amdgpu_watchdog_timer.period > 0x23)) {\n\t\tdev_warn(adev->dev, \"Watchdog period range is 1 to 0x23\\n\");\n\t\tamdgpu_watchdog_timer.period = 0x23;\n\t}\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, PERIOD_SEL,\n\t\t\t     amdgpu_watchdog_timer.period);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tgfx_v9_4_2_select_se_sh(adev, i, 0xffffffff, 0xffffffff);\n\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_CONFIG, data);\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t address)\n{\n\tWREG32_SOC15_RLC_EX(reg, GC, 0, regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(address << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK));\n\treturn RREG32_SOC15(GC, 0, regSQ_IND_DATA);\n}\n\nstatic void gfx_v9_4_2_log_cu_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t status)\n{\n\tstruct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;\n\tuint32_t i, simd, wave;\n\tuint32_t wave_status;\n\tuint32_t wave_pc_lo, wave_pc_hi;\n\tuint32_t wave_exec_lo, wave_exec_hi;\n\tuint32_t wave_inst_dw0, wave_inst_dw1;\n\tuint32_t wave_ib_sts;\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (!((i << 1) & status))\n\t\t\tcontinue;\n\n\t\tsimd = i / cu_info->max_waves_per_simd;\n\t\twave = i % cu_info->max_waves_per_simd;\n\n\t\twave_status = wave_read_ind(adev, simd, wave, ixSQ_WAVE_STATUS);\n\t\twave_pc_lo = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_LO);\n\t\twave_pc_hi = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_HI);\n\t\twave_exec_lo =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_LO);\n\t\twave_exec_hi =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_HI);\n\t\twave_inst_dw0 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW0);\n\t\twave_inst_dw1 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW1);\n\t\twave_ib_sts = wave_read_ind(adev, simd, wave, ixSQ_WAVE_IB_STS);\n\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"\\t SIMD %d, Wave %d: status 0x%x, pc 0x%llx, exec 0x%llx, inst 0x%llx, ib_sts 0x%x\\n\",\n\t\t\tsimd, wave, wave_status,\n\t\t\t((uint64_t)wave_pc_hi << 32 | wave_pc_lo),\n\t\t\t((uint64_t)wave_exec_hi << 32 | wave_exec_lo),\n\t\t\t((uint64_t)wave_inst_dw1 << 32 | wave_inst_dw0),\n\t\t\twave_ib_sts);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\tuint32_t status;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tstatus = RREG32_SOC15(GC, 0,\n\t\t\t\t\t\t      regSQ_TIMEOUT_STATUS);\n\t\t\t\tif (status != 0) {\n\t\t\t\t\tdev_info(\n\t\t\t\t\t\tadev->dev,\n\t\t\t\t\t\t\"GFX Watchdog Timeout: SE %d, SH %d, CU %d\\n\",\n\t\t\t\t\t\tse_idx, sh_idx, cu_idx);\n\t\t\t\t\tgfx_v9_4_2_log_cu_timeout_status(\n\t\t\t\t\t\tadev, status);\n\t\t\t\t}\n\t\t\t\t/* clear old status */\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n\n\nstruct amdgpu_ras_block_hw_ops  gfx_v9_4_2_ras_ops = {\n\t\t.query_ras_error_count = &gfx_v9_4_2_query_ras_error_count,\n\t\t.reset_ras_error_count = &gfx_v9_4_2_reset_ras_error_count,\n\t\t.query_ras_error_status = &gfx_v9_4_2_query_ras_error_status,\n\t\t.reset_ras_error_status = &gfx_v9_4_2_reset_ras_error_status,\n};\n\nstruct amdgpu_gfx_ras gfx_v9_4_2_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &gfx_v9_4_2_ras_ops,\n\t},\n\t.enable_watchdog_timer = &gfx_v9_4_2_enable_watchdog_timer,\n};\n", "source_code_path": "gfx_v9_4_2.c", "line_number": 427, "generated_diff": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: gfx_v9_4_2.c\nLine: 427\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nA potential buffer overflow has been detected in the function gfx_v9_4_2_log_wave_assignment(). The function uses a fixed-size buffer of 256 bytes to store a formatted string, but does not implement proper bounds checking. This could lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "gfx_v9_4_2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nThe function gfx_v9_4_2_log_wave_assignment() uses a fixed-size buffer to store a formatted string, but lacks sufficient bounds checking. This can lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size, potentially causing data corruption or system instability.\n", "diff_path": "gfx_v9_4_2-diff.txt", "diff_text": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n", "code": "/*\n * Copyright 2020 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include \"amdgpu.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n\n#include \"gc/gc_9_4_2_offset.h\"\n#include \"gc/gc_9_4_2_sh_mask.h\"\n#include \"gfx_v9_0.h\"\n\n#include \"gfx_v9_4_2.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_gfx.h\"\n\n#define SE_ID_MAX 8\n#define CU_ID_MAX 16\n#define SIMD_ID_MAX 4\n#define WAVE_ID_MAX 10\n\nenum gfx_v9_4_2_utc_type {\n\tVML2_MEM,\n\tVML2_WALKER_MEM,\n\tUTCL2_MEM,\n\tATC_L2_CACHE_2M,\n\tATC_L2_CACHE_32K,\n\tATC_L2_CACHE_4K\n};\n\nstruct gfx_v9_4_2_utc_block {\n\tenum gfx_v9_4_2_utc_type type;\n\tuint32_t num_banks;\n\tuint32_t num_ways;\n\tuint32_t num_mem_blocks;\n\tstruct soc15_reg idx_reg;\n\tstruct soc15_reg data_reg;\n\tuint32_t sec_count_mask;\n\tuint32_t sec_count_shift;\n\tuint32_t ded_count_mask;\n\tuint32_t ded_count_shift;\n\tuint32_t clear;\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x141dc920),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0x3b458b93),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x1a4f5583),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0x317717f6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x107cc1e6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x351),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_1[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x2591aa38),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0xac9e88b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x2bc3369b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0xfb74ee),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x21f0a2fe),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x49),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regGB_ADDR_CONFIG, 0xffff77ff, 0x2a114042),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTA_CNTL_AUX, 0xfffffeef, 0x10b0000),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_UTCL1_CNTL1, 0xffffffff, 0x30800400),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCI_CNTL_3, 0xff, 0x20),\n};\n\n/*\n * This shader is used to clear VGPRS and LDS, and also write the input\n * pattern into the write back buffer, which will be used by driver to\n * check whether all SIMDs have been covered.\n*/\nstatic const u32 vgpr_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xd3d94000,\n\t0x18000080, 0xd3d94001, 0x18000080, 0xd3d94002, 0x18000080, 0xd3d94003,\n\t0x18000080, 0xd3d94004, 0x18000080, 0xd3d94005, 0x18000080, 0xd3d94006,\n\t0x18000080, 0xd3d94007, 0x18000080, 0xd3d94008, 0x18000080, 0xd3d94009,\n\t0x18000080, 0xd3d9400a, 0x18000080, 0xd3d9400b, 0x18000080, 0xd3d9400c,\n\t0x18000080, 0xd3d9400d, 0x18000080, 0xd3d9400e, 0x18000080, 0xd3d9400f,\n\t0x18000080, 0xd3d94010, 0x18000080, 0xd3d94011, 0x18000080, 0xd3d94012,\n\t0x18000080, 0xd3d94013, 0x18000080, 0xd3d94014, 0x18000080, 0xd3d94015,\n\t0x18000080, 0xd3d94016, 0x18000080, 0xd3d94017, 0x18000080, 0xd3d94018,\n\t0x18000080, 0xd3d94019, 0x18000080, 0xd3d9401a, 0x18000080, 0xd3d9401b,\n\t0x18000080, 0xd3d9401c, 0x18000080, 0xd3d9401d, 0x18000080, 0xd3d9401e,\n\t0x18000080, 0xd3d9401f, 0x18000080, 0xd3d94020, 0x18000080, 0xd3d94021,\n\t0x18000080, 0xd3d94022, 0x18000080, 0xd3d94023, 0x18000080, 0xd3d94024,\n\t0x18000080, 0xd3d94025, 0x18000080, 0xd3d94026, 0x18000080, 0xd3d94027,\n\t0x18000080, 0xd3d94028, 0x18000080, 0xd3d94029, 0x18000080, 0xd3d9402a,\n\t0x18000080, 0xd3d9402b, 0x18000080, 0xd3d9402c, 0x18000080, 0xd3d9402d,\n\t0x18000080, 0xd3d9402e, 0x18000080, 0xd3d9402f, 0x18000080, 0xd3d94030,\n\t0x18000080, 0xd3d94031, 0x18000080, 0xd3d94032, 0x18000080, 0xd3d94033,\n\t0x18000080, 0xd3d94034, 0x18000080, 0xd3d94035, 0x18000080, 0xd3d94036,\n\t0x18000080, 0xd3d94037, 0x18000080, 0xd3d94038, 0x18000080, 0xd3d94039,\n\t0x18000080, 0xd3d9403a, 0x18000080, 0xd3d9403b, 0x18000080, 0xd3d9403c,\n\t0x18000080, 0xd3d9403d, 0x18000080, 0xd3d9403e, 0x18000080, 0xd3d9403f,\n\t0x18000080, 0xd3d94040, 0x18000080, 0xd3d94041, 0x18000080, 0xd3d94042,\n\t0x18000080, 0xd3d94043, 0x18000080, 0xd3d94044, 0x18000080, 0xd3d94045,\n\t0x18000080, 0xd3d94046, 0x18000080, 0xd3d94047, 0x18000080, 0xd3d94048,\n\t0x18000080, 0xd3d94049, 0x18000080, 0xd3d9404a, 0x18000080, 0xd3d9404b,\n\t0x18000080, 0xd3d9404c, 0x18000080, 0xd3d9404d, 0x18000080, 0xd3d9404e,\n\t0x18000080, 0xd3d9404f, 0x18000080, 0xd3d94050, 0x18000080, 0xd3d94051,\n\t0x18000080, 0xd3d94052, 0x18000080, 0xd3d94053, 0x18000080, 0xd3d94054,\n\t0x18000080, 0xd3d94055, 0x18000080, 0xd3d94056, 0x18000080, 0xd3d94057,\n\t0x18000080, 0xd3d94058, 0x18000080, 0xd3d94059, 0x18000080, 0xd3d9405a,\n\t0x18000080, 0xd3d9405b, 0x18000080, 0xd3d9405c, 0x18000080, 0xd3d9405d,\n\t0x18000080, 0xd3d9405e, 0x18000080, 0xd3d9405f, 0x18000080, 0xd3d94060,\n\t0x18000080, 0xd3d94061, 0x18000080, 0xd3d94062, 0x18000080, 0xd3d94063,\n\t0x18000080, 0xd3d94064, 0x18000080, 0xd3d94065, 0x18000080, 0xd3d94066,\n\t0x18000080, 0xd3d94067, 0x18000080, 0xd3d94068, 0x18000080, 0xd3d94069,\n\t0x18000080, 0xd3d9406a, 0x18000080, 0xd3d9406b, 0x18000080, 0xd3d9406c,\n\t0x18000080, 0xd3d9406d, 0x18000080, 0xd3d9406e, 0x18000080, 0xd3d9406f,\n\t0x18000080, 0xd3d94070, 0x18000080, 0xd3d94071, 0x18000080, 0xd3d94072,\n\t0x18000080, 0xd3d94073, 0x18000080, 0xd3d94074, 0x18000080, 0xd3d94075,\n\t0x18000080, 0xd3d94076, 0x18000080, 0xd3d94077, 0x18000080, 0xd3d94078,\n\t0x18000080, 0xd3d94079, 0x18000080, 0xd3d9407a, 0x18000080, 0xd3d9407b,\n\t0x18000080, 0xd3d9407c, 0x18000080, 0xd3d9407d, 0x18000080, 0xd3d9407e,\n\t0x18000080, 0xd3d9407f, 0x18000080, 0xd3d94080, 0x18000080, 0xd3d94081,\n\t0x18000080, 0xd3d94082, 0x18000080, 0xd3d94083, 0x18000080, 0xd3d94084,\n\t0x18000080, 0xd3d94085, 0x18000080, 0xd3d94086, 0x18000080, 0xd3d94087,\n\t0x18000080, 0xd3d94088, 0x18000080, 0xd3d94089, 0x18000080, 0xd3d9408a,\n\t0x18000080, 0xd3d9408b, 0x18000080, 0xd3d9408c, 0x18000080, 0xd3d9408d,\n\t0x18000080, 0xd3d9408e, 0x18000080, 0xd3d9408f, 0x18000080, 0xd3d94090,\n\t0x18000080, 0xd3d94091, 0x18000080, 0xd3d94092, 0x18000080, 0xd3d94093,\n\t0x18000080, 0xd3d94094, 0x18000080, 0xd3d94095, 0x18000080, 0xd3d94096,\n\t0x18000080, 0xd3d94097, 0x18000080, 0xd3d94098, 0x18000080, 0xd3d94099,\n\t0x18000080, 0xd3d9409a, 0x18000080, 0xd3d9409b, 0x18000080, 0xd3d9409c,\n\t0x18000080, 0xd3d9409d, 0x18000080, 0xd3d9409e, 0x18000080, 0xd3d9409f,\n\t0x18000080, 0xd3d940a0, 0x18000080, 0xd3d940a1, 0x18000080, 0xd3d940a2,\n\t0x18000080, 0xd3d940a3, 0x18000080, 0xd3d940a4, 0x18000080, 0xd3d940a5,\n\t0x18000080, 0xd3d940a6, 0x18000080, 0xd3d940a7, 0x18000080, 0xd3d940a8,\n\t0x18000080, 0xd3d940a9, 0x18000080, 0xd3d940aa, 0x18000080, 0xd3d940ab,\n\t0x18000080, 0xd3d940ac, 0x18000080, 0xd3d940ad, 0x18000080, 0xd3d940ae,\n\t0x18000080, 0xd3d940af, 0x18000080, 0xd3d940b0, 0x18000080, 0xd3d940b1,\n\t0x18000080, 0xd3d940b2, 0x18000080, 0xd3d940b3, 0x18000080, 0xd3d940b4,\n\t0x18000080, 0xd3d940b5, 0x18000080, 0xd3d940b6, 0x18000080, 0xd3d940b7,\n\t0x18000080, 0xd3d940b8, 0x18000080, 0xd3d940b9, 0x18000080, 0xd3d940ba,\n\t0x18000080, 0xd3d940bb, 0x18000080, 0xd3d940bc, 0x18000080, 0xd3d940bd,\n\t0x18000080, 0xd3d940be, 0x18000080, 0xd3d940bf, 0x18000080, 0xd3d940c0,\n\t0x18000080, 0xd3d940c1, 0x18000080, 0xd3d940c2, 0x18000080, 0xd3d940c3,\n\t0x18000080, 0xd3d940c4, 0x18000080, 0xd3d940c5, 0x18000080, 0xd3d940c6,\n\t0x18000080, 0xd3d940c7, 0x18000080, 0xd3d940c8, 0x18000080, 0xd3d940c9,\n\t0x18000080, 0xd3d940ca, 0x18000080, 0xd3d940cb, 0x18000080, 0xd3d940cc,\n\t0x18000080, 0xd3d940cd, 0x18000080, 0xd3d940ce, 0x18000080, 0xd3d940cf,\n\t0x18000080, 0xd3d940d0, 0x18000080, 0xd3d940d1, 0x18000080, 0xd3d940d2,\n\t0x18000080, 0xd3d940d3, 0x18000080, 0xd3d940d4, 0x18000080, 0xd3d940d5,\n\t0x18000080, 0xd3d940d6, 0x18000080, 0xd3d940d7, 0x18000080, 0xd3d940d8,\n\t0x18000080, 0xd3d940d9, 0x18000080, 0xd3d940da, 0x18000080, 0xd3d940db,\n\t0x18000080, 0xd3d940dc, 0x18000080, 0xd3d940dd, 0x18000080, 0xd3d940de,\n\t0x18000080, 0xd3d940df, 0x18000080, 0xd3d940e0, 0x18000080, 0xd3d940e1,\n\t0x18000080, 0xd3d940e2, 0x18000080, 0xd3d940e3, 0x18000080, 0xd3d940e4,\n\t0x18000080, 0xd3d940e5, 0x18000080, 0xd3d940e6, 0x18000080, 0xd3d940e7,\n\t0x18000080, 0xd3d940e8, 0x18000080, 0xd3d940e9, 0x18000080, 0xd3d940ea,\n\t0x18000080, 0xd3d940eb, 0x18000080, 0xd3d940ec, 0x18000080, 0xd3d940ed,\n\t0x18000080, 0xd3d940ee, 0x18000080, 0xd3d940ef, 0x18000080, 0xd3d940f0,\n\t0x18000080, 0xd3d940f1, 0x18000080, 0xd3d940f2, 0x18000080, 0xd3d940f3,\n\t0x18000080, 0xd3d940f4, 0x18000080, 0xd3d940f5, 0x18000080, 0xd3d940f6,\n\t0x18000080, 0xd3d940f7, 0x18000080, 0xd3d940f8, 0x18000080, 0xd3d940f9,\n\t0x18000080, 0xd3d940fa, 0x18000080, 0xd3d940fb, 0x18000080, 0xd3d940fc,\n\t0x18000080, 0xd3d940fd, 0x18000080, 0xd3d940fe, 0x18000080, 0xd3d940ff,\n\t0x18000080, 0xb07c0000, 0xbe8a00ff, 0x000000f8, 0xbf11080a, 0x7e000280,\n\t0x7e020280, 0x7e040280, 0x7e060280, 0x7e080280, 0x7e0a0280, 0x7e0c0280,\n\t0x7e0e0280, 0x808a880a, 0xbe80320a, 0xbf84fff5, 0xbf9c0000, 0xd28c0001,\n\t0x0001007f, 0xd28d0001, 0x0002027e, 0x10020288, 0xbe8b0004, 0xb78b4000,\n\t0xd1196a01, 0x00001701, 0xbe8a0087, 0xbefc00c1, 0xd89c4000, 0x00020201,\n\t0xd89cc080, 0x00040401, 0x320202ff, 0x00000800, 0x808a810a, 0xbf84fff8,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry vgpr_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 4 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0xbf },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x400006 },  /* 64KB LDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x3F }, /*  63 - accum-offset = 256 */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * The below shaders are used to clear SGPRS, and also write the input\n * pattern into the write back buffer. The first two dispatch should be\n * scheduled simultaneously which make sure that all SGPRS could be\n * allocated, so the dispatch 1 need check write back buffer before scheduled,\n * make sure that waves of dispatch 0 are all dispacthed to all simds\n * balanced. both dispatch 0 and dispatch 1 should be halted until all waves\n * are dispatched, and then driver write a pattern to the shared memory to make\n * all waves continue.\n*/\nstatic const u32 sgpr112_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbeda0080, 0xbedb0080, 0xbedc0080, 0xbedd0080, 0xbede0080, 0xbedf0080,\n\t0xbee00080, 0xbee10080, 0xbee20080, 0xbee30080, 0xbee40080, 0xbee50080,\n\t0xbf810000\n};\n\nconst struct soc15_reg_entry sgpr112_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 8 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x340 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic const u32 sgpr96_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr96_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0xc },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x2c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * This shader is used to clear the uninitiated sgprs after the above\n * two dispatches, because of hardware feature, dispath 0 couldn't clear\n * top hole sgprs. Therefore need 4 waves per SIMD to cover these sgprs\n*/\nstatic const u32 sgpr64_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr64_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0x10 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x1c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic int gfx_v9_4_2_run_shader(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_ib *ib,\n\t\t\t\t const u32 *shader_ptr, u32 shader_size,\n\t\t\t\t const struct soc15_reg_entry *init_regs, u32 regs_size,\n\t\t\t\t u32 compute_dim_x, u64 wb_gpu_addr, u32 pattern,\n\t\t\t\t struct dma_fence **fence_ptr)\n{\n\tint r, i;\n\tuint32_t total_size, shader_offset;\n\tu64 gpu_addr;\n\n\ttotal_size = (regs_size * 3 + 4 + 5 + 5) * 4;\n\ttotal_size = ALIGN(total_size, 256);\n\tshader_offset = total_size;\n\ttotal_size += ALIGN(shader_size, 256);\n\n\t/* allocate an indirect buffer to put the commands in */\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, total_size,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\t/* load the compute shaders */\n\tfor (i = 0; i < shader_size/sizeof(u32); i++)\n\t\tib->ptr[i + (shader_offset / 4)] = shader_ptr[i];\n\n\t/* init the ib length to 0 */\n\tib->length_dw = 0;\n\n\t/* write the register state for the compute dispatch */\n\tfor (i = 0; i < regs_size; i++) {\n\t\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);\n\t\tib->ptr[ib->length_dw++] = SOC15_REG_ENTRY_OFFSET(init_regs[i])\n\t\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\t\tib->ptr[ib->length_dw++] = init_regs[i].reg_value;\n\t}\n\n\t/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */\n\tgpu_addr = (ib->gpu_addr + (u64)shader_offset) >> 8;\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_PGM_LO)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(gpu_addr);\n\n\t/* write the wb buffer address */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 3);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_USER_DATA_0)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = pattern;\n\n\t/* write dispatch packet */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);\n\tib->ptr[ib->length_dw++] = compute_dim_x; /* x */\n\tib->ptr[ib->length_dw++] = 1; /* y */\n\tib->ptr[ib->length_dw++] = 1; /* z */\n\tib->ptr[ib->length_dw++] =\n\t\tREG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);\n\n\t/* shedule the ib on the ring */\n\tr = amdgpu_ib_schedule(ring, 1, ib, NULL, fence_ptr);\n\tif (r) {\n\t\tdev_err(adev->dev, \"ib submit failed (%d).\\n\", r);\n\t\tamdgpu_ib_free(adev, ib, NULL);\n\t}\n\treturn r;\n}\n\nstatic void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t *wb_ptr)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n\t\tfor (cu = 0; cu < CU_ID_MAX; cu++) {\n\t\t\tmemset(str, 0, 256);\n\t\t\tsize = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++) {\n\t\t\t\tsize += sprintf(str + size, \"[\");\n\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\tsize += sprintf(str + size, \"%x\", wb_ptr[offset]);\n\t\t\t\t\toffset++;\n\t\t\t\t}\n\t\t\t\tsize += sprintf(str + size, \"]  \");\n\t\t\t}\n\t\t\tdev_dbg(adev->dev, \"%s\\n\", str);\n\t\t}\n\t}\n\n\tkfree(str);\n}\n\nstatic int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t *wb_ptr, uint32_t mask,\n\t\t\t\t\t      uint32_t pattern, uint32_t num_wave, bool wait)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t loop = 0;\n\tuint32_t wave_cnt;\n\tuint32_t offset;\n\n\tdo {\n\t\twave_cnt = 0;\n\t\toffset = 0;\n\n\t\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++)\n\t\t\tfor (cu = 0; cu < CU_ID_MAX; cu++)\n\t\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++)\n\t\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\t\tif (((1 << wave) & mask) &&\n\t\t\t\t\t\t    (wb_ptr[offset] == pattern))\n\t\t\t\t\t\t\twave_cnt++;\n\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t}\n\n\t\tif (wave_cnt == num_wave)\n\t\t\treturn 0;\n\n\t\tmdelay(1);\n\t} while (++loop < 2000 && wait);\n\n\tdev_err(adev->dev, \"actual wave num: %d, expected wave num: %d\\n\",\n\t\twave_cnt, num_wave);\n\n\tgfx_v9_4_2_log_wave_assignment(adev, wb_ptr);\n\n\treturn -EBADSLT;\n}\n\nstatic int gfx_v9_4_2_do_sgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ibs[3];\n\tstruct dma_fence *fences[3];\n\tu32 pattern[3] = { 0x1, 0x5, 0xa };\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready ||\n\t\t !adev->gfx.compute_ring[1].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[0],\n\t\t\tsgpr112_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr112_init_compute_shader_aldebaran),\n\t\t\tsgpr112_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr112_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[0], &fences[0]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 224 sgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11,\n\t\t\tpattern[0],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 2,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 224 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[1],\n\t\t\t&disp_ibs[1],\n\t\t\tsgpr96_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr96_init_compute_shader_aldebaran),\n\t\t\tsgpr96_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr96_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number * 2,\n\t\t\twb_ib.gpu_addr, pattern[1], &fences[1]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear next 576 sgprs\\n\");\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11111100,\n\t\t\tpattern[1], adev->gfx.cu_info.number * SIMD_ID_MAX * 6,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 576 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp1_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fences[0], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 224 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = dma_fence_wait(fences[1], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 576 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[2],\n\t\t\tsgpr64_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr64_init_compute_shader_aldebaran),\n\t\t\tsgpr64_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr64_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[2], &fences[2]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 256 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1111,\n\t\t\tpattern[2],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 4,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 256 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp2_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\tr = dma_fence_wait(fences[2], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 256 sgprs\\n\");\n\t\tgoto disp2_failed;\n\t}\n\ndisp2_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[2], NULL);\n\tdma_fence_put(fences[2]);\ndisp1_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[1], NULL);\n\tdma_fence_put(fences[1]);\ndisp0_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[0], NULL);\n\tdma_fence_put(fences[0]);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init SGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init SGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_2_do_vgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\t/* CU_ID: 0~15, SIMD_ID: 0~3, WAVE_ID: 0 ~ 9 */\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ib;\n\tstruct dma_fence *fence;\n\tu32 pattern = 0xa;\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb.\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ib,\n\t\t\tvgpr_init_compute_shader_aldebaran,\n\t\t\tsizeof(vgpr_init_compute_shader_aldebaran),\n\t\t\tvgpr_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(vgpr_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern, &fence);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear vgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fence, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1,\n\t\t\tpattern,\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX,\n\t\t\tfalse);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to cover all simds when clearing vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\ndisp_failed:\n\tamdgpu_ib_free(adev, &disp_ib, NULL);\n\tdma_fence_put(fence);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init VGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init VGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nint gfx_v9_4_2_do_edc_gpr_workarounds(struct amdgpu_device *adev)\n{\n\t/* only support when RAS is enabled */\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn 0;\n\n\t/* Workaround for ALDEBARAN, skip GPRs init in GPU reset.\n\t   Will remove it once GPRs init algorithm works for all CU settings. */\n\tif (amdgpu_in_reset(adev))\n\t\treturn 0;\n\n\tgfx_v9_4_2_do_sgprs_init(adev);\n\n\tgfx_v9_4_2_do_vgprs_init(adev);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev);\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev);\n\nvoid gfx_v9_4_2_init_golden_registers(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t die_id)\n{\n\tsoc15_program_register_sequence(adev,\n\t\t\t\t\tgolden_settings_gc_9_4_2_alde,\n\t\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde));\n\n\t/* apply golden settings per die */\n\tswitch (die_id) {\n\tcase 0:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_0,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_0));\n\t\tbreak;\n\tcase 1:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_1,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_1));\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(adev->dev,\n\t\t\t \"invalid die id %d, ignore channel fabricid remap settings\\n\",\n\t\t\t die_id);\n\t\tbreak;\n\t}\n}\n\nvoid gfx_v9_4_2_debug_trap_config_init(struct amdgpu_device *adev,\n\t\t\t\tuint32_t first_vmid,\n\t\t\t\tuint32_t last_vmid)\n{\n\tuint32_t data;\n\tint i;\n\n\tmutex_lock(&adev->srbm_mutex);\n\n\tfor (i = first_vmid; i < last_vmid; i++) {\n\t\tdata = 0;\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, TRAP_EN, 1);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_EN, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_REPLACE,\n\t\t\t\t\t0);\n\t\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_PER_VMID_CNTL), data);\n\t}\n\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, 0);\n\tmutex_unlock(&adev->srbm_mutex);\n\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA0), 0);\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA1), 0);\n}\n\nvoid gfx_v9_4_2_set_power_brake_sequence(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tgfx_v9_0_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL, PATTERN_MODE, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL, tmp);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL1, PWRBRK_STALL_EN, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL1, tmp);\n\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_INDEX, ixPWRBRK_STALL_PATTERN_CTRL);\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, PWRBRK_STALL_PATTERN_CTRL, PWRBRK_END_STEP, 0x12);\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_DATA, tmp);\n}\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_edc_counter_regs[] = {\n\t/* CPF */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT), 0, 1, 1 },\n\t/* CPC */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT), 0, 1, 1 },\n\t/* GDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT), 0, 1, 1 },\n\t/* RLC */\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2), 0, 1, 1 },\n\t/* SPI */\n\t{ SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT), 0, 8, 1 },\n\t/* SQC */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3), 0, 8, 7 },\n\t/* SQ */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT), 0, 8, 14 },\n\t/* TCP */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW), 0, 8, 14 },\n\t/* TCI */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT), 0, 1, 69 },\n\t/* TCC */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2), 0, 1, 16 },\n\t/* TCA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT), 0, 1, 2 },\n\t/* TCX */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT), 0, 1, 2 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2), 0, 1, 2 },\n\t/* TD */\n\t{ SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT), 0, 8, 14 },\n\t/* TA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT), 0, 8, 14 },\n\t/* GCEA */\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 1, 16 },\n};\n\nstatic void gfx_v9_4_2_select_se_sh(struct amdgpu_device *adev, u32 se_num,\n\t\t\t\t  u32 sh_num, u32 instance)\n{\n\tu32 data;\n\n\tif (instance == 0xffffffff)\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX,\n\t\t\t\t     instance);\n\n\tif (se_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);\n\n\tif (sh_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);\n\n\tWREG32_SOC15_RLC_SHADOW_EX(reg, GC, 0, regGRBM_GFX_INDEX, data);\n}\n\nstatic const struct soc15_ras_field_entry gfx_v9_4_2_ras_fields[] = {\n\t/* CPF */\n\t{ \"CPF_ROQ_ME2\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME2),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME2) },\n\t{ \"CPF_ROQ_ME1\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME1) },\n\t{ \"CPF_TCIU_TAG\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT) },\n\n\t/* CPC */\n\t{ \"CPC_SCRATCH\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT) },\n\t{ \"CPC_UCODE\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT) },\n\t{ \"CPC_DC_STATE_RAM_ME1\", SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT1_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT1_ME1) },\n\n\t/* GDS */\n\t{ \"GDS_GRBM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, DED) },\n\t{ \"GDS_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED) },\n\t{ \"GDS_PHY_CMD_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED) },\n\t{ \"GDS_PHY_DATA_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_DED) },\n\t{ \"GDS_ME0_CS_PIPE_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE0_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE1_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE2_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE3_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED) },\n\t{ \"GDS_ME0_GFXHP3D_PIX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_PIX_DED) },\n\t{ \"GDS_ME0_GFXHP3D_VTX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_VTX_DED) },\n\t{ \"GDS_ME0_CS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_CS_DED) },\n\t{ \"GDS_ME0_GFXHP3D_GS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_GS_DED) },\n\t{ \"GDS_ME1_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE0_DED) },\n\t{ \"GDS_ME1_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE1_DED) },\n\t{ \"GDS_ME1_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE2_DED) },\n\t{ \"GDS_ME1_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE3_DED) },\n\t{ \"GDS_ME2_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE0_DED) },\n\t{ \"GDS_ME2_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE1_DED) },\n\t{ \"GDS_ME2_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE2_DED) },\n\t{ \"GDS_ME2_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE3_DED) },\n\n\t/* RLC */\n\t{ \"RLCG_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCG_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLCV_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCV_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_TCTAG_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_DATA_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_ADDR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE0_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE1_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE2_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE3_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE4_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE5_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE6_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE7_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_DED_COUNT) },\n\n\t/* SPI */\n\t{ \"SPI_SR_MEM\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_DED_COUNT) },\n\t{ \"SPI_GDS_EXPREQ\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_DED_COUNT) },\n\t{ \"SPI_WB_GRANT_30\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_DED_COUNT) },\n\t{ \"SPI_LIFE_CNT\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT */\n\t{ \"SQC_DATA_CU0_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU0_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT2 */\n\t{ \"SQC_INST_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_INST_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT3 */\n\t{ \"SQC_INST_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_PARITY_CNT3 */\n\t{ \"SQC_INST_BANKA_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_DED_COUNT) },\n\n\t/* SQ */\n\t{ \"SQ_LDS_D\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT) },\n\t{ \"SQ_LDS_I\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT) },\n\t{ \"SQ_SGPR\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT) },\n\t{ \"SQ_VGPR0\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT) },\n\t{ \"SQ_VGPR1\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT) },\n\t{ \"SQ_VGPR2\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT) },\n\t{ \"SQ_VGPR3\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT) },\n\n\t/* TCP */\n\t{ \"TCP_CACHE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT) },\n\t{ \"TCP_LFIFO_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT) },\n\t{ \"TCP_CMD_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_DED_COUNT) },\n\t{ \"TCP_VM_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_DED_COUNT) },\n\t{ \"TCP_DB_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO0\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO1\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT) },\n\n\t/* TCI */\n\t{ \"TCI_WRITE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_DED_COUNT) },\n\n\t/* TCC */\n\t{ \"TCC_CACHE_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT) },\n\t{ \"TCC_CACHE_DIRTY\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT) },\n\t{ \"TCC_HIGH_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_LOW_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_SRC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO_NEXT_RAM\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_DED_COUNT) },\n\t{ \"TCC_CACHE_TAG_PROBE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_DED_COUNT) },\n\t{ \"TCC_UC_ATOMIC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_DED_COUNT) },\n\t{ \"TCC_WRITE_CACHE_READ\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_DED_COUNT) },\n\t{ \"TCC_RETURN_CONTROL\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_DED_COUNT) },\n\t{ \"TCC_IN_USE_TRANSFER\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_DED_COUNT) },\n\t{ \"TCC_IN_USE_DEC\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_DED_COUNT) },\n\t{ \"TCC_WRITE_RETURN\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_DED_COUNT) },\n\t{ \"TCC_RETURN_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_DED_COUNT) },\n\n\t/* TCA */\n\t{ \"TCA_HOLE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_DED_COUNT) },\n\t{ \"TCA_REQ_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_DED_COUNT) },\n\n\t/* TCX */\n\t{ \"TCX_GROUP0\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_DED_COUNT) },\n\t{ \"TCX_GROUP1\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_DED_COUNT) },\n\t{ \"TCX_GROUP2\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_DED_COUNT) },\n\t{ \"TCX_GROUP3\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_DED_COUNT) },\n\t{ \"TCX_GROUP4\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_DED_COUNT) },\n\t{ \"TCX_GROUP5\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP5_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP6\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP6_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP7\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP7_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP8\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP8_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP9\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP9_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP10\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP10_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP11\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP11_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP12\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP12_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP13\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP13_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP14\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP14_SED_COUNT), 0, 0 },\n\n\t/* TD */\n\t{ \"TD_SS_FIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT) },\n\t{ \"TD_SS_FIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT) },\n\t{ \"TD_CS_FIFO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_DED_COUNT) },\n\n\t/* TA */\n\t{ \"TA_FS_DFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_DED_COUNT) },\n\t{ \"TA_FL_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_DED_COUNT) },\n\t{ \"TA_FX_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_DED_COUNT) },\n\t{ \"TA_FS_CFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT */\n\t{ \"EA_DRAMRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_RRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_WRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_IOWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT), 0, 0 },\n\n\t/* EA - regGCEA_EDC_CNT2 */\n\t{ \"EA_GMIRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_MAM_D0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_DED_COUNT) },\n\t{ \"EA_MAM_D1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_DED_COUNT) },\n\t{ \"EA_MAM_D2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_DED_COUNT) },\n\t{ \"EA_MAM_D3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT3 */\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IORD_CMDMEM_DED_COUNT) },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IOWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_MAM_A0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_DED_COUNT) },\n\t{ \"EA_MAM_A1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_DED_COUNT) },\n\t{ \"EA_MAM_A2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_DED_COUNT) },\n\t{ \"EA_MAM_A3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_DED_COUNT) },\n\t{ \"EA_MAM_AFMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_DED_COUNT) },\n};\n\nstatic const char * const vml2_walker_mems[] = {\n\t\"UTC_VML2_CACHE_PDE0_MEM0\",\n\t\"UTC_VML2_CACHE_PDE0_MEM1\",\n\t\"UTC_VML2_CACHE_PDE1_MEM0\",\n\t\"UTC_VML2_CACHE_PDE1_MEM1\",\n\t\"UTC_VML2_CACHE_PDE2_MEM0\",\n\t\"UTC_VML2_CACHE_PDE2_MEM1\",\n\t\"UTC_VML2_RDIF_ARADDRS\",\n\t\"UTC_VML2_RDIF_LOG_FIFO\",\n\t\"UTC_VML2_QUEUE_REQ\",\n\t\"UTC_VML2_QUEUE_RET\",\n};\n\nstatic struct gfx_v9_4_2_utc_block gfx_v9_4_2_utc_blocks[] = {\n\t{ VML2_MEM, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ VML2_WALKER_MEM, ARRAY_SIZE(vml2_walker_mems), 1, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_WALKER_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ UTCL2_MEM, 18, 1, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, UTCL2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_2M, 8, 2, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_2M_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_32K, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_32K_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_4K, 8, 2, 8,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_4K_DSM_CNTL, WRITE_COUNTERS, 1) },\n};\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_ea_err_status_regs = {\n\tSOC15_REG_ENTRY(GC, 0, regGCEA_ERR_STATUS), 0, 1, 16\n};\n\nstatic int gfx_v9_4_2_get_reg_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t  const struct soc15_reg_entry *reg,\n\t\t\t\t\t  uint32_t se_id, uint32_t inst_id,\n\t\t\t\t\t  uint32_t value, uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i;\n\tuint32_t sec_cnt, ded_cnt;\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_ras_fields); i++) {\n\t\tif (gfx_v9_4_2_ras_fields[i].reg_offset != reg->reg_offset ||\n\t\t    gfx_v9_4_2_ras_fields[i].seg != reg->seg ||\n\t\t    gfx_v9_4_2_ras_fields[i].inst != reg->inst)\n\t\t\tcontinue;\n\n\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], sec);\n\t\tif (sec_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], SEC %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t sec_cnt);\n\t\t\t*sec_count += sec_cnt;\n\t\t}\n\n\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], ded);\n\t\tif (ded_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], DED %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t ded_cnt);\n\t\t\t*ded_count += ded_cnt;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_2_query_sram_edc_count(struct amdgpu_device *adev,\n\t\t\t\tuint32_t *sec_count, uint32_t *ded_count)\n{\n\tuint32_t i, j, k, data;\n\tuint32_t sec_cnt = 0, ded_cnt = 0;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_edc_counter_regs); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_edc_counter_regs[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_2_edc_counter_regs[i].instance;\n\t\t\t     k++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, j, 0, k);\n\n\t\t\t\t/* if sec/ded_count is null, just clear counter */\n\t\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]));\n\n\t\t\t\tif (!data)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tgfx_v9_4_2_get_reg_error_count(adev,\n\t\t\t\t\t&gfx_v9_4_2_edc_counter_regs[i],\n\t\t\t\t\tj, k, data, &sec_cnt, &ded_cnt);\n\n\t\t\t\t/* clear counter after read */\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count += sec_cnt;\n\t\t*ded_count += ded_cnt;\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_log_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t struct gfx_v9_4_2_utc_block *blk,\n\t\t\t\t\t uint32_t instance, uint32_t sec_cnt,\n\t\t\t\t\t uint32_t ded_cnt)\n{\n\tuint32_t bank, way, mem;\n\tstatic const char * const vml2_way_str[] = { \"BIGK\", \"4K\" };\n\tstatic const char * const utcl2_rounter_str[] = { \"VMC\", \"APT\" };\n\n\tmem = instance % blk->num_mem_blocks;\n\tway = (instance / blk->num_mem_blocks) % blk->num_ways;\n\tbank = instance / (blk->num_mem_blocks * blk->num_ways);\n\n\tswitch (blk->type) {\n\tcase VML2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_VML2_BANK_CACHE_%d_%s_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, vml2_way_str[way], mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase VML2_WALKER_MEM:\n\t\tdev_info(adev->dev, \"GFX SubBlock %s, SED %d, DED %d\\n\",\n\t\t\t vml2_walker_mems[bank], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase UTCL2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTCL2_ROUTER_IFIF%d_GROUP0_%s, SED %d, DED %d\\n\",\n\t\t\tbank, utcl2_rounter_str[mem], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_2M:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_2M_BANK%d_WAY%d_MEM, SED %d, DED %d\\n\",\n\t\t\tbank, way, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_32K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_32K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_4K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_4K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_2_query_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i, j, data;\n\tuint32_t sec_cnt, ded_cnt;\n\tuint32_t num_instances;\n\tstruct gfx_v9_4_2_utc_block *blk;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_utc_blocks); i++) {\n\t\tblk = &gfx_v9_4_2_utc_blocks[i];\n\t\tnum_instances =\n\t\t\tblk->num_banks * blk->num_ways * blk->num_mem_blocks;\n\t\tfor (j = 0; j < num_instances; j++) {\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->idx_reg), j);\n\n\t\t\t/* if sec/ded_count is NULL, just clear counter */\n\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t\t       blk->clear);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg));\n\t\t\tif (!data)\n\t\t\t\tcontinue;\n\n\t\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, sec);\n\t\t\t*sec_count += sec_cnt;\n\t\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, ded);\n\t\t\t*ded_count += ded_cnt;\n\n\t\t\t/* clear counter after read */\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t       blk->clear);\n\n\t\t\t/* print the edc count */\n\t\t\tif (sec_cnt || ded_cnt)\n\t\t\t\tgfx_v9_4_2_log_utc_edc_count(adev, blk, j, sec_cnt,\n\t\t\t\t\t\t\t     ded_cnt);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t    void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tuint32_t sec_count = 0, ded_count = 0;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\terr_data->ue_count = 0;\n\terr_data->ce_count = 0;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n\tgfx_v9_4_2_query_utc_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n}\n\nstatic void gfx_v9_4_2_reset_utc_err_status(struct amdgpu_device *adev)\n{\n\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n}\n\nstatic void gfx_v9_4_2_reset_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\tvalue = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\t\t\tvalue = REG_SET_FIELD(value, GCEA_ERR_STATUS, CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), value);\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, NULL, NULL);\n\tgfx_v9_4_2_query_utc_edc_count(adev, NULL, NULL);\n}\n\nstatic void gfx_v9_4_2_query_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t reg_value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\treg_value = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\n\t\t\tif (REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_WRRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_DATAPARITY_ERROR)) {\n\t\t\t\tdev_warn(adev->dev, \"GCEA err detected at instance: %d, status: 0x%x!\\n\",\n\t\t\t\t\t\tj, reg_value);\n\t\t\t}\n\t\t\t/* clear after read */\n\t\t\treg_value = REG_SET_FIELD(reg_value, GCEA_ERR_STATUS,\n\t\t\t\t\t\t  CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), reg_value);\n\t\t}\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_query_utc_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX UTCL2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Walker Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_ea_err_status(adev);\n\tgfx_v9_4_2_query_utc_err_status(adev);\n\tgfx_v9_4_2_query_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_reset_utc_err_status(adev);\n\tgfx_v9_4_2_reset_ea_err_status(adev);\n\tgfx_v9_4_2_reset_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_enable_watchdog_timer(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\tuint32_t data;\n\n\tdata = REG_SET_FIELD(0, SQ_TIMEOUT_CONFIG, TIMEOUT_FATAL_DISABLE,\n\t\t\t     amdgpu_watchdog_timer.timeout_fatal_disable ? 1 :\n\t\t\t\t\t\t\t\t\t   0);\n\n\tif (amdgpu_watchdog_timer.timeout_fatal_disable &&\n\t    (amdgpu_watchdog_timer.period < 1 ||\n\t     amdgpu_watchdog_timer.period > 0x23)) {\n\t\tdev_warn(adev->dev, \"Watchdog period range is 1 to 0x23\\n\");\n\t\tamdgpu_watchdog_timer.period = 0x23;\n\t}\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, PERIOD_SEL,\n\t\t\t     amdgpu_watchdog_timer.period);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tgfx_v9_4_2_select_se_sh(adev, i, 0xffffffff, 0xffffffff);\n\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_CONFIG, data);\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t address)\n{\n\tWREG32_SOC15_RLC_EX(reg, GC, 0, regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(address << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK));\n\treturn RREG32_SOC15(GC, 0, regSQ_IND_DATA);\n}\n\nstatic void gfx_v9_4_2_log_cu_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t status)\n{\n\tstruct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;\n\tuint32_t i, simd, wave;\n\tuint32_t wave_status;\n\tuint32_t wave_pc_lo, wave_pc_hi;\n\tuint32_t wave_exec_lo, wave_exec_hi;\n\tuint32_t wave_inst_dw0, wave_inst_dw1;\n\tuint32_t wave_ib_sts;\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (!((i << 1) & status))\n\t\t\tcontinue;\n\n\t\tsimd = i / cu_info->max_waves_per_simd;\n\t\twave = i % cu_info->max_waves_per_simd;\n\n\t\twave_status = wave_read_ind(adev, simd, wave, ixSQ_WAVE_STATUS);\n\t\twave_pc_lo = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_LO);\n\t\twave_pc_hi = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_HI);\n\t\twave_exec_lo =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_LO);\n\t\twave_exec_hi =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_HI);\n\t\twave_inst_dw0 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW0);\n\t\twave_inst_dw1 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW1);\n\t\twave_ib_sts = wave_read_ind(adev, simd, wave, ixSQ_WAVE_IB_STS);\n\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"\\t SIMD %d, Wave %d: status 0x%x, pc 0x%llx, exec 0x%llx, inst 0x%llx, ib_sts 0x%x\\n\",\n\t\t\tsimd, wave, wave_status,\n\t\t\t((uint64_t)wave_pc_hi << 32 | wave_pc_lo),\n\t\t\t((uint64_t)wave_exec_hi << 32 | wave_exec_lo),\n\t\t\t((uint64_t)wave_inst_dw1 << 32 | wave_inst_dw0),\n\t\t\twave_ib_sts);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\tuint32_t status;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tstatus = RREG32_SOC15(GC, 0,\n\t\t\t\t\t\t      regSQ_TIMEOUT_STATUS);\n\t\t\t\tif (status != 0) {\n\t\t\t\t\tdev_info(\n\t\t\t\t\t\tadev->dev,\n\t\t\t\t\t\t\"GFX Watchdog Timeout: SE %d, SH %d, CU %d\\n\",\n\t\t\t\t\t\tse_idx, sh_idx, cu_idx);\n\t\t\t\t\tgfx_v9_4_2_log_cu_timeout_status(\n\t\t\t\t\t\tadev, status);\n\t\t\t\t}\n\t\t\t\t/* clear old status */\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n\n\nstruct amdgpu_ras_block_hw_ops  gfx_v9_4_2_ras_ops = {\n\t\t.query_ras_error_count = &gfx_v9_4_2_query_ras_error_count,\n\t\t.reset_ras_error_count = &gfx_v9_4_2_reset_ras_error_count,\n\t\t.query_ras_error_status = &gfx_v9_4_2_query_ras_error_status,\n\t\t.reset_ras_error_status = &gfx_v9_4_2_reset_ras_error_status,\n};\n\nstruct amdgpu_gfx_ras gfx_v9_4_2_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &gfx_v9_4_2_ras_ops,\n\t},\n\t.enable_watchdog_timer = &gfx_v9_4_2_enable_watchdog_timer,\n};\n", "source_code_path": "gfx_v9_4_2.c", "line_number": 427, "generated_diff": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: gfx_v9_4_2.c\nLine: 427\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nThe function gfx_v9_4_2_log_wave_assignment() uses a fixed-size buffer to store a formatted string, but lacks sufficient bounds checking. This can lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size, potentially causing data corruption or system instability.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "gfx_v9_4_2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nA buffer overflow vulnerability exists in the function gfx_v9_4_2_log_wave_assignment(), which uses a fixed-size buffer to store a formatted string without implementing proper length checks. If the accumulated string length exceeds the buffer size, this can result in a buffer overflow, potentially leading to system crashes or data corruption.\n", "diff_path": "gfx_v9_4_2-diff.txt", "diff_text": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n", "code": "/*\n * Copyright 2020 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include \"amdgpu.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n\n#include \"gc/gc_9_4_2_offset.h\"\n#include \"gc/gc_9_4_2_sh_mask.h\"\n#include \"gfx_v9_0.h\"\n\n#include \"gfx_v9_4_2.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_gfx.h\"\n\n#define SE_ID_MAX 8\n#define CU_ID_MAX 16\n#define SIMD_ID_MAX 4\n#define WAVE_ID_MAX 10\n\nenum gfx_v9_4_2_utc_type {\n\tVML2_MEM,\n\tVML2_WALKER_MEM,\n\tUTCL2_MEM,\n\tATC_L2_CACHE_2M,\n\tATC_L2_CACHE_32K,\n\tATC_L2_CACHE_4K\n};\n\nstruct gfx_v9_4_2_utc_block {\n\tenum gfx_v9_4_2_utc_type type;\n\tuint32_t num_banks;\n\tuint32_t num_ways;\n\tuint32_t num_mem_blocks;\n\tstruct soc15_reg idx_reg;\n\tstruct soc15_reg data_reg;\n\tuint32_t sec_count_mask;\n\tuint32_t sec_count_shift;\n\tuint32_t ded_count_mask;\n\tuint32_t ded_count_shift;\n\tuint32_t clear;\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x141dc920),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0x3b458b93),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x1a4f5583),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0x317717f6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x107cc1e6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x351),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_1[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x2591aa38),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0xac9e88b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x2bc3369b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0xfb74ee),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x21f0a2fe),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x49),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regGB_ADDR_CONFIG, 0xffff77ff, 0x2a114042),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTA_CNTL_AUX, 0xfffffeef, 0x10b0000),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_UTCL1_CNTL1, 0xffffffff, 0x30800400),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCI_CNTL_3, 0xff, 0x20),\n};\n\n/*\n * This shader is used to clear VGPRS and LDS, and also write the input\n * pattern into the write back buffer, which will be used by driver to\n * check whether all SIMDs have been covered.\n*/\nstatic const u32 vgpr_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xd3d94000,\n\t0x18000080, 0xd3d94001, 0x18000080, 0xd3d94002, 0x18000080, 0xd3d94003,\n\t0x18000080, 0xd3d94004, 0x18000080, 0xd3d94005, 0x18000080, 0xd3d94006,\n\t0x18000080, 0xd3d94007, 0x18000080, 0xd3d94008, 0x18000080, 0xd3d94009,\n\t0x18000080, 0xd3d9400a, 0x18000080, 0xd3d9400b, 0x18000080, 0xd3d9400c,\n\t0x18000080, 0xd3d9400d, 0x18000080, 0xd3d9400e, 0x18000080, 0xd3d9400f,\n\t0x18000080, 0xd3d94010, 0x18000080, 0xd3d94011, 0x18000080, 0xd3d94012,\n\t0x18000080, 0xd3d94013, 0x18000080, 0xd3d94014, 0x18000080, 0xd3d94015,\n\t0x18000080, 0xd3d94016, 0x18000080, 0xd3d94017, 0x18000080, 0xd3d94018,\n\t0x18000080, 0xd3d94019, 0x18000080, 0xd3d9401a, 0x18000080, 0xd3d9401b,\n\t0x18000080, 0xd3d9401c, 0x18000080, 0xd3d9401d, 0x18000080, 0xd3d9401e,\n\t0x18000080, 0xd3d9401f, 0x18000080, 0xd3d94020, 0x18000080, 0xd3d94021,\n\t0x18000080, 0xd3d94022, 0x18000080, 0xd3d94023, 0x18000080, 0xd3d94024,\n\t0x18000080, 0xd3d94025, 0x18000080, 0xd3d94026, 0x18000080, 0xd3d94027,\n\t0x18000080, 0xd3d94028, 0x18000080, 0xd3d94029, 0x18000080, 0xd3d9402a,\n\t0x18000080, 0xd3d9402b, 0x18000080, 0xd3d9402c, 0x18000080, 0xd3d9402d,\n\t0x18000080, 0xd3d9402e, 0x18000080, 0xd3d9402f, 0x18000080, 0xd3d94030,\n\t0x18000080, 0xd3d94031, 0x18000080, 0xd3d94032, 0x18000080, 0xd3d94033,\n\t0x18000080, 0xd3d94034, 0x18000080, 0xd3d94035, 0x18000080, 0xd3d94036,\n\t0x18000080, 0xd3d94037, 0x18000080, 0xd3d94038, 0x18000080, 0xd3d94039,\n\t0x18000080, 0xd3d9403a, 0x18000080, 0xd3d9403b, 0x18000080, 0xd3d9403c,\n\t0x18000080, 0xd3d9403d, 0x18000080, 0xd3d9403e, 0x18000080, 0xd3d9403f,\n\t0x18000080, 0xd3d94040, 0x18000080, 0xd3d94041, 0x18000080, 0xd3d94042,\n\t0x18000080, 0xd3d94043, 0x18000080, 0xd3d94044, 0x18000080, 0xd3d94045,\n\t0x18000080, 0xd3d94046, 0x18000080, 0xd3d94047, 0x18000080, 0xd3d94048,\n\t0x18000080, 0xd3d94049, 0x18000080, 0xd3d9404a, 0x18000080, 0xd3d9404b,\n\t0x18000080, 0xd3d9404c, 0x18000080, 0xd3d9404d, 0x18000080, 0xd3d9404e,\n\t0x18000080, 0xd3d9404f, 0x18000080, 0xd3d94050, 0x18000080, 0xd3d94051,\n\t0x18000080, 0xd3d94052, 0x18000080, 0xd3d94053, 0x18000080, 0xd3d94054,\n\t0x18000080, 0xd3d94055, 0x18000080, 0xd3d94056, 0x18000080, 0xd3d94057,\n\t0x18000080, 0xd3d94058, 0x18000080, 0xd3d94059, 0x18000080, 0xd3d9405a,\n\t0x18000080, 0xd3d9405b, 0x18000080, 0xd3d9405c, 0x18000080, 0xd3d9405d,\n\t0x18000080, 0xd3d9405e, 0x18000080, 0xd3d9405f, 0x18000080, 0xd3d94060,\n\t0x18000080, 0xd3d94061, 0x18000080, 0xd3d94062, 0x18000080, 0xd3d94063,\n\t0x18000080, 0xd3d94064, 0x18000080, 0xd3d94065, 0x18000080, 0xd3d94066,\n\t0x18000080, 0xd3d94067, 0x18000080, 0xd3d94068, 0x18000080, 0xd3d94069,\n\t0x18000080, 0xd3d9406a, 0x18000080, 0xd3d9406b, 0x18000080, 0xd3d9406c,\n\t0x18000080, 0xd3d9406d, 0x18000080, 0xd3d9406e, 0x18000080, 0xd3d9406f,\n\t0x18000080, 0xd3d94070, 0x18000080, 0xd3d94071, 0x18000080, 0xd3d94072,\n\t0x18000080, 0xd3d94073, 0x18000080, 0xd3d94074, 0x18000080, 0xd3d94075,\n\t0x18000080, 0xd3d94076, 0x18000080, 0xd3d94077, 0x18000080, 0xd3d94078,\n\t0x18000080, 0xd3d94079, 0x18000080, 0xd3d9407a, 0x18000080, 0xd3d9407b,\n\t0x18000080, 0xd3d9407c, 0x18000080, 0xd3d9407d, 0x18000080, 0xd3d9407e,\n\t0x18000080, 0xd3d9407f, 0x18000080, 0xd3d94080, 0x18000080, 0xd3d94081,\n\t0x18000080, 0xd3d94082, 0x18000080, 0xd3d94083, 0x18000080, 0xd3d94084,\n\t0x18000080, 0xd3d94085, 0x18000080, 0xd3d94086, 0x18000080, 0xd3d94087,\n\t0x18000080, 0xd3d94088, 0x18000080, 0xd3d94089, 0x18000080, 0xd3d9408a,\n\t0x18000080, 0xd3d9408b, 0x18000080, 0xd3d9408c, 0x18000080, 0xd3d9408d,\n\t0x18000080, 0xd3d9408e, 0x18000080, 0xd3d9408f, 0x18000080, 0xd3d94090,\n\t0x18000080, 0xd3d94091, 0x18000080, 0xd3d94092, 0x18000080, 0xd3d94093,\n\t0x18000080, 0xd3d94094, 0x18000080, 0xd3d94095, 0x18000080, 0xd3d94096,\n\t0x18000080, 0xd3d94097, 0x18000080, 0xd3d94098, 0x18000080, 0xd3d94099,\n\t0x18000080, 0xd3d9409a, 0x18000080, 0xd3d9409b, 0x18000080, 0xd3d9409c,\n\t0x18000080, 0xd3d9409d, 0x18000080, 0xd3d9409e, 0x18000080, 0xd3d9409f,\n\t0x18000080, 0xd3d940a0, 0x18000080, 0xd3d940a1, 0x18000080, 0xd3d940a2,\n\t0x18000080, 0xd3d940a3, 0x18000080, 0xd3d940a4, 0x18000080, 0xd3d940a5,\n\t0x18000080, 0xd3d940a6, 0x18000080, 0xd3d940a7, 0x18000080, 0xd3d940a8,\n\t0x18000080, 0xd3d940a9, 0x18000080, 0xd3d940aa, 0x18000080, 0xd3d940ab,\n\t0x18000080, 0xd3d940ac, 0x18000080, 0xd3d940ad, 0x18000080, 0xd3d940ae,\n\t0x18000080, 0xd3d940af, 0x18000080, 0xd3d940b0, 0x18000080, 0xd3d940b1,\n\t0x18000080, 0xd3d940b2, 0x18000080, 0xd3d940b3, 0x18000080, 0xd3d940b4,\n\t0x18000080, 0xd3d940b5, 0x18000080, 0xd3d940b6, 0x18000080, 0xd3d940b7,\n\t0x18000080, 0xd3d940b8, 0x18000080, 0xd3d940b9, 0x18000080, 0xd3d940ba,\n\t0x18000080, 0xd3d940bb, 0x18000080, 0xd3d940bc, 0x18000080, 0xd3d940bd,\n\t0x18000080, 0xd3d940be, 0x18000080, 0xd3d940bf, 0x18000080, 0xd3d940c0,\n\t0x18000080, 0xd3d940c1, 0x18000080, 0xd3d940c2, 0x18000080, 0xd3d940c3,\n\t0x18000080, 0xd3d940c4, 0x18000080, 0xd3d940c5, 0x18000080, 0xd3d940c6,\n\t0x18000080, 0xd3d940c7, 0x18000080, 0xd3d940c8, 0x18000080, 0xd3d940c9,\n\t0x18000080, 0xd3d940ca, 0x18000080, 0xd3d940cb, 0x18000080, 0xd3d940cc,\n\t0x18000080, 0xd3d940cd, 0x18000080, 0xd3d940ce, 0x18000080, 0xd3d940cf,\n\t0x18000080, 0xd3d940d0, 0x18000080, 0xd3d940d1, 0x18000080, 0xd3d940d2,\n\t0x18000080, 0xd3d940d3, 0x18000080, 0xd3d940d4, 0x18000080, 0xd3d940d5,\n\t0x18000080, 0xd3d940d6, 0x18000080, 0xd3d940d7, 0x18000080, 0xd3d940d8,\n\t0x18000080, 0xd3d940d9, 0x18000080, 0xd3d940da, 0x18000080, 0xd3d940db,\n\t0x18000080, 0xd3d940dc, 0x18000080, 0xd3d940dd, 0x18000080, 0xd3d940de,\n\t0x18000080, 0xd3d940df, 0x18000080, 0xd3d940e0, 0x18000080, 0xd3d940e1,\n\t0x18000080, 0xd3d940e2, 0x18000080, 0xd3d940e3, 0x18000080, 0xd3d940e4,\n\t0x18000080, 0xd3d940e5, 0x18000080, 0xd3d940e6, 0x18000080, 0xd3d940e7,\n\t0x18000080, 0xd3d940e8, 0x18000080, 0xd3d940e9, 0x18000080, 0xd3d940ea,\n\t0x18000080, 0xd3d940eb, 0x18000080, 0xd3d940ec, 0x18000080, 0xd3d940ed,\n\t0x18000080, 0xd3d940ee, 0x18000080, 0xd3d940ef, 0x18000080, 0xd3d940f0,\n\t0x18000080, 0xd3d940f1, 0x18000080, 0xd3d940f2, 0x18000080, 0xd3d940f3,\n\t0x18000080, 0xd3d940f4, 0x18000080, 0xd3d940f5, 0x18000080, 0xd3d940f6,\n\t0x18000080, 0xd3d940f7, 0x18000080, 0xd3d940f8, 0x18000080, 0xd3d940f9,\n\t0x18000080, 0xd3d940fa, 0x18000080, 0xd3d940fb, 0x18000080, 0xd3d940fc,\n\t0x18000080, 0xd3d940fd, 0x18000080, 0xd3d940fe, 0x18000080, 0xd3d940ff,\n\t0x18000080, 0xb07c0000, 0xbe8a00ff, 0x000000f8, 0xbf11080a, 0x7e000280,\n\t0x7e020280, 0x7e040280, 0x7e060280, 0x7e080280, 0x7e0a0280, 0x7e0c0280,\n\t0x7e0e0280, 0x808a880a, 0xbe80320a, 0xbf84fff5, 0xbf9c0000, 0xd28c0001,\n\t0x0001007f, 0xd28d0001, 0x0002027e, 0x10020288, 0xbe8b0004, 0xb78b4000,\n\t0xd1196a01, 0x00001701, 0xbe8a0087, 0xbefc00c1, 0xd89c4000, 0x00020201,\n\t0xd89cc080, 0x00040401, 0x320202ff, 0x00000800, 0x808a810a, 0xbf84fff8,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry vgpr_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 4 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0xbf },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x400006 },  /* 64KB LDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x3F }, /*  63 - accum-offset = 256 */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * The below shaders are used to clear SGPRS, and also write the input\n * pattern into the write back buffer. The first two dispatch should be\n * scheduled simultaneously which make sure that all SGPRS could be\n * allocated, so the dispatch 1 need check write back buffer before scheduled,\n * make sure that waves of dispatch 0 are all dispacthed to all simds\n * balanced. both dispatch 0 and dispatch 1 should be halted until all waves\n * are dispatched, and then driver write a pattern to the shared memory to make\n * all waves continue.\n*/\nstatic const u32 sgpr112_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbeda0080, 0xbedb0080, 0xbedc0080, 0xbedd0080, 0xbede0080, 0xbedf0080,\n\t0xbee00080, 0xbee10080, 0xbee20080, 0xbee30080, 0xbee40080, 0xbee50080,\n\t0xbf810000\n};\n\nconst struct soc15_reg_entry sgpr112_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 8 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x340 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic const u32 sgpr96_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr96_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0xc },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x2c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * This shader is used to clear the uninitiated sgprs after the above\n * two dispatches, because of hardware feature, dispath 0 couldn't clear\n * top hole sgprs. Therefore need 4 waves per SIMD to cover these sgprs\n*/\nstatic const u32 sgpr64_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr64_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0x10 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x1c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic int gfx_v9_4_2_run_shader(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_ib *ib,\n\t\t\t\t const u32 *shader_ptr, u32 shader_size,\n\t\t\t\t const struct soc15_reg_entry *init_regs, u32 regs_size,\n\t\t\t\t u32 compute_dim_x, u64 wb_gpu_addr, u32 pattern,\n\t\t\t\t struct dma_fence **fence_ptr)\n{\n\tint r, i;\n\tuint32_t total_size, shader_offset;\n\tu64 gpu_addr;\n\n\ttotal_size = (regs_size * 3 + 4 + 5 + 5) * 4;\n\ttotal_size = ALIGN(total_size, 256);\n\tshader_offset = total_size;\n\ttotal_size += ALIGN(shader_size, 256);\n\n\t/* allocate an indirect buffer to put the commands in */\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, total_size,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\t/* load the compute shaders */\n\tfor (i = 0; i < shader_size/sizeof(u32); i++)\n\t\tib->ptr[i + (shader_offset / 4)] = shader_ptr[i];\n\n\t/* init the ib length to 0 */\n\tib->length_dw = 0;\n\n\t/* write the register state for the compute dispatch */\n\tfor (i = 0; i < regs_size; i++) {\n\t\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);\n\t\tib->ptr[ib->length_dw++] = SOC15_REG_ENTRY_OFFSET(init_regs[i])\n\t\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\t\tib->ptr[ib->length_dw++] = init_regs[i].reg_value;\n\t}\n\n\t/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */\n\tgpu_addr = (ib->gpu_addr + (u64)shader_offset) >> 8;\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_PGM_LO)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(gpu_addr);\n\n\t/* write the wb buffer address */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 3);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_USER_DATA_0)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = pattern;\n\n\t/* write dispatch packet */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);\n\tib->ptr[ib->length_dw++] = compute_dim_x; /* x */\n\tib->ptr[ib->length_dw++] = 1; /* y */\n\tib->ptr[ib->length_dw++] = 1; /* z */\n\tib->ptr[ib->length_dw++] =\n\t\tREG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);\n\n\t/* shedule the ib on the ring */\n\tr = amdgpu_ib_schedule(ring, 1, ib, NULL, fence_ptr);\n\tif (r) {\n\t\tdev_err(adev->dev, \"ib submit failed (%d).\\n\", r);\n\t\tamdgpu_ib_free(adev, ib, NULL);\n\t}\n\treturn r;\n}\n\nstatic void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t *wb_ptr)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n\t\tfor (cu = 0; cu < CU_ID_MAX; cu++) {\n\t\t\tmemset(str, 0, 256);\n\t\t\tsize = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++) {\n\t\t\t\tsize += sprintf(str + size, \"[\");\n\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\tsize += sprintf(str + size, \"%x\", wb_ptr[offset]);\n\t\t\t\t\toffset++;\n\t\t\t\t}\n\t\t\t\tsize += sprintf(str + size, \"]  \");\n\t\t\t}\n\t\t\tdev_dbg(adev->dev, \"%s\\n\", str);\n\t\t}\n\t}\n\n\tkfree(str);\n}\n\nstatic int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t *wb_ptr, uint32_t mask,\n\t\t\t\t\t      uint32_t pattern, uint32_t num_wave, bool wait)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t loop = 0;\n\tuint32_t wave_cnt;\n\tuint32_t offset;\n\n\tdo {\n\t\twave_cnt = 0;\n\t\toffset = 0;\n\n\t\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++)\n\t\t\tfor (cu = 0; cu < CU_ID_MAX; cu++)\n\t\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++)\n\t\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\t\tif (((1 << wave) & mask) &&\n\t\t\t\t\t\t    (wb_ptr[offset] == pattern))\n\t\t\t\t\t\t\twave_cnt++;\n\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t}\n\n\t\tif (wave_cnt == num_wave)\n\t\t\treturn 0;\n\n\t\tmdelay(1);\n\t} while (++loop < 2000 && wait);\n\n\tdev_err(adev->dev, \"actual wave num: %d, expected wave num: %d\\n\",\n\t\twave_cnt, num_wave);\n\n\tgfx_v9_4_2_log_wave_assignment(adev, wb_ptr);\n\n\treturn -EBADSLT;\n}\n\nstatic int gfx_v9_4_2_do_sgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ibs[3];\n\tstruct dma_fence *fences[3];\n\tu32 pattern[3] = { 0x1, 0x5, 0xa };\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready ||\n\t\t !adev->gfx.compute_ring[1].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[0],\n\t\t\tsgpr112_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr112_init_compute_shader_aldebaran),\n\t\t\tsgpr112_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr112_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[0], &fences[0]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 224 sgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11,\n\t\t\tpattern[0],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 2,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 224 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[1],\n\t\t\t&disp_ibs[1],\n\t\t\tsgpr96_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr96_init_compute_shader_aldebaran),\n\t\t\tsgpr96_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr96_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number * 2,\n\t\t\twb_ib.gpu_addr, pattern[1], &fences[1]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear next 576 sgprs\\n\");\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11111100,\n\t\t\tpattern[1], adev->gfx.cu_info.number * SIMD_ID_MAX * 6,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 576 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp1_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fences[0], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 224 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = dma_fence_wait(fences[1], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 576 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[2],\n\t\t\tsgpr64_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr64_init_compute_shader_aldebaran),\n\t\t\tsgpr64_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr64_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[2], &fences[2]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 256 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1111,\n\t\t\tpattern[2],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 4,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 256 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp2_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\tr = dma_fence_wait(fences[2], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 256 sgprs\\n\");\n\t\tgoto disp2_failed;\n\t}\n\ndisp2_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[2], NULL);\n\tdma_fence_put(fences[2]);\ndisp1_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[1], NULL);\n\tdma_fence_put(fences[1]);\ndisp0_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[0], NULL);\n\tdma_fence_put(fences[0]);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init SGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init SGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_2_do_vgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\t/* CU_ID: 0~15, SIMD_ID: 0~3, WAVE_ID: 0 ~ 9 */\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ib;\n\tstruct dma_fence *fence;\n\tu32 pattern = 0xa;\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb.\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ib,\n\t\t\tvgpr_init_compute_shader_aldebaran,\n\t\t\tsizeof(vgpr_init_compute_shader_aldebaran),\n\t\t\tvgpr_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(vgpr_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern, &fence);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear vgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fence, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1,\n\t\t\tpattern,\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX,\n\t\t\tfalse);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to cover all simds when clearing vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\ndisp_failed:\n\tamdgpu_ib_free(adev, &disp_ib, NULL);\n\tdma_fence_put(fence);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init VGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init VGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nint gfx_v9_4_2_do_edc_gpr_workarounds(struct amdgpu_device *adev)\n{\n\t/* only support when RAS is enabled */\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn 0;\n\n\t/* Workaround for ALDEBARAN, skip GPRs init in GPU reset.\n\t   Will remove it once GPRs init algorithm works for all CU settings. */\n\tif (amdgpu_in_reset(adev))\n\t\treturn 0;\n\n\tgfx_v9_4_2_do_sgprs_init(adev);\n\n\tgfx_v9_4_2_do_vgprs_init(adev);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev);\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev);\n\nvoid gfx_v9_4_2_init_golden_registers(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t die_id)\n{\n\tsoc15_program_register_sequence(adev,\n\t\t\t\t\tgolden_settings_gc_9_4_2_alde,\n\t\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde));\n\n\t/* apply golden settings per die */\n\tswitch (die_id) {\n\tcase 0:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_0,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_0));\n\t\tbreak;\n\tcase 1:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_1,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_1));\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(adev->dev,\n\t\t\t \"invalid die id %d, ignore channel fabricid remap settings\\n\",\n\t\t\t die_id);\n\t\tbreak;\n\t}\n}\n\nvoid gfx_v9_4_2_debug_trap_config_init(struct amdgpu_device *adev,\n\t\t\t\tuint32_t first_vmid,\n\t\t\t\tuint32_t last_vmid)\n{\n\tuint32_t data;\n\tint i;\n\n\tmutex_lock(&adev->srbm_mutex);\n\n\tfor (i = first_vmid; i < last_vmid; i++) {\n\t\tdata = 0;\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, TRAP_EN, 1);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_EN, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_REPLACE,\n\t\t\t\t\t0);\n\t\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_PER_VMID_CNTL), data);\n\t}\n\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, 0);\n\tmutex_unlock(&adev->srbm_mutex);\n\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA0), 0);\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA1), 0);\n}\n\nvoid gfx_v9_4_2_set_power_brake_sequence(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tgfx_v9_0_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL, PATTERN_MODE, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL, tmp);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL1, PWRBRK_STALL_EN, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL1, tmp);\n\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_INDEX, ixPWRBRK_STALL_PATTERN_CTRL);\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, PWRBRK_STALL_PATTERN_CTRL, PWRBRK_END_STEP, 0x12);\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_DATA, tmp);\n}\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_edc_counter_regs[] = {\n\t/* CPF */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT), 0, 1, 1 },\n\t/* CPC */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT), 0, 1, 1 },\n\t/* GDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT), 0, 1, 1 },\n\t/* RLC */\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2), 0, 1, 1 },\n\t/* SPI */\n\t{ SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT), 0, 8, 1 },\n\t/* SQC */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3), 0, 8, 7 },\n\t/* SQ */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT), 0, 8, 14 },\n\t/* TCP */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW), 0, 8, 14 },\n\t/* TCI */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT), 0, 1, 69 },\n\t/* TCC */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2), 0, 1, 16 },\n\t/* TCA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT), 0, 1, 2 },\n\t/* TCX */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT), 0, 1, 2 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2), 0, 1, 2 },\n\t/* TD */\n\t{ SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT), 0, 8, 14 },\n\t/* TA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT), 0, 8, 14 },\n\t/* GCEA */\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 1, 16 },\n};\n\nstatic void gfx_v9_4_2_select_se_sh(struct amdgpu_device *adev, u32 se_num,\n\t\t\t\t  u32 sh_num, u32 instance)\n{\n\tu32 data;\n\n\tif (instance == 0xffffffff)\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX,\n\t\t\t\t     instance);\n\n\tif (se_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);\n\n\tif (sh_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);\n\n\tWREG32_SOC15_RLC_SHADOW_EX(reg, GC, 0, regGRBM_GFX_INDEX, data);\n}\n\nstatic const struct soc15_ras_field_entry gfx_v9_4_2_ras_fields[] = {\n\t/* CPF */\n\t{ \"CPF_ROQ_ME2\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME2),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME2) },\n\t{ \"CPF_ROQ_ME1\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME1) },\n\t{ \"CPF_TCIU_TAG\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT) },\n\n\t/* CPC */\n\t{ \"CPC_SCRATCH\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT) },\n\t{ \"CPC_UCODE\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT) },\n\t{ \"CPC_DC_STATE_RAM_ME1\", SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT1_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT1_ME1) },\n\n\t/* GDS */\n\t{ \"GDS_GRBM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, DED) },\n\t{ \"GDS_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED) },\n\t{ \"GDS_PHY_CMD_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED) },\n\t{ \"GDS_PHY_DATA_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_DED) },\n\t{ \"GDS_ME0_CS_PIPE_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE0_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE1_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE2_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE3_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED) },\n\t{ \"GDS_ME0_GFXHP3D_PIX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_PIX_DED) },\n\t{ \"GDS_ME0_GFXHP3D_VTX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_VTX_DED) },\n\t{ \"GDS_ME0_CS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_CS_DED) },\n\t{ \"GDS_ME0_GFXHP3D_GS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_GS_DED) },\n\t{ \"GDS_ME1_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE0_DED) },\n\t{ \"GDS_ME1_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE1_DED) },\n\t{ \"GDS_ME1_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE2_DED) },\n\t{ \"GDS_ME1_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE3_DED) },\n\t{ \"GDS_ME2_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE0_DED) },\n\t{ \"GDS_ME2_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE1_DED) },\n\t{ \"GDS_ME2_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE2_DED) },\n\t{ \"GDS_ME2_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE3_DED) },\n\n\t/* RLC */\n\t{ \"RLCG_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCG_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLCV_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCV_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_TCTAG_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_DATA_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_ADDR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE0_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE1_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE2_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE3_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE4_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE5_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE6_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE7_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_DED_COUNT) },\n\n\t/* SPI */\n\t{ \"SPI_SR_MEM\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_DED_COUNT) },\n\t{ \"SPI_GDS_EXPREQ\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_DED_COUNT) },\n\t{ \"SPI_WB_GRANT_30\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_DED_COUNT) },\n\t{ \"SPI_LIFE_CNT\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT */\n\t{ \"SQC_DATA_CU0_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU0_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT2 */\n\t{ \"SQC_INST_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_INST_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT3 */\n\t{ \"SQC_INST_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_PARITY_CNT3 */\n\t{ \"SQC_INST_BANKA_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_DED_COUNT) },\n\n\t/* SQ */\n\t{ \"SQ_LDS_D\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT) },\n\t{ \"SQ_LDS_I\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT) },\n\t{ \"SQ_SGPR\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT) },\n\t{ \"SQ_VGPR0\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT) },\n\t{ \"SQ_VGPR1\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT) },\n\t{ \"SQ_VGPR2\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT) },\n\t{ \"SQ_VGPR3\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT) },\n\n\t/* TCP */\n\t{ \"TCP_CACHE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT) },\n\t{ \"TCP_LFIFO_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT) },\n\t{ \"TCP_CMD_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_DED_COUNT) },\n\t{ \"TCP_VM_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_DED_COUNT) },\n\t{ \"TCP_DB_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO0\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO1\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT) },\n\n\t/* TCI */\n\t{ \"TCI_WRITE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_DED_COUNT) },\n\n\t/* TCC */\n\t{ \"TCC_CACHE_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT) },\n\t{ \"TCC_CACHE_DIRTY\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT) },\n\t{ \"TCC_HIGH_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_LOW_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_SRC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO_NEXT_RAM\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_DED_COUNT) },\n\t{ \"TCC_CACHE_TAG_PROBE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_DED_COUNT) },\n\t{ \"TCC_UC_ATOMIC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_DED_COUNT) },\n\t{ \"TCC_WRITE_CACHE_READ\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_DED_COUNT) },\n\t{ \"TCC_RETURN_CONTROL\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_DED_COUNT) },\n\t{ \"TCC_IN_USE_TRANSFER\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_DED_COUNT) },\n\t{ \"TCC_IN_USE_DEC\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_DED_COUNT) },\n\t{ \"TCC_WRITE_RETURN\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_DED_COUNT) },\n\t{ \"TCC_RETURN_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_DED_COUNT) },\n\n\t/* TCA */\n\t{ \"TCA_HOLE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_DED_COUNT) },\n\t{ \"TCA_REQ_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_DED_COUNT) },\n\n\t/* TCX */\n\t{ \"TCX_GROUP0\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_DED_COUNT) },\n\t{ \"TCX_GROUP1\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_DED_COUNT) },\n\t{ \"TCX_GROUP2\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_DED_COUNT) },\n\t{ \"TCX_GROUP3\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_DED_COUNT) },\n\t{ \"TCX_GROUP4\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_DED_COUNT) },\n\t{ \"TCX_GROUP5\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP5_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP6\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP6_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP7\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP7_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP8\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP8_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP9\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP9_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP10\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP10_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP11\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP11_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP12\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP12_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP13\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP13_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP14\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP14_SED_COUNT), 0, 0 },\n\n\t/* TD */\n\t{ \"TD_SS_FIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT) },\n\t{ \"TD_SS_FIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT) },\n\t{ \"TD_CS_FIFO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_DED_COUNT) },\n\n\t/* TA */\n\t{ \"TA_FS_DFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_DED_COUNT) },\n\t{ \"TA_FL_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_DED_COUNT) },\n\t{ \"TA_FX_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_DED_COUNT) },\n\t{ \"TA_FS_CFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT */\n\t{ \"EA_DRAMRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_RRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_WRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_IOWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT), 0, 0 },\n\n\t/* EA - regGCEA_EDC_CNT2 */\n\t{ \"EA_GMIRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_MAM_D0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_DED_COUNT) },\n\t{ \"EA_MAM_D1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_DED_COUNT) },\n\t{ \"EA_MAM_D2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_DED_COUNT) },\n\t{ \"EA_MAM_D3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT3 */\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IORD_CMDMEM_DED_COUNT) },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IOWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_MAM_A0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_DED_COUNT) },\n\t{ \"EA_MAM_A1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_DED_COUNT) },\n\t{ \"EA_MAM_A2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_DED_COUNT) },\n\t{ \"EA_MAM_A3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_DED_COUNT) },\n\t{ \"EA_MAM_AFMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_DED_COUNT) },\n};\n\nstatic const char * const vml2_walker_mems[] = {\n\t\"UTC_VML2_CACHE_PDE0_MEM0\",\n\t\"UTC_VML2_CACHE_PDE0_MEM1\",\n\t\"UTC_VML2_CACHE_PDE1_MEM0\",\n\t\"UTC_VML2_CACHE_PDE1_MEM1\",\n\t\"UTC_VML2_CACHE_PDE2_MEM0\",\n\t\"UTC_VML2_CACHE_PDE2_MEM1\",\n\t\"UTC_VML2_RDIF_ARADDRS\",\n\t\"UTC_VML2_RDIF_LOG_FIFO\",\n\t\"UTC_VML2_QUEUE_REQ\",\n\t\"UTC_VML2_QUEUE_RET\",\n};\n\nstatic struct gfx_v9_4_2_utc_block gfx_v9_4_2_utc_blocks[] = {\n\t{ VML2_MEM, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ VML2_WALKER_MEM, ARRAY_SIZE(vml2_walker_mems), 1, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_WALKER_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ UTCL2_MEM, 18, 1, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, UTCL2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_2M, 8, 2, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_2M_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_32K, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_32K_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_4K, 8, 2, 8,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_4K_DSM_CNTL, WRITE_COUNTERS, 1) },\n};\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_ea_err_status_regs = {\n\tSOC15_REG_ENTRY(GC, 0, regGCEA_ERR_STATUS), 0, 1, 16\n};\n\nstatic int gfx_v9_4_2_get_reg_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t  const struct soc15_reg_entry *reg,\n\t\t\t\t\t  uint32_t se_id, uint32_t inst_id,\n\t\t\t\t\t  uint32_t value, uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i;\n\tuint32_t sec_cnt, ded_cnt;\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_ras_fields); i++) {\n\t\tif (gfx_v9_4_2_ras_fields[i].reg_offset != reg->reg_offset ||\n\t\t    gfx_v9_4_2_ras_fields[i].seg != reg->seg ||\n\t\t    gfx_v9_4_2_ras_fields[i].inst != reg->inst)\n\t\t\tcontinue;\n\n\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], sec);\n\t\tif (sec_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], SEC %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t sec_cnt);\n\t\t\t*sec_count += sec_cnt;\n\t\t}\n\n\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], ded);\n\t\tif (ded_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], DED %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t ded_cnt);\n\t\t\t*ded_count += ded_cnt;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_2_query_sram_edc_count(struct amdgpu_device *adev,\n\t\t\t\tuint32_t *sec_count, uint32_t *ded_count)\n{\n\tuint32_t i, j, k, data;\n\tuint32_t sec_cnt = 0, ded_cnt = 0;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_edc_counter_regs); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_edc_counter_regs[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_2_edc_counter_regs[i].instance;\n\t\t\t     k++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, j, 0, k);\n\n\t\t\t\t/* if sec/ded_count is null, just clear counter */\n\t\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]));\n\n\t\t\t\tif (!data)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tgfx_v9_4_2_get_reg_error_count(adev,\n\t\t\t\t\t&gfx_v9_4_2_edc_counter_regs[i],\n\t\t\t\t\tj, k, data, &sec_cnt, &ded_cnt);\n\n\t\t\t\t/* clear counter after read */\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count += sec_cnt;\n\t\t*ded_count += ded_cnt;\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_log_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t struct gfx_v9_4_2_utc_block *blk,\n\t\t\t\t\t uint32_t instance, uint32_t sec_cnt,\n\t\t\t\t\t uint32_t ded_cnt)\n{\n\tuint32_t bank, way, mem;\n\tstatic const char * const vml2_way_str[] = { \"BIGK\", \"4K\" };\n\tstatic const char * const utcl2_rounter_str[] = { \"VMC\", \"APT\" };\n\n\tmem = instance % blk->num_mem_blocks;\n\tway = (instance / blk->num_mem_blocks) % blk->num_ways;\n\tbank = instance / (blk->num_mem_blocks * blk->num_ways);\n\n\tswitch (blk->type) {\n\tcase VML2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_VML2_BANK_CACHE_%d_%s_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, vml2_way_str[way], mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase VML2_WALKER_MEM:\n\t\tdev_info(adev->dev, \"GFX SubBlock %s, SED %d, DED %d\\n\",\n\t\t\t vml2_walker_mems[bank], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase UTCL2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTCL2_ROUTER_IFIF%d_GROUP0_%s, SED %d, DED %d\\n\",\n\t\t\tbank, utcl2_rounter_str[mem], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_2M:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_2M_BANK%d_WAY%d_MEM, SED %d, DED %d\\n\",\n\t\t\tbank, way, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_32K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_32K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_4K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_4K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_2_query_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i, j, data;\n\tuint32_t sec_cnt, ded_cnt;\n\tuint32_t num_instances;\n\tstruct gfx_v9_4_2_utc_block *blk;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_utc_blocks); i++) {\n\t\tblk = &gfx_v9_4_2_utc_blocks[i];\n\t\tnum_instances =\n\t\t\tblk->num_banks * blk->num_ways * blk->num_mem_blocks;\n\t\tfor (j = 0; j < num_instances; j++) {\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->idx_reg), j);\n\n\t\t\t/* if sec/ded_count is NULL, just clear counter */\n\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t\t       blk->clear);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg));\n\t\t\tif (!data)\n\t\t\t\tcontinue;\n\n\t\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, sec);\n\t\t\t*sec_count += sec_cnt;\n\t\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, ded);\n\t\t\t*ded_count += ded_cnt;\n\n\t\t\t/* clear counter after read */\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t       blk->clear);\n\n\t\t\t/* print the edc count */\n\t\t\tif (sec_cnt || ded_cnt)\n\t\t\t\tgfx_v9_4_2_log_utc_edc_count(adev, blk, j, sec_cnt,\n\t\t\t\t\t\t\t     ded_cnt);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t    void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tuint32_t sec_count = 0, ded_count = 0;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\terr_data->ue_count = 0;\n\terr_data->ce_count = 0;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n\tgfx_v9_4_2_query_utc_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n}\n\nstatic void gfx_v9_4_2_reset_utc_err_status(struct amdgpu_device *adev)\n{\n\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n}\n\nstatic void gfx_v9_4_2_reset_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\tvalue = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\t\t\tvalue = REG_SET_FIELD(value, GCEA_ERR_STATUS, CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), value);\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, NULL, NULL);\n\tgfx_v9_4_2_query_utc_edc_count(adev, NULL, NULL);\n}\n\nstatic void gfx_v9_4_2_query_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t reg_value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\treg_value = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\n\t\t\tif (REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_WRRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_DATAPARITY_ERROR)) {\n\t\t\t\tdev_warn(adev->dev, \"GCEA err detected at instance: %d, status: 0x%x!\\n\",\n\t\t\t\t\t\tj, reg_value);\n\t\t\t}\n\t\t\t/* clear after read */\n\t\t\treg_value = REG_SET_FIELD(reg_value, GCEA_ERR_STATUS,\n\t\t\t\t\t\t  CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), reg_value);\n\t\t}\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_query_utc_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX UTCL2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Walker Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_ea_err_status(adev);\n\tgfx_v9_4_2_query_utc_err_status(adev);\n\tgfx_v9_4_2_query_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_reset_utc_err_status(adev);\n\tgfx_v9_4_2_reset_ea_err_status(adev);\n\tgfx_v9_4_2_reset_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_enable_watchdog_timer(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\tuint32_t data;\n\n\tdata = REG_SET_FIELD(0, SQ_TIMEOUT_CONFIG, TIMEOUT_FATAL_DISABLE,\n\t\t\t     amdgpu_watchdog_timer.timeout_fatal_disable ? 1 :\n\t\t\t\t\t\t\t\t\t   0);\n\n\tif (amdgpu_watchdog_timer.timeout_fatal_disable &&\n\t    (amdgpu_watchdog_timer.period < 1 ||\n\t     amdgpu_watchdog_timer.period > 0x23)) {\n\t\tdev_warn(adev->dev, \"Watchdog period range is 1 to 0x23\\n\");\n\t\tamdgpu_watchdog_timer.period = 0x23;\n\t}\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, PERIOD_SEL,\n\t\t\t     amdgpu_watchdog_timer.period);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tgfx_v9_4_2_select_se_sh(adev, i, 0xffffffff, 0xffffffff);\n\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_CONFIG, data);\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t address)\n{\n\tWREG32_SOC15_RLC_EX(reg, GC, 0, regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(address << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK));\n\treturn RREG32_SOC15(GC, 0, regSQ_IND_DATA);\n}\n\nstatic void gfx_v9_4_2_log_cu_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t status)\n{\n\tstruct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;\n\tuint32_t i, simd, wave;\n\tuint32_t wave_status;\n\tuint32_t wave_pc_lo, wave_pc_hi;\n\tuint32_t wave_exec_lo, wave_exec_hi;\n\tuint32_t wave_inst_dw0, wave_inst_dw1;\n\tuint32_t wave_ib_sts;\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (!((i << 1) & status))\n\t\t\tcontinue;\n\n\t\tsimd = i / cu_info->max_waves_per_simd;\n\t\twave = i % cu_info->max_waves_per_simd;\n\n\t\twave_status = wave_read_ind(adev, simd, wave, ixSQ_WAVE_STATUS);\n\t\twave_pc_lo = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_LO);\n\t\twave_pc_hi = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_HI);\n\t\twave_exec_lo =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_LO);\n\t\twave_exec_hi =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_HI);\n\t\twave_inst_dw0 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW0);\n\t\twave_inst_dw1 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW1);\n\t\twave_ib_sts = wave_read_ind(adev, simd, wave, ixSQ_WAVE_IB_STS);\n\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"\\t SIMD %d, Wave %d: status 0x%x, pc 0x%llx, exec 0x%llx, inst 0x%llx, ib_sts 0x%x\\n\",\n\t\t\tsimd, wave, wave_status,\n\t\t\t((uint64_t)wave_pc_hi << 32 | wave_pc_lo),\n\t\t\t((uint64_t)wave_exec_hi << 32 | wave_exec_lo),\n\t\t\t((uint64_t)wave_inst_dw1 << 32 | wave_inst_dw0),\n\t\t\twave_ib_sts);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\tuint32_t status;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tstatus = RREG32_SOC15(GC, 0,\n\t\t\t\t\t\t      regSQ_TIMEOUT_STATUS);\n\t\t\t\tif (status != 0) {\n\t\t\t\t\tdev_info(\n\t\t\t\t\t\tadev->dev,\n\t\t\t\t\t\t\"GFX Watchdog Timeout: SE %d, SH %d, CU %d\\n\",\n\t\t\t\t\t\tse_idx, sh_idx, cu_idx);\n\t\t\t\t\tgfx_v9_4_2_log_cu_timeout_status(\n\t\t\t\t\t\tadev, status);\n\t\t\t\t}\n\t\t\t\t/* clear old status */\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n\n\nstruct amdgpu_ras_block_hw_ops  gfx_v9_4_2_ras_ops = {\n\t\t.query_ras_error_count = &gfx_v9_4_2_query_ras_error_count,\n\t\t.reset_ras_error_count = &gfx_v9_4_2_reset_ras_error_count,\n\t\t.query_ras_error_status = &gfx_v9_4_2_query_ras_error_status,\n\t\t.reset_ras_error_status = &gfx_v9_4_2_reset_ras_error_status,\n};\n\nstruct amdgpu_gfx_ras gfx_v9_4_2_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &gfx_v9_4_2_ras_ops,\n\t},\n\t.enable_watchdog_timer = &gfx_v9_4_2_enable_watchdog_timer,\n};\n", "source_code_path": "gfx_v9_4_2.c", "line_number": 427, "generated_diff": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: gfx_v9_4_2.c\nLine: 427\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nA buffer overflow vulnerability exists in the function gfx_v9_4_2_log_wave_assignment(), which uses a fixed-size buffer to store a formatted string without implementing proper length checks. If the accumulated string length exceeds the buffer size, this can result in a buffer overflow, potentially leading to system crashes or data corruption.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "gfx_v9_4_2-bug.txt", "bug_report_text": "Issue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nThe function gfx_v9_4_2_log_wave_assignment() stores a formatted string in a fixed-size buffer, but fails to validate the string length before writing to the buffer. This can lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size, resulting in potential data corruption or system instability.\n", "diff_path": "gfx_v9_4_2-diff.txt", "diff_text": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n", "code": "/*\n * Copyright 2020 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include \"amdgpu.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n\n#include \"gc/gc_9_4_2_offset.h\"\n#include \"gc/gc_9_4_2_sh_mask.h\"\n#include \"gfx_v9_0.h\"\n\n#include \"gfx_v9_4_2.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_gfx.h\"\n\n#define SE_ID_MAX 8\n#define CU_ID_MAX 16\n#define SIMD_ID_MAX 4\n#define WAVE_ID_MAX 10\n\nenum gfx_v9_4_2_utc_type {\n\tVML2_MEM,\n\tVML2_WALKER_MEM,\n\tUTCL2_MEM,\n\tATC_L2_CACHE_2M,\n\tATC_L2_CACHE_32K,\n\tATC_L2_CACHE_4K\n};\n\nstruct gfx_v9_4_2_utc_block {\n\tenum gfx_v9_4_2_utc_type type;\n\tuint32_t num_banks;\n\tuint32_t num_ways;\n\tuint32_t num_mem_blocks;\n\tstruct soc15_reg idx_reg;\n\tstruct soc15_reg data_reg;\n\tuint32_t sec_count_mask;\n\tuint32_t sec_count_shift;\n\tuint32_t ded_count_mask;\n\tuint32_t ded_count_shift;\n\tuint32_t clear;\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x141dc920),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0x3b458b93),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x1a4f5583),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0x317717f6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x107cc1e6),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x351),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde_die_1[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_0, 0x3fffffff, 0x2591aa38),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_1, 0x3fffffff, 0xac9e88b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_2, 0x3fffffff, 0x2bc3369b),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_3, 0x3fffffff, 0xfb74ee),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_4, 0x3fffffff, 0x21f0a2fe),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_CHAN_STEER_5, 0x3ff, 0x49),\n};\n\nstatic const struct soc15_reg_golden golden_settings_gc_9_4_2_alde[] = {\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regGB_ADDR_CONFIG, 0xffff77ff, 0x2a114042),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTA_CNTL_AUX, 0xfffffeef, 0x10b0000),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCP_UTCL1_CNTL1, 0xffffffff, 0x30800400),\n\tSOC15_REG_GOLDEN_VALUE(GC, 0, regTCI_CNTL_3, 0xff, 0x20),\n};\n\n/*\n * This shader is used to clear VGPRS and LDS, and also write the input\n * pattern into the write back buffer, which will be used by driver to\n * check whether all SIMDs have been covered.\n*/\nstatic const u32 vgpr_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xd3d94000,\n\t0x18000080, 0xd3d94001, 0x18000080, 0xd3d94002, 0x18000080, 0xd3d94003,\n\t0x18000080, 0xd3d94004, 0x18000080, 0xd3d94005, 0x18000080, 0xd3d94006,\n\t0x18000080, 0xd3d94007, 0x18000080, 0xd3d94008, 0x18000080, 0xd3d94009,\n\t0x18000080, 0xd3d9400a, 0x18000080, 0xd3d9400b, 0x18000080, 0xd3d9400c,\n\t0x18000080, 0xd3d9400d, 0x18000080, 0xd3d9400e, 0x18000080, 0xd3d9400f,\n\t0x18000080, 0xd3d94010, 0x18000080, 0xd3d94011, 0x18000080, 0xd3d94012,\n\t0x18000080, 0xd3d94013, 0x18000080, 0xd3d94014, 0x18000080, 0xd3d94015,\n\t0x18000080, 0xd3d94016, 0x18000080, 0xd3d94017, 0x18000080, 0xd3d94018,\n\t0x18000080, 0xd3d94019, 0x18000080, 0xd3d9401a, 0x18000080, 0xd3d9401b,\n\t0x18000080, 0xd3d9401c, 0x18000080, 0xd3d9401d, 0x18000080, 0xd3d9401e,\n\t0x18000080, 0xd3d9401f, 0x18000080, 0xd3d94020, 0x18000080, 0xd3d94021,\n\t0x18000080, 0xd3d94022, 0x18000080, 0xd3d94023, 0x18000080, 0xd3d94024,\n\t0x18000080, 0xd3d94025, 0x18000080, 0xd3d94026, 0x18000080, 0xd3d94027,\n\t0x18000080, 0xd3d94028, 0x18000080, 0xd3d94029, 0x18000080, 0xd3d9402a,\n\t0x18000080, 0xd3d9402b, 0x18000080, 0xd3d9402c, 0x18000080, 0xd3d9402d,\n\t0x18000080, 0xd3d9402e, 0x18000080, 0xd3d9402f, 0x18000080, 0xd3d94030,\n\t0x18000080, 0xd3d94031, 0x18000080, 0xd3d94032, 0x18000080, 0xd3d94033,\n\t0x18000080, 0xd3d94034, 0x18000080, 0xd3d94035, 0x18000080, 0xd3d94036,\n\t0x18000080, 0xd3d94037, 0x18000080, 0xd3d94038, 0x18000080, 0xd3d94039,\n\t0x18000080, 0xd3d9403a, 0x18000080, 0xd3d9403b, 0x18000080, 0xd3d9403c,\n\t0x18000080, 0xd3d9403d, 0x18000080, 0xd3d9403e, 0x18000080, 0xd3d9403f,\n\t0x18000080, 0xd3d94040, 0x18000080, 0xd3d94041, 0x18000080, 0xd3d94042,\n\t0x18000080, 0xd3d94043, 0x18000080, 0xd3d94044, 0x18000080, 0xd3d94045,\n\t0x18000080, 0xd3d94046, 0x18000080, 0xd3d94047, 0x18000080, 0xd3d94048,\n\t0x18000080, 0xd3d94049, 0x18000080, 0xd3d9404a, 0x18000080, 0xd3d9404b,\n\t0x18000080, 0xd3d9404c, 0x18000080, 0xd3d9404d, 0x18000080, 0xd3d9404e,\n\t0x18000080, 0xd3d9404f, 0x18000080, 0xd3d94050, 0x18000080, 0xd3d94051,\n\t0x18000080, 0xd3d94052, 0x18000080, 0xd3d94053, 0x18000080, 0xd3d94054,\n\t0x18000080, 0xd3d94055, 0x18000080, 0xd3d94056, 0x18000080, 0xd3d94057,\n\t0x18000080, 0xd3d94058, 0x18000080, 0xd3d94059, 0x18000080, 0xd3d9405a,\n\t0x18000080, 0xd3d9405b, 0x18000080, 0xd3d9405c, 0x18000080, 0xd3d9405d,\n\t0x18000080, 0xd3d9405e, 0x18000080, 0xd3d9405f, 0x18000080, 0xd3d94060,\n\t0x18000080, 0xd3d94061, 0x18000080, 0xd3d94062, 0x18000080, 0xd3d94063,\n\t0x18000080, 0xd3d94064, 0x18000080, 0xd3d94065, 0x18000080, 0xd3d94066,\n\t0x18000080, 0xd3d94067, 0x18000080, 0xd3d94068, 0x18000080, 0xd3d94069,\n\t0x18000080, 0xd3d9406a, 0x18000080, 0xd3d9406b, 0x18000080, 0xd3d9406c,\n\t0x18000080, 0xd3d9406d, 0x18000080, 0xd3d9406e, 0x18000080, 0xd3d9406f,\n\t0x18000080, 0xd3d94070, 0x18000080, 0xd3d94071, 0x18000080, 0xd3d94072,\n\t0x18000080, 0xd3d94073, 0x18000080, 0xd3d94074, 0x18000080, 0xd3d94075,\n\t0x18000080, 0xd3d94076, 0x18000080, 0xd3d94077, 0x18000080, 0xd3d94078,\n\t0x18000080, 0xd3d94079, 0x18000080, 0xd3d9407a, 0x18000080, 0xd3d9407b,\n\t0x18000080, 0xd3d9407c, 0x18000080, 0xd3d9407d, 0x18000080, 0xd3d9407e,\n\t0x18000080, 0xd3d9407f, 0x18000080, 0xd3d94080, 0x18000080, 0xd3d94081,\n\t0x18000080, 0xd3d94082, 0x18000080, 0xd3d94083, 0x18000080, 0xd3d94084,\n\t0x18000080, 0xd3d94085, 0x18000080, 0xd3d94086, 0x18000080, 0xd3d94087,\n\t0x18000080, 0xd3d94088, 0x18000080, 0xd3d94089, 0x18000080, 0xd3d9408a,\n\t0x18000080, 0xd3d9408b, 0x18000080, 0xd3d9408c, 0x18000080, 0xd3d9408d,\n\t0x18000080, 0xd3d9408e, 0x18000080, 0xd3d9408f, 0x18000080, 0xd3d94090,\n\t0x18000080, 0xd3d94091, 0x18000080, 0xd3d94092, 0x18000080, 0xd3d94093,\n\t0x18000080, 0xd3d94094, 0x18000080, 0xd3d94095, 0x18000080, 0xd3d94096,\n\t0x18000080, 0xd3d94097, 0x18000080, 0xd3d94098, 0x18000080, 0xd3d94099,\n\t0x18000080, 0xd3d9409a, 0x18000080, 0xd3d9409b, 0x18000080, 0xd3d9409c,\n\t0x18000080, 0xd3d9409d, 0x18000080, 0xd3d9409e, 0x18000080, 0xd3d9409f,\n\t0x18000080, 0xd3d940a0, 0x18000080, 0xd3d940a1, 0x18000080, 0xd3d940a2,\n\t0x18000080, 0xd3d940a3, 0x18000080, 0xd3d940a4, 0x18000080, 0xd3d940a5,\n\t0x18000080, 0xd3d940a6, 0x18000080, 0xd3d940a7, 0x18000080, 0xd3d940a8,\n\t0x18000080, 0xd3d940a9, 0x18000080, 0xd3d940aa, 0x18000080, 0xd3d940ab,\n\t0x18000080, 0xd3d940ac, 0x18000080, 0xd3d940ad, 0x18000080, 0xd3d940ae,\n\t0x18000080, 0xd3d940af, 0x18000080, 0xd3d940b0, 0x18000080, 0xd3d940b1,\n\t0x18000080, 0xd3d940b2, 0x18000080, 0xd3d940b3, 0x18000080, 0xd3d940b4,\n\t0x18000080, 0xd3d940b5, 0x18000080, 0xd3d940b6, 0x18000080, 0xd3d940b7,\n\t0x18000080, 0xd3d940b8, 0x18000080, 0xd3d940b9, 0x18000080, 0xd3d940ba,\n\t0x18000080, 0xd3d940bb, 0x18000080, 0xd3d940bc, 0x18000080, 0xd3d940bd,\n\t0x18000080, 0xd3d940be, 0x18000080, 0xd3d940bf, 0x18000080, 0xd3d940c0,\n\t0x18000080, 0xd3d940c1, 0x18000080, 0xd3d940c2, 0x18000080, 0xd3d940c3,\n\t0x18000080, 0xd3d940c4, 0x18000080, 0xd3d940c5, 0x18000080, 0xd3d940c6,\n\t0x18000080, 0xd3d940c7, 0x18000080, 0xd3d940c8, 0x18000080, 0xd3d940c9,\n\t0x18000080, 0xd3d940ca, 0x18000080, 0xd3d940cb, 0x18000080, 0xd3d940cc,\n\t0x18000080, 0xd3d940cd, 0x18000080, 0xd3d940ce, 0x18000080, 0xd3d940cf,\n\t0x18000080, 0xd3d940d0, 0x18000080, 0xd3d940d1, 0x18000080, 0xd3d940d2,\n\t0x18000080, 0xd3d940d3, 0x18000080, 0xd3d940d4, 0x18000080, 0xd3d940d5,\n\t0x18000080, 0xd3d940d6, 0x18000080, 0xd3d940d7, 0x18000080, 0xd3d940d8,\n\t0x18000080, 0xd3d940d9, 0x18000080, 0xd3d940da, 0x18000080, 0xd3d940db,\n\t0x18000080, 0xd3d940dc, 0x18000080, 0xd3d940dd, 0x18000080, 0xd3d940de,\n\t0x18000080, 0xd3d940df, 0x18000080, 0xd3d940e0, 0x18000080, 0xd3d940e1,\n\t0x18000080, 0xd3d940e2, 0x18000080, 0xd3d940e3, 0x18000080, 0xd3d940e4,\n\t0x18000080, 0xd3d940e5, 0x18000080, 0xd3d940e6, 0x18000080, 0xd3d940e7,\n\t0x18000080, 0xd3d940e8, 0x18000080, 0xd3d940e9, 0x18000080, 0xd3d940ea,\n\t0x18000080, 0xd3d940eb, 0x18000080, 0xd3d940ec, 0x18000080, 0xd3d940ed,\n\t0x18000080, 0xd3d940ee, 0x18000080, 0xd3d940ef, 0x18000080, 0xd3d940f0,\n\t0x18000080, 0xd3d940f1, 0x18000080, 0xd3d940f2, 0x18000080, 0xd3d940f3,\n\t0x18000080, 0xd3d940f4, 0x18000080, 0xd3d940f5, 0x18000080, 0xd3d940f6,\n\t0x18000080, 0xd3d940f7, 0x18000080, 0xd3d940f8, 0x18000080, 0xd3d940f9,\n\t0x18000080, 0xd3d940fa, 0x18000080, 0xd3d940fb, 0x18000080, 0xd3d940fc,\n\t0x18000080, 0xd3d940fd, 0x18000080, 0xd3d940fe, 0x18000080, 0xd3d940ff,\n\t0x18000080, 0xb07c0000, 0xbe8a00ff, 0x000000f8, 0xbf11080a, 0x7e000280,\n\t0x7e020280, 0x7e040280, 0x7e060280, 0x7e080280, 0x7e0a0280, 0x7e0c0280,\n\t0x7e0e0280, 0x808a880a, 0xbe80320a, 0xbf84fff5, 0xbf9c0000, 0xd28c0001,\n\t0x0001007f, 0xd28d0001, 0x0002027e, 0x10020288, 0xbe8b0004, 0xb78b4000,\n\t0xd1196a01, 0x00001701, 0xbe8a0087, 0xbefc00c1, 0xd89c4000, 0x00020201,\n\t0xd89cc080, 0x00040401, 0x320202ff, 0x00000800, 0x808a810a, 0xbf84fff8,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry vgpr_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 4 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0xbf },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x400006 },  /* 64KB LDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x3F }, /*  63 - accum-offset = 256 */\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * The below shaders are used to clear SGPRS, and also write the input\n * pattern into the write back buffer. The first two dispatch should be\n * scheduled simultaneously which make sure that all SGPRS could be\n * allocated, so the dispatch 1 need check write back buffer before scheduled,\n * make sure that waves of dispatch 0 are all dispacthed to all simds\n * balanced. both dispatch 0 and dispatch 1 should be halted until all waves\n * are dispatched, and then driver write a pattern to the shared memory to make\n * all waves continue.\n*/\nstatic const u32 sgpr112_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbeda0080, 0xbedb0080, 0xbedc0080, 0xbedd0080, 0xbede0080, 0xbedf0080,\n\t0xbee00080, 0xbee10080, 0xbee20080, 0xbee30080, 0xbee40080, 0xbee50080,\n\t0xbf810000\n};\n\nconst struct soc15_reg_entry sgpr112_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 8 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x340 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic const u32 sgpr96_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbeba0080, 0xbebb0080,\n\t0xbebc0080, 0xbebd0080, 0xbebe0080, 0xbebf0080, 0xbec00080, 0xbec10080,\n\t0xbec20080, 0xbec30080, 0xbec40080, 0xbec50080, 0xbec60080, 0xbec70080,\n\t0xbec80080, 0xbec90080, 0xbeca0080, 0xbecb0080, 0xbecc0080, 0xbecd0080,\n\t0xbece0080, 0xbecf0080, 0xbed00080, 0xbed10080, 0xbed20080, 0xbed30080,\n\t0xbed40080, 0xbed50080, 0xbed60080, 0xbed70080, 0xbed80080, 0xbed90080,\n\t0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr96_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0xc },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x2c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\n/*\n * This shader is used to clear the uninitiated sgprs after the above\n * two dispatches, because of hardware feature, dispath 0 couldn't clear\n * top hole sgprs. Therefore need 4 waves per SIMD to cover these sgprs\n*/\nstatic const u32 sgpr64_init_compute_shader_aldebaran[] = {\n\t0xb8840904, 0xb8851a04, 0xb8861344, 0xb8831804, 0x9208ff06, 0x00000280,\n\t0x9209a805, 0x920a8a04, 0x81080908, 0x81080a08, 0x81080308, 0x8e078208,\n\t0x81078407, 0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8e003f, 0xc0030200,\n\t0x00000000, 0xbf8c0000, 0xbf06ff08, 0xdeadbeaf, 0xbf84fff9, 0x81028102,\n\t0xc0410080, 0x00000007, 0xbf8c0000, 0xbf8a0000, 0xbefc0080, 0xbeea0080,\n\t0xbeeb0080, 0xbf00f280, 0xbee60080, 0xbee70080, 0xbee80080, 0xbee90080,\n\t0xbefe0080, 0xbeff0080, 0xbe880080, 0xbe890080, 0xbe8a0080, 0xbe8b0080,\n\t0xbe8c0080, 0xbe8d0080, 0xbe8e0080, 0xbe8f0080, 0xbe900080, 0xbe910080,\n\t0xbe920080, 0xbe930080, 0xbe940080, 0xbe950080, 0xbe960080, 0xbe970080,\n\t0xbe980080, 0xbe990080, 0xbe9a0080, 0xbe9b0080, 0xbe9c0080, 0xbe9d0080,\n\t0xbe9e0080, 0xbe9f0080, 0xbea00080, 0xbea10080, 0xbea20080, 0xbea30080,\n\t0xbea40080, 0xbea50080, 0xbea60080, 0xbea70080, 0xbea80080, 0xbea90080,\n\t0xbeaa0080, 0xbeab0080, 0xbeac0080, 0xbead0080, 0xbeae0080, 0xbeaf0080,\n\t0xbeb00080, 0xbeb10080, 0xbeb20080, 0xbeb30080, 0xbeb40080, 0xbeb50080,\n\t0xbeb60080, 0xbeb70080, 0xbeb80080, 0xbeb90080, 0xbf810000,\n};\n\nconst struct soc15_reg_entry sgpr64_init_regs_aldebaran[] = {\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_RESOURCE_LIMITS), 0x0000000 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_X), 0x40 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Y), 0x10 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_NUM_THREAD_Z), 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC1), 0x1c0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC2), 0x6 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_PGM_RSRC3), 0x0 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE0), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE1), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE2), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE3), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE4), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE5), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE6), 0xffffffff },\n\t{ SOC15_REG_ENTRY(GC, 0, regCOMPUTE_STATIC_THREAD_MGMT_SE7), 0xffffffff },\n};\n\nstatic int gfx_v9_4_2_run_shader(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_ib *ib,\n\t\t\t\t const u32 *shader_ptr, u32 shader_size,\n\t\t\t\t const struct soc15_reg_entry *init_regs, u32 regs_size,\n\t\t\t\t u32 compute_dim_x, u64 wb_gpu_addr, u32 pattern,\n\t\t\t\t struct dma_fence **fence_ptr)\n{\n\tint r, i;\n\tuint32_t total_size, shader_offset;\n\tu64 gpu_addr;\n\n\ttotal_size = (regs_size * 3 + 4 + 5 + 5) * 4;\n\ttotal_size = ALIGN(total_size, 256);\n\tshader_offset = total_size;\n\ttotal_size += ALIGN(shader_size, 256);\n\n\t/* allocate an indirect buffer to put the commands in */\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, total_size,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\t/* load the compute shaders */\n\tfor (i = 0; i < shader_size/sizeof(u32); i++)\n\t\tib->ptr[i + (shader_offset / 4)] = shader_ptr[i];\n\n\t/* init the ib length to 0 */\n\tib->length_dw = 0;\n\n\t/* write the register state for the compute dispatch */\n\tfor (i = 0; i < regs_size; i++) {\n\t\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 1);\n\t\tib->ptr[ib->length_dw++] = SOC15_REG_ENTRY_OFFSET(init_regs[i])\n\t\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\t\tib->ptr[ib->length_dw++] = init_regs[i].reg_value;\n\t}\n\n\t/* write the shader start address: mmCOMPUTE_PGM_LO, mmCOMPUTE_PGM_HI */\n\tgpu_addr = (ib->gpu_addr + (u64)shader_offset) >> 8;\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 2);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_PGM_LO)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(gpu_addr);\n\n\t/* write the wb buffer address */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_SET_SH_REG, 3);\n\tib->ptr[ib->length_dw++] = SOC15_REG_OFFSET(GC, 0, regCOMPUTE_USER_DATA_0)\n\t\t\t\t\t\t\t- PACKET3_SET_SH_REG_START;\n\tib->ptr[ib->length_dw++] = lower_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = upper_32_bits(wb_gpu_addr);\n\tib->ptr[ib->length_dw++] = pattern;\n\n\t/* write dispatch packet */\n\tib->ptr[ib->length_dw++] = PACKET3(PACKET3_DISPATCH_DIRECT, 3);\n\tib->ptr[ib->length_dw++] = compute_dim_x; /* x */\n\tib->ptr[ib->length_dw++] = 1; /* y */\n\tib->ptr[ib->length_dw++] = 1; /* z */\n\tib->ptr[ib->length_dw++] =\n\t\tREG_SET_FIELD(0, COMPUTE_DISPATCH_INITIATOR, COMPUTE_SHADER_EN, 1);\n\n\t/* shedule the ib on the ring */\n\tr = amdgpu_ib_schedule(ring, 1, ib, NULL, fence_ptr);\n\tif (r) {\n\t\tdev_err(adev->dev, \"ib submit failed (%d).\\n\", r);\n\t\tamdgpu_ib_free(adev, ib, NULL);\n\t}\n\treturn r;\n}\n\nstatic void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t *wb_ptr)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n\t\tfor (cu = 0; cu < CU_ID_MAX; cu++) {\n\t\t\tmemset(str, 0, 256);\n\t\t\tsize = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++) {\n\t\t\t\tsize += sprintf(str + size, \"[\");\n\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\tsize += sprintf(str + size, \"%x\", wb_ptr[offset]);\n\t\t\t\t\toffset++;\n\t\t\t\t}\n\t\t\t\tsize += sprintf(str + size, \"]  \");\n\t\t\t}\n\t\t\tdev_dbg(adev->dev, \"%s\\n\", str);\n\t\t}\n\t}\n\n\tkfree(str);\n}\n\nstatic int gfx_v9_4_2_wait_for_waves_assigned(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t *wb_ptr, uint32_t mask,\n\t\t\t\t\t      uint32_t pattern, uint32_t num_wave, bool wait)\n{\n\tuint32_t se, cu, simd, wave;\n\tuint32_t loop = 0;\n\tuint32_t wave_cnt;\n\tuint32_t offset;\n\n\tdo {\n\t\twave_cnt = 0;\n\t\toffset = 0;\n\n\t\tfor (se = 0; se < adev->gfx.config.max_shader_engines; se++)\n\t\t\tfor (cu = 0; cu < CU_ID_MAX; cu++)\n\t\t\t\tfor (simd = 0; simd < SIMD_ID_MAX; simd++)\n\t\t\t\t\tfor (wave = 0; wave < WAVE_ID_MAX; wave++) {\n\t\t\t\t\t\tif (((1 << wave) & mask) &&\n\t\t\t\t\t\t    (wb_ptr[offset] == pattern))\n\t\t\t\t\t\t\twave_cnt++;\n\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t}\n\n\t\tif (wave_cnt == num_wave)\n\t\t\treturn 0;\n\n\t\tmdelay(1);\n\t} while (++loop < 2000 && wait);\n\n\tdev_err(adev->dev, \"actual wave num: %d, expected wave num: %d\\n\",\n\t\twave_cnt, num_wave);\n\n\tgfx_v9_4_2_log_wave_assignment(adev, wb_ptr);\n\n\treturn -EBADSLT;\n}\n\nstatic int gfx_v9_4_2_do_sgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ibs[3];\n\tstruct dma_fence *fences[3];\n\tu32 pattern[3] = { 0x1, 0x5, 0xa };\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready ||\n\t\t !adev->gfx.compute_ring[1].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[0],\n\t\t\tsgpr112_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr112_init_compute_shader_aldebaran),\n\t\t\tsgpr112_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr112_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[0], &fences[0]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 224 sgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11,\n\t\t\tpattern[0],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 2,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 224 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[1],\n\t\t\t&disp_ibs[1],\n\t\t\tsgpr96_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr96_init_compute_shader_aldebaran),\n\t\t\tsgpr96_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr96_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number * 2,\n\t\t\twb_ib.gpu_addr, pattern[1], &fences[1]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear next 576 sgprs\\n\");\n\t\tgoto disp0_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b11111100,\n\t\t\tpattern[1], adev->gfx.cu_info.number * SIMD_ID_MAX * 6,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 576 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp1_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fences[0], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 224 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = dma_fence_wait(fences[1], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 576 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ibs[2],\n\t\t\tsgpr64_init_compute_shader_aldebaran,\n\t\t\tsizeof(sgpr64_init_compute_shader_aldebaran),\n\t\t\tsgpr64_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(sgpr64_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern[2], &fences[2]);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear first 256 sgprs\\n\");\n\t\tgoto disp1_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1111,\n\t\t\tpattern[2],\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX * 4,\n\t\t\ttrue);\n\tif (r) {\n\t\tdev_err(adev->dev, \"wave coverage failed when clear first 256 sgprs\\n\");\n\t\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\t\tgoto disp2_failed;\n\t}\n\n\twb_ib.ptr[0] = 0xdeadbeaf; /* stop waves */\n\n\tr = dma_fence_wait(fences[2], false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear first 256 sgprs\\n\");\n\t\tgoto disp2_failed;\n\t}\n\ndisp2_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[2], NULL);\n\tdma_fence_put(fences[2]);\ndisp1_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[1], NULL);\n\tdma_fence_put(fences[1]);\ndisp0_failed:\n\tamdgpu_ib_free(adev, &disp_ibs[0], NULL);\n\tdma_fence_put(fences[0]);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init SGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init SGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_2_do_vgprs_init(struct amdgpu_device *adev)\n{\n\tint r;\n\t/* CU_ID: 0~15, SIMD_ID: 0~3, WAVE_ID: 0 ~ 9 */\n\tint wb_size = adev->gfx.config.max_shader_engines *\n\t\t\t CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX;\n\tstruct amdgpu_ib wb_ib;\n\tstruct amdgpu_ib disp_ib;\n\tstruct dma_fence *fence;\n\tu32 pattern = 0xa;\n\n\t/* bail if the compute ring is not ready */\n\tif (!adev->gfx.compute_ring[0].sched.ready)\n\t\treturn 0;\n\n\t/* allocate the write-back buffer from IB */\n\tmemset(&wb_ib, 0, sizeof(wb_ib));\n\tr = amdgpu_ib_get(adev, NULL, (1 + wb_size) * sizeof(uint32_t),\n\t\t\t  AMDGPU_IB_POOL_DIRECT, &wb_ib);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to get ib (%d) for wb.\\n\", r);\n\t\treturn r;\n\t}\n\tmemset(wb_ib.ptr, 0, (1 + wb_size) * sizeof(uint32_t));\n\n\tr = gfx_v9_4_2_run_shader(adev,\n\t\t\t&adev->gfx.compute_ring[0],\n\t\t\t&disp_ib,\n\t\t\tvgpr_init_compute_shader_aldebaran,\n\t\t\tsizeof(vgpr_init_compute_shader_aldebaran),\n\t\t\tvgpr_init_regs_aldebaran,\n\t\t\tARRAY_SIZE(vgpr_init_regs_aldebaran),\n\t\t\tadev->gfx.cu_info.number,\n\t\t\twb_ib.gpu_addr, pattern, &fence);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to clear vgprs\\n\");\n\t\tgoto pro_end;\n\t}\n\n\t/* wait for the GPU to finish processing the IB */\n\tr = dma_fence_wait(fence, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"timeout to clear vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\n\tr = gfx_v9_4_2_wait_for_waves_assigned(adev,\n\t\t\t&wb_ib.ptr[1], 0b1,\n\t\t\tpattern,\n\t\t\tadev->gfx.cu_info.number * SIMD_ID_MAX,\n\t\t\tfalse);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to cover all simds when clearing vgprs\\n\");\n\t\tgoto disp_failed;\n\t}\n\ndisp_failed:\n\tamdgpu_ib_free(adev, &disp_ib, NULL);\n\tdma_fence_put(fence);\npro_end:\n\tamdgpu_ib_free(adev, &wb_ib, NULL);\n\n\tif (r)\n\t\tdev_info(adev->dev, \"Init VGPRS Failed\\n\");\n\telse\n\t\tdev_info(adev->dev, \"Init VGPRS Successfully\\n\");\n\n\treturn r;\n}\n\nint gfx_v9_4_2_do_edc_gpr_workarounds(struct amdgpu_device *adev)\n{\n\t/* only support when RAS is enabled */\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn 0;\n\n\t/* Workaround for ALDEBARAN, skip GPRs init in GPU reset.\n\t   Will remove it once GPRs init algorithm works for all CU settings. */\n\tif (amdgpu_in_reset(adev))\n\t\treturn 0;\n\n\tgfx_v9_4_2_do_sgprs_init(adev);\n\n\tgfx_v9_4_2_do_vgprs_init(adev);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev);\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev);\n\nvoid gfx_v9_4_2_init_golden_registers(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t die_id)\n{\n\tsoc15_program_register_sequence(adev,\n\t\t\t\t\tgolden_settings_gc_9_4_2_alde,\n\t\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde));\n\n\t/* apply golden settings per die */\n\tswitch (die_id) {\n\tcase 0:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_0,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_0));\n\t\tbreak;\n\tcase 1:\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\tgolden_settings_gc_9_4_2_alde_die_1,\n\t\t\t\tARRAY_SIZE(golden_settings_gc_9_4_2_alde_die_1));\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(adev->dev,\n\t\t\t \"invalid die id %d, ignore channel fabricid remap settings\\n\",\n\t\t\t die_id);\n\t\tbreak;\n\t}\n}\n\nvoid gfx_v9_4_2_debug_trap_config_init(struct amdgpu_device *adev,\n\t\t\t\tuint32_t first_vmid,\n\t\t\t\tuint32_t last_vmid)\n{\n\tuint32_t data;\n\tint i;\n\n\tmutex_lock(&adev->srbm_mutex);\n\n\tfor (i = first_vmid; i < last_vmid; i++) {\n\t\tdata = 0;\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, TRAP_EN, 1);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_EN, 0);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, EXCP_REPLACE,\n\t\t\t\t\t0);\n\t\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_PER_VMID_CNTL), data);\n\t}\n\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, 0);\n\tmutex_unlock(&adev->srbm_mutex);\n\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA0), 0);\n\tWREG32(SOC15_REG_OFFSET(GC, 0, regSPI_GDBG_TRAP_DATA1), 0);\n}\n\nvoid gfx_v9_4_2_set_power_brake_sequence(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tgfx_v9_0_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL, PATTERN_MODE, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL, tmp);\n\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, GC_THROTTLE_CTRL1, PWRBRK_STALL_EN, 1);\n\tWREG32_SOC15(GC, 0, regGC_THROTTLE_CTRL1, tmp);\n\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_INDEX, ixPWRBRK_STALL_PATTERN_CTRL);\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, PWRBRK_STALL_PATTERN_CTRL, PWRBRK_END_STEP, 0x12);\n\tWREG32_SOC15(GC, 0, regGC_CAC_IND_DATA, tmp);\n}\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_edc_counter_regs[] = {\n\t/* CPF */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT), 0, 1, 1 },\n\t/* CPC */\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT), 0, 1, 1 },\n\t/* GDS */\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT), 0, 1, 1 },\n\t/* RLC */\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT), 0, 1, 1 },\n\t{ SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2), 0, 1, 1 },\n\t/* SPI */\n\t{ SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT), 0, 8, 1 },\n\t/* SQC */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3), 0, 8, 7 },\n\t{ SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3), 0, 8, 7 },\n\t/* SQ */\n\t{ SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT), 0, 8, 14 },\n\t/* TCP */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW), 0, 8, 14 },\n\t/* TCI */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT), 0, 1, 69 },\n\t/* TCC */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2), 0, 1, 16 },\n\t/* TCA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT), 0, 1, 2 },\n\t/* TCX */\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT), 0, 1, 2 },\n\t{ SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2), 0, 1, 2 },\n\t/* TD */\n\t{ SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT), 0, 8, 14 },\n\t/* TA */\n\t{ SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT), 0, 8, 14 },\n\t/* GCEA */\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2), 0, 1, 16 },\n\t{ SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 1, 16 },\n};\n\nstatic void gfx_v9_4_2_select_se_sh(struct amdgpu_device *adev, u32 se_num,\n\t\t\t\t  u32 sh_num, u32 instance)\n{\n\tu32 data;\n\n\tif (instance == 0xffffffff)\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX, INSTANCE_INDEX,\n\t\t\t\t     instance);\n\n\tif (se_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);\n\n\tif (sh_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_BROADCAST_WRITES,\n\t\t\t\t     1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);\n\n\tWREG32_SOC15_RLC_SHADOW_EX(reg, GC, 0, regGRBM_GFX_INDEX, data);\n}\n\nstatic const struct soc15_ras_field_entry gfx_v9_4_2_ras_fields[] = {\n\t/* CPF */\n\t{ \"CPF_ROQ_ME2\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME2),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME2) },\n\t{ \"CPF_ROQ_ME1\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_ROQ_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(CPF_EDC_ROQ_CNT, DED_COUNT_ME1) },\n\t{ \"CPF_TCIU_TAG\", SOC15_REG_ENTRY(GC, 0, regCPF_EDC_TAG_CNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPF_EDC_TAG_CNT, DED_COUNT) },\n\n\t/* CPC */\n\t{ \"CPC_SCRATCH\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_SCRATCH_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_SCRATCH_CNT, DED_COUNT) },\n\t{ \"CPC_UCODE\", SOC15_REG_ENTRY(GC, 0, regCPC_EDC_UCODE_CNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, SEC_COUNT),\n\t  SOC15_REG_FIELD(CPC_EDC_UCODE_CNT, DED_COUNT) },\n\t{ \"CPC_DC_STATE_RAM_ME1\", SOC15_REG_ENTRY(GC, 0, regDC_EDC_STATE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_STATE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT_ME1) },\n\t{ \"CPC_DC_CSINVOC_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_CSINVOC_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_CSINVOC_CNT, DED_COUNT1_ME1) },\n\t{ \"CPC_DC_RESTORE_RAM1_ME1\",\n\t  SOC15_REG_ENTRY(GC, 0, regDC_EDC_RESTORE_CNT),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, SEC_COUNT1_ME1),\n\t  SOC15_REG_FIELD(DC_EDC_RESTORE_CNT, DED_COUNT1_ME1) },\n\n\t/* GDS */\n\t{ \"GDS_GRBM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_GRBM_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_GRBM_CNT, DED) },\n\t{ \"GDS_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_CNT, GDS_MEM_DED) },\n\t{ \"GDS_PHY_CMD_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_CMD_RAM_MEM_DED) },\n\t{ \"GDS_PHY_DATA_RAM_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, PHY_DATA_RAM_MEM_DED) },\n\t{ \"GDS_ME0_CS_PIPE_MEM\", SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PHY_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PHY_CNT, ME0_CS_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE0_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE0_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE1_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE1_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE2_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE2_PIPE_MEM_DED) },\n\t{ \"GDS_ME1_PIPE3_PIPE_MEM\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_PIPE_CNT),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_SEC),\n\t  SOC15_REG_FIELD(GDS_EDC_OA_PIPE_CNT, ME1_PIPE3_PIPE_MEM_DED) },\n\t{ \"GDS_ME0_GFXHP3D_PIX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_PIX_DED) },\n\t{ \"GDS_ME0_GFXHP3D_VTX_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_VTX_DED) },\n\t{ \"GDS_ME0_CS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_CS_DED) },\n\t{ \"GDS_ME0_GFXHP3D_GS_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME0_GFXHP3D_GS_DED) },\n\t{ \"GDS_ME1_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE0_DED) },\n\t{ \"GDS_ME1_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE1_DED) },\n\t{ \"GDS_ME1_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE2_DED) },\n\t{ \"GDS_ME1_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME1_PIPE3_DED) },\n\t{ \"GDS_ME2_PIPE0_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE0_DED) },\n\t{ \"GDS_ME2_PIPE1_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE1_DED) },\n\t{ \"GDS_ME2_PIPE2_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE2_DED) },\n\t{ \"GDS_ME2_PIPE3_DED\",\n\t  SOC15_REG_ENTRY(GC, 0, regGDS_EDC_OA_DED), 0, 0,\n\t  SOC15_REG_FIELD(GDS_EDC_OA_DED, ME2_PIPE3_DED) },\n\n\t/* RLC */\n\t{ \"RLCG_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCG_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCG_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLCV_INSTR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_INSTR_RAM_DED_COUNT) },\n\t{ \"RLCV_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLCV_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_TCTAG_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_TCTAG_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SPM_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_DATA_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_DATA_RAM_DED_COUNT) },\n\t{ \"RLC_SRM_ADDR_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT, RLC_SRM_ADDR_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE0_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE0_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE1_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE1_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE2_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE2_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE3_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE3_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE4_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE4_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE5_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE5_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE6_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE6_SCRATCH_RAM_DED_COUNT) },\n\t{ \"RLC_SPM_SE7_SCRATCH_RAM\", SOC15_REG_ENTRY(GC, 0, regRLC_EDC_CNT2),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(RLC_EDC_CNT2, RLC_SPM_SE7_SCRATCH_RAM_DED_COUNT) },\n\n\t/* SPI */\n\t{ \"SPI_SR_MEM\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_SR_MEM_DED_COUNT) },\n\t{ \"SPI_GDS_EXPREQ\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_GDS_EXPREQ_DED_COUNT) },\n\t{ \"SPI_WB_GRANT_30\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_WB_GRANT_30_DED_COUNT) },\n\t{ \"SPI_LIFE_CNT\", SOC15_REG_ENTRY(GC, 0, regSPI_EDC_CNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_SEC_COUNT),\n\t  SOC15_REG_FIELD(SPI_EDC_CNT, SPI_LIFE_CNT_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT */\n\t{ \"SQC_DATA_CU0_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU0_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU0_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU1_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU1_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU2_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU2_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_WRITE_DATA_BUF\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_WRITE_DATA_BUF_DED_COUNT) },\n\t{ \"SQC_DATA_CU3_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT, DATA_CU3_UTCL1_LFIFO_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT2 */\n\t{ \"SQC_INST_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_INST_UTCL1_LFIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, INST_UTCL1_LFIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT2),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT2, DATA_BANKA_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_CNT3 */\n\t{ \"SQC_INST_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, INST_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_TAG_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_TAG_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_BANK_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_BANK_RAM_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_DIRTY_BIT_RAM\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_CNT3, DATA_BANKB_DIRTY_BIT_RAM_DED_COUNT) },\n\n\t/* SQC - regSQC_EDC_PARITY_CNT3 */\n\t{ \"SQC_INST_BANKA_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKA_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKA_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_UTCL1_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_UTCL1_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_INST_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, INST_BANKB_MISS_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_HIT_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_HIT_FIFO_DED_COUNT) },\n\t{ \"SQC_DATA_BANKB_MISS_FIFO\", SOC15_REG_ENTRY(GC, 0, regSQC_EDC_PARITY_CNT3),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQC_EDC_PARITY_CNT3, DATA_BANKB_MISS_FIFO_DED_COUNT) },\n\n\t/* SQ */\n\t{ \"SQ_LDS_D\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_D_DED_COUNT) },\n\t{ \"SQ_LDS_I\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, LDS_I_DED_COUNT) },\n\t{ \"SQ_SGPR\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, SGPR_DED_COUNT) },\n\t{ \"SQ_VGPR0\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR0_DED_COUNT) },\n\t{ \"SQ_VGPR1\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR1_DED_COUNT) },\n\t{ \"SQ_VGPR2\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR2_DED_COUNT) },\n\t{ \"SQ_VGPR3\", SOC15_REG_ENTRY(GC, 0, regSQ_EDC_CNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_SEC_COUNT),\n\t  SOC15_REG_FIELD(SQ_EDC_CNT, VGPR3_DED_COUNT) },\n\n\t/* TCP */\n\t{ \"TCP_CACHE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CACHE_RAM_DED_COUNT) },\n\t{ \"TCP_LFIFO_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, LFIFO_RAM_DED_COUNT) },\n\t{ \"TCP_CMD_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, CMD_FIFO_DED_COUNT) },\n\t{ \"TCP_VM_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, VM_FIFO_DED_COUNT) },\n\t{ \"TCP_DB_RAM\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, DB_RAM_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO0\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO0_DED_COUNT) },\n\t{ \"TCP_UTCL1_LFIFO1\", SOC15_REG_ENTRY(GC, 0, regTCP_EDC_CNT_NEW),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCP_EDC_CNT_NEW, UTCL1_LFIFO1_DED_COUNT) },\n\n\t/* TCI */\n\t{ \"TCI_WRITE_RAM\", SOC15_REG_ENTRY(GC, 0, regTCI_EDC_CNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCI_EDC_CNT, WRITE_RAM_DED_COUNT) },\n\n\t/* TCC */\n\t{ \"TCC_CACHE_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DATA_DED_COUNT) },\n\t{ \"TCC_CACHE_DIRTY\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, CACHE_DIRTY_DED_COUNT) },\n\t{ \"TCC_HIGH_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, HIGH_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_LOW_RATE_TAG\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LOW_RATE_TAG_DED_COUNT) },\n\t{ \"TCC_SRC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, SRC_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_DED_COUNT) },\n\t{ \"TCC_LATENCY_FIFO_NEXT_RAM\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT, LATENCY_FIFO_NEXT_RAM_DED_COUNT) },\n\t{ \"TCC_CACHE_TAG_PROBE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, CACHE_TAG_PROBE_FIFO_DED_COUNT) },\n\t{ \"TCC_UC_ATOMIC_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, UC_ATOMIC_FIFO_DED_COUNT) },\n\t{ \"TCC_WRITE_CACHE_READ\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_CACHE_READ_DED_COUNT) },\n\t{ \"TCC_RETURN_CONTROL\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_CONTROL_DED_COUNT) },\n\t{ \"TCC_IN_USE_TRANSFER\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_TRANSFER_DED_COUNT) },\n\t{ \"TCC_IN_USE_DEC\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, IN_USE_DEC_DED_COUNT) },\n\t{ \"TCC_WRITE_RETURN\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, WRITE_RETURN_DED_COUNT) },\n\t{ \"TCC_RETURN_DATA\", SOC15_REG_ENTRY(GC, 0, regTCC_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCC_EDC_CNT2, RETURN_DATA_DED_COUNT) },\n\n\t/* TCA */\n\t{ \"TCA_HOLE_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, HOLE_FIFO_DED_COUNT) },\n\t{ \"TCA_REQ_FIFO\", SOC15_REG_ENTRY(GC, 0, regTCA_EDC_CNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCA_EDC_CNT, REQ_FIFO_DED_COUNT) },\n\n\t/* TCX */\n\t{ \"TCX_GROUP0\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP0_DED_COUNT) },\n\t{ \"TCX_GROUP1\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP1_DED_COUNT) },\n\t{ \"TCX_GROUP2\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP2_DED_COUNT) },\n\t{ \"TCX_GROUP3\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP3_DED_COUNT) },\n\t{ \"TCX_GROUP4\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_SEC_COUNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP4_DED_COUNT) },\n\t{ \"TCX_GROUP5\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP5_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP6\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP6_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP7\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP7_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP8\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP8_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP9\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP9_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP10\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT, GROUP10_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP11\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP11_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP12\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP12_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP13\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP13_SED_COUNT), 0, 0 },\n\t{ \"TCX_GROUP14\", SOC15_REG_ENTRY(GC, 0, regTCX_EDC_CNT2),\n\t  SOC15_REG_FIELD(TCX_EDC_CNT2, GROUP14_SED_COUNT), 0, 0 },\n\n\t/* TD */\n\t{ \"TD_SS_FIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_LO_DED_COUNT) },\n\t{ \"TD_SS_FIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, SS_FIFO_HI_DED_COUNT) },\n\t{ \"TD_CS_FIFO\", SOC15_REG_ENTRY(GC, 0, regTD_EDC_CNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TD_EDC_CNT, CS_FIFO_DED_COUNT) },\n\n\t/* TA */\n\t{ \"TA_FS_DFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_DFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_LO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_LO_DED_COUNT) },\n\t{ \"TA_FL_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FL_LFIFO_DED_COUNT) },\n\t{ \"TA_FX_LFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FX_LFIFO_DED_COUNT) },\n\t{ \"TA_FS_CFIFO\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_CFIFO_DED_COUNT) },\n\t{ \"TA_FS_AFIFO_HI\", SOC15_REG_ENTRY(GC, 0, regTA_EDC_CNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_SEC_COUNT),\n\t  SOC15_REG_FIELD(TA_EDC_CNT, TA_FS_AFIFO_HI_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT */\n\t{ \"EA_DRAMRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_RRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, RRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_WRET_TAGMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, WRET_TAGMEM_DED_COUNT) },\n\t{ \"EA_IOWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, DRAMWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IORD_CMDMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT, IOWR_CMDMEM_SED_COUNT), 0, 0 },\n\n\t/* EA - regGCEA_EDC_CNT2 */\n\t{ \"EA_GMIRD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_DATAMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_DATAMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIRD_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, GMIWR_PAGEMEM_SED_COUNT), 0, 0 },\n\t{ \"EA_MAM_D0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D0MEM_DED_COUNT) },\n\t{ \"EA_MAM_D1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D1MEM_DED_COUNT) },\n\t{ \"EA_MAM_D2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D2MEM_DED_COUNT) },\n\t{ \"EA_MAM_D3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT2),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_SED_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT2, MAM_D3MEM_DED_COUNT) },\n\n\t/* EA - regGCEA_EDC_CNT3 */\n\t{ \"EA_DRAMRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_DRAMWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, DRAMWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_IORD_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IORD_CMDMEM_DED_COUNT) },\n\t{ \"EA_IOWR_CMDMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, IOWR_CMDMEM_DED_COUNT) },\n\t{ \"EA_GMIRD_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIRD_PAGEMEM_DED_COUNT) },\n\t{ \"EA_GMIWR_PAGEMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3), 0, 0,\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, GMIWR_PAGEMEM_DED_COUNT) },\n\t{ \"EA_MAM_A0MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A0MEM_DED_COUNT) },\n\t{ \"EA_MAM_A1MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A1MEM_DED_COUNT) },\n\t{ \"EA_MAM_A2MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A2MEM_DED_COUNT) },\n\t{ \"EA_MAM_A3MEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_A3MEM_DED_COUNT) },\n\t{ \"EA_MAM_AFMEM\", SOC15_REG_ENTRY(GC, 0, regGCEA_EDC_CNT3),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_SEC_COUNT),\n\t  SOC15_REG_FIELD(GCEA_EDC_CNT3, MAM_AFMEM_DED_COUNT) },\n};\n\nstatic const char * const vml2_walker_mems[] = {\n\t\"UTC_VML2_CACHE_PDE0_MEM0\",\n\t\"UTC_VML2_CACHE_PDE0_MEM1\",\n\t\"UTC_VML2_CACHE_PDE1_MEM0\",\n\t\"UTC_VML2_CACHE_PDE1_MEM1\",\n\t\"UTC_VML2_CACHE_PDE2_MEM0\",\n\t\"UTC_VML2_CACHE_PDE2_MEM1\",\n\t\"UTC_VML2_RDIF_ARADDRS\",\n\t\"UTC_VML2_RDIF_LOG_FIFO\",\n\t\"UTC_VML2_QUEUE_REQ\",\n\t\"UTC_VML2_QUEUE_RET\",\n};\n\nstatic struct gfx_v9_4_2_utc_block gfx_v9_4_2_utc_blocks[] = {\n\t{ VML2_MEM, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ VML2_WALKER_MEM, ARRAY_SIZE(vml2_walker_mems), 1, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regVML2_WALKER_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(VML2_WALKER_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, VML2_WALKER_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ UTCL2_MEM, 18, 1, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regUTCL2_MEM_ECC_CNTL) },\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(UTCL2_MEM_ECC_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, UTCL2_MEM_ECC_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_2M, 8, 2, 1,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_2M_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_2M_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_2M_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_32K, 8, 2, 2,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_32K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_32K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_32K_DSM_CNTL, WRITE_COUNTERS, 1) },\n\t{ ATC_L2_CACHE_4K, 8, 2, 8,\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_INDEX) },\n\t  { SOC15_REG_ENTRY(GC, 0, regATC_L2_CACHE_4K_DSM_CNTL) },\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, SEC_COUNT),\n\t  SOC15_REG_FIELD(ATC_L2_CACHE_4K_DSM_CNTL, DED_COUNT),\n\t  REG_SET_FIELD(0, ATC_L2_CACHE_4K_DSM_CNTL, WRITE_COUNTERS, 1) },\n};\n\nstatic const struct soc15_reg_entry gfx_v9_4_2_ea_err_status_regs = {\n\tSOC15_REG_ENTRY(GC, 0, regGCEA_ERR_STATUS), 0, 1, 16\n};\n\nstatic int gfx_v9_4_2_get_reg_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t  const struct soc15_reg_entry *reg,\n\t\t\t\t\t  uint32_t se_id, uint32_t inst_id,\n\t\t\t\t\t  uint32_t value, uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i;\n\tuint32_t sec_cnt, ded_cnt;\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_ras_fields); i++) {\n\t\tif (gfx_v9_4_2_ras_fields[i].reg_offset != reg->reg_offset ||\n\t\t    gfx_v9_4_2_ras_fields[i].seg != reg->seg ||\n\t\t    gfx_v9_4_2_ras_fields[i].inst != reg->inst)\n\t\t\tcontinue;\n\n\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], sec);\n\t\tif (sec_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], SEC %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t sec_cnt);\n\t\t\t*sec_count += sec_cnt;\n\t\t}\n\n\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(\n\t\t\tvalue, gfx_v9_4_2_ras_fields[i], ded);\n\t\tif (ded_cnt) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"GFX SubBlock %s, Instance[%d][%d], DED %d\\n\",\n\t\t\t\t gfx_v9_4_2_ras_fields[i].name, se_id, inst_id,\n\t\t\t\t ded_cnt);\n\t\t\t*ded_count += ded_cnt;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_2_query_sram_edc_count(struct amdgpu_device *adev,\n\t\t\t\tuint32_t *sec_count, uint32_t *ded_count)\n{\n\tuint32_t i, j, k, data;\n\tuint32_t sec_cnt = 0, ded_cnt = 0;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_edc_counter_regs); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_edc_counter_regs[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_2_edc_counter_regs[i].instance;\n\t\t\t     k++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, j, 0, k);\n\n\t\t\t\t/* if sec/ded_count is null, just clear counter */\n\t\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]));\n\n\t\t\t\tif (!data)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tgfx_v9_4_2_get_reg_error_count(adev,\n\t\t\t\t\t&gfx_v9_4_2_edc_counter_regs[i],\n\t\t\t\t\tj, k, data, &sec_cnt, &ded_cnt);\n\n\t\t\t\t/* clear counter after read */\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\t\tgfx_v9_4_2_edc_counter_regs[i]), 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count += sec_cnt;\n\t\t*ded_count += ded_cnt;\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_log_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t struct gfx_v9_4_2_utc_block *blk,\n\t\t\t\t\t uint32_t instance, uint32_t sec_cnt,\n\t\t\t\t\t uint32_t ded_cnt)\n{\n\tuint32_t bank, way, mem;\n\tstatic const char * const vml2_way_str[] = { \"BIGK\", \"4K\" };\n\tstatic const char * const utcl2_rounter_str[] = { \"VMC\", \"APT\" };\n\n\tmem = instance % blk->num_mem_blocks;\n\tway = (instance / blk->num_mem_blocks) % blk->num_ways;\n\tbank = instance / (blk->num_mem_blocks * blk->num_ways);\n\n\tswitch (blk->type) {\n\tcase VML2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_VML2_BANK_CACHE_%d_%s_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, vml2_way_str[way], mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase VML2_WALKER_MEM:\n\t\tdev_info(adev->dev, \"GFX SubBlock %s, SED %d, DED %d\\n\",\n\t\t\t vml2_walker_mems[bank], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase UTCL2_MEM:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTCL2_ROUTER_IFIF%d_GROUP0_%s, SED %d, DED %d\\n\",\n\t\t\tbank, utcl2_rounter_str[mem], sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_2M:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_2M_BANK%d_WAY%d_MEM, SED %d, DED %d\\n\",\n\t\t\tbank, way, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_32K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_32K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\tcase ATC_L2_CACHE_4K:\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"GFX SubBlock UTC_ATCL2_CACHE_4K_BANK%d_WAY%d_MEM%d, SED %d, DED %d\\n\",\n\t\t\tbank, way, mem, sec_cnt, ded_cnt);\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_2_query_utc_edc_count(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t *sec_count,\n\t\t\t\t\t  uint32_t *ded_count)\n{\n\tuint32_t i, j, data;\n\tuint32_t sec_cnt, ded_cnt;\n\tuint32_t num_instances;\n\tstruct gfx_v9_4_2_utc_block *blk;\n\n\tif (sec_count && ded_count) {\n\t\t*sec_count = 0;\n\t\t*ded_count = 0;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_2_utc_blocks); i++) {\n\t\tblk = &gfx_v9_4_2_utc_blocks[i];\n\t\tnum_instances =\n\t\t\tblk->num_banks * blk->num_ways * blk->num_mem_blocks;\n\t\tfor (j = 0; j < num_instances; j++) {\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->idx_reg), j);\n\n\t\t\t/* if sec/ded_count is NULL, just clear counter */\n\t\t\tif (!sec_count || !ded_count) {\n\t\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t\t       blk->clear);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata = RREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg));\n\t\t\tif (!data)\n\t\t\t\tcontinue;\n\n\t\t\tsec_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, sec);\n\t\t\t*sec_count += sec_cnt;\n\t\t\tded_cnt = SOC15_RAS_REG_FIELD_VAL(data, *blk, ded);\n\t\t\t*ded_count += ded_cnt;\n\n\t\t\t/* clear counter after read */\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(blk->data_reg),\n\t\t\t       blk->clear);\n\n\t\t\t/* print the edc count */\n\t\t\tif (sec_cnt || ded_cnt)\n\t\t\t\tgfx_v9_4_2_log_utc_edc_count(adev, blk, j, sec_cnt,\n\t\t\t\t\t\t\t     ded_cnt);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_2_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t    void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tuint32_t sec_count = 0, ded_count = 0;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\terr_data->ue_count = 0;\n\terr_data->ce_count = 0;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n\tgfx_v9_4_2_query_utc_edc_count(adev, &sec_count, &ded_count);\n\terr_data->ce_count += sec_count;\n\terr_data->ue_count += ded_count;\n\n}\n\nstatic void gfx_v9_4_2_reset_utc_err_status(struct amdgpu_device *adev)\n{\n\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n}\n\nstatic void gfx_v9_4_2_reset_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\tvalue = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\t\t\tvalue = REG_SET_FIELD(value, GCEA_ERR_STATUS, CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), value);\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_sram_edc_count(adev, NULL, NULL);\n\tgfx_v9_4_2_query_utc_edc_count(adev, NULL, NULL);\n}\n\nstatic void gfx_v9_4_2_query_ea_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t i, j;\n\tuint32_t reg_value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < gfx_v9_4_2_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_2_ea_err_status_regs.instance;\n\t\t     j++) {\n\t\t\tgfx_v9_4_2_select_se_sh(adev, i, 0, j);\n\t\t\treg_value = RREG32(SOC15_REG_ENTRY_OFFSET(\n\t\t\t\tgfx_v9_4_2_ea_err_status_regs));\n\n\t\t\tif (REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_WRRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_DATAPARITY_ERROR)) {\n\t\t\t\tdev_warn(adev->dev, \"GCEA err detected at instance: %d, status: 0x%x!\\n\",\n\t\t\t\t\t\tj, reg_value);\n\t\t\t}\n\t\t\t/* clear after read */\n\t\t\treg_value = REG_SET_FIELD(reg_value, GCEA_ERR_STATUS,\n\t\t\t\t\t\t  CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32(SOC15_REG_ENTRY_OFFSET(gfx_v9_4_2_ea_err_status_regs), reg_value);\n\t\t}\n\t}\n\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_query_utc_err_status(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX UTCL2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regUTCL2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Walker Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, 0, regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_query_ea_err_status(adev);\n\tgfx_v9_4_2_query_utc_err_status(adev);\n\tgfx_v9_4_2_query_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_reset_ras_error_status(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\treturn;\n\n\tgfx_v9_4_2_reset_utc_err_status(adev);\n\tgfx_v9_4_2_reset_ea_err_status(adev);\n\tgfx_v9_4_2_reset_sq_timeout_status(adev);\n}\n\nstatic void gfx_v9_4_2_enable_watchdog_timer(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\tuint32_t data;\n\n\tdata = REG_SET_FIELD(0, SQ_TIMEOUT_CONFIG, TIMEOUT_FATAL_DISABLE,\n\t\t\t     amdgpu_watchdog_timer.timeout_fatal_disable ? 1 :\n\t\t\t\t\t\t\t\t\t   0);\n\n\tif (amdgpu_watchdog_timer.timeout_fatal_disable &&\n\t    (amdgpu_watchdog_timer.period < 1 ||\n\t     amdgpu_watchdog_timer.period > 0x23)) {\n\t\tdev_warn(adev->dev, \"Watchdog period range is 1 to 0x23\\n\");\n\t\tamdgpu_watchdog_timer.period = 0x23;\n\t}\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, PERIOD_SEL,\n\t\t\t     amdgpu_watchdog_timer.period);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tgfx_v9_4_2_select_se_sh(adev, i, 0xffffffff, 0xffffffff);\n\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_CONFIG, data);\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t address)\n{\n\tWREG32_SOC15_RLC_EX(reg, GC, 0, regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(address << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK));\n\treturn RREG32_SOC15(GC, 0, regSQ_IND_DATA);\n}\n\nstatic void gfx_v9_4_2_log_cu_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t status)\n{\n\tstruct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;\n\tuint32_t i, simd, wave;\n\tuint32_t wave_status;\n\tuint32_t wave_pc_lo, wave_pc_hi;\n\tuint32_t wave_exec_lo, wave_exec_hi;\n\tuint32_t wave_inst_dw0, wave_inst_dw1;\n\tuint32_t wave_ib_sts;\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (!((i << 1) & status))\n\t\t\tcontinue;\n\n\t\tsimd = i / cu_info->max_waves_per_simd;\n\t\twave = i % cu_info->max_waves_per_simd;\n\n\t\twave_status = wave_read_ind(adev, simd, wave, ixSQ_WAVE_STATUS);\n\t\twave_pc_lo = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_LO);\n\t\twave_pc_hi = wave_read_ind(adev, simd, wave, ixSQ_WAVE_PC_HI);\n\t\twave_exec_lo =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_LO);\n\t\twave_exec_hi =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_EXEC_HI);\n\t\twave_inst_dw0 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW0);\n\t\twave_inst_dw1 =\n\t\t\twave_read_ind(adev, simd, wave, ixSQ_WAVE_INST_DW1);\n\t\twave_ib_sts = wave_read_ind(adev, simd, wave, ixSQ_WAVE_IB_STS);\n\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"\\t SIMD %d, Wave %d: status 0x%x, pc 0x%llx, exec 0x%llx, inst 0x%llx, ib_sts 0x%x\\n\",\n\t\t\tsimd, wave, wave_status,\n\t\t\t((uint64_t)wave_pc_hi << 32 | wave_pc_lo),\n\t\t\t((uint64_t)wave_exec_hi << 32 | wave_exec_lo),\n\t\t\t((uint64_t)wave_inst_dw1 << 32 | wave_inst_dw0),\n\t\t\twave_ib_sts);\n\t}\n}\n\nstatic void gfx_v9_4_2_query_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\tuint32_t status;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tstatus = RREG32_SOC15(GC, 0,\n\t\t\t\t\t\t      regSQ_TIMEOUT_STATUS);\n\t\t\t\tif (status != 0) {\n\t\t\t\t\tdev_info(\n\t\t\t\t\t\tadev->dev,\n\t\t\t\t\t\t\"GFX Watchdog Timeout: SE %d, SH %d, CU %d\\n\",\n\t\t\t\t\t\tse_idx, sh_idx, cu_idx);\n\t\t\t\t\tgfx_v9_4_2_log_cu_timeout_status(\n\t\t\t\t\t\tadev, status);\n\t\t\t\t}\n\t\t\t\t/* clear old status */\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_2_reset_sq_timeout_status(struct amdgpu_device *adev)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines;\n\t     se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se;\n\t\t     sh_idx++) {\n\t\t\tfor (cu_idx = 0;\n\t\t\t     cu_idx < adev->gfx.config.max_cu_per_sh;\n\t\t\t     cu_idx++) {\n\t\t\t\tgfx_v9_4_2_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx);\n\t\t\t\tWREG32_SOC15(GC, 0, regSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_2_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n\n\nstruct amdgpu_ras_block_hw_ops  gfx_v9_4_2_ras_ops = {\n\t\t.query_ras_error_count = &gfx_v9_4_2_query_ras_error_count,\n\t\t.reset_ras_error_count = &gfx_v9_4_2_reset_ras_error_count,\n\t\t.query_ras_error_status = &gfx_v9_4_2_query_ras_error_status,\n\t\t.reset_ras_error_status = &gfx_v9_4_2_reset_ras_error_status,\n};\n\nstruct amdgpu_gfx_ras gfx_v9_4_2_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &gfx_v9_4_2_ras_ops,\n\t},\n\t.enable_watchdog_timer = &gfx_v9_4_2_enable_watchdog_timer,\n};\n", "source_code_path": "gfx_v9_4_2.c", "line_number": 427, "generated_diff": "diff --git a/data/raw-data/code/gfx_v9_4_2.c b/data/raw-data/code/gfx_v9_4_2.c\nindex 3f4fd2f..1f612a0 100644\n--- a/data/raw-data/code/gfx_v9_4_2.c\n+++ b/data/raw-data/code/gfx_v9_4_2.c\n@@ -424,7 +424,9 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n        char *str;\n        int size;\n\n-       str = kmalloc(256, GFP_KERNEL);\n+       int max_size = CU_ID_MAX * SIMD_ID_MAX * WAVE_ID_MAX * 64;\n+\n+       str = kmalloc(max_size, GFP_KERNEL);\n        if (!str)\n                return;\n\n@@ -432,7 +434,7 @@ static void gfx_v9_4_2_log_wave_assignment(struct amdgpu_device *adev, uint32_t\n\n        for (se = 0; se < adev->gfx.config.max_shader_engines; se++) {\n                for (cu = 0; cu < CU_ID_MAX; cu++) {\n-                       memset(str, 0, 256);\n+                       memset(str, 0, max_size);\n                        size = sprintf(str, \"SE[%02d]CU[%02d]: \", se, cu);\n                        for (simd = 0; simd < SIMD_ID_MAX; simd++) {\n                                size += sprintf(str + size, \"[\");\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: gfx_v9_4_2.c\nLine: 427\n\tuint32_t offset = 0;\n\tchar *str;\n\tint size;\n\n\tstr = kmalloc(256, GFP_KERNEL);\n\tif (!str)\n\t\treturn;\n\n\tdev_dbg(adev->dev, \"wave assignment:\\n\");\n\n====================================\nRead the following bug report.\n============ Bug Report ============\nIssue: Potential Buffer Overflow in Wave Assignment Logging\nType: Buffer Overflow\nFile: drivers/gpu/drm/amd/amdgpu/gfx_v9_4_2.c\nLine: 427\n\nDescription:\nThe function gfx_v9_4_2_log_wave_assignment() stores a formatted string in a fixed-size buffer, but fails to validate the string length before writing to the buffer. This can lead to a buffer overflow if the accumulated string length exceeds the allocated buffer size, resulting in potential data corruption or system instability.\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n  The function 'amdgpu_gem_fault' takes a pointer parameter 'vmf' of type 'struct vm_fault *'.\n  This pointer and its member 'vma' are dereferenced (struct ttm_buffer_object *bo = vmf->vma->vm_private_data;) without first checking if they are null.\n  If 'vmf' or 'vmf->vma' is null, this will result in a null pointer dereference.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf ||!vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_gem.c\nLine: 50\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n  The function 'amdgpu_gem_fault' takes a pointer parameter 'vmf' of type 'struct vm_fault *'.\n  This pointer and its member 'vma' are dereferenced (struct ttm_buffer_object *bo = vmf->vma->vm_private_data;) without first checking if they are null.\n  If 'vmf' or 'vmf->vma' is null, this will result in a null pointer dereference.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n The function amdgpu_gem_fault accesses the 'vmf' pointer and its 'vma' member without verifying their validity. This can lead to a null pointer dereference if either 'vmf' or 'vmf->vma' is null, resulting in potential system crashes or instability.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf ||!vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_gem.c\nLine: 50\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n The function amdgpu_gem_fault accesses the 'vmf' pointer and its 'vma' member without verifying their validity. This can lead to a null pointer dereference if either 'vmf' or 'vmf->vma' is null, resulting in potential system crashes or instability.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\nA null pointer dereference vulnerability exists in the function amdgpu_gem_fault, which fails to check the validity of the 'vmf' pointer and its 'vma' member before accessing them. If either 'vmf' or 'vmf->vma' is null, this can result in a null pointer dereference and potential system crashes.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf ||!vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_gem.c\nLine: 50\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\nA null pointer dereference vulnerability exists in the function amdgpu_gem_fault, which fails to check the validity of the 'vmf' pointer and its 'vma' member before accessing them. If either 'vmf' or 'vmf->vma' is null, this can result in a null pointer dereference and potential system crashes.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n The function amdgpu_gem_fault accesses the 'vmf' pointer and its 'vma' member without performing a null check. This can lead to a null pointer dereference if 'vmf' or 'vmf->vma' is null, resulting in potential system instability or crashes due to invalid memory access.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50, "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf ||!vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: amdgpu_gem.c\nLine: 50\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n====================================\nRead the following bug report.\n============ Bug Report ============\nType: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n The function amdgpu_gem_fault accesses the 'vmf' pointer and its 'vma' member without performing a null check. This can lead to a null pointer dereference if 'vmf' or 'vmf->vma' is null, resulting in potential system instability or crashes due to invalid memory access.\n\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
{"bug_report_path": "dataset/raw_data/bugs/test-generalize/sprintf1-bug.txt", "bug_report_text": "CID: 487621\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: RISKY_PROGRAMMING_PRACTICE\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_acpi_enumerate_xcc\nLine: 1092\n\nProblem:\nUnbounded string operation using sprintf can cause buffer overflow\n\nAbstract:\nThe code uses sprintf to write to buffer 'hid' without size checking. The sprintf \noperation could write beyond the buffer boundaries if the formatted output string \nexceeds the destination buffer size. This could corrupt adjacent memory and lead \nto system instability or security vulnerabilities.\n\nDetails:\nThe sprintf function is used to format a string containing \"AMD\" concatenated with \na number. Since sprintf performs no bounds checking, if AMD_XCC_HID_START + id \nproduces a large number, the resulting string could overflow the fixed-size 'hid' \nbuffer. This occurs in the GPU driver's ACPI enumeration code which runs with \nkernel privileges.\n\nFix:\nReplace sprintf with snprintf and specify the buffer size:\n  snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id)\n", "diff_path": "dataset/raw_data/bugs/test-generalize/sprintf1-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..10dfa3a37333 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -1089,7 +1089,7 @@ static int amdgpu_acpi_enumerate_xcc(void)\n        xa_init(&numa_info_xa);\n\n        for (id = 0; id < AMD_XCC_MAX_HID; id++) {\n-               sprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n+               snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n                acpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n                /* These ACPI objects are expected to be in sequential order. If\n                 * one is not found, no need to check the rest.", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c", "line_number": 1092, "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger “stop”, “enable” and “start” at D0, Driver Operational.\n\t * SBIOS trigger “stop” at D3, Driver Not Operational.\n\t * SBIOS trigger “stop” and “disable” at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n", "generated_diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..10dfa3a37333 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -1089,7 +1089,7 @@ static int amdgpu_acpi_enumerate_xcc(void)\n        xa_init(&numa_info_xa);\n\n        for (id = 0; id < AMD_XCC_MAX_HID; id++) {\n-               sprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n+               snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n                acpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n                /* These ACPI objects are expected to be in sequential order. If\n                 * one is not found, no need to check the rest.</s>", "given_prompt": "<s>[INST]Consider the following github diff format.\n============ Diff format ============\n```diff\ndiff --git a/file1 b/file2\nindex 1234567..89abcdef 100644\n--- a/file1\n+++ b/file2\n@@ -1,3 +1,3 @@\n-old line\n+new line\n```====================================\nRead through the following source code carefully.\n============ Source Code ============\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nLine: 1092\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n====================================\nRead the following bug report.\n============ Bug Report ============\nCID: 487621\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: RISKY_PROGRAMMING_PRACTICE\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_acpi_enumerate_xcc\nLine: 1092\n\nProblem:\nUnbounded string operation using sprintf can cause buffer overflow\n\nAbstract:\nThe code uses sprintf to write to buffer 'hid' without size checking. The sprintf \noperation could write beyond the buffer boundaries if the formatted output string \nexceeds the destination buffer size. This could corrupt adjacent memory and lead \nto system instability or security vulnerabilities.\n\nDetails:\nThe sprintf function is used to format a string containing \"AMD\" concatenated with \na number. Since sprintf performs no bounds checking, if AMD_XCC_HID_START + id \nproduces a large number, the resulting string could overflow the fixed-size 'hid' \nbuffer. This occurs in the GPU driver's ACPI enumeration code which runs with \nkernel privileges.\n\nFix:\nReplace sprintf with snprintf and specify the buffer size:\n  snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id)\n====================================\nBased on the source code and the bug report, write a diff that fixes the bug.\nUse github diff format.\nDon't explain your diff, answer directly with the diff.\n[/INST]"}
